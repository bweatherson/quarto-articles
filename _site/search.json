[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Online Articles",
    "section": "",
    "text": "Defending Interest Relative Invariantism\n\n\n\n\n\n\nepistemology\n\n\ninterest-relativity\n\n\n\nSince interest-relative invariantism (hereafter, IRI) was introduced into contemporary epistemology in the early 2000s, it has been criticised on a number of fronts. This paper responds to six different criticisms of IRI launched by five different authors. And it does so by noting that the best version of IRI is immune to the criticisms they have launched. The ‘best version’ in question notes three things about IRI. First, what matters for knowledge is not strictly the stakes the agent faces in any decision-problem, but really the odds at which she has to bet. Second, IRI is a relatively weak theory; it just says interests sometimes matter. Defenders of IRI have often derived it from much stronger principles about reasoning, and critics have attacked those principles, but much weaker principles would do. Third, and most importantly, interests matter because generate certain kinds of defeaters. It isn’t part of this version of IRI that an agent can know something in virtue of their interests. Rather, the theory says that whether a certain kind of consideration is a defeater to an agent’s putative knowledge that p depends on their interests. This matters for the intuitive plausibility of IRI. Critics have argued, rightly, that interests don’t behave in ways distinctive of grounds of knowledge. But interests do behave like other kinds of defeaters, and this undermines the criticisms of IRI. \n\n\n\n\n\nJan 1, 2011\n\n\nBrian Weatherson\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Good are Counterexamples?\n\n\n\n\n\n\ngames and decisions\n\n\nepistemology\n\n\nmethodology\n\n\n\nIntuitively, Gettier cases are instances of justified true beliefs that are not cases of knowledge. Should we therefore conclude that knowledge is not justified true belief? Only if we have reason to trust intuition here. But intuitions are unreliable in a wide range of cases. And it can be argued that the Gettier intuitions have a greater resemblance to unreliable intuitions than to reliable intuitions. What’s distinctive about the faulty intuitions, I argue, is that respecting them would mean abandoning a simple, systematic and largely successful theory in favour of a complicated, disjunctive and idiosyncratic theory. So maybe respecting the Gettier intuitions was the wrong reaction, we should instead have been explaining why we are all so easily misled by these kinds of cases. \n\n\n\n\n\nJul 1, 2003\n\n\nBrian Weatherson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/wgac/index.html",
    "href": "posts/wgac/index.html",
    "title": "What Good are Counterexamples?",
    "section": "",
    "text": "The following kind of scenario is familiar throughout analytic philosophy. A bold philosopher proposes that all Fs are Gs. Another philosopher proposes a particular case that is, intuitively, an F but not a G. If intuition is right, then the bold philosopher is mistaken. Alternatively, if the bold philosopher is right, then intuition is mistaken, and we have learned something from philosophy. Can this alternative ever be realised, and if so, is there a way to tell when it is? In this paper, I will argue that the answer to the first question is yes, and that recognising the right answer to the second question should lead to a change in some of our philosophical practices.\n\nPublished in Philosophical Studies 115 (2003): 1-31.\n\nThe problem is pressing because there is no agreement across the sub-disciplines of philosophy about what to do when theory and intuition clash. In epistemology, particularly in the theory of knowledge, and in parts of metaphysics, particularly in the theory of causation, it is almost universally assumed that intuition trumps theory. Shope’s The Analysis of Knowledge contains literally dozens of cases where an interesting account of knowledge was jettisoned because it clashed with intuition about a particular case. In the literature on knowledge and lotteries it is not as widely assumed that intuitions about cases are inevitably correct, but this still seems to be the working hypothesis.1 And recent work of causation by a variety of authors, with a wide variety of opinions, generally takes the same line: if a theory disagrees with intuition about a case, the theory is wrong.2 In this area exceptions to the rule are a little more frequent, particularly on the issues of whether causation is transitive and whether omissions can be causes, but in most cases the intuitions are taken to override the theories. Matters are quite different in ethics. It is certainly not a good thing for utilitarian theories that we very often feel that the action that maximises utility is not the right thing to do. But the existence of such cases is rarely taken to be obviously and immediately fatal for utilitarian theories in the way that, say, Gettier cases are taken to be obviously and immediately fatal for theories of knowledge that proclaim those cases to be cases of knowledge. Either there is some important difference here between the anti-utilitarian cases and the Gettier cases, a difference that justifies our differing reactions, or someone is making a mistake. I claim that it is (usually) the epistemologists and the metaphysicians who are wrong. In more cases than we usually imagine, a good philosophical theory can teach us that our intuitions are mistaken. Indeed, I think it is possible (although perhaps not likely) that the justified true belief (hereafter, JTB) theory of knowledge is so plausible that we should hold onto it in preference to keeping our intuition that Gettier cases are not cases of knowledge.\n1 See, for example, DeRose (1996) and Nelkin (2000)2 See, for example, Menzies (1996), or any of the papers in the special Journal of Philosophy issue on causation, April 2000.My main interests here are methodological, not epistemological. Until the last section I will be arguing for the JTB theory of knowledge, but my main interest is in showing that one particular argument against the JTB theory, the one that turns on the fact that it issues in some rather unintuitive pronouncements about Gettier cases, is not in itself decisive. Still, the epistemological issues are important, which is one reason I chose to focus on the JTB theory, and at the end I will discuss how the methodological conclusions drawn here may impact on them in an unexpected way.\n\n1 Intuitions\nLet us say that a counterexample to the theory that all Fs are Gs is a possible situation such that most people have an intuition that some particular thing in the story is an F but not a G. The kinds of intuition I have in mind are what George Bealer (1998) calls intellectual “seemings”. Bealer distinguishes intellectual seemings, such as the intuition that Hume’s Principle is true, or that punishing a person for a crime they did not commit is unjust, from physical seemings, such as the ‘intuition’ that objects fall if released, or perhaps that the sun rotates around the earth. We shall be primarily concerned here with intellectual seemings, and indeed I shall only call these intuitions in what follows.\nAs Bealer notes, whether something seems to be true can be independent of whether we believe it to be true. Bealer himself notes that Frege’s Axiom V seems to be true, though we know it is false. It does not seem to be the case, in the relevant sense, that 643 x 721 = 463603. Unless one is rather good at mental arithmetic, there is nothing that 643 x 721 seems to be; it is out of the reach of intuition. These are not the only ways that seemings and belief can come apart. One can judge that something seems to be the case while neither believing nor disbelieving it. This is a sensible attitude to take towards the view that one cannot know that a particular ticket will lose in a fair lottery. This is despite the fact that it certainly seems one cannot know this. If one’s intuitions are running rampant, one may even have an intuition about something that one believes to be strictly indeterminate. For example, some people may have the intuition that the continuum hypothesis is true, even though they believe on reflection that it is indeterminate whether it is true.\nThe distinction between intuitions and belief is important because it helps reduce the violence that revisionary philosophical views do to our pre-existing positions. When I say that Gettier cases may be cases of knowledge, I am not denying that there is a strong intuition that they are not cases of knowledge. I am not denying that a Gettier case does not seem to be a case of knowledge. The same thing occurs in ethics. Utilitarians rarely deny that it seems that punishing innocents is the wrong thing to do. They urge that in certain, rare, cases this might be one of those things that seems to be true despite being false. The case that knowledge is justified true belief is meant to be made in full awareness of the fact that certain cases of justified true beliefs seem to not be cases of knowledge.\nActually, although we will not make much of it here, this last claim is not true as a general statement about all people. Jonathan Weinberg, Stephen Stich and Shaun Nichols have reported Weinberg, Stich, and Nichols (2001) that the intuition that Gettier cases are not cases of knowledge is not universally shared. It is not entirely clear what the philosophical relevance of these discoveries is. It might show that we who have Gettier intuitions speak a different language from those who do not. It might show (though as Stich and Nichols point out it is rather hard to see how) that philosophers know a lot more about knowledge than other folk. I think it is rather unlikely that this is true, but we shall bracket such concerns for now, and continue on the assumption that all parties have the Gettier intuitions. Since I shall want to argue that knowledge may still be justified belief in any case, I am hardly tilting the playing field in my direction by making this assumption.\nGiven that intuitions are what Bealer calls intellectual seemings, and given that the example of Axiom V shows that seemings can be mistaken, what evidence have we that they are not mistaken in the cases we consider here? Arguably, we have very little indeed. Robert Cummins (1998) argues that in general intuition should not be trusted as an evidential source because it cannot be calibrated. We wouldn’t have trusted the evidence Galileo’s telescope gave us about the moon without an independent reason for thinking his telescope reliable. Fortunately, this can be done; we can point the telescope at far away terrestrial mountains, and compare its findings with the findings of examining the mountains up close and personal. There is no comparable way of calibrating intuitions. Clearly we should suspicious of any method that has been tested and found unreliable, but there are tricky questions about the appropriate level of trust in methods that have not been tested. Ernest Sosa (1998) argues in response to Cummins that this kind of reasoning leads to an untenable kind of scepticism. Sosa notes that one can make the same point about perception as Cummins makes about intuition: we have no independent way of calibrating perception as a whole. There is a distinction to be drawn here, since perception divides into natural kinds, visual perception, tactile perception, etc, and we can use each of these to calibrate the others. It is hard to see how intuitions can be so divided in ways that permit us to check some kinds of intuitions against the others. In any case, the situation is probably worse than Cummins suggests, since we know that several intuitions are just false. It is interesting to note the many ways in which intuition does, by broad agreement, go wrong.\nMany people are prone to many kinds of systematic logical mistakes. Most famously, the error rates on the Wason Selection Task are disturbingly large. Although this test directly measures beliefs rather than intuitions, it seems very likely that many of the false beliefs are generated by mistaken intuitions. As has been shown in a variety of experiments, the most famous of which were conducted by Kahneman and Tversky, most people are quite incompetent at probabilistic reasoning. In the worst cases, subjects held that a conjunction was more probable than one of its conjuncts. Again, this only directly implicates subjects’ beliefs, but it is very likely that the false beliefs are grounded in false intuitions. (The examples in this paragraph are discussed in detail in Stich (1988, 1992).)\nAs noted above, most philosophers would agree that many, if not most, people have mistaken moral intuitions. We need not agree with those consequentialists who think that vast swathes of our moral views are in error to think that (a) people make systematic moral mistakes and (b) some of these mistakes can be traced to mistaken intuitions. To take the most dramatic example, for thousands of years it seemed to many people that slavery was morally acceptable. On a more mundane level, many of us find that our intuitive judgements about a variety of cases cannot be all acceptable, for it is impossible to find a plausible theory that covers them all.3 Whenever we make a judgement inconsistent with such an intuition, we are agreeing that some of our original intuitions were mistaken.\n3 The myriad examples in Unger (1996) are rather useful for reminding us just how unreliable our moral intuitions are, and how necessary it is to employ reflection and considered judgement in regimenting such intuitions.From a rather different direction, there are many mistaken conceptual intuitions, with the error traceable to the way Gricean considerations are internalised in the process of learning a language. Having learned that it would be improper to use t to describe a particular case, we can develop the intuition that this case is not an F, where F is the property denoted by t. For example, if one is careless, one can find oneself sharing the intuition expressed by Ryle in The Concept of Mind that morally neutral actions, like scratching one’s head, are neither voluntary nor involuntary (Ryle 1949). The source of this intuition is the simple fact that it would be odd to describe an action as voluntary or involuntary unless there was some reason to do so, with the most likely such reason being that the action was in some way morally suspect. The fact that the intuition has a natural explanation does not stop it being plainly false. We can get errors in conceptual intuitions from another source. At one stage it was thought that whales are fish, that the Mars is a star, the sun isn’t. These are beliefs, not intuitions, but there are clearly related intuitions. Anyone who had these beliefs would have had the intuition that in a situation like this (here demonstrating the world) the object in the Mars position was a star, and the objects in the whale position were fish. The empirical errors in the person’s belief will correlate to conceptual errors in their intuition. To note further that the kind of error being made here is conceptual not empirical, and hence the kind of error that occurs in intuition, note that we need not have learned anything new about whales, the sun or Mars to come to our modern beliefs. (In fact we did, but that’s a different matter.) Rather, we need only have learned something about the vast bulk of the objects that are fish, or stars, to realise that these objects had been wrongly categorised. The factor we had thought to be the most salient similarity to the cases grouped under the term, being a heavenly body visible in the night sky for ‘star’, living in water for ‘fish’, turned out not to be the most important similarity between most things grouped under that term. So there is an important sense in which saying whales are fish, or that the sun is not a star, may reveal a conceptual (rather than an empirical) error.\nThere seems to be a link between these two kinds of conceptual error. The reason we say that the Rylean intuitions, or more generally the intuitions of what Grice (1989, Ch. 1) called the Type-A philosophers, are mistaken is that the rival, Gricean, theory attaches to each word a relatively natural property. There is no natural property that actions satisfy when, and only when, we ordinarily describe them as voluntary. There is a natural property that covers all these cases, and other more mundane actions like scratching one’s head, and that is the property we now think is denoted by ‘voluntary’. This notion of naturalness, and the associated drive for systematicity in our philosophical and semantic theories, will play an important role in what follows.\n\n\n2 Correcting Mistakes\nThe following would be a bad defence of the JTB theory against counterexamples. We can tell that all counterexamples to the JTB theory are based on mistaken intuitions, because the JTB theory is true, so all counterexamples to it are false. Unless we have some support for the crucial premise that the JTB theory is true, this argument is rather weak. And that support should be enough to not only make the theory prima facie plausible, but so convincing that we are prepared to trust it rather than our judgements about Gettier cases.\nIn short, the true theory of knowledge is the one that does best at (a) accounting for as many as possible of our intuitions about knowledge while (b) remaining systematic. A ‘theory’ that simply lists our intuitions is no theory at all, so condition (b) is vital. And it is condition (b), when fully expressed, that will do most of the work in justifying the preservation of the JTB theory in the face of the counterexamples.\nThe idea that our theory should be systematic is accepted across a wide range of philosophical disciplines. This idea seems to be behind the following plausible claims by Michael Smith: “Not only is it a platitude that rightness is a property that we can discover to be instantiated by engaging in rational argument, it is also a platitude that such arguments have a characteristic coherentist form.” (1994: 40) The second so-called platitude just points out that it is a standard way of arguing in ethics to say, you think we should do X in circumstances C1, circumstances C2 are just like C1, so we should do X in C1. The first points out that not only is this standard, it can yield surprising ethical knowledge. But this is only plausible if it is more important that final ethics is systematic than that first ethics, the ethical view delivered by intuition, is correct. In other words, it is only plausible if ethical intuitions are classified as mistaken to the extent that they conflict with the most systematic plausible theory. So, for example, it would be good news for utilitarianism if there was no plausible rival with any reasonable degree of systematicity.\nThis idea also seems to do important work in logic. If we just listed intuitions about entailment, we would have a theory on which disjunctive syllogism (A and ~A \\({\\vee}\\) B entail B) is valid, while ex falso quadlibet (A and ~A entail B) is not. Such a theory is unsystematic because no concept of entailment that satisfies these two intuitions will satisfy a generalised transitivity requirement: that if C and D entail E, and F entails D then C and F entail E. (This last step assumes that ~A entails ~A \\({\\vee}\\) B, but that is rarely denied.) Now one can claim that a theory of entailment that gives up this kind of transitivity can still be systematic enough, and Neil Tennant (1992) does exactly this, but it is clear that we have a serious cost of the theory here, and many people think avoiding this cost is more important than preserving all intuitions.\nIn more detail, there are four criteria by which we can judge a philosophical theory. First, counterexamples to a theory count against it. While a theory can be reformist, it cannot be revolutionary. A theory that disagreed with virtually all intuitions about possible cases is, for that reason, false. The theory: X knows that p iff X exists and p is true is systematic, but hardly plausible. As a corollary, while intuitions about any particular possible case can be mistaken, not too many of them could be. Counterexamples are problematic for a theory, the fewer reforms needed the better, it’s just not that they are not fatal. Importantly, not all counterexamples are as damaging to a theory as others. Intuitions come in various degrees of strength, and theories that violate weaker intuitions are not as badly off as those that violate stronger intuitions. Many people accept that the more obscure or fantastic a counterexample is, the less damaging it is to a theory. This seems to be behind the occasional claim that certain cases are “spoils to the victor” – the idea is that the case is so obscure or fantastic that we should let theory rather than intuition be our guide. Finally, if we can explain why we have the mistaken intuition, that counts for a lot in reducing the damage the counterexample does. Grice did not just assert that the theory on which an ordinary head scratch was voluntary was more systematic than the theory of voluntariness Ryle proposed, he provided an explanation of why it might seem that his theory was wrong in certain cases.\nSecondly, the analyses must not have too many theoretical consequences which are unacceptable. Consider Kahneman and Tversky’s account of how agents actually make decisions, prospect theory, as an analysis of ‘good decision’. (Disclaimer: This is not how Kahneman and Tversky intend it.) So the analysis of ‘good decision’ is ‘decision authorised by prospect theory’. It is a consequence of prospect theory that which decision is “best” depends on which outcome is considered to be the neutral point. In practice this is determined by contextual factors. Redescribing a story to make different points neutral, which can be done by changing the context, licences different decisions. I take it this would be unacceptable in an analysis of ‘good decision’, even though it means the theory gives intuitively correct results in more possible cases than its Bayesian rivals4. In general, we want our normative theories to eliminate arbitrariness as much as possible, and this is usually taken to be more important than agreeing with our pre-theoretic intuitions about particular cases. Unger uses a similar argument in Living High and Letting Die to argue against the reliance on intuitions about particular cases in ethics. We have differing ethical intuitions towards particular cases that differ only in the conspicuousness of the suffering caused (or not prevented), we know that conspicuousness is not a morally salient difference, so we should stop trusting the particular intuitions. (Presumably this is part of the reason that we find Tennant’s theory of entailment so incredible, prima facie. It is not just that violating transitivity seems unsystematic, it is that we have a theoretical intuition that transitivity should be maintained.)\n4 A point very similar to this is made in Horowitz (1998).Thirdly, the concept so analysed should be theoretically significant, and should be analysed in other theoretically significant terms. This is why we now analyse ‘fish’ in such a way that whales aren’t fish, and ‘star’ in such a way that the sun is a star. This is not just an empirical fact about our language. Adopting such a constraint on categories is a precondition of building a serious classificatory scheme, so it is a constraint on languages, which are classificatory schemes par excellance. Even if I’m wrong about this, the fact that we do reform our language with the advance of science to make our predicates refer to theoretically more significant properties shows that we have a commitment to this restriction.\nFinally, the analysis must be simple. This is an important part of why we don’t accept Ryle’s analysis of ‘voluntary’. His analysis can explain all the intuitive data, even without recourse to Gricean implicature, and arguably it doesn’t do much worse than the Gricean explanation on the second and third tests. But Grice’s theory can explain away the intuitions that it violates, and importantly it does so merely with the aid of theories of pragmatics that should be accepted for independent reasons, and it is simpler, so it trumps Ryle’s theory.\nMy main claim is that even once we have accepted that the JTB theory seems to say the wrong thing about Gettier cases, we should still keep an open mind to the question of whether it is true. The right theory of knowledge, the one that attributes the correct meaning to the word ‘knows’, will do best on balance at these four tests. Granted that the JTB theory does badly on test one, it seems to do better than its rivals on tests two, three and four, and this may be enough to make it correct.\n\n\n3 Naturalness in a theory of meaning\nLet’s say I have convinced you that it would be better to use ‘knows’ in such a way that we all now assent to “She knows” whenever the subject of that pronoun truly, justifiably, believes. You may have been convinced that only by doing this will our term pick out a natural relation, and there is evident utility in having our words pick out relations that carve nature at something like its joints. Only in that way, you may concede, will our language be a decent classificatory scheme of the kind described above, and it is a very good thing to have one’s language be a decent classificatory scheme. I have implicitly claimed above that if you concede this you should agree that I will have thereby corrected a mistake in your usage. But, an objector may argue, it is much more plausible to say that in doing so I simply changed the meaning of ‘knows’ and its cognates in your idiolect. The meaning of your words is constituted by your responses to cases like Gettier cases, so when I convince you to change your response, I change the meaning of your words.\nThis objection relies on a faulty theory of meaning, one that equates meaning with use in a way which is quite implausible. If this objection were right, it would imply infallibilism about knowledge ascriptions. Still, the objection does point to a rather important point. There is an implicit folk theory of the meaning of ‘knows’, one according to which it does not denote justified true belief. I claim this folk theory is mistaken. It is odd to say that we can all be mistaken about the meanings of our words; it is odd to say that we can’t make errors in word usage. I think the latter is the greater oddity, largely because I have a theory which explains how we can all make mistakes about meanings in our own language.\nHow can we make such mistakes? The short answer is that meanings ain’t in the head. The long answer turns on the kind of tests on analyses I discussed in section two. The meaning of a predicate is a property in the sense described by Lewis (1983)5: a set, or class, or plurality of possibilia. (That is, in general the meaning of a predicate is its intension.6) The interesting question is determining which property it is. In assigning a property to a predicate, there are two criteria we would like to follow. The first is that it validates as many as possible of our pre-theoretic beliefs. The second is that it is, in some sense, simple and theoretically important. How to make sense of this notion of simplicity is a rather complex matter. Lewis canvasses the idea that there is a primitive ‘naturalness’ of properties which measures simplicity and theoretical significance7, and I will adopt this idea. Space restrictions prevent me going into greater detail concerning ‘naturalness’, but if something more definite is wanted, for the record I mean by it here just what Lewis means by it in the works previously cited.8\n5 The theory of meaning outlined here is deeply indebted to Lewis (1983, 1984, 1992).6 There are tricky questions concerning cointensional predicates, but these have fairly familiar solutions, which I accept. For ease of expression here I will ignore the distinction between properties and relations – presumably ‘knows’ denotes a relation, that is a set of ordered pairs.7 ‘Measures’ may be inappropriate here. Plausibly a property is simple because it is natural.8 For more recent applications of naturalness in Lewis’s work, see Langton and Lewis (1998, 2001) and Lewis (2001).So, recapitulating what I said in section two, for any predicate t and property F, we want F meet two requirements before we say it is the meaning of t. We want this meaning assignment to validate many of our pre-theoretic intuitions (this is what we test for in tests one and two) and we want F to be reasonably natural (this is what we test for in tests three and four). In hard cases, these requirements pull in opposite directions; the meaning of t is the property which on balance does best. Saying ‘knows’ means ‘justifiably truly believes’ does not do particularly well on the first requirement. Gettier isolated a large class of cases where it goes wrong. But it does very well on the second, as it analyses knowledge in terms of a short list of simple and significant features. I claim that all its rivals don’t do considerably better on the first, and arguably do much worse on the second. (There are considerations pulling either way here, as I note in section seven, but it is prima facie plausible that it does very well on the second, which is all that we consider for now.) That the JTB theory is the best trade-off is still a live possibility, even considering Gettier cases.\nThis little argument will be perfectly useless this theory of meaning (owing in all its essential features to Lewis) is roughly right. There are several reasons for believing it. First, it can account for the possibility of mistaken intuitions, while still denying the possibility that intuitions about meaning can be systematically and radically mistaken. This alone is a nice consequence, and not one which is shared by every theory of meaning on the market. Secondly, as was shown in sections one and two, it seems to make the right kinds of predictions about when meaning will diverge from intuitions about meaning.\nThirdly, it can account for the fact that some, but not all, disagreements about the acceptability of assertions are disputes about matters of fact, not matters of meaning. This example is from Cummins: “If a child, asked to use ‘fair’ in a sentence, says,”It isn’t fair for girls to get as much as boys,” we should suspect the child’s politics, not his language” (1998, 120). This seems right; but if the child had said “It is fair that dreams are purple”, we would suspect his language. Perhaps by ‘fair’ he means ‘nonsensical’ or something similar. A theory of meaning needs to account for this divergence, and for the fact that it is a vague matter when we say the problem is with the child’s language, and when with his politics. In short, saying which disputes are disputes about facts (or values or whatever), and which about meanings, is a compulsory question for a theory of meaning.\nThe balance theory of meaning I am promoting can do this, as the following demonstration shows. This theory of meaning is determinedly individualistic. Every person has an idiolect determined by her dispositions to apply terms; a shared language is a collection of closely-enough overlapping idiolects. So the child’s idiolect might differ from ours, especially if he uses ‘fair’ to mean ‘nonsensical’. But if the idiolect differs in just how a few sentences are used, it is likely that the meaning postulate which does best at capturing his dispositions to use according to our two criteria, is the same as the meaning postulate which does best at capturing our dispositions to use. The reason is that highly natural properties are pretty thin on the ground; one’s dispositions to use a term have to change quite a lot before they get into the orbit of a distinct natural property. So despite the fact that I allow for nothing more than overlapping idiolects, in practice the overlap is much closer to being exact than on most ‘overlapping idiolect’ theories.\nWith this, I can now distinguish which disputes are disputes about facts, and which are disputes about meaning. Given that there is a dispute, the parties must have different dispositions to use some important term. In some disputes, the same meaning postulate does best on balance at capturing the dispositions of each party. I say that here the parties mean the same thing by their words, and the dispute is a dispute about facts. In others, the difference will be so great that different meaning postulates do best at capturing the dispositions of the competing parties. In these cases, I say the dispute is a dispute about meaning.\nNow, I can explain the intuition that the JTB theorist means something different to the rest of us by ‘knows. That is, I can explain this intuition away. It seems a fair assumption that the reasonably natural properties will be evenly distributed throughout the space of possible linguistic dispositions. If this is right, then any change of usage beyond a certain magnitude will, on my theory, count as a change of meaning. And it is plausible to suppose the change I am urging to our usage, affirming rather than denying sentences like, “Smith knows Jones owns a Ford” is beyond that certain magnitude. But the assumption of even distribution of the reasonably natural properties is false. That, I claim, is what the failure of the ’analysis of knowledge’ merry-go-round to stop shows us. There are just no reasonably natural properties in the neighbourhood of our disposition to use ‘knows’. If this is right, then even some quite significant changes to usage will not be changes in meaning, because they will not change which is the closest reasonably natural property to our usage pattern. The assumption that the reasonably natural properties are reasonably evenly distributed is plausible, but false. Hence the hunch that I am trying to change the meaning of ‘knows’ is plausible, but false.\nThe hypothesis that when we alter intuitions because of a theory we always change meanings, on the other hand, is not even plausible. When the ancients said “Whales are fish”, or “The sun is not a star”, they simply said false sentences. That is, they said that whales are fish, and believed that the sun is not a star. This seems platitudinous, but the ‘use-change implies meaning-change’ hypothesis would deny it.\nIt has sometimes been suggested to me that conceptual intuitions should be given greater privilege than other intuitions; that I am wrong to generalise from the massive fallibility of logical, ethical or semantic intuitions to the massive fallibility of conceptual intuitions. Since I am on much firmer ground when talking about these non-conceptual cases, if such an attack were justified it would severely weaken my argument. Given what has been said so far we should be able to see what is wrong with this suggestion. Consider a group of people who systematically assent to “If A then B implies if B then A.” On this view these people are expressing a mistaken logical intuition, but a correct conceptual intuition. So their concept of ‘implication’ doesn’t pick out implication, or at the very least doesn’t pick out our concept of ‘implication’. Now if we are in that group, this summary becomes incoherent, so this position immediately implies that we can’t be mistaken about our logical intuitions. Further, we are no longer able to say that when these people say “If A then B implies if B then A,” they are saying something false, because given the reference of ‘implies’ in their idiolect, this sentence expresses a true proposition. This is odd, but odder is to come. Assuming again we are in this group, it turns out to be vitally important in debates concerning philosophical logic to decide whether we are engaging in logical analysis or conceptual analysis. It might turn out a correct piece of conceptual analysis of ‘implication’ picks out a different relation to the correct implication relation we derive from purely logical considerations. If logical intuitions are less reliable than conceptual intuitions, as proposed, and assent to sentences like “If A then B implies if B then A” reveals simultaneously a logical and a conceptual intuition, this untenable conclusion seems forced. I conclude that conceptual intuitions are continuous with other intuitions, and should be treated in a similar way.\n\n\n4 Keeping Conceptual Analysis\nThe following would be a bad way to respond to the worry that the JTB theory amounts to a change in the meaning of the word ‘knows’. For the worry to have any bite, facts about the meaning of ‘knows’ will have to be explicable in terms of facts about the use of ’knows. But facts about use can only tell us about the beliefs of this community about knowledge, not what knowledge really is. Since different communities adopt different standards for knowledge, we should only trust ours over theirs if (a) we have special evidence that our is correct or (b) we are so xenophobic that we trust ours simply because it is ours. “Many of us care very much whether or cognitive processes lead to beliefs that are true, or give us power over nature, or lead to happiness. But only those with a deep and free-floating conservatism in matters epistemic will care whether their cognitive processes are sanctioned by the evaluative standards that happen to be woven into our language” (Stich (1988), 109). “The intuitions and tacit knowledge of the man or woman in the street are quite irrelevant. The theory seeks to say what knowledge really is, not what folk epistemology takes it to be” (Stich (1992), 252).9 Facts about use can only give us the latter, so they are not what are relevant to my inquiry.\n9 The paper from which this quote is drawn is about the content of mental states, so originally it had ‘mental representation’ for ‘knowledge’ and ‘psychology’ for ‘epistemology’. But I take it that (a) this isn’t an unfair representation of Stich’s views and (b) even if it is, it is an admirably clear statement of the way many people feel about the use of intuitions about possible cases, and worth considering for that reason alone.Stich takes this to be a general reason for abandoning conceptual analysis. Now while I think, and have argued above, that conceptual analysis need not slavishly follow intuition, I do not think that we should abandon it altogether. Stich’s worry seems to be conceptual analysis can only tell us about our words, not about our world. But is this kind of worry coherent? Can we say what will be found when we get to this real knowledge about the world? Will we be saying, “This belief of Smith’s shouldn’t be called knowledge, but really it is”? We need to attend to facts about the meaning of ‘knows’ in order to define the target of our search. If not, we have no way to avoid incoherencies like this one.\nTo put the same point another way, when someone claims to find this deep truth about knowledge, why should anyone else care? She will say, “Smith really knows that Jones owns a Ford, but I don’t mean what everyone else means by ‘knows’.” Why is this any more interesting than saying, “Smith really is a grapefruit, but I don’t mean what everyone else means by ‘grapefruit’”? If she doesn’t use words in the way that we do, we can ignore what she says about our common word usage. Or at least we can ignore it until she (or one of her colleagues) provides us with a translation manual. But to produce a translation manual, or to use words the way we do, she needs to attend to facts about our meanings. Again, incoherence threatens if she doesn’t attend to these facts but claims nevertheless to be participating in a debate with us. These points are all to be found in Chapter 2 of Jackson (1998).\nAn underlying assumption of the first reply is that there is a hard division between facts about meaning and facts about the world at large; that a principle like: No ‘is’ from a ‘means’ holds. This principle is, however, mistaken. All instances of the following argument pattern, where t ranges over tokenings of referring terms, are valid.\n\nP1.\n\nt refers unequivocally to \\({\\alpha}\\).\n\nP2.\n\nt refers unequivocally to \\({\\beta}\\).\n\nC.\n\n\\({\\alpha}\\) = \\({\\beta}\\)\n\n\nFor example, from the premise that ‘POTUS’ refers unequivocally to the President of the United States, and the premise that ‘POTUS’ refers unequivocally to Bush, we can validly infer that Bush is President of the United States. Since P1 and P2 are facts about meaning, and C is a fact about the world, any principle like No ‘is’ from a ‘means’ must be mistaken. So this worry about how much we can learn from conceptual analysis, from considerations of meaning, is mistaken.\nI call this inference pattern the R-inference. That the R-inference is valid doesn’t just show Stich’s critique rests on the false assumption No ‘is’ from a ‘means’. It can be used to provide a direct response to his critique. The problem is meant to be that conceptual analysis, the method of counterexamples, can at best provide us with claims like: ‘knows’ refers to the relation justifiably truly believes. We want to know facts about knowledge, not about the term ‘knows’, so the conceptual analyst seems to have been looking in the wrong place. But it is a platitude that ‘knows’ refers to the relation knows. I call such platitudes, that ‘t’ refers to t, instances of the R-schema10. We can use the R-schema together with the R-inference to get the kind of conclusion our opponents are looking for.\n10 Horwich (1999, 115–30) discusses similar schema, noting that instances involving words in foreign languages, or indexical expressions, will not be platitudinous. He also notes a way to remove the presumption that there is such a thing as knowledge, by stating the schema as \\({\\forall}\\)x (‘knowledge’ refers to x iff knowledge = x). For ease of expression I will stick with the simpler formulation in the text.\nP1.\n\n‘Knowledge’ refers unequivocally to the relation justifiably truly believes.\n\nP2.\n\n‘Knowledge’ refers unequivocally to the relation knows.\n\nC.\n\nThe relation knows is the relation justifiably truly believes.\n\n\nMore colloquially, the conclusion says that knowledge is justified true belief. Everyone agrees (I take it) that conceptual analysis could, in principle, give us knowledge of facts of the form of P1. So the opponents of conceptual analysis must either deny P2, or deny that C follows from P1 and P2. In other words, for any such argument they must deny that the R-schema is true, or that the R-inference is valid. I hope the reader will agree that neither option looks promising.\n\n\n5 Against the Psychologists\nSomeone excessively impressed by various results in the psychological study of concepts may make the following objection to the theory of meaning here proffered. “Why think that we should prefer short lists of necessary and sufficient conditions? This seems like another one of those cases where philosophers take their aesthetic preferences to be truth-indicative, much like the ‘taste for desert landscapes’ argument. Besides, haven’t psychologists like Eleanor Rosch shown that our concepts don’t have simple necessary and sufficient conditions? If that’s right, your argument falls down in several different places.”\nStrictly speaking, my preference is not just for short lists of necessary and sufficient conditions. But it is, for reasons set out more fully in the next section, for short theories that fit the meaning of some term into a network of other properties. And my argument would fall down if there was no reason to prefer such short theories. And, of course, short lists of necessary and sufficient conditions are paradigmatically short theories. One reason I prefer the JTB analysis to its modern rivals is its brevity. Some of the reasons for preferring short lists are brought out by considering the objections to this approach developed by psychologists. I’ll just focus on one of the experiments performed by Rosch and Mervis, the points I make can be generalised.\nRosch and Mervis (1975) claim that “subjects rate superordinate semantic categories as having few, if any, attributes common to all members.” (p. 20) (A superordinate semantic category is one, like ‘fruit’, which has other categories, like ‘apple’, ‘pear’ and ‘banana’, as sub-categories.) Here’s the experiment they ran to show this. For each of six superordinate categories (‘furniture’, ‘fruit’, ‘weapon’, ‘vegetable’, ‘vehicle’ and ‘clothing’) they selected twenty category members. So for ‘fruit’ the members ranged from ‘orange’ and ‘apple’ to ‘tomato’ and ‘olive’. They then asked a range of subjects to list the attributes they associated with some of these 120 category members. Each subject was presented with six members, one from each category, and for each member had a minute and a half to write down its salient attributes.\n\n[F]ew attributes were given that were true of all twenty members of the category – for four of the categories there was only one such item; for two of the categories, none. Furthermore, the single attribute that did apply to all members, in three cases was true of many items besides those within that superordinate (for example, “you eat it” for fruit). Rosch and Mervis (1975)\n\nThey go on to conclude that the superordinate is not defined by necessary and sufficient conditions, but by a ‘family resemblance’ between members. This particular experiment was taken to confirm that the number of attributes a member has with other members of the category is correlated with a previously defined measure of prototypicality.11 They claim that the intuition, commonly held amongst philosophers, that there must be some attribute in common to all the members, is explicable by the fact that the highly prototypical members of the category all do share quite a few attributes in common, ranging from 3 attributes in common to the highly prototypical vegetables, to 36 for the highly prototypical vehicles.\n11 In previous work they had done some nice experiments aimed at getting a grip on our intuition that apples are more prototypical exemplars of fruit than olives are.One occasionally hears people deride the assumption that there are necessary and sufficient conditions for the application of a term, as if this was the most preposterous piece of philosophy possible. Really, this assumption is no more than the assumption that dictionaries can be written, and without any reason to think otherwise, seems perfectly harmless. Perhaps, though, the Rosch and Mervis experiments provide a reason to think otherwise, a reason for thinking that the conditions of applicability for terms like ‘fruit’, ‘weapon’, and perhaps ‘knowledge’ are Wittgensteinian family resemblance conditions, rather than short lists of necessary and sufficient conditions, the kinds of conditions that fill traditional dictionaries.\nWhen we look closely, we see that the experiments do not show this at all. One could try and knock any such argument away by claiming the proposal is incoherent. The psychologists claim that there are no necessary and sufficient conditions for being a weapon, but something is a weapon iff it bears a suitable resemblance to paradigmatic weapons. In one sense, bearing a suitable resemblance to a paradigmatic weapon is a condition, so it looks like we just have a very short list of necessary and sufficient conditions, a list of length one. Jackson (1998, 61) makes a similar point in response to Stich’s invocation of Rosch’s experiments. This feels like it’s cheating, so I’ll move onto other objections. I’ll explain below just why it feels like cheating.\nPhilosophers aren’t particularly interested in terms like ‘weapon’, so these experiments only have philosophical interest if the results can be shown to generalise to terms philosophers care about. In other words, if can be shown that terms like ‘property’, ‘justice’, ‘cause’ and particularly ‘knows’ are cluster concepts, or family resemblance terms. But there is a good reason to think this is false. As William Ramsey (1998) notes, if F refers to a cluster concept, then for any proposed list of necessary and sufficient properties for F-hood, it should be easy to find an individual which is an F but which lacks some of these properties. To generate such an example, just find an individual which lacks one of the proposed properties, but which has several other properties from the cluster. It should be harder to find an individual which has the properties without being an F. If the proposed analysis is even close to being right, then having these conditions will entail having enough of the cluster of properties that are constitutive of F-hood to be an F. Note, for example, that all of the counterexamples Wittgenstein (1953) lists to purported analyses of ‘game’ are cases where something is, intuitively, a game but which does not satisfy the analysis. If game is really a cluster concept, this is how things should be. But it is not how things are with knowledge; virtually all counterexamples, from Gettier on, are cases which are intuitively not cases of knowledge, but which satisfy the proposed analysis. This is good evidence that even if some terms in English refer to cluster concepts, ‘knows’ is not one of them.\nSecondly, Rosch and Mervis’s conclusions about the nature of the superordinate categories makes some rather mundane facts quite inexplicable. In this experiment the subjects weren’t told which category each member was in, but for other categories they were. Imagine, as seems plausible, one of the subjects objected to putting the member in that category. Many people, even undergraduates, don’t regard olives and tomatoes as fruit. (“Fruit on pasta? How absurd!”) When the student asks why is this thing called a fruit, other speakers can provide a response. It is not a brute fact of language that tomatoes are fruit. It is not just by magic that we happened to come to a shared meaning for fruit that includes tomatoes, and that if faced with a new kind of object, we would generally agree about whether it is a fruit. It is because we know how to answer such questions. This answer to the Why is it called ‘fruit’? question had better be a sufficient condition for fruitness. If not, the subject is entitled to ask why having that property makes it a fruit. And unless there are very many possible distinct answers to this question, which seems very improbable, there will be a short list of necessary and sufficient conditions for being a fruit. But for this example, at least, ‘fruit’ was relatively arbitrary, so there will be a short list of necessary and sufficient conditions for being an F, for pretty much any F.\nThirdly, returning to ‘fruit’, we can see that Rosch and Mervis’s experiments could not possibly show that many superordinate predicates in English are cluster concepts. For they would, if successful, show that ‘fruit’ is a cluster concept, and it quite plainly is not. So by modus tollens, there is something wrong with their methodology. Some of the other categories they investigate, particularly ‘weapon’ and ‘furniture’ might be relatively cluster-ish, in a sense to be explained soon, but not ‘fruit’. As the OED says, a fruit is “the edible product of a tree, shrub or other plant, consisting of the seed and its envelope.” If nothing like this is right, then we couldn’t explain to the sceptical why we call tomatoes, olives and so on fruit.\nSo the conclusion that philosophically significant terms are likely to be cluster concepts is mistaken. To close, I note one way the cluster concept view could at least be coherent. Many predicates do have necessary and sufficient conditions for their applicability, just as traditional conceptual analysis assumed. In other words, they have analyses. However, any analysis must be in words, and sometimes the words needed will refer to quite recherche properties. The properties in the analysans may, that is, be significantly less natural than the analysandum.\nIn some contexts, we only consider properties that are above a certain level of naturalness. If I claim two things say my carpet and the Battle of Agincourt, have nothing in common, I will not feel threatened by an objector who points out that they share some gruesome, gerrymandered property, like being elements of {my carpet, the Battle of Agincourt}. Say that the best analysis of F-hood requires us to use predicates denoting properties which are below the contextually defined border between the ‘natural enough’ and ‘too gruesome to use’. Then there will be a sense in which there is no analysis of F into necessary and sufficient conditions; just the sense in which my carpet and the Battle of Avignon have nothing in common. Jackson’s argument feels like a cheat because he just shows that there will be necessary and sufficient conditions for any concept provided we are allowed to use gruesome properties, but he makes it sound like this proviso is unnecessary. If Rosch and Mervis’s experiments show anything at all, it is that this is true of some common terms in some everyday-ish contexts. In particular, if we restrict our attention to the predicates that might occur to us within ninety seconds (which plausibly correlates well with some level of naturalness), very few terms have analyses. Thus far, Rosch and Mervis are correct. They go wrong by projecting truths of a particular context to all contexts.\n\n\n6 In defence of analysis\nIn the previous section I argued that various empirical arguments gave us no reason to doubt that ‘knows’ will have a short analysis. In this section we look at various philosophical arguments to this conclusion. One might easily imagine the following objection to what has been claimed so far. At best, the above reasoning shows that if ‘knows’ has a short analysis, then the JTB analysis is correct, notwithstanding the intuitions provoked by Gettier cases. But there is little reason to think English terms have analyses, as evidenced by the failure of philosophers to analyse even one interesting term, and particular reasons to think that ‘knows’ does not have an analysis. These reasons are set out by Williamson (2000 Ch. 3), who argues, by appeal to intuitions about a particular kind of case, that there can be no analysis of ‘knows’ into independent clauses, one of which describes an internal state of the agent and the other of which describes an external state of the agent. This does not necessarily refute the JTB analysis, since the concepts of justification and belief in use may be neither internal nor external in Williamson’s sense. And if we are going to revise intuitions about the Gettier cases, we may wish to revise intuitions about Williamson’s cases as well, though here it is probably safest to not do this, because it is unclear just what philosophical benefit is derived from this revision. In response to these arguments I will make two moves: one defensive and one offensive. The defensive move is to distinguish the assumptions made here about the structure of the meaning of ‘knows’, and show how these assumptions do not have some of the dreadful consequences suggested by various authors. The offensive move, with which we begin, is to point out the rather unattractive consequences of not making these assumptions about the structure of the meaning of ‘knows’.\nIn terms of the concept of naturalness used above, the relation denoted by ‘knows’ might fall into one of three broad camps:\n\nIt might be rather unnatural;\nIt might be fairly natural in virtue of its relation to other, more natural, properties; or\nIt might be a primitive natural property, one that does not derive its naturalness from anything else.\n\nMy preferred position is (b). I think that the word ‘knows’, like every other denoting term in English, denotes something fairly natural. And I don’t think there are any primitively natural properties or relations in the vicinity of the denotation of this word, so it must derive its naturalness from its relation to other properties or relations. If this is so, we can recover some of the structure of its meaning by elucidating those relationships. If it is correct, that is exactly what I think the JTB theory does. This is not to say that justification, truth or belief are themselves primitively natural properties, but rather that we can make some progress towards recovering the source of the naturalness of knowledge via its decomposition into justification, truth and belief. But before investigating the costs of (b), let us look at the costs of (a) and (c).\nI think we can dispense with (c) rather quickly. It would be surprising, to say the least, if knowledge was a primitive relation. That X knows that p can hardly be one of the foundational facts that make up the universe. If X knows that p, this fact obtains in virtue of the obtaining of other facts. We may not be able to tell exactly what these facts are in general, but we have fairly strong opinions about whether they obtain or not in a particular case. This is why we are prepared to say whether or not a character knows something in a story, perhaps a philosophical story, without being told exactly that. We see the facts in virtue of which the character does, or does not, know this. This does not conclusively show that knowledge is not a primitively natural property. Electrical charge presumably is a primitively natural property, yet sometimes we can figure out the charge of an object by the behaviour of other objects. For example, if we know it is repulsed by several different negatively charged things, it is probably negatively charged. But in these cases it is clear our inference is from some facts to other facts that are inductively implied, not to facts that are constituted by the facts we know. (Only a rather unreformed positivist would say that charge is constituted by repulsive behaviour.) And it does not at all feel that in philosophical examples we are inductively (or abductively) inferring whether the character knows that p.\nThe more interesting question is whether (a) might be correct. This is, perhaps surprisingly, consistent with the theory of meaning advanced above. I held, following Lewis, that the meaning of a denoting term is the most natural object, property or relation that satisfies most of our usage dispositions. It is possible that the winner of this contest will itself be quite unnatural. This is what happens all the time with vague terms, and indeed it is what causes, or perhaps constitutes, their vagueness. None of the properties (or relations) that we may pick out by ‘blue’ is much more natural than several other properties (or relations) that would do roughly as well at capturing our usage dispositions, were they the denotation of ‘blue’.12 And indeed none of these properties (or relations) are particularly natural; they are all rather arbitrary divisions of the spectrum. The situation is possibly worse when we consider what Theodore Sider (2001) calls maximal properties. A property F is maximal iff things that massively overlap an F are not themselves an F. So being a coin is maximal, since large parts of a coin, or large parts of a coin fused with some nearby atoms outside the coin, are not themselves coins. Sider adopts the following useful notation: something is an F* iff it is suitable to be an F in every respect save that it may massively overlap an F. So a coin* is a piece of metal (or suitable substance) that is (roughly) coin-shaped and is (more or less) the deliberate outcome of a process designed to produce legal tender. Assuming that any collection of atoms has a fusion, in the vicinity of any coin there will be literally trillions of coin*s. At most one of these will be a coin, since coins do not, in general, overlap. That is, the property being a coin must pick out exactly one of these coin*s. Since the selection will be ultimately arbitrary, this property is not very natural. There are just no natural properties in the area, so the denotation of ‘coin’ is just not natural.\n12 I include the parenthetical comments here so as not to prejudge the question of whether colours are properties or relations. It seems unlikely to me that colours are relations, either the viewers or environments, but it is not worth quibbling over this here.These kind of considerations show that option (a) is a live possibility. But they do not show that it actually obtains. And there are several contrasts between ‘knows’, on the one hand, and ‘blue’ and ‘coin’ on the other, which suggest that it does not obtain. First, we do not take our word ‘knows’ to be as indeterminate as ‘blue’ or ‘coin’, despite the existence of some rather strong grounds for indeterminacy in it. Secondly, we take apparent disputes between different users of the word ‘knows’ to be genuine disputes, ones in which at most one side is correct, which we do not necessarily do with ‘blue’ and ‘coin’. Finally, we are prepared to use the relation denoted by ‘knows’ in inductive arguments in ways that seem a little suspect with genuinely unnatural relations, as arguably evidenced by our attitudes towards ‘coin’ and ‘blue’. Let’s look at these in more detail.\nIf we insisted that the meaning of ‘knows’ must validate all of our dispositions to use the term, we would find that the word has no meaning. If we just look at intuitions, we will find that our intuitions about ‘knows’ are inconsistent with some simple known facts. (Beliefs, being regimented by reflection, might not be inconsistent, depending on how systematic the regimentation has been.) For example, the following all seem true to many people.\n\nKnowledge supervenes on evidence: if two people (not necessarily in the same possible world) have the same evidence, they know the same things.\nWe know many things about the external world.\nWe have the same evidence as some people who are the victims of massive deception, and who have few true beliefs about their external world.\nWhatever is known is true.\n\nThese are inconsistent, so they cannot all be true. We could take any three of these as an argument for the negation of the fourth, though probably the argument from (1) (2) and (3) to the negation of (4) is less persuasive than the other three such arguments. I don’t want to adjudicate here which such argument is sound. All I want to claim here is that there is a fact of the matter about which of these arguments is sound, and hence about which of these four claims is false. If two people are disagreeing about which of these is false, at most one of them is right, and the other is wrong. If ‘knows’ denoted a rather unnatural relation, there would be little reason to believe these things to be true. Perhaps by more carefully consulting intuitions we could determine that one of them is false by seeing that it had the weakest intuitive pull. If we couldn’t do this, it would follow that in general there was no fact of the matter about which is false, and if someone wanted to use ‘know’ in their idiolect so that one particular one of these is false, there would be no way we could argue that they were wrong. It is quite implausible that this is what should happen in such a situation. It is more plausible that the dispute should be decided by figuring out which group of three can be satisfied by a fairly natural relation. This, recall, is just how we resolve disputes in many other areas of philosophy, from logic to ethics. If there is no natural relation eligible to be the meaning of ‘knows’, then probably this dispute has no resolution, just like the dispute about what ‘mass’ means in Newtonian mechanics.13\n13 Note that in that dispute the rivals are quite natural properties, but seem to be matched in their naturalness. In the dispute envisaged here, the rivals are quite unnatural, but still seem to be matched. For more on ‘mass’, see Field (1973).The above case generalises quite widely. If one speaker says that a Gettier case is a case of knowledge and another denies this (as Stich assures us actually happens if we cast our linguistic net wide enough) we normally assume that one of them is making a mistake. But if ‘knows’ denotes something quite unnatural, then probably each is saying something true in her own idiolect. Each party may make other mistaken claims, that for example what they say is also true in the language of all their compatriots, but in just making these claims about knowledge they would not be making a mistake. Perhaps there really is no fact of the matter here about who is right, but thinking so would be a major change to our common way of viewing matters, and hence would be a rather costly consequence of accepting option (a). Note here the contrast with ‘blue’ and ‘coin’. If one person adopts an idiosyncratic usage of ‘blue’ and ‘coin’, one on which there are determinate facts about matters where, we say, there are none, the most natural thing to say is that they are using the terms differently to us. If they insist that it is part of their intention in using the terms to speak the same way as their fellows we may (but only may) revise this judgement. But in general there is much more inclination to say that a dispute over whether, say, a patch is blue is merely verbal than to say this about a dispute over whether X knows that p.\nFinally, if knowledge was a completely unnatural relation, we would no more expect it to play a role in inductive or analogical arguments than does grue, but it seems it can play such a role. One might worry here that blueness also plays a role in inductive arguments, as in: The sky has been blue the last n days, so probably it will be blue tomorrow. If blueness is not natural, this might show that unnatural properties can play a role in inductive arguments. But what is really happening here is that there is, implicitly, an inductive argument based on a much narrow colour spectrum, and hence a much more natural property. To see this, note that we would be just as surprised tomorrow if the sky was navy blue, or perhaps of the dominant blue in Picasso’s blue period paintings, as if it were not blue at all.\nSo there are substantial costs to (a) and (c). Are there similar costs to (b)? If we take (b) to mean that there is a decomposition of the meaning of ‘knows’ into conditions, expressible in English, which we can tell a priori are individually necessary and jointly sufficient for knowledge, and such that it is also a priori that they represent natural properties, then (b) would be wildly implausible. To take just one part of this, Williamson (2000) notes it is clear that there are some languages in which such conditions cannot be expressed, so perhaps English is such a language too. And if this argument for ‘knows’ works it presumably works for other terms, like ‘pain’, but it is hard to find such an a priori decomposition of ‘pain’ into more natural properties. Really, all (b) requires is that there be some connection, perhaps only discoverable a posteriori, perhaps not even humanly comprehensible, between knowledge and other more primitively natural properties. These properties need not be denoted by any terms of English, or any other known language.\nMost importantly, this connection need not be a decomposition. If knowledge is the most general factive mental state, as Williamson proposes, and being factive and being a mental state are natural properties, then condition (b) will be thereby satisfied. If knowledge is the norm of assertion, as Williamson also proposes, then that could do as the means by which knowledge is linked into the network of natural properties. This last assumes that being an assertion is a natural property, and more dangerously that norms as natural, but these are relatively plausible assumptions in general. In neither case do we have a factorisation, in any sense, of knowledge into constituent properties, but we do have, as (b) requires, a means by which knowledge is linked into the network of natural properties. It is quite plausible that for every term which, unlike ‘blue’ and ‘coin’ are not excessively vague and do not denote maximal properties, something like (b) is correct. Given the clarifications made here to (b), this is consistent with most positions normally taken to be anti-reductionist about those terms, or their denotata.\n\n\n7 Naturalness and the JTB theory\nI have argued here that the following argument against the JTB theory is unsound.\n\nP1.\n\nThe JTB theory says that Gettier cases are cases of knowledge.\n\nP2.\n\nIntuition says that Gettier cases are not cases of knowledge.\n\nP3.\n\nIntuition is trustworthy in these cases.\n\nC.\n\nThe JTB theory is false.\n\n\nThe objection has been that P3 is false in those cases where following intuition slavishly would mean concluding that some common term denoted a rather unnatural property while accepting deviations from intuition would allow us to hold that it denoted a rather natural property. Peter Klein (in conversation) has suggested that there is a more sophisticated argument against the JTB theory that we can draw out of the Gettier cases. Since this argument is a good illustration of the way counterexamples should be used in philosophy, I’ll close with it.\nKlein’s idea, in effect, is that we can use Gettier cases to argue that being a justified true belief is not a natural property, and hence that P3 is after all true. Remember that P3 only fails when following intuition too closely would lead too far away from naturalness. If being a justified true belief is not a natural property to start with, there is no great danger of this happening. What the Gettier cases show us, goes the argument, is that there are two ways to be a justified true belief. The first way is where the belief is justified in some sense because it is true. The second way is where it is quite coincidental that the belief is both justified and true. These two ways of being a justified true belief may be natural enough, but the property being a justified true belief is just the disjunction of these two not especially related properties.\nI think this is, at least, a prima facie compelling argument. There are, at least, three important points to note about it. First, this kind of reasoning does not obviously generalise. Few of the examples described in Shope (1983) could be used to show that some target theory in fact made knowledge into a disjunctive kind. The second point is that accepting this argument is perfectly consistent with accepting everything I said above against the (widespread) uncritical use of appeal to intuition. Indeed, if what I said above is broadly correct then this is just the kind of reasoning we should be attempting to use when looking at fascinating counterexamples. Thirdly, if the argument works it shows something much more interesting than just that the JTB theory is false. It shows that naturalness is not always transferred to a conjunctive property by its conjuncts.\nI assume here that being a justified belief and being a true belief are themselves natural properties, and being a justified true belief is the conjunction of these. The only point here that seems possibly contentious is that being a true belief is not natural. On some forms of minimalism about truth this may be false, but those forms seem quite implausibly strong. Remember that saying being a true belief is natural does not imply that has an analysis – truth might be a primitively natural component of this property. And remember also that naturalness is intensional rather than hyperintensional. If all true beliefs correspond with reality in a suitable way, and corresponding with reality in that way is a natural property, then so is being a true belief, even if truth of belief cannot be explained in terms of correspondence.\nThis is a surprising result, because the way naturalness was originally set up by Lewis suggested that it would be transferred to a conjunctive property by its conjuncts. Lewis gave three accounts of naturalness. The first is that properties are perfectly natural in virtue of being co-intensive with a genuine universal. The third is that properties are natural in virtue of the mutual resemblance of their members, where resemblance is taken to be a primitive. On either account, it seems that whenever being F is natural, and so is being G, then being F and G will be natural.14 The second account, if it can be called that, is that naturalness is just primitive. If the Gettier cases really do show that being a justified true belief is not natural, then they will have shown that we have to fall back on just this account of naturalness.\n\n\n\n14 I follow Armstrong (1978) here in assuming that there are conjunctive universals.\n\n\nReferences\n\nArmstrong, D. M. 1978. Universals and Scientific Realism. Cambridge: Cambridge University Press.\n\n\nBealer, George. 1998. “Intuition and the Autonomy of Philosophy.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 201–40. Lanham: Rowman & Littlefield.\n\n\nCummins, Robert. 1998. “Reflection on Reflective Equilibrium.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 113–28. Lanham: Rowman & Littlefield.\n\n\nDeRose, Keith. 1996. “Knowledge, Assertion and Lotteries.” Australasian Journal of Philosophy 74 (4): 568–79. https://doi.org/10.1080/00048409612347531.\n\n\nField, Hartry. 1973. “Theory Change and the Indeterminacy of Reference.” Journal of Philosophy 70 (14): 462–81. https://doi.org/10.2307/2025110.\n\n\nGrice, H. Paul. 1989. Studies in the Way of Words. Cambridge, MA.: Harvard University Press.\n\n\nHorowitz, Tamara. 1998. “Philosophical Intuitions and Psychological Theory.” Ethics 108 (2): 367–85. https://doi.org/10.1086/233809.\n\n\nHorwich, Paul. 1999. Meaning. Oxford: Oxford University Press.\n\n\nJackson, Frank. 1998. From Metaphysics to Ethics: A Defence of Conceptual Analysis. Clarendon Press: Oxford.\n\n\nLangton, Rae, and David Lewis. 1998. “Defining ‘Intrinsic’.” Philosophy and Phenomenological Research 58 (2): 333–45. https://doi.org/10.2307/2653512.\n\n\n———. 2001. “Marshall and Parsons on ‘Intrinsic’.” Philosophy and Phenomenological Research 63 (2): 353–55. https://doi.org/10.2307/3071068.\n\n\nLewis, David. 1983. “New Work for a Theory of Universals.” Australasian Journal of Philosophy 61 (4): 343–77. https://doi.org/10.1080/00048408312341131.\n\n\n———. 1984. “Putnam’s Paradox.” Australasian Journal of Philosophy 62 (3): 221–36. https://doi.org/10.1080/00048408412340013.\n\n\n———. 1992. “Meaning Without Use: Reply to Hawthorne.” Australasian Journal of Philosophy 70 (1): 106–10. https://doi.org/10.1080/00048408112340093.\n\n\n———. 2001. “Redefining ’Intrinsic’.” Philosophy and Phenomenological Research 63 (2): 381–98. https://doi.org/10.2307/3071071.\n\n\nMenzies, Peter. 1996. “Probabilistic Causation and the Pre-Emption Problem.” Mind 105 (417): 85–117. https://doi.org/10.1093/mind/105.417.85.\n\n\nNelkin, Dana. 2000. “The Lottery Paradox, Knowledge, and Rationality.” Philosophical Review 109 (3): 373–409. https://doi.org/10.2307/2693695.\n\n\nRamsey, William. 1998. “Prototypes and Conceptual Analysis.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 161–77. Lanham: Rowman & Littlefield.\n\n\nRosch, Eleanor, and Carolyn Mervis. 1975. “Family Resemblances: Studies in the Internal Structure of Categories.” Cognitive Science 7 (4): 573–605. https://doi.org/10.1016/0010-0285(75)90024-9.\n\n\nRyle, Gilbert. 1949. The Concept of Mind. New York: Barnes; Noble.\n\n\nShope, Robert. 1983. The Analysis of Knowledge. Princeton: Princeton University Press.\n\n\nSider, Theodore. 2001. “Maximality and Intrinsic Properties.” Philosophy and Phenomenological Research 63 (2): 357–64. https://doi.org/10.1111/j.1933-1592.2001.tb00109.x.\n\n\nSosa, Ernest. 1998. “Minimal Intuition.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 257–69. Lanham: Rowman & Littlefield.\n\n\nStich, Stephen. 1988. “Reflective Equilibrium, Analytic Epistemology and the Problem of Cognitive Diversity.” Synthese 74 (3): 391–413. https://doi.org/10.1007/bf00869637.\n\n\n———. 1992. “What Is a Theory of Mental Representation?” Mind 101 (402): 243–63. https://doi.org/10.1093/mind/101.402.243.\n\n\nTennant, Neil. 1992. Autologic. Edinburgh: Edinburgh University Press.\n\n\nUnger, Peter. 1996. Living High and Letting Die. Oxford: Oxford University Press.\n\n\nWeinberg, Jonathan, Stephen Stich, and Shaun Nichols. 2001. “Normativity and Epistemic Intuitions.” Philosophical Topics 29 (1): 429–60. https://doi.org/10.5840/philtopics2001291/217.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nWittgenstein, Ludwig. 1953. Philosophical Investigations. London: Macmillan.\n\nCitationBibTeX citation:@online{weatherson2003,\n  author = {Weatherson, Brian},\n  title = {What {Good} Are {Counterexamples?}},\n  volume = {115},\n  number = {1},\n  pages = {1-31},\n  date = {2003-07},\n  doi = {10.1023/A:1024961917413},\n  langid = {en}\n}"
  },
  {
    "objectID": "posts/diri/index.html",
    "href": "posts/diri/index.html",
    "title": "Defending Interest Relative Invariantism",
    "section": "",
    "text": "In recent years a number of authors have defended the interest-relativity of knowledge and justification. Views of this form are floated by John Hawthorne (2004), and endorsed by Jeremy Fantl and Matthew McGrath (2002, 2009), Jason Stanley (2005) and Brian Weatherson (2005). The various authors differ quite a lot in how much interest-relativity they allow, but what is common is the defence of interest-relativity.\n\nPublished in Logos and Episteme 2 (2011): 591-609.\nImage from Wikimedia Commons.\n\nThese views have, quite naturally, drawn a range of criticisms. The primary purpose of this paper is to respond to these criticisms and, as it says on the tin, defend interest-relative invariantism, or IRI for short. But I don’t plan to defend every possible version of IRI, only a particular one. Most of the critics of IRI have assumed that it must have some or all of the following features.\n\nIt is harder to know things in high-stakes situations than in low-stakes situations.\nThere is an interest-sensitive constituent of knowledge.\nIRI stands and falls with some principles connecting knowledge and action, such as the principles found in Hawthorne and Stanley (2008).\n\nMy preferred version of IRI has none of these three features.1\n1 It is a tricky exegetical question how many of the three features here must be read into defences of IRI in the literature. My reading is that they do not have to be read in, so it is not overly original of me to defend a version of IRI that does away with all three. But I know many people disagree with that. If they’re right, this paper is more original than I think it is, and so I’m rather happy to be wrong. But I’m going to mostly set these exegetical issues aside, and compare different theories without taking a stand on who originally promulgated them.First, it says that knowledge changes when the odds an agent faces change, not when the stakes change. More precisely, interests affect belief because whether someone believes \\(p\\) depends inter alia on whether their credence in \\(p\\) is high enough that any bet on \\(p\\) they actually face is a good bet. And interests affect knowledge largely because they affect belief. Raising the stakes of any bet on \\(p\\) does not directly change whether an agent believes \\(p\\), but changing the odds of the bets on \\(p\\) they face does change it. In practice raising the stakes changes the odds due to the declining marginal utility of material goods. So in practice high-stakes situations are typically long-odds situations. But knowledge is hard in those situations because they are long-odds situations, not because they are high-stakes situations.\nSo my version of IRI says that knowledge differs between these two cases.\n\nHigh Cost Map:\n\nZeno is walking to the Mysterious Bookshop in lower Manhattan. He’s pretty confident that it’s on the corner of Warren Street and West Broadway. But he’s been confused about this in the past, forgetting whether the east-west street is Warren or Murray, and whether the north-south street is Greenwich, West Broadway or Church. In fact he’s right about the location this time, but he isn’t justified in having a credence in his being correct greater than about 0.95. While he’s walking there, he has two options. He could walk to where he thinks the shop is, and if it’s not there walk around for a few minutes to the nearby corners to find where it is. Or he could call up directory assistance, pay $1, and be told where the shop is. Since he’s confident he knows where the shop is, and there’s little cost to spending a few minutes walking around if he’s wrong, he doesn’t do this, and walks directly to the shop.\n\nLow Cost Map:\n\nJust like the previous case, except that Zeno has a new phone with more options. In particular, his new phone has a searchable map, so with a few clicks on the phone he can find where the store is. Using the phone has some very small costs. For example, it distracts him a little, which marginally raises the likelihood of bumping into another pedestrian. But the cost is very small compared to the cost of getting the location wrong. So even though he is very confident about where the shop is, he double checks while walking there.\n\n\nI think the Map Cases are like the various cases that have been used to motivate interest-relativity2 in all important respects. I think Zeno knows where the shop is in High Cost Map, and doesn’t know in Low Cost Map. And he doesn’t know in Low Cost Map because the location of the shop has suddenly become the subject matter of a bet at very long odds. You should think of Zeno’s not checking the location of the shop on his phone-map as a bet on the location of the shop. If he wins the bet, he wins a few seconds of undistracted strolling. If he loses, he has to walk around a few blocks looking for a store. The disutility of the loss seems easily twenty times greater than the utility of the gain, and by hypothesis the probability of winning the bet is no greater than 0.95. So he shouldn’t take the bet. Yet if he knew where the store was, he would be justified in taking the bet. So he doesn’t know where the store is. Now this is not a case where higher stakes defeat knowledge. If anything, the stakes are lower in Low Cost Map. But the relevant odds are longer, and that’s what matters to knowledge.\n2 Such as the Bank Cases in Stanley (2005), or the Train Cases in Fantl and McGrath (2002).Second, on this version of IRI, interests matter because there are interest-sensitive defeaters, not because interests form any kind of new condition on knowledge, alongside truth, justification, belief and so on. In particular, interests matter because there are interest-relative coherence constraints on knowledge. Some coherence constraints, I claim, are not interest-relative. If an agent believes \\(\\neg p\\), that belief defeats her purported knowledge that \\(p\\), even if the belief that \\(p\\) is true, justified, safe, sensitive and so on. It is tempting to try to posit a further coherence condition.\n\nPractical Coherence\n\nAn agent does not know that \\(p\\) if she prefers \\(\\varphi\\) to \\(\\psi\\) unconditionally, but prefers \\(\\psi\\) to \\(\\varphi\\) conditional on \\(p\\).\n\n\nBut that is too strong. For reasons similar to those gone over at the start of Hawthorne (2004), it would mean we know nearly nothing. A more plausible condition is:\n\nRelevant Practical Coherence\n\nAn agent does not know that \\(p\\) if she prefers \\(\\varphi\\) to \\(\\psi\\) unconditionally, but prefers \\(\\psi\\) to \\(\\varphi\\) conditional on \\(p\\), for any \\(\\varphi, \\psi\\) that are relevant given her interests.\n\n\nWhen this condition is violated, the agent’s claim to knowledge is defeated. As we’ll see below, defeaters behave rather differently to constituents of knowledge. Some things which could not plausibly be grounds for knowledge could be defeaters to defeaters for knowledge.\nRelevant Practical Coherence suffices, at least among agents who are trying to maximise expected value, to generate an interest-relativity to knowledge. The general structure of the case should be familiar from the existing literature. Let \\(p\\) be a proposition that is true, believed by the agent, and strongly but not quite conclusively supported by their evidence. Let \\(B\\) be a bet that has a small positive return if \\(p\\), and a huge negative return if \\(\\neg p\\). Assume the agent is now offered the bet, and let \\(\\varphi\\) be declining the bet, and \\(\\psi\\) be accepting the bet. Conditional on \\(p\\), the bet wins, so the agent prefers the small positive payout, so prefers \\(\\psi\\) to \\(\\varphi\\) conditional on \\(p\\). But the bet has a massively negative expected return, so unconditionally the agent does not want it. That is, unconditionally she prefers \\(\\varphi\\) to \\(\\psi\\). Once the bet is offered, the actions \\(\\varphi\\) and \\(\\psi\\) become relevant given her interests, so by Relevant Practical Coherence she no longer knows \\(p\\). So for such an agent, knowledge is interest-relative.\nCases where knowledge is defeated because if the agent did know \\(p\\), that would lead to problems elsewhere in their cognitive system, have a few quirky features. In particular, whether the agent knows \\(p\\) can depend on very distant features. Consider the following kind of case.\n\nConfused Student\nCon is systematically disposed to affirm the consequent. That is, if he notices that he believes both \\(p\\) and \\(q \\rightarrow p\\), he’s disposed to either infer \\(q\\), or if that’s impermissible given his evidence, to ditch his belief in the conjunction of \\(p\\) and \\(q \\rightarrow p\\). Con has completely compelling evidence for both \\(q \\rightarrow p\\) and \\(\\neg q\\). He has good but less compelling evidence for \\(p\\). And this evidence tracks the truth of \\(p\\) in just the right way for knowledge. On the basis of this evidence, Con believes \\(p\\). Con has not noticed that he believes both \\(p\\) and \\(q \\rightarrow p\\). If he did, he’s unhesitatingly drop his belief that \\(p\\), since he’d realise the alternatives (given his dispositions) involved dropping belief in a compelling proposition. Two questions:\n\nDoes Con know that \\(p\\)?\nIf Con were to think about the logic of conditionals, and reason himself out of the disposition to affirm the consequent, would he know that \\(p\\)?\n\n\nI think the answer to the first question is No, and the answer to the second question is Yes. As it stands, Con’s disposition to affirm the consequent is a doxastic defeater of his putative knowledge that \\(p\\). Put another way, \\(p\\) doesn’t cohere well enough with the rest of Con’s views for his belief that \\(p\\) to count as knowledge. To be sure, \\(p\\) coheres well enough with those beliefs by objective standards, but it doesn’t cohere at all by Con’s lights. Until he changes those lights, it doesn’t cohere well enough to be knowledge. Moreover (as a referee pointed out), Con’s belief is not safe. Since he could easily have ‘reasoned’ himself out of his belief that \\(p\\), the belief isn’t safe in the way that knowledge is safe.\nI think that beliefs which violate Relevant Practical Coherence fail to be knowledge for the same reason that Con’s belief that \\(p\\) fails to be knowledge. In what follows, I’ll make frequent use of this analogy; many of the objections to IRI turn out to be equally strong objections to the view that there are ever defeaters of the type Con suffers from.\nThis suggests our third point. This version of IRI does not take IRI to be a consequence of more general principles about knowledge and action. It simply says that there exist at least one pair of cases where the only relevant difference between agents in the two cases concerns their interests, but one knows that \\(p\\) and the other does not.3 I happen to think that most of the general principles that philosophers have used to try to derive IRI are false. But since IRI is much weaker than those principles, that is no reason to conclude IRI is false.4\n3 And this is true even though \\(p\\) is not a proposition about their interests, or something that is supported by propositions about their interests, and so on.4 I will consider, and tentatively support, one principle stronger than IRI in the final section. But the key point is that these general principles are not needed to defend IRI.The existence of interest-relativity is then quite a weak claim. There are plenty of stronger claims in the area we could make. I prefer, for instance, a version of IRI where being offered bets like \\(B\\) defeats knowledge that \\(p\\) even if the agent does not have the preferences I ascribed above. (That could be because she isn’t trying to maximise expected value, or because she’s messed up the expected value calculations.) But knowledge could be interest-relative even if I’m wrong about those cases.\nSo I’ve set out a version of IRI that lacks three features often attributed to IRI. I haven’t argued for that theory here - I do that at much greater length in (Author Paper 1). But I hope I’ve done enough to convince you that the theory is both a version of IRI, and not obviously false. In what follows, I’ll argue that the theory is immune to the various challenges to IRI that have been put forward in the literature. This immunity is, I think, a strong reason to prefer this version of IRI.\n\n1 Experimental Objections\nI don’t place as much weight as some philosophers do on the correlation between the verdicts of an epistemological theory and the gut reactions that non-experts have to tricky cases. And I don’t think the best cases for IRI relies on such a correlation holding. The best case for IRI is that it integrates nicely with an independently supported theory of belief, and that it lets us keep a number of plausible principles without drifting into skepticism.5 But still, it is nice to not have one’s theory saying exorbitantly counterintuitive things. Various experimental results, such as the results in May et al. (2010) and Feltz and Zarpentine (2010), might be thought to suggest that IRI does have consequences which are counterintuitive, or which at least run counter to the intuitions of some experimental subjects. I’m going to concentrate on the latter set of results here, though I think that what I say will generalise to related experimental work. In fact, I think the experiments don’t really tell against IRI, because IRI, at least in my preferred version, doesn’t make any unambiguous predictions about the cases at the centre of the experiments. The reason for this is related to my insistence that we concentrate on the odds an agent faces, not the stakes she faces.\n5 This points are expanded upon greatly in (Author Paper 1).Feltz and Zarpentine gave subjects related vignettes, such as the following pair. (Each subject only received one of the pair.)\n\nHigh Stakes Bridge\n\nJohn is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a yawning thousand foot drop. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, ‘I know that my truck will make it across the bridge.’\n\nLow Stakes Bridge\n\nJohn is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a three foot ditch. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, ‘I know that my truck will make it across the bridge.’ (Feltz and Zarpentine 2010, 696)\n\n\nSubjects were asked to evaluate John’s thought. And the result was that 27% of the participants said that John does not know that the truck will make it across in Low Stakes Bridge, while 36% said he did not know this in High Stakes Bridge. Feltz and Zarpentine say that these results should be bad for interest-relativity views. But it is hard to see just why this is so.\nNote that the change in the judgments between the cases goes in the direction that IRI seems to predict. The change isn’t trivial, even if due to the smallish sample size it isn’t statistically significant in this sample. But should a view like IRI have predicted a larger change? To figure this out, we need to ask three questions.\n\nWhat are the costs of the bridge collapsing in the two cases?\nWhat are the costs of not taking the bet, i.e., not driving across the bridge?\nWhat is the rational credence to have in the bridge’s sturdiness given the evidence John has?\n\nConditional on the bridge not collapsing, the drivers presumably prefer taking the bridge to not taking it. And the actions of taking the bridge or going around the long way are relevant. So by Relevant Practical Coherence, the drivers know the bridge will not collapse in Low Stakes Bridge but not High Stakes Bridge if the following equation is true. (I assume all the other conditions for knowledge are met, and that there are no other salient instances of Relevant Practical Coherence to consider.)\n\\[\\frac{C_H}{G + C_H} &gt; x &gt; \\frac{C_L}{G + C_L}\\]\nwhere \\(G\\) is the gain the driver gets from taking a non-collapsing bridge rather than driving around (or whatever the alternative is), \\(C_H\\) is the cost of being on a collapsing bridge in High Stakes Bridge, \\(C_L\\) is the cost of being on a collapsing bridge in Low Stakes Bridge, and \\(x\\) is the probability that the bridge will collapse. I assume \\(x\\) is constant between the two cases. If that equation holds, then taking the bridge, i.e., acting as if the bridge won’t collapse, maximises expected utility in Low Stakes Bridge but not High Stakes Bridge. So in High Stakes Bridge, adding the proposition that the bridge won’t collapse to the agent’s cognitive system produces incoherence, since the agent won’t (at least rationally) act as if the bridge won’t collapse. So if the equation holds, the agent’s interests in avoiding \\(C_H\\) creates a doxastic defeater in High Stakes Bridge.\nBut does the equation hold? Or, more relevantly, did the subjects of the experiment believe that the equation hold? None of the four variables has their values clearly entailed by the story, so we have to guess a little as to what the subjects’ views would be.\nFeltz and Zarpentine say that the costs in “High Stakes Bridge are very costly—certain death—whereas the costs in Low Stakes Bridge are likely some minor injuries and embarrassment.” (Feltz and Zarpentine 2010, 702) I suspect both of those claims are wrong, or at least not universally believed. A lot more people survive bridge collapses than you may expect, even collapses from a great height.6 And once the road below a truck collapses, all sorts of things can go wrong, even if the next bit of ground is only 3 feet away. (For instance, if the bridge collapses unevenly, the truck could roll, and the driver would probably suffer more than minor injuries.)\n6 In the West Gate bridge collapse in Melbourne in 1971, a large number of the victims were underneath the bridge; the people on top of the bridge had a non-trivial chance of survival. That bridge was 200 feet above the water, not 1000, but I’m not sure the extra height would matter greatly. Again from a slightly lower height, over 90% of people on the bridge survived the I-35W collapse in Minneapolis in 2007.We aren’t given any information as to the costs of not crossing the bridge. But given that 15 other trucks, with less evidence than John, have decided to cross the bridge, it seems plausible to think they are substantial. If there was an easy way to avoid the bridge, presumably the first truck would have taken it. If \\(G\\) is large enough, and \\(C_H\\) small enough, then the only way for this equation to hold will be for \\(x\\) to be low enough that we’d have independent reason to say that the driver doesn’t know the bridge will hold.\nBut what is the value of \\(x\\)? John has a lot of information that the bridge will support his truck. If I’ve tested something for sturdiness two or three times, and it has worked, I won’t even think about testing it again. Consider what evidence you need before you’ll happily stand on a particular chair to reach something in the kitchen, or put a heavy television on a stand. Supporting a weight is the kind of thing that either fails the first time, or works fairly reliably. Obviously there could be some strain-induced effects that cause a subsequent failure7, but John really has a lot of evidence that the bridge will support him.\n7 As I believe was the case in the I-35W collapse.Given those three answers, it seems to me that it is a reasonable bet to cross the bridge. At the very least, it’s no more of an unreasonable bet than the bet I make every day crossing a busy highway by foot. So I’m not surprised that 64% of the subjects agreed that John knew the bridge would hold him. At the very least, that result is perfectly consistent with IRI, if we make plausible assumptions about how the subjects would answer the three numbered questions above.\nAnd as I’ve stressed, these experiments are only a problem for IRI if the subjects are reliable. I can think of two reasons why they might not be. First, subjects tend to massively discount the costs and likelihoods of traffic related injuries. In most of the country, the risk of death or serious injury through motor vehicle accident is much higher than the risk of death or serious injury through some kind of crime or other attack, yet most people do much less to prevent vehicles harming them than they do to prevent criminals or other attackers harming them.8 Second, only 73% of these subjects in this very experiment said that John knows the bridge will support him in Low Stakes Bridge. This is rather striking. Unless the subjects endorse an implausible kind of scepticism, something has gone wrong with the experimental design. But if the subjects are implausibly sceptical, then we shouldn’t require our epistemological theory to track their gut reactions. (And if something has gone wrong with the experimental design, then obviously can’t be used as the basis for any objection.) So given the fact that the experiment points broadly in the direction of IRI, and that with some plausible assumptions it is perfectly consistent with that theory, and that the subjects seem unreasonably sceptical to the point of unreliability about epistemology, I don’t think this kind of experimental work threatens IRI.\n8 See the massive drop in the numbers of students walking or biking to school, reported in Ham, Martin, and Kohl III (2008), for a sense of how big an issue this is.\n\n2 Knowledge By Indifference and By Wealth\nGillian Russell and John Doris (2009) argue that Jason Stanley’s account of knowledge leads to some implausible attributions of knowledge, and if successful their objections would generalise to other forms of IRI. I’m going to argue that Russell and Doris’s objections turn on principles that are prima facie rather plausible, but which ultimately we can reject for independent reasons.9\n9 I think the objections I make here are similar in spirit to those Stanley made in a comments thread on Certain Doubts, though the details are new. The thread is at http://el-prod.baylor.edu/certain_doubts/?p=616.Their objection relies on variants of the kind of case Stanley uses heavily in his (2005) to motivate a pragmatic constraint on knowledge. Stanley considers the kinds of cases we used to derive IRI from Relevant Practical Coherence. So imagine an agent who faces a choice between accepting the status quo, call that \\(\\varphi\\), and taking some giant risk, call that \\(\\psi\\). The giant risk in this case will involve a huge monetary loss if \\(\\neg p\\), and a small non-monetary gain if \\(p\\). Stanley says, and I agree, that in such a case the agent doesn’t know \\(p\\), even if their belief in \\(p\\) is true, well supported by evidence, and so on. Moreover, he says, had \\(\\psi\\) not been a relevant option, the agent could have known \\(p\\). I agree, and I think Relevant Practical Coherence explains these intuitions well.\nRussell and Doris imagine two kinds of variants on Stanley’s case. In one variant the agent doesn’t care about the material loss associated with \\(\\psi \\wedge \\neg p\\). As I would put it, although their material wealth would decline precipitously in that case, their utility would not, because their utility is not tightly correlated with material wellbeing. Given that, the agent may well prefer \\(\\psi\\) to \\(\\varphi\\) unconditionally, and so would still know \\(p\\). Russell and Doris don’t claim this is a problem in itself, but they do think the conjunction of this with the previous paragraph is a problem. As they put it, “you should have reservations ... about what makes the knowledge claim true: not giving a damn, however enviable in other respects, should not be knowledge-making.” (Russell and Doris 2009, 432).\nTheir other variant involves an agent with so much money that the material loss is trifling to them. Since the difference in utility between having, say, eight billion dollars and seven billion dollars is not that high, perhaps they will again prefer \\(\\psi\\) to \\(\\varphi\\) unconditionally, so still know \\(p\\). But it is, allegedly, counterintuitive to have the knowledge that \\(p\\) turn on the agent’s wealth. As Russell and Doris say, “matters are now even dodgier for practical interest accounts, because money turns out to be knowledge making.” (Russell and Doris 2009, 433) And this isn’t just because wealth can purchase knowledge. As they say, “money may buy the instruments of knowledge ... but here the connection between money and knowledge seems rather too direct.” (Russell and Doris 2009, 433)\nThe first thing to note about this case is that indifference and wealth aren’t really producing knowledge. What they are doing is more like defeating a defeater. Remember that the agent in question had enough evidence, and enough confidence, that they would know \\(p\\) were it not for the practical circumstances. As I said in the introduction, practical considerations enter debates about knowledge in part because they are distinctive kinds of defeaters. It seems that’s what is going on here. And we have, somewhat surprisingly, independent evidence to think that indifference and wealth do matter to defeaters.\nConsider two variants on Gilbert Harman’s ‘dead dictator’ example (Harman 1973, 75). In the original example, an agent reads that the dictator has died through an actually reliable source. But there are many other news sources around, such that if the agent read them, she would lose her belief. Even if the agent doesn’t read those sources, their presence can constitute defeaters to her putative knowledge that the dictator died.\nIn our first variant on Harman’s example, the agent simply does not care about politics. It’s true that there are many other news sources around that are ready to mislead her about the dictator’s demise. But she has no interest in looking them up, nor is she at all likely to look them up. She mostly cares about literature, and will spend her day reading old novels. In this case, the misleading news sources are too distant, in a sense, to be defeaters. So she still knows the dictator has died. Her indifference towards politics doesn’t generate knowledge - the original reliable report is the knowledge generator - but her indifference means that a would-be defeater doesn’t gain traction.\nIt might be objected here that the agent doesn’t know the dictator has died because there are misleading reports around saying the dictator is alive, and she is in no position to rebut them. But this is too high a standard for knowledge. There are millions of people in Australia who know that humans are contributing to global warming on purely testimonial grounds. Many, perhaps even most, of these people would not be able to answer a carefully put together argument that humans are not contributing to global warming, such as an argument that picked various outlying statistics to mislead the reader. And such arguments certainly exist; the conservative parts of the media do as much as they can to play them up. But the mere existence of such arguments doesn’t defeat the average person’s testimonial knowledge about anthropogenic global warming. Similarly, the mere existence of misleading reports does not defeat our agent’s knowledge of the dictator’s death, as long as there is no nearby world where she is exposed to the reports. (Thanks here to an anonymous referee.)\nIn the second variant, the agent cares deeply about politics, and has masses of wealth at hand to ensure that she knows a lot about it. Were she to read the misleading reports that the dictator has survived, then she would simply use some of the very expensive sources she has to get more reliable reports. Again this suffices for the misleading reports not to be defeaters. Even before the rich agent exercises her wealth, the fact that her wealth gives her access to reports that will correct for misleading reports means that the misleading reports are not actually defeaters. So with her wealth she knows things she wouldn’t otherwise know, even before her money goes to work. Again, her money doesn’t generate knowledge – the original reliable report is the knowledge generator – but her wealth means that a would-be defeater doesn’t gain traction.\nThe same thing is true in Russell and Doris’s examples. The agent has quite a bit of evidence that \\(p\\). That’s why she knows \\(p\\). There’s a potential practical defeater for \\(p\\). But due to either indifference or wealth, the defeater is immunised. Surprisingly perhaps, indifference and/or wealth can be the difference between knowledge and ignorance. But that’s not because they can be in any interesting sense ‘knowledge makers’, any more than I can make a bowl of soup by preventing someone from tossing it out. Rather, they can be things that block defeaters, both when the defeaters are the kind Stanley talks about, and when they are more familiar kinds of defeaters.\n\n\n3 Temporal Embeddings\nMichael Blome-Tillmann (2009) has argued that tense-shifted knowledge ascriptions can be used to show that his version of Lewisian contextualism is preferable to IRI. Like Russell and Doris, his argument uses a variant of Stanley’s Bank Cases.10 Let \\(O\\) be that the bank is open Saturday morning. If Hannah has a large debt, she is in a high-stakes situation with respect to \\(O\\). In Blome-Tillmann’s version of the example, Hannah had in fact incurred a large debt, but on Friday morning the creditor waived this debt. Hannah had no way of anticipating this on Thursday. She has some evidence for \\(O\\), but not enough for knowledge if she’s in a high-stakes situation. Blome-Tillmann says that this means after Hannah discovers the debt waiver, she could say\n10 In the interests of space, I won’t repeat those cases yet again here.\nI didn’t know \\(O\\) on Thursday, but on Friday I did.\n\nBut I’m not sure why this case should be problematic for any version of IRI, and very unsure why it should even look like a reductio of IRI. As Blome-Tillmann notes, it isn’t really a situation where Hannah’s stakes change. She was never actually in a high stakes situation. At most her perception of her stakes change; she thought she was in a high-stakes situation, then realised that she wasn’t. Blome-Tillmann argues that even this change in perceived stakes can be enough to make (1) true if IRI is true. Now actually I agree that this change in perception could be enough to make (1) true, but when we work through the reason that’s so, we’ll see that it isn’t because of anything distinctive, let alone controversial, about IRI.\nIf Hannah is rational, then given her interests she won’t be ignoring \\(\\neg O\\) possibilities on Thursday. She’ll be taking them into account in her plans. Someone who is anticipating \\(\\neg O\\) possibilities, and making plans for them, doesn’t know \\(O\\). That’s not a distinctive claim of IRI. Any theory should say that if a person is worrying about \\(\\neg O\\) possibilities, and planning around them, they don’t know \\(O\\). And that’s simply because knowledge requires a level of confidence that such a person simply does not show. If Hannah is rational, that will describe her on Thursday, but not on Friday. So (1) is true not because Hannah’s practical situation changes between Thursday and Friday, but because her psychological state changes, and psychological states are relevant to knowledge.\nWhat if Hannah is, on Thursday, irrationally ignoring \\(\\neg O\\) possibilities, and not planning for them even though her rational self wishes she were planning for them? In that case, it seems she still believes \\(O\\). After all, she makes the same decisions as she would as if \\(O\\) were sure to be true. But it’s worth remembering that if Hannah does irrationally ignore \\(\\neg O\\) possibilities, she is being irrational with respect to \\(O\\). And it’s very plausible that this irrationality defeats knowledge. That is, you can’t be irrational with respect to a proposition and know it. Irrationality excludes knowledge. In any case, I doubt this is the natural way to read Blome-Tillmann’s example. We naturally read Hannah as being rational, and if she is rational she won’t have the right kind of confidence to count as knowing \\(O\\) on Thursday.\nThere’s a methodological point here worth stressing. Doing epistemology with imperfect agents often results in facing tough choices, where any way to describe a case feels a little counterintuitive. If we simply hew to intuitions, we risk being led astray by just focussing on the first way a puzzle case is described to us. But once we think through Hannah’s case, we see perfectly good reasons, independent of IRI, to endorse IRI’s prediction about the case.\n\n\n4 Problematic Conjunctions\nBlome-Tillmann offers another argument against IRI, that makes heavy use of the notion of having enough evidence to know something. Here is how he puts the argument. (Again I’ve changed the numbering and some terminology for consistency with this paper.)\n\nSuppose that John and Paul have exactly the same evidence, while John is in a low-stakes situation towards \\(p\\) and Paul in a high-stakes situation towards \\(p\\). Bearing in mind that IRI is the view that whether one knows \\(p\\) depends on one’s practical situation, IRI entails that one can truly assert:\n\nJohn and Paul have exactly the same evidence for \\(p\\), but only John has enough evidence to know \\(p\\), Paul doesn’t.\n\n(Blome-Tillmann 2009, 328–29)\n\nAnd this is meant to be a problem, because (2) is intuitively false.\nBut IRI doesn’t entail any such thing. We can see this by looking at a simpler example that illustrates the way ‘enough’ works.\nGeorge and Ringo both have $6000 in their bank accounts. They both are thinking about buying a new computer, which would cost $2000. Both of them also have rent due tomorrow, and they won’t get any more money before then. George lives in New York, so his rent is $5000. Ringo lives in Syracuse, so his rent is $1000. Clearly, (REC) and (RAC) are true.\n\nREC\n\nRingo has enough money to buy the computer.\n\nRAC\n\nRingo can afford the computer.\n\n\nAnd (GEC) is true as well, though there’s at least a reading of (GAC) where it is false.\n\nGEC\n\nGeorge has enough money to buy the computer.\n\nGAC\n\nGeorge can afford the computer.\n\n\nFocus for now on (GEC). It is a bad idea for George to buy the computer; he won’t be able to pay his rent. But he has enough money to do so; the computer costs $2000, and he has $6000 in the bank. So (GEC) is true. Admittedly there are things close to (GEC) that aren’t true. He hasn’t got enough money to buy the computer and pay his rent. You might say that he hasn’t got enough money to buy the computer given his other financial obligations. But none of this undermines (GEC).\nNow just like George has enough money to buy the computer, Paul has enough evidence to know that \\(p\\). Paul can’t know that \\(p\\), just like George can’t buy the computer, because of his practical situation. But that doesn’t mean he doesn’t have enough evidence to know it. He clearly does have enough evidence, since he has the same evidence John has, and John knows that \\(p\\). So, contra Blome-Tillmann, IRI doesn’t entail this problematic conjunction.\nIn a footnote attached to this, Blome-Tillmann offers a reformulation of the argument.\n\nI take it that having enough evidence to ‘know \\(p\\)’ in \\(C\\) just means having evidence such that one is in a position to ‘know \\(p\\)’ in \\(C\\), rather than having evidence such that one ‘knows \\(p\\)’. Thus, another way to formulate (2) would be as follows: ‘John and Paul have exactly the same evidence for \\(p\\), but only John is in a position to know \\(p\\), Paul isn’t.’ (Blome-Tillmann 2009, 329n23)\n\nNow having enough evidence to know \\(p\\) isn’t the same as being in a position to know it, any more than having enough money to buy the computer puts George in a position to buy it. So I think this is more of a new objection than a reformulation of the previous point. But might it be a stronger objection? Might it be that IRI entails (PosK), which is false?\n\nPosK\n\nJohn and Paul have exactly the same evidence for \\(p\\), but only John is in a position to know \\(p\\), Paul isn’t.\n\n\nActually, it isn’t a problem that IRI says that (PosK) is true. In fact, almost any epistemological theory will imply that conjunctions like that are true. In particular, any epistemological theory that allows for the existence of defeaters which do not supervene on the possession of evidence will imply that conjunctions like (PosK) are true. For example, anyone who thinks that whether you can know that a barn-like structure is really a barn depends on whether there are non-barns in the neighbourhood that look like the structure you’re looking at will think that conjunctions like (PosK) are true. Again, it matters a lot that IRI is suggesting that traditional epistemologists did not notice that there are distinctively pragmatic defeaters. Once we see that, we’ll see that conjunctions like (PosK) are not surprising at all.\nConsider again Con, and his friend Mod who is disposed to reason by modus ponens and not by affirming the consequent. We could say that Con and Mod have the same evidence for \\(p\\), but only Mod is in a position to know \\(p\\). There are only two ways to deny that conjunction. One is to interpret ‘position to know’ so broadly that Con is in a position to know \\(p\\) because he could change his inferential dispositions. But then we might as well say that Paul is in a position to know \\(p\\) because he could get into a different ‘stakes’ situation. Alternatively, we could say that Con’s inferential dispositions count as a kind of evidence against \\(p\\). But that stretches the notion of evidence beyond a breaking point. Note that we didn’t say Con had any reason to affirm the consequent, just that he does. Someone might adopt, or change, a poor inferential habit because they get new evidence. But they need not do so, and we shouldn’t count their inferential habits as evidence they have.\nIf that case is not convincing, we can make the same point with a simple Gettier-style case.\n\nGetting the Job\nIn world 1, at a particular workplace, someone is about to be promoted. Agnetha knows that Benny is the management’s favourite choice for the promotion. And she also knows that Benny is Swedish. So she comes to believe that the promotion will go to someone Swedish. Unsurprisingly, management does choose Benny, so Agnetha’s belief is true.\nWorld 2 is similar, except there it is Anni-Frid who knows that Benny is the management’s favourite choice for the promotion, that Benny is Swedish. So she comes to believe that the promotion will go to someone Swedish. But in this world Benny quits the workplace just before the promotion is announced, and the management unexpectedly passes over a lot of Danish workers to promote another Swede, namely Björn. So Anni-Frid’s belief that the promotion will go to someone Swedish is true, but not in a way that she could have expected.\n\nIn that story, I think it is clear that Agnetha and Anni-Frid have exactly the same evidence that the job will go to someone Swedish, but only Agnetha is in a position to know this, Anni-Frid is not. The fact that an intermediate step is false in Anni-Frid’s reasoning, but not Agnetha’s, means that Anni-Frid’s putative knowledge is defeated, but Agnetha’s is not. And when that happens, we can have differences in knowledge without differences in evidence. So it isn’t an argument against IRI that it allows differences in knowledge without differences in evidence.\n\n\n5 Holism and Defeaters\nThe big lesson of the last few sections is that interests create defeaters. Sometimes an agent can’t know \\(p\\) because adding \\(p\\) to her stock of beliefs would introduce either incoherence or irrationality. The reason is normally that the agent faces some decision where it is, say, bad to do \\(\\varphi\\), but good to do \\(\\varphi\\) given \\(p\\). In that situation, if she adds \\(p\\), she’ll either incoherently think that it’s bad to do \\(\\varphi\\) although it’s good to do it given what is (by her lights) true. Moreover, the IRI theorist says, being incoherent in this way blocks knowledge, so the agent doesn’t know \\(p\\).\nBut there are other, more roundabout, ways in which interests can mean that believing \\(p\\) would entail incoherence. One of these is illustrated by an example alleged by Ram Neta to be hard for interest-relative theorists to accommodate.\n\nKate needs to get to Main Street by noon: her life depends upon it. She is desperately searching for Main Street when she comes to an intersection and looks up at the perpendicular street signs at that intersection. One street sign says “State Street” and the perpendicular street sign says “Main Street.” Now, it is a matter of complete indifference to Kate whether she is on State Street–nothing whatsoever depends upon it. (Neta 2007, 182)\n\nLet’s assume for now that Kate is rational; dropping this assumption introduces mostly irrelevant complications. That is, we will assume Kate is an expected utility maximiser. Kate will not believe she’s on Main Street. She would only have that belief if she took it to be settled that she’s on Main, and hence not worthy of spending further effort investigating. But presumably she won’t do that. The rational thing for her to do is to get confirming (or, if relevant, confounding) evidence for the appearance that she’s on Main. If it were settled that she was on Main, the rational thing to do would be to try to relax, and be grateful that she had found Main Street. Since she has different attitudes about what to do simpliciter and conditional on being on Main Street, she doesn’t believe she’s on Main Street.\nSo far so good, but what about her attitude towards the proposition that she’s on State Street? She has enough evidence for that proposition that her credence in it should be rather high. And no practical issues turn on whether she is on State. So she believes she is on State, right?\nNot so fast! Believing that she’s on State has more connections to her cognitive system than just producing actions. Note in particular that street signs are hardly basic epistemic sources. They are the kind of evidence we should be ‘conservative’ about in the sense of Pryor (2004). We should only use them if we antecedently believe they are correct. So for Kate to believe she’s on State, she’d have to believe the street signs she can see are correct. If not, she’d incoherently be relying on a source she doesn’t trust, even though it is not a basic source.11 But if she believes the street signs are correct, she’d believe she was on Main, and that would lead to practical incoherence. So there’s no way to coherently add the belief that she’s on State Street to her stock of beliefs. So she doesn’t know, and can’t know, that she’s either on State or on Main. This is, in a roundabout way, due to the high stakes Kate faces.\n11 The caveats here about basic sources are to cancel any suggestion that Kate has to antecedently believe that any source is reliable before she uses it. As Pryor (2000) notes, that view is problematic. The view that we only get knowledge from a street sign if we antecedently have reason to trust it is not so implausible.Neta thinks that the best way for the interest-relative theorist to handle this case is to say that the high stakes associated with the proposition that Kate is on Main Street imply that certain methods of belief formation do not produce knowledge. And he argues, plausibly, that such a restriction will lead to implausibly sceptical results. But that’s not the only way for the interest-relative theorist to go. What they could, and I think should, say is that Kate can’t know she’s on State Street because the only grounds for that belief are intimately connected to a proposition that, in virtue of her interests, she needs very large amounts of evidence to believe.\n\n\n6 Non-Consequentialist Cases\nNone of the replies yet have leaned heavily on the last of the three points from the introduction, the fact that IRI is an existential claim. This reply will make heavy use of that fact.\nIf an agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. But when agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is not so clear that orthodox decision theory is the appropriate way to model the agents. That’s relevant to cases like this one, which Jessica Brown has argued are problematic for the epistemological theories John Hawthorne and Jason Stanley have recently been defending.12\n12 The target here is not directly the interest-relativity of their theories, but more general principles about the role of knowledge in action and assertion. But it’s important to see how IRI handles the cases that Brown discusses, since these cases are among the strongest challenges that have been raised to IRI.\nA student is spending the day shadowing a surgeon. In the morning he observes her in clinic examining patient A who has a diseased left kidney. The decision is taken to remove it that afternoon. Later, the student observes the surgeon in theatre where patient A is lying anaesthetised on the operating table. The operation hasn’t started as the surgeon is consulting the patient’s notes. The student is puzzled and asks one of the nurses what’s going on:\nStudent: I don’t understand. Why is she looking at the patient’s records? She was in clinic with the patient this morning. Doesn’t she even know which kidney it is?\nNurse: Of course, she knows which kidney it is. But, imagine what it would be like if she removed the wrong kidney. She shouldn’t operate before checking the patient’s records. (Brown 2008, 1144–45)\n\nIt is tempting, but I think mistaken, to represent the payoff table associated with the surgeon’s choice as follows. Let Left mean the left kidney is diseased, and Right mean the right kidney is diseased.\n\n\n\n\n\nLeft\nRight\n\n\nRemove left kidney\n\\(1\\)\n\\(-1\\)\n\n\nRemove right kidney\n\\(-1\\)\n\\(1\\)\n\n\nCheck notes\n\\(1-\\varepsilon\\)\n\\(1-\\varepsilon\\)\n\n\n\n\nHere \\(\\varepsilon\\) is the trivial but non-zero cost of checking the chart. Given this table, we might reason that since the surgeon knows that she’s in the left column, and removing the left kidney is the best option in that column, she should remove the left kidney rather than checking the notes.\nBut that reasoning assumes that the surgeon does not have any obligations over and above her duty to maximise expected utility. And that’s very implausible, since consequentialism is a fairly implausible theory of medical ethics.13\n13 I’m not saying that consequentialism is wrong as a theory of medical ethics. But if it is right, so many intuitions about medical ethics are going to be mistaken that such intuitions have no evidential force. And Brown’s argument relies on intuitions about this case having evidential value. So I think for her argument to work, we have to suppose non-consequentialism about medical ethics.It’s not clear exactly what obligation the surgeon has. Perhaps it is an obligation to not just know which kidney to remove, but to know this on the basis of evidence she has obtained while in the operating theatre. Or perhaps it is an obligation to make her belief about which kidney to remove as sensitive as possible to various possible scenarios. Before she checked the chart, this counterfactual was false: Had she misremembered which kidney was to be removed, she would have a true belief about which kidney was to be removed. Checking the chart makes that counterfactual true, and so makes her belief that the left kidney is to be removed a little more sensitive to counterfactual possibilities.\nHowever we spell out the obligation, it is plausible given what the nurse says that the surgeon has some such obligation. And it is plausible that the ‘cost’ of violating this obligation, call it \\(\\delta\\), is greater than the cost of checking the notes. So here is the decision table the surgeon faces.\n\n\n\n\n\nLeft\nRight\n\n\nRemove left kidney\n\\(1-\\delta\\)\n\\(-1-\\delta\\)\n\n\nRemove right kidney\n\\(-1-\\delta\\)\n\\(1-\\delta\\)\n\n\nCheck notes\n\\(1-\\varepsilon\\)\n\\(1-\\varepsilon\\)\n\n\n\n\nAnd it isn’t surprising, or a problem for an interest-relative theory of knowledge, that the surgeon should check the notes, even if she believes and knows that the left kidney is the diseased one. This is not to say that the surgeon does know that the left kidney is diseased, just that the version of IRI being defended here is neutral on that question.\nThere is a very general point here. It suffices to derive IRI that we defend principles like the following:\n\nWhenever maximising expected value is called for, one should maximise expected value conditional on everything one knows.\nMaximising expected value is called for often enough that there exist the kinds of pairs of cases IRI claims exist. That’s because in some cases, changing the options facing an agent will make it the case that which live option is best differs from which live option is best given \\(p\\), even though the agent antecedently knew \\(p\\).\n\nBut that doesn’t imply that maximising expected value is always called for. Especially in a medical case, it is hard to square an injunction like “Do No Harm!” with a view that one should maximise expected value, since maximising expected value requires treating harms and benefits symmetrically. What would be a problem for the version of IRI defended here was a case with the following four characteristics.\n\nMaximising expected value is called for in the case.\nConditional on \\(p\\), the action with the highest expected value is \\(\\varphi\\).\nIt would be wrong to do \\(\\varphi\\).\nThe agent knows \\(p\\).\n\nIt is tempting for the proponent of IRI to resist any attempted counterexample by claiming it is not really a case of knowledge. That might be the right thing to say in Brown’s case. But IRI defenders should remember that it is often a good move to deny that the first condition holds. Consequentialism is not an obviously correct theory of decision making in morally fraught situations; purported counterexamples that rely on it can therefore be resisted.\n\n\n\n\n\n\nReferences\n\nBlome-Tillmann, Michael. 2009. “Contextualism, Subject-Sensitive Invariantism, and the Interaction of ‘Knowledge’-Ascriptions with Modal and Temporal Operators.” Philosophy and Phenomenological Research 79 (2): 315–31. https://doi.org/10.1111/j.1933-1592.2009.00280.x.\n\n\nBrown, Jessica. 2008. “Knowledge and Practical Reason.” Philosophy Compass 3 (6): 1135–52. https://doi.org/10.1111/j.1747-9991.2008.00176.x.\n\n\nFantl, Jeremy, and Matthew McGrath. 2002. “Evidence, Pragmatics, and Justification.” Philosophical Review 111: 67–94. https://doi.org/10.2307/3182570.\n\n\n———. 2009. Knowledge in an Uncertain World. Oxford: Oxford University Press.\n\n\nFeltz, Adam, and Chris Zarpentine. 2010. “Do You Know More When It Matters Less?” Philosophical Psychology 23 (5): 683–706. https://doi.org/10.1080/09515089.2010.514572.\n\n\nHam, Sandra A., Sarah Martin, and Harold W. Kohl III. 2008. “Changes in the Percentage of Students Who Walk or Bike to School-United States, 1969 and 2001.” Journal of Physical Activity and Health 5 (2): 205–15. https://doi.org/10.1123/jpah.5.2.205.\n\n\nHarman, Gilbert. 1973. Thought. Princeton: Princeton University Press.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHawthorne, John, and Jason Stanley. 2008. “Knowledge and Action.” Journal of Philosophy 105 (10): 571–90. https://doi.org/10.5840/jphil20081051022.\n\n\nMay, Joshua, Walter Sinnott-Armstrong, Jay G. Hull, and Aaron Zimmerman. 2010. “Practical Interests, Relevant Alternatives, and Knowledge Attributions: An Empirical Study.” Review of Philosophy and Psychology 1 (2): 265–73. https://doi.org/10.1007/s13164-009-0014-3.\n\n\nNeta, Ram. 2007. “Anti-Intellectualism and the Knowledge-Action Principle.” Philosophy and Phenomenological Research 75 (1): 180–87. https://doi.org/10.1111/j.1933-1592.2007.00069.x.\n\n\nPryor, James. 2000. “The Skeptic and the Dogmatist.” Noûs 34 (4): 517–49. https://doi.org/10.1111/0029-4624.00277.\n\n\n———. 2004. “What’s Wrong with Moore’s Argument?” Philosophical Issues 14 (1): 349–78. https://doi.org/10.1111/j.1533-6077.2004.00034.x.\n\n\nRussell, Gillian, and John M. Doris. 2009. “Knowledge by Indifference.” Australasian Journal of Philosophy 86 (3): 429–37. https://doi.org/10.1080/00048400802001996.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\nCitationBibTeX citation:@online{weatherson2003,\n  author = {Weatherson, Brian},\n  title = {What {Good} Are {Counterexamples?}},\n  volume = {115},\n  number = {1},\n  pages = {1-31},\n  date = {2003-07},\n  doi = {10.5840/logos-episteme2011248},\n  langid = {en}\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]
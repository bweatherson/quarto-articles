<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.479">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">
<meta name="dcterms.date" content="2005-12-13">
<meta name="description" content="I argue that interests primarily affect the relationship between credence and belief. A view is set out and defended where evidence and rational credence are not interest-relative, but belief, rational belief, and knowledge are.">

<title>Online Articles - Brian Weatherson - Can We Do Without Pragmatic Encroachment?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://use.typekit.net/uzz2drx.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Online Articles - Brian Weatherson</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://brian.weatherson.org"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/bweatherson.bsky.social"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Can We Do Without Pragmatic Encroachment?</h1>
                  <div>
        <div class="description">
          <p>I argue that interests primarily affect the relationship between credence and belief. A view is set out and defended where evidence and rational credence are not interest-relative, but belief, rational belief, and knowledge are.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">epistemology</div>
                <div class="quarto-category">interest-relativity</div>
                <div class="quarto-category">philosophy of mind</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="http://brian.weatherson.org">Brian Weatherson</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Michigan
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 13, 2005</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.1111/j.1520-8583.2005.00068.x">10.1111/j.1520-8583.2005.00068.x</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">0.1</span> Introduction</a></li>
  <li><a href="#belief-and-degree-of-belief" id="toc-belief-and-degree-of-belief" class="nav-link" data-scroll-target="#belief-and-degree-of-belief"><span class="header-section-number">0.2</span> Belief and Degree of Belief</a></li>
  <li><a href="#impractical-propositions" id="toc-impractical-propositions" class="nav-link" data-scroll-target="#impractical-propositions"><span class="header-section-number">0.3</span> Impractical Propositions</a></li>
  <li><a href="#defending-closure" id="toc-defending-closure" class="nav-link" data-scroll-target="#defending-closure"><span class="header-section-number">0.4</span> Defending Closure</a></li>
  <li><a href="#too-little-closure" id="toc-too-little-closure" class="nav-link" data-scroll-target="#too-little-closure"><span class="header-section-number">0.5</span> Too Little Closure?</a></li>
  <li><a href="#examples-of-pragmatic-encroachment" id="toc-examples-of-pragmatic-encroachment" class="nav-link" data-scroll-target="#examples-of-pragmatic-encroachment"><span class="header-section-number">0.6</span> Examples of Pragmatic Encroachment</a></li>
  <li><a href="#justification-and-practical-reasoning" id="toc-justification-and-practical-reasoning" class="nav-link" data-scroll-target="#justification-and-practical-reasoning"><span class="header-section-number">0.7</span> Justification and Practical Reasoning</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">0.8</span> Conclusions</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="can-we-do-without-pragmatic-encroachment.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<section id="introduction" class="level3 page-columns page-full" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">0.1</span> Introduction</h3>
<p>Recently several authors have defended claims suggesting that there is a closer connection between practical interests and epistemic justification than has traditionally been countenanced. Jeremy Fantl and Matthew McGrath <span class="citation" data-cites="Fantl2002">(<a href="#ref-Fantl2002" role="doc-biblioref">2002</a>)</span> argue that there is a “pragmatic necessary condition on epistemic justification” (77), namely the following.</p>
<aside>
Published in <em>Philosophical Perspectives</em> 19: 417-43.
</aside>
<dl>
<dt>(PC)</dt>
<dd>
<p><em>S</em> is justified in believing that <em>p</em> only if <em>S</em> is rational to prefer as if <em>p</em>. (77)</p>
</dd>
</dl>
<p>And John <span class="citation" data-cites="Hawthorne2004">Hawthorne (<a href="#ref-Hawthorne2004" role="doc-biblioref">2004</a>)</span> and Jason <span class="citation" data-cites="Stanley2005-STAKAP">Stanley (<a href="#ref-Stanley2005-STAKAP" role="doc-biblioref">2005</a>)</span> have argued that what it takes to turn true belief into knowledge is sensitive to the practical environment the subject is in. These authors seem to be suggesting there is, to use Jonathan Kvanvig’s phrase “pragmatic encroachment” in epistemology. In this paper I’ll argue that their arguments do not quite show this is true, and that concepts of epistemological justification need not be pragmatically sensitive. The aim here isn’t to show that (PC) is false, but rather that it shouldn’t be described as a pragmatic condition on <em>justification</em>. Rather, it is best thought of as a pragmatic condition on <em>belief</em>. There are two ways to spell out the view I’m taking here. These are both massive simplifications, but they are close enough to the truth to show the kind of picture I’m aiming for.</p>
<aside>
Thanks to Michael Almeida, Tamar Szabó Gendler, Peter Gerdes, Jon Kvanvig, Barry Lam, Ishani Maitra, Robert Stalnaker, Jason Stanley, Matthew Weiner for helpful discussions, and especially to Matthew McGrath for correcting many mistakes in an earlier draft of this paper.
</aside>
<p>First, imagine a philosopher who holds a very simplified version of functionalism about belief, call it (B).</p>
<dl>
<dt>(B)</dt>
<dd>
<p><em>S</em> believes that <em>p</em> iff <em>S</em> prefers as if <em>p</em></p>
</dd>
</dl>
<p>Our philosopher one day starts thinking about justification, and decides that we can get a principle out of (B) by adding normative operators to both sides, inferring (JB).</p>
<dl>
<dt>(JB)</dt>
<dd>
<p><em>S</em> is justified in believing that <em>p</em> only if <em>S</em> is justified to prefer as if <em>p</em></p>
</dd>
</dl>
<p>Now it would be a mistake to treat (JB) as a pragmatic condition on <em>justification</em> (rather than belief) if it was derived from (B) by this simple means. And if our philosopher goes on to infer (PC) from (JB), by replacing ‘justified’ with ‘rational’, and inferring the conditional from the biconditional, we still don’t get a pragmatic condition on <em>justification</em>.</p>
<p>Second, Fantl and McGrath focus their efforts on attacking the following principle.</p>
<dl>
<dt>Evidentialism</dt>
<dd>
<p>For any two subjects <em>S</em> and S<span class="math inline">\(^\prime\)</span>, necessarily, if <em>S</em> and S<span class="math inline">\(^\prime\)</span> have the same evidence for/against <em>p</em>, then <em>S</em> is justified in believing that <em>p</em> iff S<span class="math inline">\(^\prime\)</span> is, too.</p>
</dd>
</dl>
<p>I agree, evidentialism is false. And I agree that there are counterexamples to evidentialism from subjects who are in different practical situations. What I don’t agree is that we learn much about the role of pragmatic factors in <em>epistemology</em> properly defined from these counterexamples to evidentialism. Evidentialism follows from the following three principles.</p>
<dl>
<dt>Probabilistic Evidentialism</dt>
<dd>
<p>For any two subjects <em>S</em> and S<span class="math inline">\(^\prime\)</span>, and any degree of belief <span class="math inline">\(\alpha\)</span> necessarily, if <em>S</em> and S<span class="math inline">\(^\prime\)</span> have the same evidence for/against <em>p</em>, then <em>S</em> is justified in believing that <em>p</em> to degree <span class="math inline">\(\alpha\)</span> iff S<span class="math inline">\(^\prime\)</span> is, too.</p>
</dd>
<dt>Threshold View</dt>
<dd>
<p>For any two subjects <em>S</em> and S<span class="math inline">\(^\prime\)</span>, and any degree of belief <span class="math inline">\(\alpha\)</span>, if <em>S</em> and S<span class="math inline">\(^\prime\)</span> both believe <em>p</em> to degree <span class="math inline">\(\alpha\)</span>, then <em>S</em> believes that <em>p</em> iff S<span class="math inline">\(^\prime\)</span> does too.</p>
</dd>
<dt>Probabilistic Justification</dt>
<dd>
<p>For any <span class="math inline">\(S, S\)</span> is justified in believing <em>p</em> iff there is some degree of belief <span class="math inline">\(\alpha\)</span> such that <em>S</em> is justified in believing <em>p</em> to degree <span class="math inline">\(\alpha\)</span>, and in S’s situation, believing <em>p</em> to degree <span class="math inline">\(\alpha\)</span> suffices for believing <em>p</em>.</p>
</dd>
</dl>
<p>(Degrees of belief here are meant to be the subjective correlates of Keynesian probabilities. See <span class="citation" data-cites="Keynes1921">Keynes (<a href="#ref-Keynes1921" role="doc-biblioref">1921</a>)</span> for more details. They need not, and usually will not, be numerical values. The Threshold View is so-called because given some other plausible premises it implies that <span class="math inline">\(S\)</span> believes that <em>p</em> iff S’s degree of belief in <em>p</em> is above a threshold.)</p>
<p>I endorse Probabilistic Justification, and for present purposes at least I endorse Probabilistic Evidentialism. The reason I think Evidentialism fails is because the Threshold View is false. It is plausible that Probabilistic Justification and Probabilistic Evidentialism are epistemological principles, while the Threshold View is a principle from philosophy of mind. So this matches up with the earlier contention that the failure of Evidentialism tells us something interesting about the role of pragmatics in philosophy of mind, rather than something about the role of pragmatics in epistemology.</p>
<p>As noted, Hawthorne and Stanley are both more interested in knowledge than justification. So my discussion of their views will inevitably be somewhat distorting. I think what I say about justification here should carry over to a theory of knowledge, but space prevents a serious examination of that question. The primary bit of ‘translation’ I have to do to make their works relevant to a discussion of justification is to interpret their defences of the principle (KP) below as implying some support for (JP), which is obviously similar to (PC).</p>
<dl>
<dt>(KP)</dt>
<dd>
<p>If <em>S</em> knows that <em>p</em>, then <em>S</em> is justified in using <em>p</em> as a premise in practical reasoning.</p>
</dd>
<dt>(JP)</dt>
<dd>
<p>If <em>S</em> justifiably believes that <em>p</em>, then <em>S</em> is justified in using <em>p</em> as a premise in practical reasoning.</p>
</dd>
</dl>
<p>I think (JP) is just as plausible as (KP). In any case it is independently plausible whether or not Hawthorne and Stanley are committed to it. So I’ll credit recognition of (JP)’s importance to a theory of justification to them, and hope that in doing so I’m not irrepairably damaging the public record.</p>
<p>The overall plan here is to use some philosophy of mind, specifically functionalist analyses of belief to respond to some arguments in epistemology. But, as you can see from the role the Threshold View plays in the above argument, our starting point will be the question what is the relation between the credences decision theory deals with, and our traditional notion of a belief? I’ll offer an analysis of this relation that supports my above claim that we should work with a pragmatic notion of belief rather than a pragmatic notion of justification. The analysis I offer has a hole in it concerning propositions that are not relevant to our current plans, and I’ll fix the hold in section 3. Sections 4 and 5 concern the role that closure principles play in my theory, in particular the relationship between having probabilistically coherent degrees of belief and logically coherent beliefs. In this context, a closure principle is a principle that says probabilistic coherence implies logical coherence, at least in a certain domain. (It’s called a closure principle because we usually discuss it by working out properties of probabilistically coherent agents, and show that their beliefs are closed under entailment in the relevant domain.) In section 4 I’ll defend the theory against the objection, most commonly heard from those wielding the preface paradox, that we need not endorse as strong a closure principle as I do. In section 5 I’ll defend the theory against those who would endorse an even stronger closure principle than is defended here. Once we’ve got a handle on the relationship between degrees of belief and belief <em>tout court</em>, we’ll use that to examine the arguments for pragmatic encroachment. In section 6 I’ll argue that we can explain the intuitions behind the cases that seem to support pragmatic encroachment, while actually keeping all of the pragmatic factors in our theory of belief. In section 7 I’ll discuss how to endorse principles like (PC) and (JP) (as far as they can be endorsed) while keeping a non-pragmatic theory of probabilistic justification. The interesting cases here are ones where agents have mistaken and/or irrational beliefs about their practical environment, and intuitions in those cases are cloudy. But it seems the most natural path in these cases is to keep a pragmatically sensitive notion of belief, and a pragmatically insensitive notion of justification.</p>
</section>
<section id="belief-and-degree-of-belief" class="level3 page-columns page-full" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="belief-and-degree-of-belief"><span class="header-section-number">0.2</span> Belief and Degree of Belief</h3>
<p>Traditional epistemology deals with beliefs and their justification. Bayesian epistemology deals with degrees of belief and their justification. In some sense they are both talking about the same thing, namely epistemic justification. Two questions naturally arise. Do we really have two subject matters here (degrees of belief and belief <em>tout court</em>) or two descriptions of the one subject matter? If just one subject matter, what relationship is there between the two modes of description of this subject matter?</p>
<p>The answer to the first question is I think rather easy. There is no evidence to believe that the mind contains two representational systems, one to represent things as being probable or improbable and the other to represent things as being true or false. The mind probably does contain a vast plurality of representational systems, but they don’t divide up the doxastic duties this way. If there are distinct visual and auditory representational systems, they don’t divide up duties between degrees of belief and belief <em>tout court</em>, for example. If there were two distinct systems, then we should imagine that they could vary independently, at least as much as is allowed by constitutive rationality. But such variation is hard to fathom. So I’ll infer that the one representational system accounts for our credences and our categorical beliefs. (It follows from this that the question <span class="citation" data-cites="Bovens1999">Bovens and Hawthorne (<a href="#ref-Bovens1999" role="doc-biblioref">1999</a>)</span> ask, namely what beliefs <em>should</em> an agent have given her degrees of belief, doesn’t have a non-trivial answer. If fixing the degrees of belief in an environment fixes all her doxastic attitudes, as I think it does, then there is no further question of what she should believe given these are her degrees of belief.)</p>
<p>The second question is much harder. It is tempting to say that <span class="math inline">\(S\)</span> believes that <em>p</em> iff S’s credence in <em>p</em> is greater than some salient number <span class="math inline">\(r\)</span>, where <span class="math inline">\(r\)</span> is made salient either by the context of belief ascription, or the context that <em>S</em> is in. I’m following Mark <span class="citation" data-cites="Kaplan1996">Kaplan (<a href="#ref-Kaplan1996" role="doc-biblioref">1996</a>)</span> in calling this the threshold view. There are two well-known problems with the threshold view, both of which seem fatal to me.</p>
<p>As Robert Stalnaker <span class="citation" data-cites="Stalnaker1984">(<a href="#ref-Stalnaker1984" role="doc-biblioref">1984, 91</a>)</span> emphasised, any number <span class="math inline">\(r\)</span> is bound to seem arbitrary. Unless these numbers are made salient by the environment, there is no special difference between believing <em>p</em> to degree 0.9786 and believing it to degree 0.9875. But if <span class="math inline">\(r\)</span> is 0.98755, this will be <em>the difference</em> between believing <em>p</em> and not believing it, which is an important difference. The usual response to this, as found in <span class="citation" data-cites="Foley1993">(<a href="#ref-Foley1993" role="doc-biblioref">Foley 1993</a> Ch. 4)</span> and <span class="citation" data-cites="Hunter1996">Hunter (<a href="#ref-Hunter1996" role="doc-biblioref">1996</a>)</span> is to say that the boundary is vague. But it’s not clear how this helps. On an epistemic theory of vagueness, there is still a number such that degrees of belief above that count, and degrees below that do not, and any such number is bound to seem unimportant. On supervaluational theories, the same is true. There won’t be a <em>determinate</em> number, to be sure, but there will a number, and that seems false. My preferred degree of belief theory of vagueness, as set out in <span class="citation" data-cites="Weatherson2005-WEATTT">Weatherson (<a href="#ref-Weatherson2005-WEATTT" role="doc-biblioref">2005</a>)</span> has the same consequence. Hunter defends a version of the threshold view combined with a theory of vagueness based around fuzzy logic, which seems to be the only theory that could avoid the arbitrariness objection. But as <span class="citation" data-cites="Williamson1994-WILV">Williamson (<a href="#ref-Williamson1994-WILV" role="doc-biblioref">1994</a>)</span> showed, there are deep and probably insurmountable difficulties with that position. So I think the vagueness response to the arbitrariness objection is (a) the only prima facie plausible response and (b) unsuccessful.</p>
<p>The second problem concerns conjunction. It is also set out clearly by Stalnaker.</p>
<blockquote class="blockquote">
<p>Reasoning in this way from accepted premises to their deductive consequences (<span class="math inline">\(P\)</span>, also <span class="math inline">\(Q\)</span>, therefore <span class="math inline">\(R\)</span>) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. <span class="citation" data-cites="Stalnaker1984">(<a href="#ref-Stalnaker1984" role="doc-biblioref">Stalnaker 1984, 92</a>)</span></p>
</blockquote>
<p>If categorical belief is having a credence above the threshold, then one can coherently do exactly this. Let <span class="math inline">\(x\)</span> be a number between <span class="math inline">\(r\)</span> and than <span class="math inline">\(r\)</span> <sup><span class="math inline">\(\nicefrac{1}{2}\)</span></sup>, such that for an atom of type U has probability <span class="math inline">\(x\)</span> of decaying within a time <span class="math inline">\(t\)</span>, for some <span class="math inline">\(t\)</span> and U. Assume our agent knows this fact, and is faced with two (isolated) atoms of U. Let <em>p</em> be that the first decays within <span class="math inline">\(t\)</span>, and <span class="math inline">\(q\)</span> be that the second decays within <span class="math inline">\(t\)</span>. She should, given her evidence, believe <em>p</em> to degree <span class="math inline">\(x, q\)</span> to degree <span class="math inline">\(x\)</span>, and <span class="math inline">\(p \wedge q\)</span> to degree <span class="math inline">\(x ^2\)</span>. If she believed <span class="math inline">\(p \wedge q\)</span> to a degree greater than <span class="math inline">\(r\)</span>, she’d have to either have credences that were not supported by her evidence, or credences that were incoherent. (Or, most likely, both.) So this theory violates the platitude. This is a well-known argument, so there are many responses to it, most of them involving something like appeal to the preface paradox. I’ll argue in section 4 that the preface paradox doesn’t in fact offer the threshold view proponent much support here. But even before we get to there, we should note that the arbitrariness objection gives us sufficient reason to reject the threshold view.</p>
<p>A better move is to start with the functionalist idea that to believe that <em>p</em> is to treat <em>p</em> as true for the purposes of practical reasoning. To believe <em>p</em> is to have preferences that make sense, by your own lights, in a world where <em>p</em> is true. So, if you prefer A to B and believe that <em>p</em>, you prefer A to B given <em>p</em>. For reasons that will become apparent below, we’ll work in this paper with a notion of preference where <em>conditional</em> preferences are primary.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> So the core insight we’ll work with is the following:</p>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;To say the agent prefers A to B given <span class="math inline">\(q\)</span> is not to say that if the agent were to learn <span class="math inline">\(q\)</span>, she would prefer A to B. It’s rather to say that she prefers the state of the world where she does A and <span class="math inline">\(q\)</span> is true to the state of the world where she does B and <span class="math inline">\(q\)</span> is true. These two will come apart in cases where learning <span class="math inline">\(q\)</span> changes the agent’s preferences. We’ll return to this issue below.</p></li></div><blockquote class="blockquote">
<p>If you prefer A to B given <span class="math inline">\(q\)</span>, and you believe that <em>p</em>, then you prefer A to B given <span class="math inline">\(p \wedge q\)</span></p>
</blockquote>
<p>The bold suggestion here is that if that is true for all the A, B and <em>q</em> that matter, then you believe <em>p</em>. Put formally, where <em>Bel</em>(<em>p</em>) means that the agent believes that <em>p</em>, and A <span class="math inline">\(\geq _q\)</span> B means that the agent thinks A is at least as good as B given <span class="math inline">\(q\)</span>, we have the following</p>
<ol type="1">
<li><em>Bel</em>(<em>p</em>) <span class="math inline">\(\leftrightarrow \forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall q\)</span> (A <span class="math inline">\(\geq _q\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q}\)</span> B)</li>
</ol>
<p>In words, an agent believes that <em>p</em> iff conditionalising on <em>p</em> doesn’t change any conditional preferences over things that matter.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The left-to-right direction of this seems trivial, and the right-to-left direction seems to be a plausible way to operationalise the functionalist insight that belief is a functional state. There is some work to be done if (1) is to be interpreted as a truth though.</p>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;This might seem <em>much</em> too simple, especially when compared to all the bells and whistles that functionalists usually put in their theories to (further) distinguish themselves from crude versions of behaviourism. The reason we don’t need to include those complications here is that they will all be included in the analysis of <em>preference</em>. Indeed, the theory here is compatible with a thoroughly anti-functionalist treatment of preference. The claim is not that we can offer a functional analysis of belief in terms of non-mental concepts, just that we can offer a functionalist reduction of belief to other mental concepts. The threshold view is <em>also</em> such a reduction, but it is such a crude reduction that it doesn’t obviously fall into any category.</p></li></div><p>If we interpret the quantifiers in (1) as unrestricted, then we get the (false) conclusion that just about no one believes no contingent propositions. To prove this, consider a bet that wins iff the statue in front of me waves back at me due to random quantum effects when I wave at it. If I take the bet and win, I get to live forever in paradise. If I take the bet and lose, I lose a penny. Letting A be that I take the bet, B be that I decline the bet, <span class="math inline">\(q\)</span> be a known tautology (so my preferences given <span class="math inline">\(q\)</span> are my preferences <em>tout court</em>) and <em>p</em> be that the statue does not wave back, we have that I prefer A to B, but not A to B given <em>p</em>. So by this standard I don’t believe that <em>p</em>. This is false – right now I believe that statues won’t wave back at me when I wave at them.</p>
<p>This seems like a problem. But the solution to it is not to give up on functionalism, but to insist on its pragmatic foundations. The quantifiers in (1) should be restricted, with the restrictions motivated pragmatically. What is crucial to the theory is to say what the restrictions on A and B are, and what the restrictions on <span class="math inline">\(q\)</span> are. We’ll deal with these in order.</p>
<p>For better or worse, I don’t right now have the option taking that bet and hence spending eternity in paradise if the statue waves back at me. Taking or declining such unavailable bets are not open choices. For any option that is open to me, assuming that statues do not in fact wave does not change its utility. That’s to say, I’ve already factored in the non-waving behaviour of statues into my decision-making calculus. That’s to say, I believe statues don’t wave.</p>
<p>An action A is a live option for the agent if it is really possible for the agent to perform A. An action A is a salient option if it is an option the agent takes seriously in deliberation. Most of the time gambling large sums of money on internet gambling sites over my phone is a live option, but not a salient option. I know this option is suboptimal, and I don’t have to recompute every time whether I should do it. Whenever I’m making a decision, I don’t have to add in to the list of choices <em>bet thousands of dollars on internet gambling sites</em>, and then rerule that out every time. I just don’t consider that option, and properly so. If I have a propensity to daydream, then becoming the centrefielder for the Boston Red Sox might be a salient option to me, but it certainly isn’t a live option. We’ll say the two initial quantifiers range over the options that are live and salient options for the agent.</p>
<p>Note that we <em>don’t</em> say that the quantifiers range over the options that are live and salient for the person making the belief ascription. That would lead us to a form of contextualism for which we have little evidence. We also don’t say that an option becomes salient for the agent iff they <em>should</em> be considering it. At this stage we are just saying what the agent does believe, not what they should believe, so we don’t have any clauses involving normative concepts.</p>
<p>Now we’ll look at the restrictions on the quantifier over propositions. Say a proposition is <em>relevant</em> if the agent is disposed to take seriously the question of whether it is true (whether or not she is currently considering that question) and conditionalising on that proposition or its negation changes some of the agents <em>unconditional</em> preferences over live, salient options.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The first clause is designed to rule out wild hypotheses that the agent does not take at all seriously. If <span class="math inline">\(q\)</span> is not such a proposition, if the agent is disposed to take it seriously, then it is relevant if there are live, salient A and B such that A <span class="math inline">\(\geq _q\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq\)</span> B is false. Say a proposition is <em>salient</em> if the agent is currently considering whether it is true. Finally, say a proposition is <em>active</em> relative to <em>p</em> iff it is a (possibly degenerate) conjunction of propositions such that each conjunct is either relevant or salient, and such that the conjunction is consistent with <em>p</em>. (By a degenerate conjunction I mean a conjunction with just one conjunct. The consistency requirement is there because it might be hard in some cases to make sense of preferences given inconsistencies.) Then the propositional quantifier in (1) ranges over active propositions.</p>
<div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;Conditionalising on the proposition <em>There are space aliens about to come down and kill all the people writing epistemology papers</em> will make me prefer to stop writing this paper, and perhaps grab some old metaphysics papers I could be working on. So that proposition satisfies the second clause of the definition of relevance. But it clearly doesn’t satisfy the first clause. This part of the definition of relevance won’t do much work until the discussion of agents with mistaken environmental beliefs in section 7.</p></li></div><p>We will expand and clarify this in the next section, but our current solution to the relationship between beliefs and degrees of belief is that degrees of belief determine an agent’s preferences, and she believes that <em>p</em> iff the claim (1) about her preferences is true when the quantifiers over options are restricted to live, salient actions, and the quantifier over propositions is restricted to salient propositions. The simple view would be to say that the agent believes that <em>p</em> iff conditioning on <em>p</em> changes none of her preferences. The more complicated view here is that the agent believes that <em>p</em> iff conditioning on <em>p</em> changes none of her conditional preferences over live, salient options, where the conditions are also active relative to <em>p</em>.</p>
</section>
<section id="impractical-propositions" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="impractical-propositions"><span class="header-section-number">0.3</span> Impractical Propositions</h3>
<p>The theory sketched in the previous paragraph seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and as we’ll see it handles some otherwise difficult cases with aplomb. But it needs to be supplemented a little to handle beliefs about propositions that are practically irrelevant. I’ll illustrate the problem, then note how I prefer to solve it.</p>
<p>I don’t know what Julius Caeser had for breakfast the morning he crossed the Rubicon. But I think he would have had <em>some</em> breakfast. It is hard to be a good general without a good morning meal after all. Let <em>p</em> be the proposition that he had breakfast that morning. I believe <em>p</em>. But this makes remarkably little difference to my practical choices in most situations. True, I wouldn’t have written this paragraph as I did without this belief, but it is rare that I have to write about Caeser’s dietary habits. In general whether <em>p</em> is true makes no practical difference to me. This makes it hard to give a pragmatic account of whether I believe that <em>p</em>. Let’s apply (1) to see whether I really believe that <em>p</em>.</p>
<ol type="1">
<li><em>Bel</em>(<em>p</em>) <span class="math inline">\(\leftrightarrow \forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall q\)</span> (A <span class="math inline">\(\geq _q\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q}\)</span> B)</li>
</ol>
<p>Since <em>p</em> makes no practical difference to any choice I have to make, the right hand side is true. So the left hand side is true, as desired. The problem is that the right hand side of (2) is also true here.</p>
<ol start="2" type="1">
<li><em>Bel</em>(<span class="math inline">\(\neg p\)</span>) <span class="math inline">\(\leftrightarrow \forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall q\)</span> (A <span class="math inline">\(\geq _q\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{\neg p \wedge q}\)</span> B)</li>
</ol>
<p>Adding the assumption that Caeser had no breakfast that morning doesn’t change any of my practical choices either. So I now seem to <em>inconsistently</em> believe both <em>p</em> and <span class="math inline">\(\neg p\)</span>. I have some inconsistent beliefs, I’m sure, but those aren’t among them. We need to clarify what (1) claims.</p>
<p>To do so, I supplement the theory sketched in section 2 with the following principles.</p>
<ul>
<li><p>A proposition <em>p</em> is <em>eligible</em> <em>for belief</em> if it satisfies <span class="math inline">\(\forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall q\)</span> (A <span class="math inline">\(\geq _q\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q}\)</span> B), where the first two quantifiers range over the open, salient actions in the sense described in section 2.</p></li>
<li><p>For any proposition <em>p</em>, and any proposition <span class="math inline">\(q\)</span> that is relevant or salient, among the actions that are (by stipulation!) open and salient with respect to <em>p</em> are <em>believing that p</em>, <em>believing that q</em>, <em>not believing that p</em> and <em>not believing that q</em></p></li>
<li><p>For any proposition, the subject prefers believing it to not believing it iff (a) it is eligible for belief and (b) the agents degree of belief in the proposition is greater than <span class="math inline">\(\nicefrac{1}{2}\)</span>.</p></li>
<li><p>The previous stipulation holds both unconditionally and conditional on <em>p</em>, for any <em>p</em>.</p></li>
<li><p>The agent believes that <em>p</em> iff <span class="math inline">\(\forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall q\)</span> (A <span class="math inline">\(\geq _q\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q}\)</span> B), where the first two quantifiers range over all actions that are either open and salient <em>tout court</em> (i.e.&nbsp;in the sense of section 2) or open and salient with respect to <em>p</em> (as described above).</p></li>
</ul>
<p>This all looks moderately complicated, but I’ll explain how it works in some detail as we go along. One simple consequence is that an agent only believes that <em>p</em> iff their degree of belief in <em>p</em> is greater than <span class="math inline">\(\nicefrac{1}{2}\)</span>. Since my degree of belief in Caeser’s foodless morning is not greater than <span class="math inline">\(\nicefrac{1}{2}\)</span>, in fact it is considerably less, I don’t believe <span class="math inline">\(\neg p\)</span>. On the other hand, since my degree of belief in <em>p</em> is considerably greater than <span class="math inline">\(\nicefrac{1}{2}\)</span>, I prefer to believe it than disbelieve it, so I believe it.</p>
<p>There are many possible objections to this position, which I’ll address sequentially.</p>
<p><em>Objection</em>: Even if I have a high degree of belief in <em>p</em>, I might prefer to not believe <em>p</em> because I think that belief in <em>p</em> is bad for some other reason. Perhaps, if <em>p</em> is a proposition about my brilliance, it might be immodest to believe that <em>p</em>.</p>
<p><em>Reply</em>: Any of these kinds of considerations should be put into the credences. If it is immodest to believe that you are a great philosopher, it is equally immodest to believe to a high degree that you are a great philosopher.</p>
<p><em>Objection</em>: Belief that <em>p</em> is not an action in the ordinary sense of the term.</p>
<p><em>Reply</em>: True, which is why this is described as a supplement to the original theory, rather than just cashing out its consequences.</p>
<p><em>Objection</em>: It is impossible to choose to believe or not believe something, so we shouldn’t be applying these kinds of criteria.</p>
<p><em>Reply</em>: I’m not as convinced of the impossibility of belief by choice as others are, but I won’t push that for present purposes. Let’s grant that beliefs are always involuntary. So these ‘actions’ aren’t open actions in any interesting sense, and the theory is section 2 was really incomplete. As I said, this is a supplement to the theory in section 2.</p>
<p>This doesn’t prevent us using principles of constitutive rationality, such as we prefer to believe <em>p</em> iff our credence in <em>p</em> is over <span class="math inline">\(\nicefrac{1}{2}\)</span>. Indeed, on most occasions where we use constitutive rationality to infer that a person has some mental state, the mental state we attribute to them is one they could not fail to have. But functionalists are committed to constitutive rationality <span class="citation" data-cites="Lewis1994b">(<a href="#ref-Lewis1994b" role="doc-biblioref">Lewis 1994</a>)</span>. So my approach here is consistent with a broadly functionalist outlook.</p>
<p><em>Objection</em>: This just looks like a roundabout way of stipulating that to believe that <em>p</em>, your degree of belief in <em>p</em> has to be greater than <span class="math inline">\(\nicefrac{1}{2}\)</span>. Why not just add that as an extra clause than going through these little understood detours about preferences about beliefs?</p>
<p><em>Reply</em>: There are three reasons for doing things this way rather than adding such a clause.</p>
<p>First, it’s nice to have a systematic theory rather than a theory with an ad hoc clause like that.</p>
<p>Second, the effect of this constraint is much more than to restrict belief to propositions whose credence is greater than <span class="math inline">\(\nicefrac{1}{2}\)</span>. Consider a case where <em>p</em> and <span class="math inline">\(q\)</span> and their conjunction are all salient, <em>p</em> and <span class="math inline">\(q\)</span> are probabilistically independent, and the agent’s credence in each is 0.7. Assume also that <span class="math inline">\(p, q\)</span> and <span class="math inline">\(p \wedge q\)</span> are completely irrelevant to any practical deliberation the agent must make. Then the criteria above imply that the agent does not believe that <em>p</em> or that <span class="math inline">\(q\)</span>. The reason is that the agent’s credence in <span class="math inline">\(p \wedge q\)</span> is 0.49, so she prefers to not believe <span class="math inline">\(p \wedge q\)</span>. But conditional on <em>p</em>, her credence in <span class="math inline">\(p \wedge q\)</span> is 0.7, so she prefers to believe it. So conditionalising on <em>p</em> does change her preferences with respect to believing <span class="math inline">\(p \wedge q\)</span>, so she doesn’t believe <em>p</em>. So the effect of these stipulations rules out much more than just belief in propositions whose credence is below <span class="math inline">\(\nicefrac{1}{2}\)</span>.</p>
<p>This suggests the third, and most important point. The problem with the threshold view was that it led to violations of closure. Given the theory as stated, we can prove the following theorem. Whenever <em>p</em> and <span class="math inline">\(q\)</span> and their conjunction are all open or salient, and both are believed, and the agent is probabilistically coherent, the agent also believes <span class="math inline">\(p \wedge q\)</span>. This is a quite restricted closure principle, but this is no reason to deny that it is <em>true</em>, as it fails to be true on the threshold view.</p>
<p>The proof of this theorem is a little complicated, but worth working through. First we’ll prove that if the agent believes <em>p</em>, believes <span class="math inline">\(q\)</span>, and <em>p</em> and <span class="math inline">\(q\)</span> are both salient, then the agent prefers believing <span class="math inline">\(p \wedge q\)</span> to not believing it, if <span class="math inline">\(p \wedge q\)</span> is eligible for belief. In what follows <em>Pr</em>(<span class="math inline">\(x | y\)</span>) is the agent’s conditional degree of belief in <span class="math inline">\(x\)</span> given <span class="math inline">\(y\)</span>. Since the agent is coherent, we’ll assume this is a probability function (hence the name).</p>
<ol type="1">
<li><p>Since the agent believes that <span class="math inline">\(q\)</span>, they prefer believing that <span class="math inline">\(q\)</span> to not believing that <span class="math inline">\(q\)</span> (by the criteria for belief)</p></li>
<li><p>So the agent prefers believing that <span class="math inline">\(q\)</span> to not believing that <span class="math inline">\(q\)</span> given <em>p</em> (From 1 and the fact that they believe that <em>p</em>, and that <span class="math inline">\(q\)</span> is salient)</p></li>
<li><p>So <em>Pr</em>(<span class="math inline">\(q | p\)</span>) <span class="math inline">\(&gt; \nicefrac{1}{2}\)</span> (from 2)</p></li>
<li><p><em>Pr</em>(<span class="math inline">\(q | p\)</span>) = <em>Pr</em>(<span class="math inline">\(p \wedge q | p\)</span>) (by probability calculus)</p></li>
<li><p>So <em>Pr</em>(<span class="math inline">\(p \wedge q | p\)</span>) <span class="math inline">\(&gt; \nicefrac{1}{2}\)</span> (from 3, 4)</p></li>
<li><p>So, if <span class="math inline">\(p \wedge q\)</span> is eligible for belief, then the agent prefers believing that <span class="math inline">\(p \wedge q\)</span> to not believing it, given <em>p</em> (from 5)</p></li>
<li><p>So, if <span class="math inline">\(p \wedge q\)</span> is eligible for belief, the agent prefers believing that <span class="math inline">\(p \wedge q\)</span> to not believing it (from 6, and the fact that they believe that <em>p</em>, and <span class="math inline">\(p \wedge q\)</span> is salient)</p></li>
</ol>
<p>So whenever, <span class="math inline">\(p, q\)</span> and <span class="math inline">\(p \wedge q\)</span> are salient, and the agent believes each conjunct, the agent prefers believing the conjunction <span class="math inline">\(p \wedge q\)</span> to not believing it, if <span class="math inline">\(p \wedge q\)</span> is eligible. Now we have to prove that <span class="math inline">\(p \wedge q\)</span> is eligible for belief, to prove that it is actually believed. That is, we have to prove that (5) follows from (4) and (3), where the initial quantifiers range over actions that are open and salient <em>tout court</em>.</p>
<ol type="1">
<li><p><span class="math inline">\(\forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall r\)</span> (A <span class="math inline">\(\geq_r\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _p \wedge r\)</span> B)</p></li>
<li><p><span class="math inline">\(\forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall r\)</span> (A <span class="math inline">\(\geq_r\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _q \wedge r\)</span> B)</p></li>
<li><p><span class="math inline">\(\forall\)</span>A<span class="math inline">\(\forall\)</span>B<span class="math inline">\(\forall r\)</span> (A <span class="math inline">\(\geq_r\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q \wedge r}\)</span> B)</p></li>
</ol>
<p>Assume that (5) isn’t true. That is, there are A, B and <em>S</em> such that <span class="math inline">\(\neg\)</span>(A <span class="math inline">\(\geq_s\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q \wedge s}\)</span>B). By hypothesis <em>S</em> is active, and consistent with <span class="math inline">\(p \wedge q\)</span>. So it is the conjunction of relevant, salient propositions. Since <span class="math inline">\(q\)</span> is salient, this means <span class="math inline">\(q \wedge s\)</span> is also active. Since <em>S</em> is consistent with <span class="math inline">\(p \wedge q\)</span>, it follows that <span class="math inline">\(q \wedge s\)</span> is consistent with <em>p</em>. So <span class="math inline">\(q \wedge s\)</span> is a possible substitution instance for <span class="math inline">\(r\)</span> in (3). Since (3) is true, it follows that A <span class="math inline">\(\geq _{q \wedge s}\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q \wedge s}\)</span> B. By similar reasoning, it follows that <span class="math inline">\(s\)</span> is a permissible substitution instance in (4), giving us A <span class="math inline">\(\geq_s\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{q \wedge s}\)</span> B. Putting the last two biconditionals together we get A <span class="math inline">\(\geq_s\)</span> B <span class="math inline">\(\leftrightarrow\)</span> A <span class="math inline">\(\geq _{p \wedge q \wedge s}\)</span>B, contradicting our hypothesis that there is a counterexample to (5). So whenever (3) and (4) are true, (5) is true as well, assuming <span class="math inline">\(p, q\)</span> and <span class="math inline">\(p \wedge q\)</span> are all salient.</p>
</section>
<section id="defending-closure" class="level3" data-number="0.4">
<h3 data-number="0.4" class="anchored" data-anchor-id="defending-closure"><span class="header-section-number">0.4</span> Defending Closure</h3>
<p>So on my account of the connection between degrees of belief and belief <em>tout court</em>, probabilistic coherence implies logical coherence amongst salient propositions. The last qualification is necessary. It is possible for a probabilistically coherent agent to not believe the <em>non</em>-salient consequences of things they believe, and even for a probabilistically coherent agent to have inconsistent beliefs as long as not all the members of the inconsistent set are active. Some people argue that even this weak a closure principle is implausible. David <span class="citation" data-cites="Christensen2005">Christensen (<a href="#ref-Christensen2005" role="doc-biblioref">2005</a>)</span>, for example, argues that the preface paradox provides a reason for doubting that beliefs must be closed under entailment, or even must be consistent. Here is his description of the case.</p>
<blockquote class="blockquote">
<p>We are to suppose that an apparently rational person has written a long non-fiction book—say, on history. The body of the book, as is typical, contains a large number of assertions. The author is highly confident in each of these assertions; moreover, she has no hesitation in making them unqualifiedly, and would describe herself (and be described by others) as believing each of the book’s many claims. But she knows enough about the difficulties of historical scholarship to realize that it is almost inevitable that at least a few of the claims she makes in the book are mistaken. She modestly acknowledges this in her preface, by saying that she believes the book will be found to contain some errors, and she graciously invites those who discover the errors to set her straight. <span class="citation" data-cites="Christensen2005">(<a href="#ref-Christensen2005" role="doc-biblioref">Christensen 2005, 33–34</a>)</span></p>
</blockquote>
<p>Christensen thinks such an author might be rational in every one of her beliefs, even though these are all inconsistent. Although he does not say this, nothing in his discussion suggests that he is using the irrelevance of some of the propositions in the author’s defence. So here is an argument that we should abandon closure amongst relevant beliefs.</p>
<p>Christensen’s discussion, like other discussions of the preface paradox, makes frequent use of the fact that examples like these are quite common. We don’t have to go to fake barn country to find a counterexample to closure. But it seems to me that we need two quite strong idealisations in order to get a real counterexample here.</p>
<p>The first of these is discussed in forthcoming work by Ishani Maitra <span class="citation" data-cites="MaitraANG">(<a href="#ref-MaitraANG" role="doc-biblioref">Maitra 2010</a>)</span>, and is briefly mentioned by Christensen in setting out the problem. We only have a counterexample to closure if the author <em>believes</em> every thing she writes in her book. (Indeed, we only have a counterexample if she reasonably believes every one of them. But we’ll assume a rational author who only believes what she ought to believe.) This seems unlikely to be true to me. An author of a historical book is like a detective who, when asked to put forward her best guess about what explains the evidence, says “If I had to guess, I’d say …” and then launches into spelling out her hypothesis. It seems clear that she need not <em>believe</em> the truth of her hypothesis. If she did that, she could not later learn it was true, because you can’t learn the truth of something you already believe. And she wouldn’t put any effort into investigating alternative suspects. But she can come to learn her hypothesis was true, and it would be rational to investigate other suspects. It seems to me (following here Maitra’s discussion) that we should understand scholarly assertions as being governed by the same kind of rules that govern detectives making the kind of speech being contemplated here. And those rules don’t require that the speaker believe the things they say without qualification. The picture is that the little prelude the detective explicitly says is implicit in all scholarly work.</p>
<p>There are three objections I know to this picture, none of them particularly conclusive. First, Christensen says that the author doesn’t qualify their assertions. But neither does our detective qualify most individual sentences. Second, Christensen says that most people would describe our author as believing her assertions. But it is also natural to describe our detective as believing the things she says in her speech. It’s natural to say things like “She thinks it was the butler, with the lead pipe,” in reporting her hypothesis. Third, Timothy <span class="citation" data-cites="Williamson2000-WILKAI">Williamson (<a href="#ref-Williamson2000-WILKAI" role="doc-biblioref">2000</a>)</span> has argued that if speakers don’t believe what they say, we won’t have an explanation of why Moore’s paradoxical sentences, like “The butler did it, but I don’t believe the butler did it,” are always defective. Whatever the explanation of the paradoxicality of these sentences might be, the alleged requirement that speakers believe what they say can’t be it. For our detective cannot properly say “The butler did it, but I don’t believe the butler did it” in setting out her hypothesis, even though <em>believing</em> the butler did it is not necessary for her to say “The butler did it” in setting out just that hypothesis.</p>
<p>It is plausible that for <em>some</em> kinds of books, the author should only say things they believe. This is probably true for travel guides, for example. Interestingly, casual observation suggests that authors of such books are much less likely to write modest prefaces. This makes some sense if those books can only include statements their authors believe, and the authors believe the conjunctions of what they believe.</p>
<p>The second idealisation is stressed by Simon <span class="citation" data-cites="Evnine1999">Evnine (<a href="#ref-Evnine1999" role="doc-biblioref">1999</a>)</span> in his paper “Believing Conjunctions”. The following situation does not involve me believing anything inconsistent.</p>
<ul>
<li><p>I believe that what Manny just said, whatever it was, is false.</p></li>
<li><p>Manny just said that the stands at Fenway Park are green.</p></li>
<li><p>I believe that the stands at Fenway Park are green.</p></li>
</ul>
<p>If we read the first claim <em>de dicto</em>, that I believe that Manny just said something false, then there is no inconsistency. (Unless I also believe that what Manny just said was that the stands in Fenway Park are green.) But if we read it <em>de re</em>, that the thing Manny just said is one of the things I believe to be false, then the situation does involve me being inconsistent. The same is true when the author believes that one of the things she says in her book is mistaken. If we understand what she says <em>de dicto</em>, there is no contradiction in her beliefs. It has to be understood <em>de re</em> before we get a logical problem. And the fact is that most authors do not have <em>de re</em> attitudes towards the claims made in their book. Most authors don’t even remember everything that’s in their books. (I’m not sure I remember how this section started, let alone this paper.) Some may argue that authors don’t even have the capacity to consider a proposition as long and complicated as the conjunction of all the claims in their book. Christensen considers this objection, but says it isn’t a serious problem.</p>
<blockquote class="blockquote">
<p>It is undoubtedly true that ordinary humans cannot entertain book-length conjunctions. But surely, agents who do not share this fairly <em>superficial</em> limitation are easily conceived. And it seems just as wrong to say of such agents that they are rationally required to believe in the inerrancy of the books they write. (38: my emphasis)</p>
</blockquote>
<p>I’m not sure this is undoubtedly true; it isn’t clear that propositions (as opposed to their representations) have lengths. And humans can believe propositions that <em>can</em> be represented by sentences as long as books. But even without that point, Christensen is right that there is an idealisation here, since ordinary humans do not know exactly what is in a given book, and hence don’t have <em>de re</em> attitudes towards the propositions expressed in the book.</p>
<p>I’m actually rather suspicious of the intuition that Christensen is pushing here, that idealising in this way doesn’t change intuitions about the case. The preface paradox gets a lot of its (apparent) force from intuitions about what attitude we should have towards real books. Once we make it clear that the real life cases are not relevant to the paradox, I find the intuitions become rather murky. But I won’t press this point.</p>
<p>A more important point is that we believers in closure don’t think that authors should think their books are inerrant. Rather, following <span class="citation" data-cites="Stalnaker1984">Stalnaker (<a href="#ref-Stalnaker1984" role="doc-biblioref">1984</a>)</span>, we think that authors shouldn’t unqualifiedly <em>believe</em> the individual statements in their book if they don’t believe the conjunction of those statements. Rather, their attitude towards those propositions (or at least some of them) should be that they are probably true. (As Stalnaker puts it, they accept the story without believing it.) Proponents of the preface paradox know that this is a possible response, and tend to argue that it is impractical. Here is Christensen on this point.</p>
<blockquote class="blockquote">
<p>It is clear that our everyday binary way of talking about beliefs has immense practical advantages over a system which insisted on some more fine-grained reporting of degrees of confidence … At a minimum, talking about people as believing, disbelieving, or withholding belief has at least as much point as do many of the imprecise ways we have of talking about things that can be described more precisely. (96)</p>
</blockquote>
<p>Richard Foley makes a similar point.</p>
<blockquote class="blockquote">
<p>There are <em>deep</em> reasons for wanting an epistemology of beliefs, reasons that epistemologies of degrees of belief by their very nature cannot possibly accommodate. <span class="citation" data-cites="Foley1993">(<a href="#ref-Foley1993" role="doc-biblioref">Foley 1993, 170</a>, my emphasis)</span></p>
</blockquote>
<p>It’s easy to make too much of this point. It’s a lot easier to triage propositions into TRUE, FALSE and NOT SURE and work with those categories than it is to work assign precise numerical probabilities to each proposition. But these are not the only options. Foley’s discussion subsequent to the above quote sometimes suggests they are, especially when he contrasts the triage with “indicat[ing] as accurately as I can my degree of confidence in each assertion that I defend.” (171) But really it isn’t <em>much</em> harder to add two more categories, PROBABLY TRUE and PROBABLY FALSE to those three, and work with that five-way division rather than a three-way division. It’s not clear that humans as they are actually constructed have a <em>strong</em> preference for the three-way over the five-way division, and even if they do, I’m not sure in what sense this is a ‘deep’ fact about them.</p>
<p>Once we have the five-way division, it is clear what authors should do if they want to respect closure. For any conjunction that they don’t believe (i.e.&nbsp;classify as true), they should not believe one of the conjuncts. But of course they can classify every conjunct as probably true, even if they think the conjunction is false, or even certainly false. Still, might it not be considered something of an idealisation to say rational authors must make this five-way distinction amongst propositions they consider? Yes, but it’s no more of an idealisation than we need to set up the preface paradox in the first place. To use the preface paradox to find an example of someone who reasonably violates closure, we need to insist on the following three constraints.</p>
<ol type="1">
<li><p>They are part of a research community where only asserting propositions you believe is compatible with active scholarship;</p></li>
<li><p>They know exactly what is in their book, so they are able to believe that one of the propositions in the book is mistaken, where this is understood <em>de re</em>; but</p></li>
<li><p>They are unable to effectively function if they have to effect a five-way, rather than a three-way, division amongst the propositions they consider.</p></li>
</ol>
<p>Put more graphically, to motivate the preface paradox we have to think that our inability to have <em>de re</em> thoughts about the contents of books is a “superficial constraint”, but our preference for working with a three-way rather than a five-way division is a “deep” fact about our cognitive system. Maybe each of these attitudes could be plausible taken on its own (though I’m sceptical of that) but the conjunction seems just absurd.</p>
<p>I’m not entirely sure an agent subject to exactly these constraints is even fully conceivable. (Such an agent is negatively conceivable, in David Chalmers’s terminology, but I rather doubt they are positively conceivable.) But even if they are a genuine possibility, why the norms applicable to an agent satisfying that very gerrymandered set of constraints should be considered relevant norms for our state is far from clear. I’d go so far as to say it’s clear that the applicability (or otherwise) of a given norm to such an odd agent is no reason whatsoever to say it applies to us. But since the preface paradox only provides a reason for just these kinds of agents to violate closure, we have no reason for ordinary humans to violate closure. So I see no reason here to say that we can have probabilistic coherence without logical coherence, as proponents of the threshold view insist we can have, but which I say we can’t have <em>at least when the propositions involved are salient</em>. The more pressing question, given the failure of the preface paradox argument, is why I don’t endorse a much stronger closure principle, one that drops the restriction to salient propositions. The next section will discuss that point.</p>
<p>I’ve used Christensen’s book as a stalking horse in this section, because it is the clearest and best statement of the preface paradox. Since Christensen is a paradox-mongerer and I’m a paradox-denier, it might be thought we have a deep disagreement about the relevant epistemological issues. But actually I think our overall views are fairly close despite this. I favour an epistemological outlook I call “Probability First”, the view that getting the epistemology of partial belief right is of the first importance, and everything else should flow from that. Christensen’s view, reduced to a slogan, is “Probability First and Last”. This section has been basically about the difference between those two slogans. It’s an important dispute, but it’s worth bearing in mind that it’s a factional squabble within the Probability Party, not an outbreak of partisan warfare.</p>
</section>
<section id="too-little-closure" class="level3" data-number="0.5">
<h3 data-number="0.5" class="anchored" data-anchor-id="too-little-closure"><span class="header-section-number">0.5</span> Too Little Closure?</h3>
<p>In the previous section I defended the view that a coherent agent has beliefs that are deductively cogent with respect to salient propositions. Here I want to defend the importance of the qualification. Let’s start with what I take to be the most important argument for closure, the passage from Stalnaker’s <em>Inquiry</em> that I quoted above.</p>
<blockquote class="blockquote">
<p>Reasoning in this way from accepted premises to their deductive consequences (<span class="math inline">\(P\)</span>, also <span class="math inline">\(Q\)</span>, therefore <span class="math inline">\(R\)</span>) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. <span class="citation" data-cites="Stalnaker1984">(<a href="#ref-Stalnaker1984" role="doc-biblioref">Stalnaker 1984, 92</a>)</span></p>
</blockquote>
<p>Stalnaker’s wording here is typically careful. The relevant question isn’t whether we can accept <em>p</em>, accept <span class="math inline">\(q\)</span>, accept <em>p</em> and <span class="math inline">\(q\)</span> entail <span class="math inline">\(r\)</span>, and reject <span class="math inline">\(r\)</span>. As Christensen <span class="citation" data-cites="Christensen2005">(<a href="#ref-Christensen2005" role="doc-biblioref">2005</a> Ch. 4)</span> notes, this is impossible even on the threshold view, as long as the threshold is above 2/3. The real question is whether we can accept <em>p</em>, accept <span class="math inline">\(q\)</span>, accept <em>p</em> and <span class="math inline">\(q\)</span> entail <span class="math inline">\(r\)</span>, and <em>fail</em> to accept <span class="math inline">\(r\)</span>. And this is always a live possibility on any threshold view, though it seems absurd at first that this could be coherent.</p>
<p>But it’s important to note how <em>active</em> the verbs in Stalnaker’s description are. When faced with a valid argument we have to <em>object</em> to one of the premises, or the validity of the argument. What we can’t do is <em>agree</em> to the premises and the validity of the argument, while <em>objecting</em> to the conclusion. I agree. If we are really <em>agreeing</em> to some propositions, and <em>objecting</em> to others, then all those propositions are salient. And in that case closure, deductive coherence, is mandatory. This doesn’t tell us what we have to do if we haven’t previously made the propositions salient in the first place.</p>
<p>The position I endorse here is very similar in its conclusions to that endorsed by Gilbert Harman in <em>Change in View</em>. There Harman endorses the following principle. (At least he endorses it as true – he doesn’t seem to think it is particularly explanatory because it is a special case of a more general interesting principle.)</p>
<dl>
<dt>Recognized Logical Implication Principle</dt>
<dd>
<p>One has reason to believe <em>p</em> if one <em>recognizes</em> that <em>p</em> is logically implied by one’s view. <span class="citation" data-cites="Harman1986">(<a href="#ref-Harman1986" role="doc-biblioref">Harman 1986, 17</a>)</span></p>
</dd>
</dl>
<p>This seems right to me, both what it says and its implicature that the reason in question is not a conclusive reason. My main objection to those who use the preface paradox to argue against closure is that they give us a mistaken picture of what we have <em>to do</em> epistemically. When I have inconsistent beliefs, or I don’t believe some consequence of my beliefs, that is something I have a reason to deal with at some stage, something I have to do. When we say that we have things to do, we don’t mean that we have to do them <em>right now</em>, or instead of everything else. My current list of things to do includes cleaning my bathroom, yet here I am writing this paper, and (given the relevant deadlines) rightly so. We can have the job of cleaning up our epistemic house as something to do while recognising that we can quite rightly do other things first. But it’s a serious mistake to infer from the permissibility of doing other things that cleaning up our epistemic house (or our bathroom) isn’t something to be done. The bathroom won’t clean itself after all, and eventually this becomes a problem.</p>
<p>There is a possible complication when it comes to tasks that are very low priority. My attic is to be cleaned, or at least it could be cleaner, but there are no imaginable circumstances under which something else wouldn’t be higher priority. Given that, should we really leave <em>clean the attic</em> on the list of things to be done? Similarly, there might be implications I haven’t followed through that it couldn’t possibly be worth my time to sort out. Are they things to be done? I think it’s worthwhile recording them as such, because otherwise we might miss opportunities to deal with them in the process of doing something else. I don’t need to put off anything else in order to clean the attic, but if I’m up there for independent reasons I should bring down some of the garbage. Similarly, I don’t need to follow through implications mostly irrelevant to my interests, but if those propositions come up for independent reasons, I should deal with the fact that some things I believe imply something I don’t believe. Having it be the case that all implications from things we believe to things we don’t believe constitute jobs to do (possibly in the loose sense that cleaning my attic is something to do) has the right implications for what epistemic duties we do and don’t have.</p>
<p>While waxing metaphorical, it seems time to pull out a rather helpful metaphor that Gilbert <span class="citation" data-cites="Ryle1949">Ryle (<a href="#ref-Ryle1949" role="doc-biblioref">1949</a>)</span> develops in <em>The Concept of Mind</em> at a point where he’s covering what we’d now call the inference/implication distinction. (This is a large theme of chapter 9, see particularly pages 292-309.) Ryle’s point in these passages, as it frequently is throughout the book, is to stress that minds are fundamentally active, and the activity of a mind cannot be easily recovered from its end state. Although Ryle doesn’t use this language, his point is that we shouldn’t confuse the difficult activity of drawing inferences with the smoothness and precision of a logical implication. The language Ryle does use is more picturesque. He compares the easy work a farmer does when sauntering down a path from the hard work he did when building the path. A good argument, in philosophy or mathematics or elsewhere, is like a well made path that permits sauntering from the start to finish without undue strain. But from that it doesn’t follow that the task of coming up with that argument, of building that path in Ryle’s metaphor, was easy work. The easiest paths to walk are often the hardest to build. Path-building, smoothing out our beliefs so they are consistent and closed under implication, is hard work, even when the finished results look clean and straightforward. Its work that we shouldn’t do unless we need to. But making sure our beliefs are closed under entailment even with respect to irrelevant propositions is suspiciously like the activity of buildings paths between points without first checking you need to walk between them.</p>
<p>For a less metaphorical reason for doubting the wisdom of this unchecked commitment to closure, we might notice the difficulties theorists tend to get into all sorts of difficulties. Consider, for example, the view put forward by Mark Kaplan in <em>Decision Theory as Philosophy</em>. Here is his definition of belief.</p>
<blockquote class="blockquote">
<p>You count as believing P just if, were your sole aim to assert the truth (as it pertains to P), and you only options were to assert that P, assert that <span class="math inline">\(\neg\)</span>P or make neither assertion, you would prefer to assert that P. (109)</p>
</blockquote>
<p>Kaplan notes that conditional definitions like this are prone to Shope’s conditional fallacy. If my sole aim were to assert the truth, I might have different beliefs to what I now have. He addresses one version of this objection (namely that it appears to imply that everyone believes their sole desire is to assert the truth) but as we’ll see presently he can’t avoid all versions of it.</p>
<p>These arguments are making me thirsty. I’d like a beer. Or at least I think I would. But wait! On Kaplan’s theory I can’t think that I’d like a beer, for if my sole aim were to assert the truth as it pertains to my beer-desires, I wouldn’t have beer desires. And then I’d prefer to assert that I wouldn’t like a beer, I’d merely like to assert the truth as it pertains to my beer desires.</p>
<p>Even bracketing this concern, Kaplan ends up being committed to the view that I can (coherently!) believe that <em>p</em> even while regarding <em>p</em> as highly improbable. This looks like a refutation of the view to me, but Kaplan accepts it with some equanimity. He has two primary reasons for saying we should live with this. First, he says that it only looks like an absurd consequence if we are committed to the Threshold View. To this all I can say is that <em>I</em> don’t believe the Threshold View, but it still seems absurd to me. Second, he says that any view is going to have to be revisionary to some extent, because our ordinary concept of belief is not “coherent” (142). His view is that, “Our ordinary notion of belief both construes belief as a state of confidence short of certainty and takes consistency of belief to be something that is at least possible and, perhaps, even desirable” and this is impossible. I think the view here interprets belief as a state less than confidence and allows for as much consistency as the folk view does (i.e.&nbsp;consistency amongst salient propositions), so this defence is unsuccessful as well.</p>
<p>None of the arguments here in favour of our restrictions on closure are completely conclusive. In part the argument at this stage rests on the lack of a plausible rival theory that doesn’t interpret belief as certainty but implements a stronger closure principle. It’s possible that tomorrow someone will come up with a theory that does just this. Until then, we’ll stick with the account here, and see what its epistemological implications might be.</p>
</section>
<section id="examples-of-pragmatic-encroachment" class="level3 page-columns page-full" data-number="0.6">
<h3 data-number="0.6" class="anchored" data-anchor-id="examples-of-pragmatic-encroachment"><span class="header-section-number">0.6</span> Examples of Pragmatic Encroachment</h3>
<p>Fantl and McGrath’s case for pragmatic encroachment starts with cases like the following. (The following case is not quite theirs, but is similar enough to suit their plan, and easier to explain in my framework.)</p>
<blockquote class="blockquote">
<p><em>Local and Express</em></p>
<p>There are two kinds of trains that run from the city to the suburbs: the local, which stops at all stations, and the express, which skips the first eight stations. Harry and Louise want to go to the fifth station, so they shouldn’t catch the Express. Though if they do it isn’t too hard to catch a local back the other way, so it isn’t usually a large cost. Unfortunately, the trains are not always clearly labelled. They see a particular train about to leave. If it’s a local they are better off catching it, if it is an express they should wait for the next local, which they can see is already boarding passengers and will leave in a few minutes. While running towards the train, they hear a fellow passenger say “It’s a local.” This gives them good, but far from overwhelming, reason to believe that the train is a local. Passengers get this kind of thing wrong fairly frequently, but they don’t have time to get more information. So each of them face a gamble, which they can take by getting on the train. If the train is a local, they will get home a few minutes early. If it is an express they will get home a few minutes later. For Louise, this is a low stakes gamble, as nothing much turns on whether she is a few minutes early or late, but she does have a weak preference for arriving earlier rather than later. But for Harry it is a high stakes gamble, because if he is late he won’t make the start of his daughter’s soccer game, which will highly upset her. There is no large payoff for Harry arriving early.</p>
</blockquote>
<p>What should each of them do? What should each of them believe?</p>
<p>The first question is relatively easy. Louise should catch the train, and Harry should wait for the next. For each of them that’s the utility maximising thing to do. The second one is harder. Fantl and McGrath suggest that, despite being in the same epistemic position with respect to everything except their interests, Louise is justified in believing the train is a local and Harry is not. I agree. (If you don’t think the particular case fits this pattern, feel free to modify it so the difference in interests grounds a difference in what they are justified in believing.) Does this show that our notion of epistemic justification has to be pragmatically sensitive? I’ll argue that it does not.</p>
<p>The fundamental assumption I’m making is that what is primarily subject to epistemic evaluation are degrees of belief, or what are more commonly called states of confidence in ordinary language. When we think about things this way, we see that Louise and Harry are justified in adopting <em>the very same degrees of belief</em>. Both of them should be confident, but not absolutely certain, that the train is a local. We don’t have even the appearance of a counterxample to Probabilistic Evidentialism here. If we like putting this in numerical terms, we could say that each of them is justified in assigning a probability of around 0.9 to the proposition <em>That train is a local</em>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> So as long as we adopt a Probability First epistemology, where we in the first instance evaluate the probabilities that agents assign to propositions, Harry and Louise are evaluated alike iff they do the same thing.</p>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;I think putting things numerically is misleading because it suggests that the kind of bets we usually use to measure degrees of belief are open, salient options for Louise and Harry. But if those bets were open and salient, they wouldn’t <em>believe</em> the train is a local. Using qualitative rather than quantitative language to describe them is just as accurate, and doesn’t have misleading implications about their practical environment.</p></li></div><p>How then can we say that Louise alone is justified in believing that the train is a local? Because that state of confidence they are justified in adopting, the state of being fairly confident but not absolutely certain that the train is a local, counts as believing that the train is a local given Louise’s context but not Harry’s context. Once Louise hears the other passenger’s comment, conditionalising on <em>That’s a local</em> doesn’t change any of her preferences over open, salient actions, including such ‘actions’ as believing or disbelieving propositions. But conditional on the train being a local, Harry prefers catching the train, which he actually does not prefer.</p>
<p>In cases like this, interests matter not because they affect the degree of confidence that an agent can reasonably have in a proposition’s truth. (That is, not because they matter to epistemology.) Rather, interests matter because they affect whether those reasonable degrees of confidence amount to belief. (That is, because they matter to philosophy of mind.) There is no reason here to let pragmatic concerns into epistemology.</p>
</section>
<section id="justification-and-practical-reasoning" class="level3" data-number="0.7">
<h3 data-number="0.7" class="anchored" data-anchor-id="justification-and-practical-reasoning"><span class="header-section-number">0.7</span> Justification and Practical Reasoning</h3>
<p>The discussion in the last section obviously didn’t show that there is no encroachment of pragmatics into epistemology. There are, in particular, two kinds of concerns one might have about the prospects for extending my style of argument to block all attempts at pragmatic encroachment. The biggest concern is that it might turn out to be impossible to defend a Probability First epistemology, particularly if we do not allow ourselves pragmatic concerns. For instance, it is crucial to this project that we have a notion of evidence that is not defined in terms of traditional epistemic concepts (e.g.&nbsp;as knowledge), or in terms of interests. This is an enormous project, and I’m not going to attempt to tackle it here. The second concern is that we won’t be able to generalise the discussion of that example to explain the plausibility of (JP) without conceding something to the defenders of pragmatic encroachment.</p>
<dl>
<dt>(JP)</dt>
<dd>
<p>If <em>S</em> justifiably believes that <em>p</em>, then <em>S</em> is justified in using <em>p</em> as a premise in practical reasoning.</p>
</dd>
</dl>
<p>And that’s what we will look at in this section. To start, we need to clarify exactly what (JP) means. Much of this discussion will be indebted to Fantl and McGrath’s discussion of various ways of making (JP) more precise. To see some of the complications at issue, consider a simple case of a bet on a reasonably well established historical proposition. The agent has a lot of evidence that supports <em>p</em>, and is offered a bet that returns $1 if <em>p</em> is true, and loses $500 if <em>p</em> is false. Since her evidence doesn’t support <em>that</em> much confidence in <em>p</em>, she properly declines the bet. One might try to reason intuitively as follows. Assume that she justifiably believed that <em>p</em>. Then she’d be in a position to make the following argument.</p>
<blockquote class="blockquote">
<p><em>p</em><br>
If <em>p</em>, then I should take the bet<br>
So, I should take the bet</p>
</blockquote>
<p>Since she isn’t in a position to draw the conclusion, she must not be in a position to endorse both of the premises. Hence (arguably) she isn’t justified in believing that <em>p</em>. But we have to be careful here. If we assume also that <em>p</em> is true (as Fantl and McGrath do, because they are mostly concerned with knowledge rather than justified belief), then the second premise is clearly false, since it is a conditional with a true antecedent and a false consequent. So the fact that she can’t draw the conclusion of this argument only shows that she can’t endorse <em>both</em> of the premises, and that’s not surprising since one of the premises is most likely false. (I’m not assuming here that the conditional is true iff it has a true antecendent or a false consequent, just that it is only true if it has a false antecedent or a true consequent.)</p>
<p>In order to get around this problem, Fantl and McGrath suggest a few other ways that our agent might reason to the bet. They suggest each of the following principles.</p>
<blockquote class="blockquote">
<p>S knows that p only if, for any act A, if <em>S</em> knows that if p, then A is the best thing she can do, then <em>S</em> is rational to do A. (72)</p>
<p><em>S</em> knows that p only if, for any states of affairs A and B, if <span class="math inline">\(S\)</span> knows that if p, then A is better for her than B, then <em>S</em> is rational to prefer A to B. (74)</p>
<p><strong>(PC)</strong> <em>S</em> is justified in believing that p only if <em>S</em> is rational to prefer as if p.&nbsp;(77)</p>
</blockquote>
<p>Hawthorne <span class="citation" data-cites="Hawthorne2004">(<a href="#ref-Hawthorne2004" role="doc-biblioref">2004, 174–81</a>)</span> appears to endorse the second of these principles. He considers an agent who endorses the following implication concerning a proposed sell of a lottery ticket for a cent, which is well below its actuarially fair value.</p>
<blockquote class="blockquote">
<p>I will lose the lottery.<br>
If I keep the ticket, I will get nothing.<br>
If I sell the ticket, I will get a cent.<br>
So I ought to sell the ticket. (174)</p>
</blockquote>
<p>(To make this fully explicit, it helps to add the tacit premise that a cent is better than nothing.) Hawthorne says that this is intuitively a <em>bad</em> argument, and concludes that the agent who attempts to use it is not in a position to know its first premise. But that conclusion only follows if we assume that the argument form is acceptable. So it is plausible to conclude that he endorses Fantl and McGrath’s second principle.</p>
<p>The interesting question here is whether the theory endorsed in this paper can validate the true principles that Fantl and McGrath articulate. (Or, more precisely, we can validate the equivalent true principles concerning justified belief, since knowledge is outside the scope of the paper.) I’ll argue that it can in the following way. First, I’ll just note that given the fact that the theory here implies the closure principles we outlined in section 5, we can easily enough endorse Fantl and McGrath’s first two principles. This is good, since they seem true. The longer part of the argument involves arguing that their principle (PC), which doesn’t hold on the theory endorsed here, is in fact incorrect.</p>
<p>One might worry that the qualification on the closure principles in section 5 mean that we can’t fully endorse the principles Fantl and McGrath endorse. In particular, it might be worried that there could be an agent who believes that <em>p</em>, believes that if <em>p</em>, then A is better than B, but doesn’t put these two beliefs together to infer that A is better than B. This is certainly a possibility given the qualifications listed above. But note that in this position, if those two beliefs were justified, the agent would certainly be <em>rational</em> to conclude that A is better than B, and hence rational to prefer A to B. So the constraints on the closure principles don’t affect our ability to endorse these two principles.</p>
<p>The real issue is (PC). Fantl and McGrath offer a lot of cases where (PC) holds, as well as arguing that it is plausibly true given the role of implications in practical reasoning. What’s at issue is that (PC) is stronger than a deductive closure principle. It is, in effect, equivalent to endorsing the following schema as a valid principle of implication.</p>
<blockquote class="blockquote">
<p><em>p</em><br>
Given <em>p</em>, A is preferable to B<br>
So, A is preferable to B</p>
</blockquote>
<p>I call this Practical Modus Ponens, or PMP. The middle premise in PMP is <em>not</em> a conditional. It is not to be read as <em>If p, then A is preferable to B</em>. Conditional valuations are not conditionals. To see this, again consider the proposed bet on (true) <em>p</em> at exorbitant odds, where A is the act of taking the bet, and B the act of declining the bet. It’s true that given <em>p</em>, A is preferable to B. But it’s not true that if <em>p</em>, then A is preferable to B. Even if we restrict our attention to cases where the preferences in question are perfectly valid, this is a case where PMP is invalid. Both premises are true, and the conclusion is false. It might nevertheless be true that whenever an agent is justified in believing both of the premises, she is justified in believing the conclusion. To argue against this, we need a <em>very</em> complicated case, involving embedded bets and three separate agents, Quentin, Robby and Thom. All of them have received the same evidence, and all of them are faced with the same complex bet, with the following properties.</p>
<ul>
<li><p><em>p</em> is an historical proposition that is well (but not conclusively) supported by their evidence, and happens to be true. All the agents have a high credence in <em>p</em>, which is exactly what the evidence supports.</p></li>
<li><p>The bet A, which they are offered, wins if <em>p</em> is true, and loses if <em>p</em> is false.</p></li>
<li><p>If they win the bet, the prize is the bet B.</p></li>
<li><p><em>S</em> is also an historical proposition, but the evidence tells equally for and against it. All the agents regard <em>S</em> as being about as likely as not. Moreover, <em>S</em> turns out to be false.</p></li>
<li><p>The bet B is worth $2 if <em>S</em> is true, and worth -$1 if <em>S</em> is false. Although it is actually a losing bet, the agents all rationally value it at around 50 cents.</p></li>
<li><p>How much A costs is determined by which proposition from the partition {<span class="math inline">\(q, r, s\)</span>} is true.</p></li>
<li><p>If <span class="math inline">\(q\)</span> is true, A costs $2</p></li>
<li><p>If <span class="math inline">\(r\)</span> is true, A costs $500</p></li>
<li><p>If <span class="math inline">\(t\)</span> is true, A costs $1</p></li>
<li><p>The evidence the agents has strongly supports <span class="math inline">\(r\)</span>, though <span class="math inline">\(t\)</span> is in fact true</p></li>
<li><p>Quentin believes <span class="math inline">\(q\)</span></p></li>
<li><p>Robby believes <span class="math inline">\(r\)</span></p></li>
<li><p>Thom believes <span class="math inline">\(t\)</span></p></li>
</ul>
<p>All of the agents make the utility calculations that their beliefs support, so Quentin and Thom take the bet and lose a dollar, while Robby declines it. Although Robby has a lot of evidence in favour of <em>p</em>, he correctly decides that it would be unwise to bet on <em>p</em> at effective odds of 1000 to 1 against. I’ll now argue that both Quentin and Thom are potential counterexamples to (PC). There are three possibilities for what we can say about those two.</p>
<p>First, we could say that they are justified in believing <em>p</em>, and rational to take the bet. The problem with this position is that if they had rational beliefs about the partition {<span class="math inline">\(q, r, t\)</span>} they would realise that taking the bet does not maximise expected utility. If we take rational decisions to be those that maximise expected utility given a rational response to the evidence, then the decisions are clearly not rational.</p>
<p>Second, we could say that although Quentin and Thom are not rational in accepting the bet, nor are they justified in believing that <em>p</em>. This doesn’t seem particularly plausible for several reasons. The irrationality in their belief systems concerns whether <span class="math inline">\(q, r\)</span> or <span class="math inline">\(t\)</span> is true, not whether <em>p</em> is true. If Thom suddenly got a lot of evidence that <span class="math inline">\(t\)</span> is true, then all of his (salient) beliefs would be well supported by the evidence. But it is bizarre to think that whether his belief in <em>p</em> is rational turns on how much evidence he has for <span class="math inline">\(t\)</span>. Finally, even if we accept that agents in higher stakes situations need more evidence to have justified beliefs, the fact is that the agents are in a low-risk situation, since <span class="math inline">\(t\)</span> is actually true, so the most they could lose is $1.</p>
<p>So it seems like the natural thing to say is that Quentin and Thom <em>are</em> justified in believing that <em>p</em>, and are justified in believing that given <em>p</em>, it maximises expected utility to take the bet, but they are not rational to take the bet. (At least, in the version of the story where they are thinking about which of <span class="math inline">\(q, r\)</span> and <span class="math inline">\(t\)</span> are correct given their evidence when thinking about whether to take the bet they are counterexamples to (PC).) Against this, one might respond that if belief in <em>p</em> is justified, there are arguments one might make to the conclusion that the bet should be taken. So it is inconsistent to say that the belief is justified, but the decision to take the bet is not rational. The problem is finding a premise that goes along with <em>p</em> to get the conclusion that taking the bet is rational. Let’s look at some of the premises the agent might use.</p>
<ul>
<li>If <em>p</em>, then the best thing to do is to take the bet.</li>
</ul>
<p>This isn’t true (<em>p</em> is true, but the best thing to do isn’t to take the bet). More importantly, the agents think this is only true if <em>S</em> is true, and they think <em>S</em> is a 50/50 proposition. So they don’t believe this premise, and it would not be rational to believe it.</p>
<ul>
<li>If <em>p</em>, then probably the best thing to do is to take the bet.</li>
</ul>
<p>Again this isn’t true, and it isn’t well supported, and it doesn’t even support the conclusion, for it doesn’t follow from the fact that <span class="math inline">\(x\)</span> is probably the best thing to do that <span class="math inline">\(x\)</span> should be done.</p>
<ul>
<li>If <em>p</em>, then taking the bet maximises rational expected utility.</li>
</ul>
<p>This isn’t true – it is a conditional with a true antecedent and a false consequent. Moreover, if Quentin and Thom were rational, like Robby, they would recognise this.</p>
<ul>
<li>If <em>p</em>, then taking the bet maximises expected utility relative to their beliefs.</li>
</ul>
<p>This is true, and even reasonable to believe, but it doesn’t imply that they should take the bet. It doesn’t follow from the fact that doing something maximises expected utility relative to my crazy beliefs that I should do that thing.</p>
<ul>
<li>Given <em>p</em>, taking the bet maximises rational expected utility.</li>
</ul>
<p>This is true, and even reasonable to believe, but it isn’t clear that it supports the conclusion that the agents should take the bet. The implication appealed to here is PMP, and in this context that’s close enough to equivalent to (PC). If we think that this case is a prima facie problem for (PC), as I think is intuitively plausible, then we can’t use (PC) to show that it <em>doesn’t</em> post a problem. We could obviously continue for a while, but it should be clear it will be very hard to find a way to justify taking the bet even spotting the agents <em>p</em> as a premise they can use in rational deliberation. So it seems to me that (PC) is not in general true, which is good because as we’ll see in cases like this one the theory outlined here does not support it.</p>
<p>The theory we have been working with says that belief that <em>p</em> is justified iff the agent’s degree of belief in <em>p</em> is sufficient to amount to belief in their context, and they are justified in believing <em>p</em> to that degree. Since by hypothesis Quentin and Thom are justified in believing <em>p</em> to the degree that they do, the only question left is whether this amounts to belief. This turns out not to be settled by the details of the case as yet specified. At first glance, assuming there are no other relevant decisions, we might think they believe that <em>p</em> because (a) they prefer (in the relevant sense) believing <em>p</em> to not believing <em>p</em>, and (b) conditionalising on <em>p</em> doesn’t change their attitude towards the bet. (They prefer taking the bet to declining it, both unconditionally and conditional on <em>p</em>.)</p>
<p>But that isn’t all there is to the definition of belief <em>tout court</em>. We must also ask whether conditionalising on <em>p</em> changes any preferences conditional on any active proposition. And that may well be true. Conditional on <span class="math inline">\(r\)</span>, Quentin and Thom prefer not taking the bet to taking it. But conditional on <span class="math inline">\(r\)</span> and <em>p</em>, they prefer taking the bet to not taking it. So if <span class="math inline">\(r\)</span> is an active proposition, they don’t believe that <em>p</em>. If <span class="math inline">\(r\)</span> is not active, they do believe it. In more colloquial terms, if they are concerned about the possible truth of <span class="math inline">\(r\)</span> (if it is salient, or at least not taken for granted to be false) then <em>p</em> becomes a potentially high-stakes proposition, so they don’t believe it without extraordinary evidence (which they don’t have). Hence they are only a counterexample to (PC) if <span class="math inline">\(r\)</span> is not active. But if <span class="math inline">\(r\)</span> is not active, our theory predicts that they are a counterexample to (PC), which is what we argued above is intuitively correct.</p>
<p>Still, the importance of <span class="math inline">\(r\)</span> suggests a way of saving (PC). Above I relied on the position that if Quentin and Thom are not maximising rational expected utility, then they are being irrational. This is perhaps too harsh. There is a position we could take, derived from some suggestions made by Gilbert Harman in <em>Change in View</em>, that an agent can rationally rely on their beliefs, even if those beliefs were not rationally formed, if they cannot be expected to have kept track of the evidence they used to form that belief. If we adopt this view, then we might be able to say that (PC) is compatible with the correct normative judgments about this case.</p>
<p>To make this compatibility explicit, let’s adjust the case so Quentin takes <span class="math inline">\(q\)</span> for granted, and cannot be reasonably expected to have remembered the evidence for <span class="math inline">\(q\)</span>. Thom, on the other hand, forms the belief that <span class="math inline">\(t\)</span> rather than <span class="math inline">\(r\)</span> is true in the course of thinking through his evidence that bears on the rationality of taking or declining the bet. (In more familiar terms, <span class="math inline">\(t\)</span> is part of the inference Thom uses in coming to conclude that he should take the bet, though it is not part of the final implication he endorses whose conclusion is that he should take the bet.) Neither Quentin nor Thom is a counterexample to (PC) thus understood. (That is, with the notion of rationality in (PC) understood as Harman suggests that it should be.) Quentin is not a counterexample, because he is <em>rational</em> in taking the bet. And Thom is not a counterexample, because in his context, where <span class="math inline">\(r\)</span> is active, his credence in <em>p</em> does not amount to belief in <em>p</em>, so he is not justified in believing <em>p</em>.</p>
<p>We have now two readings of (PC). On the strict reading, where a rational choice is one that maximises rational expected utility, the principle is subject to counterexample, and seems generally to be implausible. On the loose reading, where we allow agents to rely on beliefs formed irrationally in the past in rational decision making, (PC) <em>is</em> plausible. Happily, the theory sketched here agrees with (PC) on the plausible loose reading, but not on the implausible strict reading. In the previous section I argued that the theory also accounts for intuitions about particular cases like <em>Local and Express</em>. And now we’ve seen that the theory accounts for our considered opinions about which principles connecting justified belief to rational decision making we should endorse. So it seems at this stage that we can account for the intuitions behind the pragmatic encroachment view while keeping a concept of probabilistic epistemic justification that is free of pragmatic considerations.</p>
</section>
<section id="conclusions" class="level3" data-number="0.8">
<h3 data-number="0.8" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">0.8</span> Conclusions</h3>
<p>Given a pragmatic account of belief, we don’t need to have a pragmatic account of justification in order to explain the intuitions that whether <span class="math inline">\(S\)</span> justifiably believes that <em>p</em> might depend on pragmatic factors. My focus here has been on sketching a theory of belief on which it is the belief part of the concept of a justified belief which is pragmatically sensitive. I haven’t said much about why we should prefer to take that option than say that the notion of epistemic justification is a pragmatic notion. I’ve mainly been aiming to show that a particular position is an open possibility, namely that we can accept that whether a particular agent is justified in believing <em>p</em> can be sensitive to their practical environment without thinking that the primary epistemic concepts are themselves pragmatically sensitive.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Bovens1999" class="csl-entry" role="listitem">
Bovens, Luc, and James Hawthorne. 1999. <span>“The Preface, the Lottery, and the Logic of Belief.”</span> <em>Mind</em> 108 (430): 241–64. <a href="https://doi.org/10.1093/mind/108.430.241">https://doi.org/10.1093/mind/108.430.241</a>.
</div>
<div id="ref-Christensen2005" class="csl-entry" role="listitem">
Christensen, David. 2005. <em>Putting Logic in Its Place</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Evnine1999" class="csl-entry" role="listitem">
Evnine, Simon. 1999. <span>“Believing Conjunctions.”</span> <em>Synthese</em> 118: 201–27. <a href="https://doi.org/10.1023/A:1005114419965">https://doi.org/10.1023/A:1005114419965</a>.
</div>
<div id="ref-Fantl2002" class="csl-entry" role="listitem">
Fantl, Jeremy, and Matthew McGrath. 2002. <span>“Evidence, Pragmatics, and Justification.”</span> <em>Philosophical Review</em> 111: 67–94. <a href="https://doi.org/10.2307/3182570">https://doi.org/10.2307/3182570</a>.
</div>
<div id="ref-Foley1993" class="csl-entry" role="listitem">
Foley, Richard. 1993. <em>Working Without a Net</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Harman1986" class="csl-entry" role="listitem">
Harman, Gilbert. 1986. <em>Change in View</em>. Cambridge, MA: Bradford.
</div>
<div id="ref-Hawthorne2004" class="csl-entry" role="listitem">
Hawthorne, John. 2004. <em>Knowledge and Lotteries</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Hunter1996" class="csl-entry" role="listitem">
Hunter, Daniel. 1996. <span>“On the Relation Between Categorical and Probabilistic Belief.”</span> <em>No<span>û</span>s</em> 30: 75–98. <a href="https://doi.org/10.2307/2216304">https://doi.org/10.2307/2216304</a>.
</div>
<div id="ref-Kaplan1996" class="csl-entry" role="listitem">
Kaplan, Mark. 1996. <em>Decision Theory as Philosophy</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Keynes1921" class="csl-entry" role="listitem">
Keynes, John Maynard. 1921. <em>Treatise on Probability</em>. London: Macmillan.
</div>
<div id="ref-Lewis1994b" class="csl-entry" role="listitem">
Lewis, David. 1994. <span>“Reduction of Mind.”</span> In <em>A Companion to the Philosophy of Mind</em>, edited by Samuel Guttenplan, 412–31. Oxford: Blackwell. <a href="https://doi.org/10.1017/CBO9780511625343.019">https://doi.org/10.1017/CBO9780511625343.019</a>.
</div>
<div id="ref-MaitraANG" class="csl-entry" role="listitem">
Maitra, Ishani. 2010. <span>“Assertion, Norms and Games.”</span> In <em>Assertion: New Philosophical Essays</em>, edited by Jessica Brown and Herman Cappelen, 277–96. Oxford: Oxford University Press.
</div>
<div id="ref-Ryle1949" class="csl-entry" role="listitem">
Ryle, Gilbert. 1949. <em>The Concept of Mind</em>. New York: Barnes; Noble.
</div>
<div id="ref-Stalnaker1984" class="csl-entry" role="listitem">
Stalnaker, Robert. 1984. <em>Inquiry</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Stanley2005-STAKAP" class="csl-entry" role="listitem">
Stanley, Jason. 2005. <em><span class="nocase">Knowledge and Practical Interests</span></em>. Oxford University Press.
</div>
<div id="ref-Weatherson2005-WEATTT" class="csl-entry" role="listitem">
Weatherson, Brian. 2005. <span>“<span>True, Truer, Truest</span>.”</span> <em>Philosophical Studies</em> 123 (1-2): 47–70. <a href="https://doi.org/10.1007/s11098-004-5218-x">https://doi.org/10.1007/s11098-004-5218-x</a>.
</div>
<div id="ref-Williamson1994-WILV" class="csl-entry" role="listitem">
Williamson, Timothy. 1994. <em><span>Vagueness</span></em>. Routledge.
</div>
<div id="ref-Williamson2000-WILKAI" class="csl-entry" role="listitem">
———. 2000. <em><span class="nocase">Knowledge and its Limits</span></em>. Oxford University Press.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
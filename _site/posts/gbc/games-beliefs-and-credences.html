<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.479">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">
<meta name="dcterms.date" content="2016-03-01">
<meta name="description" content="In previous work I’ve defended an interest-relative theory of belief. This paper continues the defence. I have four aims. First, to offer a new kind of reason for being unsatisfied with the simple Lockean reduction of belief to credence. Second, to defend the legitimacy of appealing to credences in a theory of belief. Third, to illustrate the importance of theoretical, as well as practical, interests in an interest-relative account of belief. And finally, to have another try at extending my basic account of belief to cover propositions that are practically and theoretically irrelevant to the agent.">

<title>Online Articles - Brian Weatherson - Games, Beliefs and Credences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://use.typekit.net/uzz2drx.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Online Articles - Brian Weatherson</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://brian.weatherson.org"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/bweatherson.bsky.social"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Games, Beliefs and Credences</h1>
                  <div>
        <div class="description">
          <p>In previous work I’ve defended an interest-relative theory of belief. This paper continues the defence. I have four aims. First, to offer a new kind of reason for being unsatisfied with the simple Lockean reduction of belief to credence. Second, to defend the legitimacy of appealing to credences in a theory of belief. Third, to illustrate the importance of theoretical, as well as practical, interests in an interest-relative account of belief. And finally, to have another try at extending my basic account of belief to cover propositions that are practically and theoretically irrelevant to the agent.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">epistemology</div>
                <div class="quarto-category">interest-relativity</div>
                <div class="quarto-category">games and decisions</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="http://brian.weatherson.org">Brian Weatherson</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Michigan
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 1, 2016</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.1111/phpr.12088">10.1111/phpr.12088</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#playing-games-with-a-lockean" id="toc-playing-games-with-a-lockean" class="nav-link active" data-scroll-target="#playing-games-with-a-lockean"><span class="header-section-number">0.1</span> Playing Games with a Lockean</a></li>
  <li><a href="#holton-on-credence" id="toc-holton-on-credence" class="nav-link" data-scroll-target="#holton-on-credence"><span class="header-section-number">0.2</span> Holton on Credence</a></li>
  <li><a href="#the-power-of-theoretical-interests" id="toc-the-power-of-theoretical-interests" class="nav-link" data-scroll-target="#the-power-of-theoretical-interests"><span class="header-section-number">0.3</span> The Power of Theoretical Interests</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">0.4</span> Conclusion</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="games-beliefs-and-credences.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>In previous work <span class="citation" data-cites="Weatherson2005-WEACWD Weatherson2011-WEADIR Weatherson2012-WEAKBI">(<a href="#ref-Weatherson2005-WEACWD" role="doc-biblioref">Weatherson 2005</a>, <a href="#ref-Weatherson2011-WEADIR" role="doc-biblioref">2011</a>, <a href="#ref-Weatherson2012-WEAKBI" role="doc-biblioref">2012b</a>)</span> I’ve defended an interest-relative theory of belief. This paper continues the defence. I have four aims.</p>
<aside>
Published in <em>Philosophy and Phenomenological Research</em> 92: 209-236.
</aside>
<ol type="1">
<li><p>To offer a new kind of reason for being unsatisfied with the simple Lockean reduction of belief to credence.</p></li>
<li><p>To defend the legitimacy of appealing to credences in a theory of belief.</p></li>
<li><p>To illustrate the importance of theoretical, as well as practical, interests in an interest-relative account of belief.</p></li>
<li><p>To have another try at extending my basic account of belief to cover propositions that are practically and theoretically irrelevant to the agent.</p></li>
</ol>
<p>You’re probably familiar with the following dialectic. We want there to be some systematic connection between credences and beliefs. At first blush, saying that a person believes <span class="math inline">\(p\)</span> and has a very low credence in <span class="math inline">\(p\)</span> isn’t just an accusation of irrationality, it is literally incoherent. The simplest such connection would be a reduction of beliefs to credences. But the simplest reductions don’t work.</p>
<aside>
Image via <a href="https://www.flickr.com/photos/7796992@N08">Meagan</a> via <a href="https://search.creativecommons.org/photos/326f0c20-275b-479b-98e0-aa7945751de6">Creative Commons</a>.
</aside>
<p>If we identify beliefs with credence 1, and take credences to support betting dispositions, then a rational agent will have very few beliefs. There are lots of things that an agent, we would normally say, believes even though she wouldn’t bet on them at absurd odds. Note that this argument doesn’t rely on <em>reducing</em> credences to betting dispositions; as long as credences support the betting dispositions, the argument goes through.</p>
<p>A simple retreat is to the so-called <strong>Lockean thesis</strong>, which holds that to believe that <span class="math inline">\(p\)</span> is to have credence in <span class="math inline">\(p\)</span> greater than some threshold <span class="math inline">\(t\)</span>, where <span class="math inline">\(t &lt; 1\)</span>. Just how the threshold is determined could be a matter of some discretion. Perhaps it is a function of the agent’s situation, or of the person ascribing beliefs to the agent, or to the person evaluating that ascription. Never mind these complexities; assuming all such things are held fixed, the Lockean thesis says that there is a threshold <span class="math inline">\(t\)</span> such that everything with credence above <span class="math inline">\(t\)</span> is believed.</p>
<p>There’s a simple objection to the Lockean thesis. Given some very weak assumptions about the world, it implies that there are plenty of quadruples <span class="math inline">\(\langle S, A, B, A \wedge B \rangle\)</span> such that</p>
<ul>
<li><p><span class="math inline">\(S\)</span> is a rational agent.</p></li>
<li><p><span class="math inline">\(A, B\)</span> and <span class="math inline">\(A \wedge B\)</span> are propositions.</p></li>
<li><p><span class="math inline">\(S\)</span> believes <span class="math inline">\(A\)</span> and believes <span class="math inline">\(B\)</span>.</p></li>
<li><p><span class="math inline">\(S\)</span> does not believe <span class="math inline">\(A \wedge B\)</span>.</p></li>
<li><p><span class="math inline">\(S\)</span> knows that she has all these states, and consciously reflectively endorses them.</p></li>
</ul>
<p>Now one might think, indeed I do think, that such quadruples do not exist at all. But set that objection aside. If the Lockean is correct, these quadruples should be everywhere. That’s because for any <span class="math inline">\(t \in (0, 1)\)</span> you care to pick, quadruples of the form <span class="math inline">\(\langle S, C, D, C \wedge D \rangle\)</span> are very very common.</p>
<ul>
<li><p><span class="math inline">\(S\)</span> is a rational agent.</p></li>
<li><p><span class="math inline">\(C, D\)</span> and <span class="math inline">\(C \wedge D\)</span> are propositions.</p></li>
<li><p><span class="math inline">\(S\)</span>’s credence in <span class="math inline">\(C\)</span> is greater than <span class="math inline">\(t\)</span>, and her credence in <span class="math inline">\(D\)</span> is greater than <span class="math inline">\(t\)</span>.</p></li>
<li><p><span class="math inline">\(S\)</span>’s credence in <span class="math inline">\(C \wedge D\)</span> is less than <span class="math inline">\(t\)</span>.</p></li>
<li><p><span class="math inline">\(S\)</span> knows that she has all these states, and reflectively endorses them.</p></li>
</ul>
<p>The best arguments for the existence of quadruples <span class="math inline">\(\langle S, A, B, A \wedge B \rangle\)</span> are non-constructive existence proofs. David <span class="citation" data-cites="Christensen2005">Christensen (<a href="#ref-Christensen2005" role="doc-biblioref">2005</a>)</span> for instance, argues from the existence of the preface paradox to the existence of these quadruples. I’ve expressed some reservations about that argument in the past <span class="citation" data-cites="Weatherson2005-WEACWD">(<a href="#ref-Weatherson2005-WEACWD" role="doc-biblioref">Weatherson 2005</a>)</span>. But what I want to stress here is that even if these existence proofs work, they don’t really prove what the Lockean needs. They don’t show that quadruples satisfying the constraints we associated with <span class="math inline">\(\langle S, A, B, A \wedge B \rangle\)</span> are just as common as quadruples satisfying the constraints we associated with <span class="math inline">\(\langle S, C, D, C \wedge D \rangle\)</span>, for any <span class="math inline">\(t\)</span>. But if the Lockean were correct, they should be exactly as common.</p>
<p>This kind of consideration pushes some of us, well me in any case, towards an interest-relative account of belief. But I’m going to set that move aside to start by investigating a different objection. This objection holds that the Lockean thesis could not be true, because credence 1 is not <em>sufficient</em> for belief. That is, the Lockean is committed to the thesis known as <em>regularity</em>; that everything left open by belief gets a positive credence. I think regularity is false. That’s hardly news, there are plenty of good arguments against it, though most of these involve cases with some idealisations. Timothy <span class="citation" data-cites="Williamson2007-WILHPI">Williamson (<a href="#ref-Williamson2007-WILHPI" role="doc-biblioref">2007a</a>)</span> has a compelling argument against regularity turning on reflections about a case involving infinite coin flips.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> I’m going to offer a ‘finite’ argument against regularity, which I hope is of independent interest, and from that conclude the Lockean is mistaken. There is a worry that my argument against the Lockean also undermines my preferred positive view, and I’ll suggest an independently motivated patch. I’ll then turn to Richard Holton’s attack on the very notion of credence, which obviously would have repercussions for attempts to understand beliefs in terms of credences were it to succeed. I think it doesn’t succeed, but it does show there are important and underappreciated constraints on a theory of belief. I’ll conclude with a comparison between my preferred interest-relative account of belief, and a recent account suggested by Jacob Ross and Mark Schroeder. The short version of the comparison is that I think there’s less difference between the views than Ross and Schroeder think, though naturally I think what differences there are favour my view.</p>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;If there’s any gap in Williamson’s argument, it is I think at the point where he concludes that any two infinite sequences of coin flips have the same probability of landing all heads. I think that the defender of non-numerical, comparative approaches to probability can deny that with some plausibility. Perhaps the two sequences of coin flips have <em>incomparable</em> probabilities of landing all heads. But this leads us into complications that are irrlevant to this paper, especially since I think it turns out there is a sound Williamsonian argument against the Lockean who lets different sequences have incomparable probabilities. For a more pessimistic take on Williamson’s argument, see <span class="citation" data-cites="Weintraub2008">Weintraub (<a href="#ref-Weintraub2008" role="doc-biblioref">2008</a>)</span>.</p></li></div><section id="playing-games-with-a-lockean" class="level3 page-columns page-full" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="playing-games-with-a-lockean"><span class="header-section-number">0.1</span> Playing Games with a Lockean</h3>
<p>I’m going to raise problems for Lockeans, and for defenders of regularity in general, by discussing a simple game. The game itself is a nice illustration of how a number of distinct solution concepts in game theory come apart. (Indeed, the use I’ll make of it isn’t a million miles from the use that <span class="citation" data-cites="KohlbergMertens1986">Kohlberg and Mertens (<a href="#ref-KohlbergMertens1986" role="doc-biblioref">1986</a>)</span> make of it.) To set the problem up, I need to say a few words about how I think of game theory. This won’t be at all original - most of what I say is taken from important works by Robert <span class="citation" data-cites="Stalnaker1994 Stalnaker1996 Stalnaker1998 Stalnaker1999">Stalnaker (<a href="#ref-Stalnaker1994" role="doc-biblioref">1994</a>, <a href="#ref-Stalnaker1996" role="doc-biblioref">1996</a>, <a href="#ref-Stalnaker1998" role="doc-biblioref">1998</a>, <a href="#ref-Stalnaker1999" role="doc-biblioref">1999</a>)</span>. But it is different to what I used to think, and perhaps to what some other people think too, so I’ll set it out slowly.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;I’m grateful to the participants in a game theory seminar at Arché in 2011, especially Josh Dever and Levi Spectre, for very helpful discussions that helped me see through my previous confusions.</p></li></div><p>Start with a simple decision problem, where the agent has a choice between two acts <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>, and there are two possible states of the world, <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>, and the agent knows the payouts for each act-state pair are given by the following able.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><span class="math inline">\(S_1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(S_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_1\)</span></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_2\)</span></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
</div>
<p>What to do? I hope you share the intuition that it is radically underdetermined by the information I’ve given you so far. If <span class="math inline">\(S_2\)</span> is much more probable than <span class="math inline">\(S_1\)</span>, then <span class="math inline">\(A_2\)</span> should be chosen; otherwise <span class="math inline">\(A_1\)</span> should be chosen. But I haven’t said anything about the relative probability of those two states. Now compare that to a simple game. Row has two choices, which I’ll call <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>. Column also has two choices, which I’ll call <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>. It is common knowledge that each player is rational, and that the payouts for the pairs of choices are given in the following table. (As always, Row’s payouts are given first.)</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><span class="math inline">\(S_1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(S_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_1\)</span></td>
<td style="text-align: center;">4, 0</td>
<td style="text-align: center;">0, 1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_2\)</span></td>
<td style="text-align: center;">1, 0</td>
<td style="text-align: center;">1, 1</td>
</tr>
</tbody>
</table>
</div>
<p>What should Row do? This one is easy. Column gets 1 for sure if she plays <span class="math inline">\(S_2\)</span>, and 0 for sure if she plays <span class="math inline">\(S_1\)</span>. So she’ll play <span class="math inline">\(S_2\)</span>. And given that she’s playing <span class="math inline">\(S_2\)</span>, it is best for Row to play <span class="math inline">\(A_2\)</span>.</p>
<p>You probably noticed that the game is just a version of the decision problem that we discussed a couple of paragraphs ago. The relevant states of the world are choices of Column. But that’s fine; we didn’t say in setting out the decision problem what constituted the states <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>. And note that we solved the problem without explicitly saying anything about probabilities. What we added was some information about Column’s payouts, and the fact that Column is rational. From there we deduced something about Column’s play, namely that she would play <span class="math inline">\(S_2\)</span>. And from that we concluded what Row should do.</p>
<p>There’s something quite general about this example. What’s distinctive about game theory isn’t that it involves any special kinds of decision making. Once we get the probabilities of each move by the other player, what’s left is (mostly) expected utility maximisation. (We’ll come back to whether the ‘mostly’ qualification is needed below.) The distinctive thing about game theory is that the probabilities aren’t specified in the setup of the game; rather, they are solved for. Apart from special cases, such as where one option strictly dominates another, we can’t say much about a decision problem with unspecified probabilities. But we can and do say a lot about games where the setup of the game doesn’t specify the probabilities, because we can solve for them given the other information we have.</p>
<p>This way of thinking about games makes the description of game theory as ‘interactive epistemology’ <span class="citation" data-cites="Aumann1999">(<a href="#ref-Aumann1999" role="doc-biblioref">Aumann 1999</a>)</span> rather apt. The theorist’s work is to solve for what a rational agent should think other rational agents in the game should do. From this perspective, it isn’t surprising that game theory will make heavy use of equilibrium concepts. In solving a game, we must deploy a theory of rationality, and attribute that theory to rational actors in the game itself. In effect, we are treating rationality as something of an unknown, but one that occurs in every equation we have to work with. Not surprisingly, there are going to be multiple solutions to the puzzles we face.</p>
<p>This way of thinking lends itself to an epistemological interpretation of one of the most puzzling concepts in game theory, the mixed strategy. Arguably the core solution concept in game theory is the Nash equilibrium. As you probably know, a set of moves is a Nash equilibrium if no player can improve their outcome by deviating from the equilibrium, conditional on no other player deviating. In many simple games, the only Nash equilibria involve mixed strategies. Here’s one simple example.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><span class="math inline">\(S_1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(S_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_1\)</span></td>
<td style="text-align: center;">0, 1</td>
<td style="text-align: center;">10, 0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_2\)</span></td>
<td style="text-align: center;">9, 0</td>
<td style="text-align: center;">-1, 1</td>
</tr>
</tbody>
</table>
</div>
<p>This game is reminiscent of some puzzles that have been much discussed in the decision theory literature, namely asymmetric Death in Damascus puzzles. Here Column wants herself and Row to make the ‘same’ choice, i.e., <span class="math inline">\(A_1\)</span> and <span class="math inline">\(S_1\)</span> or <span class="math inline">\(A_2\)</span> and <span class="math inline">\(S_2\)</span>. She gets 1 if they do, 0 otherwise. And Row wants them to make different choices, and gets 10 if they do. Row also dislikes playing <span class="math inline">\(A_2\)</span>, and this costs her 1 whatever else happens. It isn’t too hard to prove that the only Nash equilibrium for this game is that Row plays a mixed strategy playing both <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> with probability , while Column plays the mixed strategy that gives <span class="math inline">\(S_1\)</span> probability , and <span class="math inline">\(S_2\)</span> with probability .</p>
<p>Now what is a mixed strategy? It is easy enough to take away form the standard game theory textbooks a <strong>metaphysical</strong> interpretation of what a mixed strategy is. Here, for instance, is the paragraph introducing mixed strategies in Dixit and Skeath’s <em>Games of Strategy</em>.</p>
<blockquote class="blockquote">
<p>When players choose to act unsystematically, they pick from among their pure strategies in some random way …We call a random mixture between these two pure strategies a mixed strategy. <span class="citation" data-cites="DixitSkeath2004">(<a href="#ref-DixitSkeath2004" role="doc-biblioref">Dixit and Skeath 2004, 186</a>)</span></p>
</blockquote>
<p>Dixit and Skeath are saying that it is definitive of a mixed strategy that players use some kind of randomisation device to pick their plays on any particular run of a game. That is, the probabilities in a mixed strategy must be in the world; they must go into the players’ choice of play. That’s one way, the paradigm way really, that we can think of mixed strategies metaphysically.</p>
<p>But the understanding of game theory as interactive epistemology naturally suggests an <strong>epistemological</strong> interpretation of mixed strategies.</p>
<blockquote class="blockquote">
<p>One could easily …[model players] …turning the choice over to a randomizing device, but while it might be harmless to permit this, players satisfying the cognitive idealizations that game theory and decision theory make could have no motive for playing a mixed strategy. So how are we to understand Nash equilibrium in model theoretic terms as a solution concept? We should follow the suggestion of Bayesian game theorists, interpreting mixed strategy profiles as representations, not of players’ choices, but of their beliefs. <span class="citation" data-cites="Stalnaker1994">(<a href="#ref-Stalnaker1994" role="doc-biblioref">Stalnaker 1994, 57–58</a>)</span></p>
</blockquote>
<p>One nice advantage of the epistemological interpretation, as noted by Binmore <span class="citation" data-cites="Binmore2007">(<a href="#ref-Binmore2007" role="doc-biblioref">2007, 185</a>)</span> is that we don’t require players to have <span class="math inline">\(n\)</span>-sided dice in their satchels, for every <span class="math inline">\(n\)</span>, every time they play a game.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> But another advantage is that it lets us make sense of the difference between playing a pure strategy and playing a mixed strategy where one of the ‘parts’ of the mixture is played with probability one.</p>
<div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;Actually, I guess it is worse than if some games have the only equilibria involving mixed strategies with irrational probabilities. And it might be noted that Binmore’s introduction of mixed strategies, on page 44 of his <span class="citation" data-cites="Binmore2007">(<a href="#ref-Binmore2007" role="doc-biblioref">2007</a>)</span>, sounds much more like the metaphysical interpretation. But I think the later discussion is meant to indicate that this is just a heuristic introduction; the epistemological interpretation is the correct one.</p></li></div><p>With that in mind, consider the below game, which I’ll call Red-Green. I’ve said something different about this game in earlier work <span class="citation" data-cites="Weatherson2012-WEAGAT">(<a href="#ref-Weatherson2012-WEAGAT" role="doc-biblioref">Weatherson 2012a</a>)</span>. But I now think that to understand what’s going on, we need to think about mixed strategies where one element of the mixture has probability one.</p>
<p>Informally, in this game <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> must each play either a green or red card. I will capitalise <span class="math inline">\(A\)</span>’s moves, i.e., <span class="math inline">\(A\)</span> can play GREEN or RED, and italicise <span class="math inline">\(B\)</span>’s moves, i.e., <span class="math inline">\(B\)</span> can play <em>green</em> or <em>red</em>. If two green cards, or one green card and one red card are played, each player gets $1. If two red cards are played, each gets nothing. Each cares just about their own wealth, so getting $1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with <span class="math inline">\(A\)</span> on the row and <span class="math inline">\(B\)</span> on the column.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><em>green</em></td>
<td style="text-align: center;"><em>red</em></td>
</tr>
<tr class="even">
<td>GREEN</td>
<td style="text-align: center;">1, 1</td>
<td style="text-align: center;">1, 1</td>
</tr>
<tr class="odd">
<td>RED</td>
<td style="text-align: center;">1, 1</td>
<td style="text-align: center;">0, 0</td>
</tr>
</tbody>
</table>
</div>
<p>When I write game tables like this, and I think this is the usual way game tables are to be interpreted <span class="citation" data-cites="Weatherson2012-WEAKBI">(<a href="#ref-Weatherson2012-WEAKBI" role="doc-biblioref">Weatherson 2012b</a>)</span>, I mean that the players know that these are the payouts, that the players know the other players to be rational, and these pieces of knowledge are common knowledge to at least as many iterations as needed to solve the game. With that in mind, let’s think about how the agents should approach this game.</p>
<p>I’m going to make one big simplifying assumption at first. We’ll relax this later, but it will help the discussion to start with this assumption. This assumption is that the doctrine of <strong>Uniqueness</strong> applies here; there is precisely one rational credence to have in any salient proposition about how the game will play. Some philosophers think that Uniqueness always holds <span class="citation" data-cites="White2005-WHIEP">(<a href="#ref-White2005-WHIEP" role="doc-biblioref">White 2005</a>)</span>. I join with those such as <span class="citation" data-cites="North2010">North (<a href="#ref-North2010" role="doc-biblioref">2010</a>)</span> and <span class="citation" data-cites="Schoenfield2013">Schoenfield (<a href="#ref-Schoenfield2013" role="doc-biblioref">2013</a>)</span> who don’t. But it does seem like Uniqueness might <em>often</em> hold; there might often be a right answer to a particular problem. Anyway, I’m going to start by assuming that it does hold here.</p>
<p>The first thing to note about the game is that it is symmetric. So the probability of <span class="math inline">\(A\)</span> playing GREEN should be the same as the probability of <span class="math inline">\(B\)</span> playing <em>green</em>, since <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> face exactly the same problem. Call this common probability <span class="math inline">\(x\)</span>. If <span class="math inline">\(x &lt; 1\)</span>, we get a quick contradiction. The expected value, to Row, of GREEN, is 1. Indeed, the known value of GREEN is 1. If the probability of <em>green</em> is <span class="math inline">\(x\)</span>, then the expected value of RED is <span class="math inline">\(x\)</span>. So if <span class="math inline">\(x &lt; 1\)</span>, and Row is rational, she’ll definitely play GREEN. But that’s inconsistent with the claim that <span class="math inline">\(x &lt; 1\)</span>, since that means that it isn’t definite that Row will play GREEN.</p>
<p>So we can conclude that <span class="math inline">\(x = 1\)</span>. Does that mean we can know that Row will play GREEN? No.&nbsp;Assume we could conclude that. Whatever reason we would have for concluding that would be a reason for any rational person to conclude that Column will play <em>green</em>. Since any rational person can conclude this, Row can conclude it. So Row knows that she’ll get 1 whether she plays GREEN or RED. But then she should be indifferent between playing GREEN and RED. And if we know she’s indifferent between playing GREEN and RED, and our only evidence for what she’ll play is that she’s a rational player who’ll maximise her returns, then we can’t be in a position to know she’ll play GREEN.</p>
<p>I think the arguments of the last two paragraphs are sound. We’ll turn to an objection presently, but let’s note how bizarre is the conclusion we’ve reached. One argument has shown that it could not be more probable that Row will play GREEN. A second argument has shown that we can’t know that Row will play GREEN. It reminds me of examples involving blindspots <span class="citation" data-cites="Sorensen1988">(<a href="#ref-Sorensen1988" role="doc-biblioref">Sorensen 1988</a>)</span>. Consider this case:</p>
<ol start="2" type="A">
<li>Brian does not know (B).</li>
</ol>
<p>That’s true, right? Assume it’s false, so I do know (B). Knowledge is factive, so (B) is true. But that contradicts the assumption that it’s false. So it’s true. But I obviously don’t know that it’s true; that’s what this very true proposition says.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;It’s received wisdom in philosophy that one can never properly say something of the form <em>p, but I don’t know that p</em>. This is used as a data point in views as far removed from each other as those defended in <span class="citation" data-cites="Heal1994">Heal (<a href="#ref-Heal1994" role="doc-biblioref">1994</a>)</span> and <span class="citation" data-cites="Williamson1996-WILKAA">Williamson (<a href="#ref-Williamson1996-WILKAA" role="doc-biblioref">1996</a>)</span>. But I don’t feel the force of this alleged datum at all, and (B) is just one reason. For a different kind of case that makes the same point, see <span class="citation" data-cites="MaitraWeatherson">Maitra and Weatherson (<a href="#ref-MaitraWeatherson" role="doc-biblioref">2010</a>)</span>.</p></li><li id="fn5"><p><sup>5</sup>&nbsp;As an aside, the existence of these cases is why I get so irritated when epistemologists try to theorise about ‘Gettier Cases’ as a class. What does (B) have in common with inferences from a justified false belief, or with otherwise sound reasoning that is ever so close to issuing in a false conclusion due to relatively bad luck? As far as I can tell, the class of justified true beliefs that aren’t knowledge is a disjunctive mess, and this should matter for thinking about the nature of knowledge. For further examples, see <span class="citation" data-cites="WilliamsonLofoten">Williamson (<a href="#ref-WilliamsonLofoten" role="doc-biblioref">2013</a>)</span> and <span class="citation" data-cites="Nagel2013-Williamson">Nagel (<a href="#ref-Nagel2013-Williamson" role="doc-biblioref">2013</a>)</span>.</p></li></div><p>Now I’m not going to rest anything on this case, because there are so many tricky things one can say about blindspots, and about the paradoxes generally. It does suggest that there are other finite cases where one can properly have maximal credence in a true proposition without knowledge.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> And, assuming that we shouldn’t believe things we know we don’t know, that means we can have maximal credence in things we don’t believe. All I want to point out is that this phenomena of maximal credence without knowledge, and presumably without full belief, isn’t a quirky feature of self-reference, or of games, or of puzzles about infinity; it comes up in a wide range of cases.</p>
<p>For the rest of this section I want to reply to one objection, and weaken an assumption I made earlier. The objection is that I’m wrong to assume that agents will only maximise expected utility. They may have tie-breaker rules, and those rules might undermine the arguments I gave above. The assumption is that there’s a uniquely rational credence to have in any given situation.</p>
<p>I argued that if we knew that <span class="math inline">\(A\)</span> would play GREEN, we could show that <span class="math inline">\(A\)</span> had no reason to play GREEN. But actually what we showed was that the expected utility of playing GREEN would be the same as playing RED. Perhaps <span class="math inline">\(A\)</span> has a reason to play GREEN, namely that GREEN weakly dominates RED. After all, there’s one possibility on the table where GREEN does better than RED, and none where RED does better. And perhaps that’s a reason, even if it isn’t a reason that expected utility considerations are sensitive to.</p>
<p>Now I don’t want to insist on expected utility maximisation as the only rule for rational decision making. Sometimes, I think some kind of tie-breaker procedure is part of rationality. In the papers by Stalnaker I mentioned above, he often appeals to this kind of weak dominance reasoning to resolve various hard cases. But I don’t think weak dominance provides a reason to play GREEN in this particular case. When Stalnaker says that agents should use weak dominance reasoning, it is always in the context of games where the agents’ attitude towards the game matrix is different to their attitude towards each other. One case that Stalnaker discusses in detail is where the game table is common knowledge, but there is merely common (justified, true) belief in common rationality. Given such a difference in attitudes, it does seem there’s a good sense in which the most salient departure from equilibrium will be one in which the players end up somewhere else on the table. And given that, weak dominance reasoning seems appropriate.</p>
<p>But that’s not what we’ve got here. Assuming that rationality requires playing GREEN/<em>green</em>, the players know we’ll end up in the top left corner of the table. There’s no chance that we’ll end up elsewhere. Or, perhaps better, there is just as much chance we’ll end up ‘off the table’, as that we’ll end up in a non-equilibrium point on the table. To make this more vivid, consider the ‘possibility’ that <span class="math inline">\(B\)</span> will play <em>blue</em>, and if <span class="math inline">\(B\)</span> plays <em>blue</em>, <span class="math inline">\(A\)</span> will receive 2 if she plays RED, and -1 if she plays GREEN. Well hold on, you might think, didn’t I say that <em>green</em> and <em>red</em> were the only options, and this was common knowledge? Well, yes, I did, but if the exercise is to consider what would happen if something the agent knows to be true doesn’t obtain, then the possibility that one agent will play blue certainly seems like one worth considering. It is, after all, a metaphysical possibility. And if we take it seriously, then it isn’t true that under <em>any</em> possible play of the game, GREEN does better than RED.</p>
<p>We can put this as a dilemma. Assume, for <em>reductio</em>, that GREEN/<em>green</em> is the only rational play. Then if we restrict our attention to possibilities that are epistemically open to <span class="math inline">\(A\)</span>, then GREEN does just as well as RED; they both get 1 in every possibility. If we allow possibilities that are epistemically closed to <span class="math inline">\(A\)</span>, then the possibility where <span class="math inline">\(B\)</span> plays <em>blue</em> is just as relevant as the possibility that <span class="math inline">\(B\)</span> is irrational. After all, we stipulated that this is a case where rationality is common knowledge. In neither case does the weak dominance reasoning get any purchase.</p>
<p>With that in mind, we can see why we don’t need the assumption of Uniqueness. Let’s play through how a failure of Uniqueness could undermine the argument. Assume, again for <strong>reductio</strong>, that we have credence <span class="math inline">\(\varepsilon &gt; 0\)</span> that <span class="math inline">\(A\)</span> will play RED. Since <span class="math inline">\(A\)</span> maximises expected utility, that means <span class="math inline">\(A\)</span> must have credence 1 that <span class="math inline">\(B\)</span> will play <em>green</em>. But this is already odd. Even if you think people can have different reactions to the same evidence, it is odd to think that one rational agent could regard a possibility as infinitely less likely than another, given isomorphic evidence. And that’s not all of the problems. Even if <span class="math inline">\(A\)</span> has credence 1 that <span class="math inline">\(B\)</span> will play <em>green</em>, it isn’t obvious that playing RED is rational. After all, relative to the space of epistemic possibilities, GREEN weakly dominates RED. Remember that we’re no longer assuming that it can be known what <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> will play. So even without Uniqueness, there are two reasons to think that it is wrong to have credence <span class="math inline">\(\varepsilon &gt; 0\)</span> that <span class="math inline">\(A\)</span> will play RED. So we’ve still shown that credence 1 doesn’t imply knowledge, and since the proof is known to us, and full belief is incompatible with knowing that you can’t know, this is a case where credence 1 doesn’t imply full belief. So whether <span class="math inline">\(A\)</span> plays GREEN, like whether the coin will ever land tails, is a case the Lockean cannot get right. No matter where they set the threshold for belief our credence is above that threshold, but we don’t believe.</p>
<p>So I think this case is a real problem for a Lockean view about the relationship between credence and belief. If <em>A</em> is rational, she can have credence 1 that <em>B</em> will play <em>green</em>, but won’t believe that <em>B</em> will play <em>green</em>. But now you might worry that my own account of the relationship between belief and credence is in just as much trouble. After all, I said that to believe <span class="math inline">\(p\)</span> is, roughly, to have the same attitudes towards all salient questions as you have conditional on <span class="math inline">\(p\)</span>. And it’s hard to identify a question that rational <em>A</em> would answer differently upon conditionalising on the proposition that <em>B</em> plays <em>green</em>.</p>
<p>I think what went wrong in my earlier view was that I’d too quickly equated updating with conditionalisation. The two can come apart. Here’s an example from <span class="citation" data-cites="Gillies2010">Gillies (<a href="#ref-Gillies2010" role="doc-biblioref">2010</a>)</span> that makes the point well.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;A similar example is in Kratzer <span class="citation" data-cites="Kratzer2012">(<a href="#ref-Kratzer2012" role="doc-biblioref">2012, 94</a>)</span>.</p></li></div><blockquote class="blockquote">
<p>I have lost my marbles. I know that just one of them – Red or Yellow – is in the box. But I don’t know which. I find myself saying things like …“If Yellow isn’t in the box, the Red must be.” (4:13)</p>
</blockquote>
<p>As Gillies goes on to point out, this isn’t really a problem for the Ramsey test view of conditionals.</p>
<blockquote class="blockquote">
<p>The Ramsey test – the schoolyard version, anyway – is a test for when an indicative conditional is acceptable given your beliefs. It says that (if <em>p</em>)(<em>q</em>) is acceptable in belief state <em>B</em> iff <em>q</em> is acceptable in the derived or subordinate state <em>B</em>-plus-the-information-that-<em>p</em>. (4:27)</p>
</blockquote>
<p>And he notes that this can explain what goes on with the marbles conditional. Add the information that Yellow isn’t in the box, and it isn’t just true, but must be true, that Red is in the box.</p>
<p>Note though that while we can explain this conditional using the Ramsey test, we can’t explain it using any version of the idea that probabilities of conditionals are conditional probabilities. The probability that Red must be in the box is 0. The probability that Yellow isn’t in the box is not 0. So conditional on Yellow not being in the box, the probability that Red must be in the box is still 0. Yet the conditional is perfectly assertable.</p>
<p>There is, and this is Gillies’s key point, something about the behaviour of modals in the consequents of conditionals that we can’t capture using conditional probabilities, or indeed many other standard tools. And what goes for consequents of conditionals goes for updated beliefs too. Learn that Yellow isn’t in the box, and you’ll conclude that Red must be. But that learning can’t go via conditionalisation; just conditionalise on the new information and the probability that Red must be in the box goes from 0 to 0.</p>
<p>Now it’s a hard problem to say exactly how this alternative to updating by conditionalisation should work. But very roughly, the idea is that at least some of the time, we update by eliminating worlds from the space of possibilities. This affects dramatically the probability of propositions whose truth is sensitive to which worlds are in the space of possibiilties.</p>
<p>For example, in the game I’ve been discussing, we should believe that rational <em>B</em> might play <em>red</em>. Indeed, the probability of that is, I think, 1. And whether or not <em>B</em> might play red is highly salient; it matters to the probability of whether <em>A</em> will play GREEN or RED. Conditionalising on something that has probability 1, such as that <em>B</em> will play <em>green</em>, can hardly change that probability. But updating on the proposition that <em>B</em> will play <em>green</em> can make a difference. We can see that by simply noting that the conditional <em>If B plays green, she might play red</em> is incoherent.</p>
<p>So I conclude that a theory of belief like mine can handle the puzzle this game poses, as long as it distinguishes between conditionalising and updating, in just the way Gillies suggests. To believe that <em>p</em> is to be disposed to not change any attitude towards a salient question on updating that <em>p</em>. (Plus some bells and whistles to deal with propositions that are not relevant to salient questions. We’ll return to them below.) Updating often goes by conditionalisation, so we can often say that belief means having attitudes that match unconditionally and conditionally on <em>p</em>. But not all updating works that way, and the theory of belief needs to acknowledge this.</p>
</section>
<section id="holton-on-credence" class="level3 page-columns page-full" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="holton-on-credence"><span class="header-section-number">0.2</span> Holton on Credence</h3>
<p>While I don’t agree with the Lockeans, I do endorse a lot of similar theses to them about the relationship between belief and credence. These theses include that both beliefs and credences exist and that the two are constitutively (as opposed to merely causally) connected. I differ from the Lockeans in holding that both belief and credence have important explanatory roles, and that the connection between the two goes via the interests of the agent. As with most work in this area, my views start off from considerations of cases much like DeRose’s famous bank cases.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> Here’s another contribution to the genre. I know it’s an overcrowded field, but I wanted a case that (a) is pretty realistic, and (b) doesn’t involve the attribution (either to oneself or others) of a propositional attitude. In the example, X and Y are parents of a child, Z.</p>
<div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;The idea of using allergies to illustrate the kind of case we’re interested in is due to <span class="citation" data-cites="SchroederRoss2012">Ross and Schroeder (<a href="#ref-SchroederRoss2012" role="doc-biblioref">2014</a>)</span>, and I’m grateful for the idea. It makes the intuitions much more vivid. The kind of cases we’re considering play a big role in, <em>inter alia</em>, <span class="citation" data-cites="DeRose1992 Cohen1999">DeRose (<a href="#ref-DeRose1992" role="doc-biblioref">1992</a>; <a href="#ref-Cohen1999" role="doc-biblioref">Cohen 1999</a>)</span> and <span class="citation" data-cites="Fantl2002">Fantl and McGrath (<a href="#ref-Fantl2002" role="doc-biblioref">2002</a>)</span>.</p></li></div><blockquote class="blockquote">
<p>Y: This salad you bought is very good. Does it have nuts in it?<br>
X: No.&nbsp;The nuttiness you’re tasting is probably from the beans.<br>
Y: Oh, so we could pack it for Z’s lunch tomorrow.<br>
X: Hang on, I better check about the nuts. Z’s pre-school is very fussy about nuts. One of the children there might have an allergy, and it would be awful to get in trouble over her lunch.</p>
</blockquote>
<p>Here’s what I think is going on in that exchange.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> At <span class="math inline">\(t_2\)</span> (I’ll use <span class="math inline">\(t_i\)</span> for the time of the <span class="math inline">\(i\)</span>’th utterance in the exchange), X believes that the salad has no nuts in it. Indeed, the one word sentence “No” expresses that belief. But by <span class="math inline">\(t_4\)</span>, X has lost that belief. It would be fine to pack the salad for lunch if it has no nuts, but X isn’t willing to do this for the simple reason that X no longer believes that it has no nuts. Moreover, this change of belief was, or at least could have been for all we’ve said so far, rational on X’s part.</p>
<div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;What I say here obviously has some similarities to a view put forward by Jennifer <span class="citation" data-cites="Nagel2008">Nagel (<a href="#ref-Nagel2008" role="doc-biblioref">2008</a>)</span>, but I ultimately end up drawing rather different conclusions to the ones she draws.</p></li></div><p>There’s something a little puzzling about that. Jacob Ross and Mark Schroeder <span class="citation" data-cites="SchroederRoss2012">(<a href="#ref-SchroederRoss2012" role="doc-biblioref">2014</a>)</span> voice a common intuition when they say that beliefs should only change when new evidence comes in. Indeed, they use this intuition as a key argument against my view of belief. But X doesn’t get any evidence that bears on the nuttiness of the salad. Yet X rationally changes beliefs. So I just conclude that sometimes we can change beliefs without new evidence coming in; sometimes our interests, broadly construed, change, and that is enough to change beliefs.</p>
<p>We’ll come back to Ross and Schroeder’s arguments in the next section, because first I want to concede something to the view that only evidence changes beliefs. That view is false, but there might be a true view in the area. And that’s the view that only change in evidence can change <em>credences</em>. But that view only makes sense if there are such things as credences, and that’s something that Richard <span class="citation" data-cites="Holton2013">Holton (<a href="#ref-Holton2013" role="doc-biblioref">2014</a>)</span> has recently launched an intriguing argument against.</p>
<p>Holton’s broader project is a much more sweeping attack on the Lockean thesis than I have proposed. Actually, it is a pair of more sweeping attacks. One of the pair is that the Lockeans identify something that exists, namely belief, with something that doesn’t, namely high credence. I would not, could not, sign up for that critique. But I am much more sympathetic to the other attack in the pair, namely that credences and beliefs have very different dynamics.</p>
<p>Credences are, by their nature, exceedingly unstable. Whether an agent’s credence that <span class="math inline">\(p\)</span> is above or below any number <em>x</em> is liable to change according to any number of possible changes in evidence. But, at least if the agent is rational, beliefs are not so susceptible to change. Holton thinks that rational agents, or at least rational humans, frequently instantiate the following pattern. They form a belief that <span class="math inline">\(p\)</span>, on excellent grounds. They later get some evidence that <span class="math inline">\(\neg p\)</span>. The evidence is strong enough that, had they had it to begin with, they would have remained uncertain about <span class="math inline">\(p\)</span>. But they do not decide to reopen the investigation into whether <span class="math inline">\(p\)</span>. They hold on to their belief that <span class="math inline">\(p\)</span>, the matter having been previously decided.</p>
<p>Such an attitude might look like unprincipled dogmatism. But it need not be, I think, as long as four criteria are met. (I think Holton agrees with these criteria.) One is that the agent’s willingness to reopen the question of whether <span class="math inline">\(p\)</span> must increase. She must be more willing, in the light of yet more evidence against <span class="math inline">\(p\)</span>, to consider whether <span class="math inline">\(p\)</span> is really true. A second is that, should the agent (irrationally) reopen the question of whether <span class="math inline">\(p\)</span>, she should not use the fact that she previously closed that question as evidence. Once the genie is out of the box, only reasoning about <span class="math inline">\(p\)</span> can get it back in. A third is that the costs of the inquiry must be high enough to warrant putting it off. If simply turning one’s head fifteen degrees to the left will lead to acquiring evidence that definitively settles whether <span class="math inline">\(p\)</span>, it is a little dogmatic to refuse to do so in the face of evidence against one’s previously formed opinion that <span class="math inline">\(p\)</span>. And finally, the costs of being wrong about <span class="math inline">\(p\)</span> must not be too high. X, in our little dialogue above, would be terribly dogmatic if they didn’t reopen the question of whether the salad had nuts in it, on being informed that this information was being used in a high stakes inquiry.</p>
<p>So beliefs should have a kind of resilience. Credences, if they exist, should not have this kind of resilience. So this suggests that a simple reduction of belief to credence, as the Lockeans suggest, cannot be right. You might worry that things are worse, that no reduction of belief to credence can be compatible with the difference in resilience between belief and credence. We’ll return to that point, because first I want to look at Holton’s stronger claim: that there are no such things as credences.</p>
<p>Holton acknowledges, as of course he must, that we have probabilistic truth-directed attitudes. We can imagine a person, call her Paula, who thinks it’s likely that Richard III murdered his nephews, for instance. But Holton offers several reasons for thinking that in these probabilistic truth-directed attitudes, the probability goes in the content, not in the attitude. That is, we should interpret Paula as believing the probabilistic claim, Richard III probably murdered his nephews, and not as having some graded attitude towards the simple proposition <em>Richard III murdered his nephews</em>. More precisely, Holton thinks we should understand Paula’s <em>explicit</em> attitudes that way, and that independent of having reason to think that agents explicitly have probabilistic attitudes, there’s no good way to make sense of the claim that they implicitly have probabilistic attitudes. So there’s no such thing as credences, as usually understood. Or, at least, there’s no good sense to be made of the claim that there are credences.</p>
<p>In response, I want to make six points.</p>
<ol type="1">
<li><p>Holton is right about cases like Paula’s, and the possibility of iterating terms like <em>probably</em> provides independent support for this view.</p></li>
<li><p>Beliefs like the one Paula has are odd; they seem to have very strange truth conditions.</p></li>
<li><p>Our theory of mind needs some mechanism for explaining the relationship between confidence and action.</p></li>
<li><p>The ‘explanatory gap’ here could be filled by positing a binary attitude <em>is more confident that</em>.</p></li>
<li><p>This binary attitude can do all the work that graded attitudes were posited to do, and in a (historically sensitive) way saves the credences story.</p></li>
<li><p>Credences (or at least confidences) can have a key role within a Holton-like story about graded belief. They can both explain why agents reconsider some beliefs, and provide a standard of correctness for decisions to reconsider.</p></li>
</ol>
<p>Let’s take those in order.</p>
<p>I’m not going to rehearse Holton’s argument for the ‘content view’: that in cases like Paula’s the content of her attitude, and not the attitude itself, is probabilistic. But I do want to offer one extra consideration in its favour. (I’m indebted here to work in progress by my colleague Sarah <span class="citation" data-cites="MossPragmaticsEpistemicModals">Moss (<a href="#ref-MossPragmaticsEpistemicModals" role="doc-biblioref">2015</a>)</span>, though I’m not sure she’d approve of this conclusion.) As well as Paula, we can imagine a person Pip who isn’t sure that Paula is right, but thinks she’s probably right. That is, Pip thinks that Richard III probably probably murdered his nephews. It’s easy to make sense of Pip on the content view. Modalities in propositions iterate smoothly; that’s what they are designed to do. But it’s much harder to iterate attitudes. The possibility of cases like Pip suggests Holton must be right about Paula’s case.</p>
<p>But Paula’s case is odd. Beliefs have truth conditions. What are the truth conditions for Paula’s belief? On the one hand, it seems they must be sensitive to her evidence. If she later acquires conclusive evidence that Richard III was framed, she won’t think her earlier self had a false belief. But if we put the evidence into the content of the belief, we get the strange result that her belief can’t be preserved by uttering the same words to herself over again. That is, if the content of Paula’s belief is <em>Given the evidence I have now, Richard III likely murdered his nephews</em>, she can’t have the very same belief tomorrow by retaining the thought <em>Richard III likely murdered his nephews</em>. And she can’t have a belief with the same content as anyone else by the two of them both thinking <em>Richard III likely murdered his nephews</em>. Those seem like unhappy conclusions, especially in the midst of a project that wants to emphasise the resiliency of belief. So perhaps we should say, following <span class="citation" data-cites="Stephenson2007">Stephenson (<a href="#ref-Stephenson2007" role="doc-biblioref">2007</a>)</span> or <span class="citation" data-cites="MacFarlane2011">MacFarlane (<a href="#ref-MacFarlane2011" role="doc-biblioref">2011</a>)</span>, that the truth conditions of the belief are agent-relative. Or, if we’re unhappy with the MacFarlane story, we might be pushed towards a kind of expressivism (perhaps a la <span class="citation" data-cites="Yalcin2011">Yalcin (<a href="#ref-Yalcin2011" role="doc-biblioref">2011</a>)</span>), which isn’t quite like either the content view or the attitude view that Holton discusses. I’m personally partial to the relativist view, but I don’t want to argue for that here, just note that the content view raises some interesting problems, and that natural solutions to them could in a way blur the boundaries between the content and attitude views.</p>
<p>As Holton notes in his discussion of Brutus, when our confidence in a proposition changes, our actions will change. Paula gets a little evidence that Richard III was framed, and her actions may change. Of course, not much of what we do in everyday life is sensitive to facts about English royal history, but there may be some effects. Maybe she’ll be less inclined to speak up if the topic of the princes’ murder comes up, or she’ll take a slightly more jaundiced view of Shakespeare’s play (compare <span class="citation" data-cites="Friend2003">Friend (<a href="#ref-Friend2003" role="doc-biblioref">2003</a>)</span>.) Holton says that these falling confidences need not have all the precise structure of credences. In particular, they may not have the topology of the interval <span class="math inline">\([0, 1]\)</span>. But lots of credence lovers think that’s too demanding. There’s a long tradition of thinking that credences need not all be comparable.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> What’s important is that the relative confidences exist, and that they have a robust relationship to action.</p>
<div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;Notable members of the tradition include <span class="citation" data-cites="Levi1974">Levi (<a href="#ref-Levi1974" role="doc-biblioref">1974</a>)</span>, <span class="citation" data-cites="Jeffrey1983">Jeffrey (<a href="#ref-Jeffrey1983" role="doc-biblioref">1983</a>)</span> and <span class="citation" data-cites="vanFraassen1989">Fraassen (<a href="#ref-vanFraassen1989" role="doc-biblioref">1989</a>)</span>.</p></li></div><p>There’s an old fashioned way of doing this. The idea is implicit in <span class="citation" data-cites="RamseyTruthProb">Ramsey (<a href="#ref-RamseyTruthProb" role="doc-biblioref">1926</a>)</span>, and made central in <span class="citation" data-cites="DeFinetti1964">DeFinetti (<a href="#ref-DeFinetti1964" role="doc-biblioref">1964</a>)</span>. Take the binary attitude <em>is more confident that p than q</em> as primitive. As Holton notes, surface structure of our attitude reports suggest that this attitude, unlike the graded attitude of credence, is part of folk psychology. Lay down some constraints on this attitude. To get enough constraints that the binary relation determines a unique probability function, the constraints will have to be very tight. In particular, you’ll need some kind of Archimedean principle, and a principle of universal comparability. Those aren’t very plausible, especially the second. But even weaker constraints will get you something interesting. In particular, it isn’t hard to lay down enough constraints that there is a unique set <span class="math inline">\(S\)</span> of probability functions such that the agent is more confident that <span class="math inline">\(p\)</span> than <span class="math inline">\(q\)</span> just in case <span class="math inline">\(\Pr( p) &gt; \Pr(q)\)</span> for all <span class="math inline">\(\Pr \in S\)</span>. (For much more detail, see for instance <span class="citation" data-cites="Walley1991">Walley (<a href="#ref-Walley1991" role="doc-biblioref">1991</a>)</span>.)</p>
<p>In that way, we can derive credences from the relative confidences of a reasonably coherent agent. But we can do with even less coherence than that I think. A throwaway remark from <span class="citation" data-cites="Ramsey1929">Ramsey (<a href="#ref-Ramsey1929" role="doc-biblioref">1929/1990</a>)</span> provides a key clue. What is it to have credence <span class="math inline">\(\frac{2}{3}\)</span> in <span class="math inline">\(p\)</span>? Don’t say it’s a betting disposition; mental states and behavioural dispositions aren’t that tightly linked. Here’s Ramsey’s idea. To have credence <span class="math inline">\(\frac{2}{3}\)</span> in <span class="math inline">\(p\)</span> is to be exactly as confident in <span class="math inline">\(p\)</span> as in <span class="math inline">\(q \vee r\)</span>, where <span class="math inline">\(q, r\)</span> and <span class="math inline">\(s\)</span> are taken to be exclusive and exhaustive, and one has equal confidence in all three. It’s easy to see how to extend that to a definition of credence <span class="math inline">\(\frac{m}{n}\)</span> for any integer <span class="math inline">\(m, n\)</span>. It’s a little trickier to say precisely what, say, credence <span class="math inline">\(\frac{1}{\pi}\)</span> is, but rational credences are probably credences enough to explain action. And just like that, we have a way of talking about credences, i.e., graded attitudes, without positing anything more than a binary attitude <em>more confident than</em>.</p>
<p>Perhaps Holton could argue that we only have unary attitudes, not binary attitudes like <em>more confident than</em>. If Maury is more confident that Oswald shot JFK than that Richard III murdered his nephews, that means he really believes the proposition <em>It is more likely that Oswald shot JFK than that Richard III murdered his nephews</em>. But such a view seems forced at best, and isn’t motivated by Holton’s other arguments for the ‘content view’. This attitude of <em>more confident than</em> isn’t iterable. It isn’t subject to the particular kind of reasoning errors that Holton takes to be evidence for the content view in the probabilistic case. It is an attitude we ordinarily report as a binary attitude in normal speech. In short, it looks like a genuine binary attitude.</p>
<p>Given that the binary attitude exists, and that we can define numerical (at least rational) credences in terms of it, I’d say that’s enough to say that credences exist. In a sense, credences will be epiphenomenal. What does the explanatory work is the binary relation <em>more confident that</em>. Maury might stay away from a showing of <em>Richard III</em> because he is less confident that it is historically accurate than he used to be. We can work out from Maury’s other relative confidences what his credence in Richard III’s guilt is and was. Or, at least, we can work out bounds on these. But those numbers aren’t in a fundamental sense explanatory, and neither are the complicated sets of relative confidences that constitute the numbers. What’s <em>really</em> explanatory are relative confidences. But it’s a harmless enough mode of speech to talk as if credences are explanatory; they are easier to talk about than the underlying relative confidences.</p>
</section>
<section id="the-power-of-theoretical-interests" class="level3 page-columns page-full" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="the-power-of-theoretical-interests"><span class="header-section-number">0.3</span> The Power of Theoretical Interests</h3>
<p>So I think we should accept that credences exist. And we can just about reduce beliefs to credences. In previous work I argued that we could do such a reduction. I’m not altogether sure whether the amendments to that view I’m proposing here means it no longer should count as a reductive view; we’ll come back to that question in the conclusion.</p>
<p>The view I defended in previous work is that the reduction comes through the relationship between conditional and unconditional attitudes. Very roughly, to believe that <em>p</em> is simply to have the same attitudes, towards all salient questions, unconditionally as you have conditional on <em>p</em>. In a syrupy slogan, belief means never having to say you’ve conditionalised. For reasons I mentioned in section 1, I now think that was inaccurate; I should have said that belief means never having to say you’ve updated, or at least that you’ve updated your view on any salient question.</p>
<p>The restriction to salient questions is important. Consider any <em>p</em> that I normally take for granted, but such that I wouldn’t bet on it at insane odds. I prefer declining such a bet to taking it. But conditional on <em>p</em>, I prefer taking the bet. So that means I don’t believe any such <em>p</em>. But just about any <em>p</em> satisfies that description, for at least some ‘insane’ odds. So I believe almost nothing. That would be a <em>reductio</em> of the position. I respond by saying that the choice of whether to take an insane bet is not normally salient.</p>
<p>But now there’s a worry that I’ve let in too much. For many <em>p</em>, there is no salient decision that they even bear on. What I would do conditional on <em>p</em>, conditional on <span class="math inline">\(\neg p\)</span>, and unconditionally is exactly the same, over the space of salient choices. (And this isn’t a case where updating and conditionalising come apart; I’ll leave this proviso mostly implicit from now on.) So with the restriction in place, I believe <em>p</em> and <span class="math inline">\(\neg p\)</span>. That seems like a <em>reductio</em> of the view too. I probably do have inconsistent beliefs, but not in virtue of <em>p</em> being irrelevant to me right now. I’ve changed my mind a little about what the right way to avoid this problem is, in part because of some arguments by Jacob Ross and Mark Schroeder.</p>
<p>They have what looks like, on the surface, a rather different view to mine. They say that to believe <em>p</em> is to have a <strong>default reasoning disposition</strong> to use <em>p</em> in reasoning. Here’s how they describe their own view.</p>
<blockquote class="blockquote">
<p>What we should expect, therefore, is that for some propositions we would have a <em>defeasible</em> or <em>default</em> disposition to treat them as true in our reasoning–a disposition that can be overridden under circumstances where the cost of mistakenly acting as if these propositions are true is particularly salient. And this expectation is confirmed by our experience. We do indeed seem to treat some uncertain propositions as true in our reasoning; we do indeed seem to treat them as true automatically, without first weighing the costs and benefits of so treating them; and yet in contexts such as High where the costs of mistakenly treating them as true is salient, our natural tendency to treat these propositions as true often seems to be overridden, and instead we treat them as merely probable.</p>
<p>But if we concede that we have such defeasible dispositions to treat particular propositions as true in our reasoning, then a hypothesis naturally arises, namely, that beliefs consist in or involve such dispositions. More precisely, at least part of the functional role of belief is that believing that <em>p</em> defeasibly disposes the believer to treat <em>p</em> as true in her reasoning. Let us call this hypothesis the <em>reasoning disposition account</em> of belief. <span class="citation" data-cites="SchroederRoss2012">(<a href="#ref-SchroederRoss2012" role="doc-biblioref">Ross and Schroeder 2014, 9–10</a>)</span></p>
</blockquote>
<p>There are, relative to what I’m interested in, three striking characteristics of Ross and Schroeder’s view.</p>
<ol type="1">
<li><p>Whether you believe <em>p</em> is sensitive to how you reason; that is, your theoretical interests matter.</p></li>
<li><p>How you would reason about some questions that are not live is relevant to whether you believe <em>p</em>.</p></li>
<li><p>Dispositions can be masked, so you can believe <em>p</em> even though you don’t actually use <em>p</em> in reasoning now.</p></li>
</ol>
<p>I think they take all three of these points to be reasons to favour their view over mine. As I see it, we agree on point 1 (and I always had the resources to agree with them), I can accommodate point 2 with a modification to my theory, and point 3 is a cost of their theory, not a benefit. Let’s take those points in order.</p>
<p>There are lots of reasons to dislike what Ross and Schroeder call <em>Pragmatic Credal Reductionism</em> (PCR). This is, more or less, the view that the salient questions, in the sense relevant above, are just those which are practically relevant to the agent. So to believe <span class="math inline">\(p\)</span> just is to have the same attitude towards all practically relevant questions unconditionally as conditional on <span class="math inline">\(p\)</span>. There are at least three reasons to resist this view.</p>
<p>One reason comes from the discussions of Ned Block’s example Blockhead &nbsp;<span class="citation" data-cites="Block1978">(<a href="#ref-Block1978" role="doc-biblioref">Block 1978</a>)</span>. As Braddon-Mitchell and Jackson point out, the lesson to take from that example is that beliefs are constituted in part by their relations to other mental states &nbsp;<span class="citation" data-cites="DBMJackson2007">(<a href="#ref-DBMJackson2007" role="doc-biblioref">Braddon-Mitchell and Jackson 2007, 114ff</a>)</span>. There’s a quick attempted refutation of PCR via the Blockhead case which doesn’t quite work. We might worry that if all that matters to belief given PCR is how it relates to action, PCR will have the implausible consequence that Blockhead has a rich set of beliefs. That isn’t right; PCR is compatible with the view that Blockhead doesn’t have credences, and hence doesn’t have credences that constitute beliefs. But the Blockhead example’s value isn’t exhausted by its use in quick refutations.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> The lesson is that beliefs are, by their nature, interactive. It seems to me that PCR doesn’t really appreciate that lesson.</p>
<div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;The point I’m making here is relevant I think to recent debates about the proper way to formalise counterexamples in philosophy, as in &nbsp;<span class="citation" data-cites="Williamson2007-WILTPO-17 IchikawaJarvis2009 Malmgren2011">(<a href="#ref-Williamson2007-WILTPO-17" role="doc-biblioref">Williamson 2007b</a>; <a href="#ref-IchikawaJarvis2009" role="doc-biblioref">Ichikawa and Jarvis 2009</a>; <a href="#ref-Malmgren2011" role="doc-biblioref">Malmgren 2011</a>)</span>. I worry that too much of that debate is focussed on the role that examples play in one-step refutations. But there’s more, much more, to a good example than that.</p></li></div><p>Another reason comes from recent work by Jessica <span class="citation" data-cites="Brown2013">Brown (<a href="#ref-Brown2013" role="doc-biblioref">2014</a>)</span>. Compare these two situations.</p>
<ol type="1">
<li><p><em>S</em> is in circumstances <em>C</em>, and has to decide whether to do <em>X</em>.</p></li>
<li><p><em>S</em> is in completely different circumstances to <em>C</em>, but is seriously engaged in planning for future contingencies. She’s currently trying to decide whether in circumstances <em>C</em> to do <em>X</em>.</p></li>
</ol>
<p>Intuitively, <em>S</em> can bring exactly the same evidence, knowledge and beliefs to bear on the two problems. If <em>C</em> is a particularly high stakes situation, say it is a situation where one has to decide what to feed someone with a severe peanut allergy, then a lot of things that can ordinarily be taken for granted cannot, in this case, be taken for granted. And that’s true whether <em>S</em> is actually in <em>C</em>, or she is just planning for the possibility that she finds herself in <em>C</em>.</p>
<p>So I conclude that both practical and theoretical interests matter for what we can take for granted in inquiry. The things we can take for granted into a theoretical inquiry into what to do in high stakes contexts as restricted, just as they are when we are in a high stakes context, and must make a practical decision. Since the latter restriction on what we can take for granted is explained by (and possibly constituted by) a restriction on what we actually believe in those contexts, we should similarly conclude that agents simply believe less when they are reasoning about high stakes contexts, whatever their actual context.</p>
<p>A third reason to dislike PCR comes from the ‘Renzi’ example in Ross and Schroeder’s paper. I’ll run through a somewhat more abstract version of the case, because I don’t think the details are particularly important. Start with a standard decision problem. The agent knows that X is better to do if <em>p</em>, and Y is better to do if <span class="math inline">\(\neg p\)</span>. The agent should then go through calculating the relative gains to doing X or Y in the situations they are better, and the probability of <em>p</em>. But the agent imagined doesn’t do that. Rather, the agent divides the possibility space in four, taking the salient possibilities to be <span class="math inline">\(p \wedge q, p \wedge \neg q, \neg p \wedge q\)</span> and <span class="math inline">\(\neg p \wedge \neg q\)</span>, and then calculates the expected utility of X and Y accordingly. This is a bad bit of reasoning on the agent’s part. In the cases we are interested in, <em>q</em> is exceedingly likely. Moreover, the expected utility of each act doesn’t change a lot depending on <em>q</em>’s truth value. So it is fairly obvious that we’ll end up making the same decision whether we take the ‘small worlds’ in our decision model to be just the world where <em>p</em>, and the world where <span class="math inline">\(\neg p\)</span>, or the four worlds this agent uses. But the agent does use these four, and the question is what to say about them.</p>
<p>Ross and Schroeder say that such an agent should not be counted as believing that <em>q</em>. If they are consciously calculating the probability that <em>q</em>, and taking <span class="math inline">\(\neg q\)</span> possibilities into account when calculating expected utilities, they regard <em>q</em> as an open question. And regarding <em>q</em> as open in this way is incompatible with believing it. I agree with all this.</p>
<p>They also think that PCR implies that the agent <em>does</em> believe <em>q</em>. The reason is that conditionalising on <em>q</em> doesn’t change the agent’s beliefs about any practical question. I think that’s right too, at least on a natural understanding of what ‘practical’ is.</p>
<p>My response to all these worries is to say that whether someone believes that <em>p</em> depends not just on how conditionalising (or more generally updating) on <em>p</em> would affect someone’s action, but on how it would affect their reasoning. That is, just as we learned from the Blockhead example, to believe that <em>p</em> requires having a mental state that is connected to the rest of one’s cognitive life in roughly the way a belief that <em>p</em> should be connected. Let’s go through both the last two cases to see how this works on my theory.</p>
<p>One of the things that happens when the stakes go up is that conditionalising on very probable things can change the outcome of interesting decisions. Make the probability that some nice food is peanut-free be high, but short of one. Conditional on it being peanut-free, it’s a good thing to give to a peanut-allergic guest. But unconditionally, it’s a bad thing to give to such a guest, because the niceness of the food doesn’t outweigh the risk of killing them. And that’s true whether the guest is actually there, or you’re just thinking about what to do should such a guest arrive in the future. In general, the same questions will be relevant whether you’re in <em>C</em> trying to decide whether to do <em>X</em>, or simply trying to decide whether to <em>X</em> in <em>C</em>. In one case they will be practically relevant questions, in the other they will be theoretically relevant questions. But this feels a lot like a distinction without a difference, since the agent should have similar beliefs in the two cases.</p>
<p>The same response works for Ross and Schroeder’s case. The agent was trying to work out the expected utility of X and Y by working out the utility of each action in each of four ‘small worlds’, then working out the probability of each of these. Conditional on <em>q</em>, the probability of two of them (<span class="math inline">\(p \wedge \neg q, \neg p \wedge \neg q\)</span>), will be 0. Unconditionally, this probability won’t be 0. So the agent has a different view on some question they have taken an interest in unconditionally to their view conditional on <em>q</em>. So they don’t believe <em>q</em>. The agent shouldn’t care about that question, and conditional on each question they should care about, they have the same attitude unconditionally and conditional on <em>q</em>. But they do care about these probabilistic questions, so they don’t believe <em>q</em>.</p>
<p>So I think that Ross and Schroeder and I agree on point 1; something beyond practical interests is relevant to belief.</p>
<p>They have another case that I think does suggest a needed revision to my theory. I’m going to modify their case a little to change the focus a little, and to avoid puzzles about vagueness. (What follows is a version of their example about Dalı́’s moustache, purged of any worries about vagueness, and without the focus on consistency. I don’t think the problem they true to press on me, that my theory allows excessive inconsistency of belief among rational agents, really sticks. Everyone will have to make qualifications to consistency to deal with the preface paradox, and for reasons I went over in &nbsp;<span class="citation" data-cites="Weatherson2005-WEACWD">(<a href="#ref-Weatherson2005-WEACWD" role="doc-biblioref">Weatherson 2005</a>)</span>, I think the qualifications I make are the best ones to make.)</p>
<p>Let <em>D</em> be the proposition that the number of games the Detroit Tigers won in 1976 (in the MLB regular season) is not a multiple of 3. At most times, <em>D</em> is completely irrelevant to anything I care about, either practically or theoretically. My attitudes towards any relevant question are the same unconditionally as conditional on <em>D</em>. So there’s a worry that I’ll count as believing <em>D</em>, and believing <span class="math inline">\(\neg D\)</span>, by default.</p>
<p>In earlier work, I added a clause meant to help with cases like this. I said that for determining whether an agent believes that <em>p</em>, we should treat the question of whether <em>p</em>’s probability is above or below 0.5 as salient, even if the agent doesn’t care about it. Obviously this won’t help with this particular case. The probability of <em>D</em> is around , and is certainly above 0.5. My ‘fix’ avoids the consequence that I implausibly count as believing <span class="math inline">\(\neg D\)</span>. But I still count, almost as implausibly, as believing <em>D</em>. This needs to be fixed.</p>
<p>Here’s my proposed change. For an agent to count as believing <em>p</em>, it must be possible for <em>p</em> to do some work for them in reasoning. Here’s what I mean by work. Consider a very abstract set up of a decision problem, as follows.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><em>p</em></td>
<td style="text-align: center;"><em>q</em></td>
</tr>
<tr class="even">
<td>X</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td>Y</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
</div>
<p>That table encodes a lot of information. It encodes that <span class="math inline">\(p \vee q\)</span> is true; otherwise there are some columns missing. It encodes that the only live choices are X or Y; otherwise there are rows missing. It encodes that doing X is better than doing Y if <em>p</em>, and worse if <em>q</em>.</p>
<p>For any agent, and any decision problem, there is a table like this that they would be disposed to use to resolve that problem. Or, perhaps, there are a series of tables and there is no fact about which of them they would be most disposed to use.</p>
<p>Given all that terminology, here’s my extra constraint on belief. To believe that <em>p</em>, there must be some decision problem such that some table the agent would be disposed to use to solve it encodes that <em>p</em>. If there is no such problem, the agent does not believe that <em>p</em>. For anything that I intuitively believe, this is an easy condition to satisfy. Let the problem be whether to take a bet that pays 1 if <em>p</em>, and loses 1 otherwise. Here’s the table I’d be disposed to use to solve the problem.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><em>p</em></td>
</tr>
<tr class="even">
<td>Take bet</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td>Decline bet</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p>This table encodes that <em>p</em>, so it is sufficient to count as believing that <em>p</em>. And it doesn’t matter that this bet isn’t on the table. I’m disposed to use this table, so that’s all that matters.</p>
<p>But might there be problems in the other direction. What about an agent who, if offered such a bet on <em>D</em>, would use such a simple table? I simply say that they believe that <em>D</em>. I would not use any such table. I’d use this table.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><em>D</em></td>
<td style="text-align: center;"><span class="math inline">\(\neg D\)</span></td>
</tr>
<tr class="even">
<td>Take bet</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">–1</td>
</tr>
<tr class="odd">
<td>Decline bet</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<p>Now given the probability of <em>D</em>, I’d still end up taking the bet; it has an expected return of . (Well, actually I’d probably decline the bet because being offered the bet would change the probability of <em>D</em> for reasons made clear in &nbsp;<span class="citation" data-cites="RunyonGuysDolls">Runyon (<a href="#ref-RunyonGuysDolls" role="doc-biblioref">1992, 14–15</a>)</span>. But that hardly undermines the point I’m making.) But this isn’t some analytic fact about me, or even I think some respect in which I’m obeying the dictates of rationality. It’s simply a fact that I wouldn’t take <em>D</em> for granted in any inquiry. And that’s what my non-belief that <em>D</em> consists in.</p>
<p>This way of responding to the Tigers example helps respond to a nice observation that Ross and Schroeder make about correctness. A belief that <em>p</em> is, in some sense, <em>incorrect</em> if <span class="math inline">\(\neg p\)</span>. It isn’t altogether clear how to capture this sense given a simple reduction of beliefs to credences. I propose to capture it using this idea that decision tables encode propositions. A table is incorrect if it encodes something that’s false. To believe something is, <em>inter alia</em>, to be disposed to use a table that encodes it. So if it is false, it involves a disposition to do something incorrect.</p>
<p>It also helps capture Holton’s observation that beliefs should be resilient. If someone is disposed to use decision tables that encode that <em>p</em>, that disposition should be fairly resilient. And to the extent that it is resilient, they will satisfy all the other clauses in my preferred account of belief. So anyone who believes <em>p</em> should have a resilient belief that <em>p</em>.</p>
<p>The last point is where I think my biggest disagreement with Ross and Schroeder lies. They think it is very important that a theory of belief vindicate a principle they call <strong>Stability</strong>.</p>
<blockquote class="blockquote">
<p><strong>Stability</strong>: A fully rational agent does not change her beliefs purely in virtue of an evidentially irrelevant change in her credences or preferences. (20)</p>
</blockquote>
<p>Here’s the kind of case that is meant to motivate Stability, and show that views like mine are in tension with it.</p>
<blockquote class="blockquote">
<p>Suppose Stella is extremely confident that steel is stronger than Styrofoam, but she’s not so confident that she’d bet her life on this proposition for the prospect of winning a penny. PCR implies, implausibly, that if Stella were offered such a bet, she’d cease to believe that steel is stronger than Styrofoam, since her credence would cease to rationalize acting as if this proposition is true. (22)</p>
</blockquote>
<p>Ross and Schroeder’s own view is that if Stella has a defeasible disposition to treat as true the proposition that steel is stronger than Styrofoam, that’s enough for her to believe it. And that can be true if the disposition is not only defeasible, but actually defeated in the circumstances Stella is in. This all strikes me as just as implausible as the failure of Stability. Let’s go over its costs.</p>
<p>The following propositions are clearly not mutually consistent, so one of them must be given up. We’re assuming that Stella is facing, and knows she is facing, a bet that pays a penny if steel is stronger than Styrofoam, and costs her life if steel is not stronger than Styrofoam.</p>
<ol type="1">
<li><p>Stella believes that steel is stronger than Styrofoam.</p></li>
<li><p>Stella believes that if steel is stronger than Styrofoam, she’ll win a penny and lose nothing by taking the bet.</p></li>
<li><p>If 1 and 2 are true, and Stella considers the question of whether she’ll win a penny and lose nothing by taking the bet, she’ll believe that she’ll win a penny and lose nothing by taking the bet.</p></li>
<li><p>Stella prefers winning a penny and losing nothing to getting nothing.</p></li>
<li><p>If Stella believes that she’ll win a penny and lose nothing by taking the bet, and prefers winning a penny and losing nothing to getting nothing, she’ll take the bet.</p></li>
<li><p>Stella won’t take the bet.</p></li>
</ol>
<p>It’s part of the setup of the problem that 2 and 4 are true. And it’s common ground that 6 is true, at least assuming that Stella is rational. So we’re left with 1, 3 and 5 as the possible candidates for falsehood.</p>
<p>Ross and Schroeder say that it’s implausible to reject 1. After all, Stella believed it a few minutes ago, and hasn’t received any evidence to the contrary. And I guess rejecting 1 isn’t the most intuitive philosophical conclusion I’ve ever drawn. But compare the alternatives!</p>
<p>If we reject 3, we must say that Stella will simply refuse to infer <em>r</em> from <em>p</em>, <em>q</em> and <span class="math inline">\((p \wedge q) \rightarrow r\)</span>. Now it is notoriously hard to come up with a general principle for closure of beliefs. But it is hard to see why this particular instance would fail. And in any case, it’s hard to see why Stella wouldn’t have a general, defeasible, disposition to conclude <em>r</em> in this case, so by Ross and Schroeder’s own lights, it seems 3 should be acceptable.</p>
<p>That leaves 5. It seems on Ross and Schroeder’s view, Stella simply must violate a very basic principle of means-end reasoning. She desires something, she believes that taking the bet will get that thing, and come with no added costs. Yet, she refuses to take the bet. And she’s rational to do so! At this stage, I think I’ve lost what’s meant to be belief-like about their notion of belief. I certainly think attributing this kind of practical incoherence to Stella is much less plausible than attributing a failure of Stability to her.</p>
<p>Put another way, I don’t think presenting Stability on its own as a desideratum of a theory is exactly playing fair. The salient question isn’t whether we should accept or reject Stability. The salient question is whether giving up Stability is a fair price to pay for saving basic tenets of means-end rationality. And I think that it is. Perhaps there will be some way of understanding cases like Stella’s so that we don’t have to choose between theories of belief that violate Stability constraints, and theories of belief that violate coherence constraints. But I don’t see one on offer, and I’m not sure what such a theory could look like.</p>
<p>I have one more argument against Stability, but it does rest on somewhat contentious premises. There’s often a difference between the best <em>methodology</em> in an area, and the correct <em>epistemology</em> of that area. When that happens, it’s possible that there is a good methodological rule saying that if such-and-such happens, re-open a certain inquiry. But that rule need not be epistemologically significant. That is, it need not be the case that the happening of such-and-such provides evidence against the conclusion of the inquiry. It just provides a reason that a good researcher will re-open the inquiry. And, as we’ve stated above, an open inquiry is incompatible with belief.</p>
<p>Here’s one way that might happen. Like other non-conciliationists about disagreement, e.g., &nbsp;<span class="citation" data-cites="Kelly2010-KELPDA">Kelly (<a href="#ref-Kelly2010-KELPDA" role="doc-biblioref">2010</a>)</span>, I hold that disagreement by peers with the same evidence as you doesn’t provide <em>evidence</em> that you are wrong. But it might provide an excellent reason to re-open an inquiry. We shouldn’t draw conclusions about the methodological significance of disagreement from the epistemology of disagreement. So learning that your peers all disagree with a conclusion might be a reason to re-open inquiry into that conclusion, and hence lose belief in the conclusion, without providing evidence that the conclusion is false. This example rests on a very contentious claim about the epistemology of disagreement. But any gap that opens up between methodology and epistemology will allow such an example to be constructed, and hence provide an independent reason to reject Stability.</p>
</section>
<section id="conclusion" class="level3" data-number="0.4">
<h3 data-number="0.4" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">0.4</span> Conclusion</h3>
<p>You might well worry that the view here is too <em>complex</em> to really be a theory of belief. Belief is a simple state; why all the epicycles? This is a good question, and I’m not sure I have a sufficiently good answer to it.</p>
<p>At heart, the theory I’ve offered here is simple. To believe <em>p</em> is to take <em>p</em> for granted, to take it as given, to take it as a settled question. But one doesn’t take a question as settled in a vacuum. I will take some questions as settled in some circumstances and not others. It’s here that the complexities enter in.</p>
<p>To believe <em>p</em>, it isn’t necessary that we take it as settled in all contexts. That would mean that anything one believes one would bet on at any odds. But it isn’t sufficient to take it as settled in some context or other. If I’m facing a tricky bet on <em>p</em>, the fact that I’d take <em>p</em> as settled in some other context doesn’t mean that I believe <em>p</em>. After all, I might even decline the bet, although I desire the reward for winning the bet, and believe that if <em>p</em> I will win. And we can’t just focus on the actual circumstances. Five minutes ago, I neither took it as settled or as open that the Cubs haven’t won the World Series for quite a while. I simply wasn’t thinking about that proposition, and didn’t really take it to be one thing or another.</p>
<p>This is why things get so complex. To believe <em>p</em> is to hold a fairly simple attitude towards <em>p</em> in some relevant circumstances. But which circumstances? That’s what’s hard to say, and it’s why the theory is so messy. And I think we have an argument that it must be a little hard to say, namely an argument by exhaustion of all the possible simple things to say. The previous paragraph starts such an argument.</p>
<p>I’d be a little surprised if the account here is the best or last word on the matter though. It does feel a little disjunctive, as if there is a simpler reduction to be had. But I think it’s better than what came before, so I’m putting it forward.</p>
<p>The previous version of the theory I put forward was clearly reductive; beliefs were reduced to credences and preferences. This version is not quite as clearly reductive. Which decision tables the agent is disposed to use, and which propositions those tables encode, are not obviously facts about credences and preferences. So it feels like I’ve given up on the reductive project.</p>
<p>I’m not altogether happy about this; reduction is a good aim to have. But if reduction of belief to other states fails, I’d think this kind of reason is why it is going to fail. Facts about how an agent conceptualises a problem, how she sets up the decision table, are distinct from facts about which values she writes into the table. This is the deepest reason why the Lockean theory is false. Belief is not the difference between one column in the decision table getting probability 0.98 rather than 0.97; it is the difference between one column being excluded rather than included. If that difference can’t be accounted for in terms of actual credences and preferences, the reductionist project will fail.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Aumann1999" class="csl-entry" role="listitem">
Aumann, Robert J. 1999. <span>“Interactive Epistemology i: Knowledge.”</span> <em>International Journal of Game Theory</em> 28 (3): 263–300. <a href="https://doi.org/10.1007/s001820050111">https://doi.org/10.1007/s001820050111</a>.
</div>
<div id="ref-Binmore2007" class="csl-entry" role="listitem">
Binmore, Ken. 2007. <em>Playing for Real: A Text on Game Theory</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Block1978" class="csl-entry" role="listitem">
Block, Ned. 1978. <span>“Troubles with Functionalism.”</span> <em>Minnesota Studies in the Philosophy of Science</em> 9: 261–325.
</div>
<div id="ref-DBMJackson2007" class="csl-entry" role="listitem">
Braddon-Mitchell, David, and Frank Jackson. 2007. <em>The Philosophy of Mind and Cognition, <span class="roman">Second Edition</span></em>. Malden, MA: Blackwell.
</div>
<div id="ref-Brown2013" class="csl-entry" role="listitem">
Brown, Jessica. 2014. <span>“Impurism, Practical Reasoning and the Threshold Problem.”</span> <em>No<span>û</span>s</em> 48 (1): 179–92. <a href="https://doi.org/10.1111/nous.12008">https://doi.org/10.1111/nous.12008</a>.
</div>
<div id="ref-Christensen2005" class="csl-entry" role="listitem">
Christensen, David. 2005. <em>Putting Logic in Its Place</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Cohen1999" class="csl-entry" role="listitem">
Cohen, Stewart. 1999. <span>“Contextualism, Skepticism, and the Structure of Reasons.”</span> <em>Philosophical Perspectives</em> 13: 57–89. <a href="https://doi.org/10.1111/0029-4624.33.s13.3">https://doi.org/10.1111/0029-4624.33.s13.3</a>.
</div>
<div id="ref-DeFinetti1964" class="csl-entry" role="listitem">
DeFinetti, Bruno. 1964. <span>“Foresight: Its Logical Laws, Its Subjective Sources.”</span> In <em>Studies in Subjective Probability</em>, edited by Henry E. Kyburg and Howard E. Smokler, 93–156. New York: Wiley.
</div>
<div id="ref-DeRose1992" class="csl-entry" role="listitem">
DeRose, Keith. 1992. <span>“Contextualism and Knowledge Attributions.”</span> <em>Philosophy and Phenomenological Research</em> 52 (4): 913–29. <a href="https://doi.org/10.2307/2107917">https://doi.org/10.2307/2107917</a>.
</div>
<div id="ref-DixitSkeath2004" class="csl-entry" role="listitem">
Dixit, Avinash K., and Susan Skeath. 2004. <em>Games of Strategy</em>. Second. New York: W. W. Norton &amp; Company.
</div>
<div id="ref-Fantl2002" class="csl-entry" role="listitem">
Fantl, Jeremy, and Matthew McGrath. 2002. <span>“Evidence, Pragmatics, and Justification.”</span> <em>Philosophical Review</em> 111: 67–94. <a href="https://doi.org/10.2307/3182570">https://doi.org/10.2307/3182570</a>.
</div>
<div id="ref-vanFraassen1989" class="csl-entry" role="listitem">
Fraassen, Bas van. 1989. <em>Laws and Symmetry</em>. Oxford: Clarendon Press.
</div>
<div id="ref-Friend2003" class="csl-entry" role="listitem">
Friend, Stacie. 2003. <span>“How i Really Feel about <em>JFK</em>.”</span> In <em>Imagination, Philosophy and the Arts</em>, edited by Matthew Kieran and Dominic McIver Lopes, 35–53. London. Routledge.
</div>
<div id="ref-Gillies2010" class="csl-entry" role="listitem">
Gillies, Anthony S. 2010. <span>“Iffiness.”</span> <em>Semantics and Pragmatics</em> 3 (4): 1–42. <a href="https://doi.org/10.3765/sp.3.4">https://doi.org/10.3765/sp.3.4</a>.
</div>
<div id="ref-Heal1994" class="csl-entry" role="listitem">
Heal, Jane. 1994. <span>“Moore’s Paradox: A Wittgensteinian Approach.”</span> <em>Mind</em> 103 (409): 5–24. <a href="https://doi.org/10.1093/mind/103.409.5">https://doi.org/10.1093/mind/103.409.5</a>.
</div>
<div id="ref-Holton2013" class="csl-entry" role="listitem">
Holton, Richard. 2014. <span>“Intention as a Model for Belief.”</span> In <em>Rational and Social Agency: Essays on the Philosophy of Michael Bratman</em>, edited by Manuel Vargas and Gideon Yaffe, 12–37. Oxford: Oxford University Press.
</div>
<div id="ref-IchikawaJarvis2009" class="csl-entry" role="listitem">
Ichikawa, Jonathan, and Benjamin Jarvis. 2009. <span>“Thought-Experiment Intuitions and Truth in Fiction.”</span> <em>Philosophical Studies</em> 142 (2): 221–46. <a href="https://doi.org/10.1007/s11098-007-9184-y">https://doi.org/10.1007/s11098-007-9184-y</a>.
</div>
<div id="ref-Jeffrey1983" class="csl-entry" role="listitem">
Jeffrey, Richard. 1983. <span>“Bayesianism with a Human Face.”</span> In <em>Testing Scientific Theories</em>, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.
</div>
<div id="ref-Kelly2010-KELPDA" class="csl-entry" role="listitem">
Kelly, Thomas. 2010. <span>“Peer Disagreement and Higher Order Evidence.”</span> In <em>Disagreement</em>, edited by Ted Warfield and Richard Feldman, 111–74. Oxford: Oxford University Press.
</div>
<div id="ref-KohlbergMertens1986" class="csl-entry" role="listitem">
Kohlberg, Elon, and Jean-Francois Mertens. 1986. <span>“On the Strategic Stability of Equilibria.”</span> <em>Econometrica</em> 54 (5): 1003–37. <a href="https://doi.org/10.2307/1912320">https://doi.org/10.2307/1912320</a>.
</div>
<div id="ref-Kratzer2012" class="csl-entry" role="listitem">
Kratzer, Angelika. 2012. <em>Modals and Conditionals</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Levi1974" class="csl-entry" role="listitem">
Levi, Isaac. 1974. <span>“On Indeterminate Probabilities.”</span> <em>Journal of Philosophy</em> 71 (13): 391–418. <a href="https://doi.org/10.2307/2025161">https://doi.org/10.2307/2025161</a>.
</div>
<div id="ref-MacFarlane2011" class="csl-entry" role="listitem">
MacFarlane, John. 2011. <span>“Epistemic Modals Are Assessment-Sensitive.”</span> In <em>Epistemic Modality</em>, edited by Andy Egan and Brian Weatherson, 144–78. Oxford: Oxford University Press.
</div>
<div id="ref-MaitraWeatherson" class="csl-entry" role="listitem">
Maitra, Ishani, and Brian Weatherson. 2010. <span>“Assertion, Knowledge and Action.”</span> <em>Philosophical Studies</em> 149 (1): 99–118. <a href="https://doi.org/10.1007/s11098-010-9542-z">https://doi.org/10.1007/s11098-010-9542-z</a>.
</div>
<div id="ref-Malmgren2011" class="csl-entry" role="listitem">
Malmgren, Anna-Sara. 2011. <span>“Rationalism and the Content of Intuitive Judgements.”</span> <em>Mind</em> 120 (478): 263–327. <a href="https://doi.org/10.1093/mind/fzr039">https://doi.org/10.1093/mind/fzr039</a>.
</div>
<div id="ref-MossPragmaticsEpistemicModals" class="csl-entry" role="listitem">
Moss, Sarah. 2015. <span>“On the Semantics and Pragmatics of Epistemic Vocabulary.”</span> <em>Semantics and Pragmatics</em> 8: 1–81. <a href="https://doi.org/10.3765/sp.8.5">https://doi.org/10.3765/sp.8.5</a>.
</div>
<div id="ref-Nagel2008" class="csl-entry" role="listitem">
Nagel, Jennifer. 2008. <span>“Knowledge Ascriptions and the Psychological Consequences of Changing Stakes.”</span> <em>Australasian Journal of Philosophy</em> 86 (2): 279–94. <a href="https://doi.org/10.1080/00048400801886397">https://doi.org/10.1080/00048400801886397</a>.
</div>
<div id="ref-Nagel2013-Williamson" class="csl-entry" role="listitem">
———. 2013. <span>“Motivating Williamson’s Model Gettier Cases.”</span> <em>Inquiry</em> 56 (1): 54–62. <a href="https://doi.org/10.1080/0020174X.2013.775014">https://doi.org/10.1080/0020174X.2013.775014</a>.
</div>
<div id="ref-North2010" class="csl-entry" role="listitem">
North, Jill. 2010. <span>“An Empirical Approach to Symmetry and Probability.”</span> <em>Studies In History and Philosophy of Science Part B: Studies In History and Philosophy of Modern Physics</em> 41 (1): 27–40. <a href="https://doi.org/10.1016/j.shpsb.2009.08.008">https://doi.org/10.1016/j.shpsb.2009.08.008</a>.
</div>
<div id="ref-Ramsey1929" class="csl-entry" role="listitem">
Ramsey, Frank. 1929/1990. <span>“Probability and Partial Belief.”</span> In <em>Philosophical Papers</em>, edited by D. H. Mellor, 95–96. Cambridge University Press.
</div>
<div id="ref-RamseyTruthProb" class="csl-entry" role="listitem">
———. 1926. <span>“Truth and Probability.”</span> In <em>Philosophical Papers</em>, edited by D. H. Mellor, 52–94. Cambridge: Cambridge University Press.
</div>
<div id="ref-SchroederRoss2012" class="csl-entry" role="listitem">
Ross, Jacob, and Mark Schroeder. 2014. <span>“Belief, Credence, and Pragmatic Encroachment.”</span> <em>Philosophy and Phenomenological Research</em> 88 (2): 259–88. <a href="https://doi.org/10.1111/j.1933-1592.2011.00552.x">https://doi.org/10.1111/j.1933-1592.2011.00552.x</a>.
</div>
<div id="ref-RunyonGuysDolls" class="csl-entry" role="listitem">
Runyon, Damon. 1992. <em>Guys &amp; Dolls: The Stories of <span>D</span>amon <span>R</span>unyon</em>. New York: Penguin.
</div>
<div id="ref-Schoenfield2013" class="csl-entry" role="listitem">
Schoenfield, Miriam. 2013. <span>“Permission to Believe: Why Permissivism Is True and What It Tells Us about Irrelevant Influences on Belief.”</span> <em>No<span>û</span>s</em> 47 (1): 193–218. <a href="https://doi.org/10.1111/nous.12006">https://doi.org/10.1111/nous.12006</a>.
</div>
<div id="ref-Sorensen1988" class="csl-entry" role="listitem">
Sorensen, Roy A. 1988. <em>Blindspots</em>. Oxford: Clarendon Press.
</div>
<div id="ref-Stalnaker1994" class="csl-entry" role="listitem">
Stalnaker, Robert. 1994. <span>“On the Evaluation of Solution Concepts.”</span> <em>Theory and Decision</em> 37 (1): 49–73. <a href="https://doi.org/10.1007/BF01079205">https://doi.org/10.1007/BF01079205</a>.
</div>
<div id="ref-Stalnaker1996" class="csl-entry" role="listitem">
———. 1996. <span>“Knowledge, Belief and Counterfactual Reasoning in Games.”</span> <em>Economics and Philosophy</em> 12: 133–63. <a href="https://doi.org/10.1017/S0266267100004132">https://doi.org/10.1017/S0266267100004132</a>.
</div>
<div id="ref-Stalnaker1998" class="csl-entry" role="listitem">
———. 1998. <span>“Belief Revision in Games: Forward and Backward Induction.”</span> <em>Mathematical Social Sciences</em> 36 (1): 31–56. <a href="https://doi.org/10.1016/S0165-4896(98)00007-9">https://doi.org/10.1016/S0165-4896(98)00007-9</a>.
</div>
<div id="ref-Stalnaker1999" class="csl-entry" role="listitem">
———. 1999. <span>“Extensive and Strategic Forms: Games and Models for Games.”</span> <em>Research in Economics</em> 53 (3): 293–319. <a href="https://doi.org/10.1006/reec.1999.0200">https://doi.org/10.1006/reec.1999.0200</a>.
</div>
<div id="ref-Stephenson2007" class="csl-entry" role="listitem">
Stephenson, Tamina. 2007. <span>“Judge Dependence, Epistemic Modals, and Predicates of Personal Taste.”</span> <em>Linguistics and Philosophy</em> 30 (4): 487–525. <a href="https://doi.org/10.1007/s10988-008-9023-4">https://doi.org/10.1007/s10988-008-9023-4</a>.
</div>
<div id="ref-Walley1991" class="csl-entry" role="listitem">
Walley, Peter. 1991. <em>Statisical Reasoning with Imprecise Probabilities</em>. London: Chapman &amp; Hall.
</div>
<div id="ref-Weatherson2005-WEACWD" class="csl-entry" role="listitem">
Weatherson, Brian. 2005. <span>“<span>Can We Do Without Pragmatic Encroachment?</span>”</span> <em>Philosophical Perspectives</em> 19 (1): 417–43. <a href="https://doi.org/10.1111/j.1520-8583.2005.00068.x">https://doi.org/10.1111/j.1520-8583.2005.00068.x</a>.
</div>
<div id="ref-Weatherson2011-WEADIR" class="csl-entry" role="listitem">
———. 2011. <span>“Defending Interest-Relative Invariantism.”</span> <em>Logos &amp; Episteme</em> 2 (4): 591–609. <a href="https://doi.org/10.5840/logos-episteme2011248">https://doi.org/10.5840/logos-episteme2011248</a>.
</div>
<div id="ref-Weatherson2012-WEAGAT" class="csl-entry" role="listitem">
———. 2012a. <span>“Games and the Reason-Knowledge Principle.”</span> <em>The Reasoner</em> 6 (1): 6–8.
</div>
<div id="ref-Weatherson2012-WEAKBI" class="csl-entry" role="listitem">
———. 2012b. <span>“Knowledge, Bets and Interests.”</span> In <em>Knowledge Ascriptions</em>, edited by Jessica Brown and Mikkel Gerken, 75–103. Oxford: Oxford University Press.
</div>
<div id="ref-Weintraub2008" class="csl-entry" role="listitem">
Weintraub, Ruth. 2008. <span>“How Probable Is an Infinite Sequence of Heads? A Reply to Williamson.”</span> <em>Analysis</em> 68 (3): 247–50. <a href="https://doi.org/10.1093/analys/68.3.247">https://doi.org/10.1093/analys/68.3.247</a>.
</div>
<div id="ref-White2005-WHIEP" class="csl-entry" role="listitem">
White, Roger. 2005. <span>“Epistemic Permissiveness.”</span> <em>Philosophical Perspectives</em> 19: 445–59. <a href="https://doi.org/10.1111/j.1520-8583.2005.00069.x">https://doi.org/10.1111/j.1520-8583.2005.00069.x</a>.
</div>
<div id="ref-Williamson1996-WILKAA" class="csl-entry" role="listitem">
Williamson, Timothy. 1996. <span>“<span class="nocase">Knowing and Asserting</span>.”</span> <em>Philosophical Review</em> 105 (4): 489–523. <a href="https://doi.org/10.2307/2998423">https://doi.org/10.2307/2998423</a>.
</div>
<div id="ref-Williamson2007-WILHPI" class="csl-entry" role="listitem">
———. 2007a. <span>“How Probable Is an Infinite Sequence of Heads?”</span> <em>Analysis</em> 67 (295): 173–80. <a href="https://doi.org/10.1111/j.1467-8284.2007.00671.x">https://doi.org/10.1111/j.1467-8284.2007.00671.x</a>.
</div>
<div id="ref-Williamson2007-WILTPO-17" class="csl-entry" role="listitem">
———. 2007b. <em><span class="nocase">The Philosophy of Philosophy</span></em>. Blackwell.
</div>
<div id="ref-WilliamsonLofoten" class="csl-entry" role="listitem">
———. 2013. <span>“Gettier Cases in Epistemic Logic.”</span> <em>Inquiry</em> 56 (1): 1–14. <a href="https://doi.org/10.1080/0020174X.2013.775010">https://doi.org/10.1080/0020174X.2013.775010</a>.
</div>
<div id="ref-Yalcin2011" class="csl-entry" role="listitem">
Yalcin, Seth. 2011. <span>“Nonfactualism about Epistemic Modality.”</span> In <em>Epistemic Modality</em>, edited by Andy Egan and Brian Weatherson, 295–332. Oxford: Oxford University Press.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.479">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">
<meta name="dcterms.date" content="2016-03-11">
<meta name="description" content="Gordon Belot has recently developed a novel argument against Bayesianism. He shows that there is an interesting class of problems that, intuitively, no rational belief forming method is likely to get right. But a Bayesian agent’s credence, before the problem starts, that she will get the problem right has to be 1. This is an implausible kind of immodesty on the part of Bayesians. My aim is to show that while this is a good argument against traditional, precise Bayesians, the argument doesn’t neatly extend to imprecise Bayesians. As such, Belot’s argument is a reason to prefer imprecise Bayesianism to precise Bayesianism.">

<title>Online Articles - Brian Weatherson - For Bayesians, Rational Modesty Requires Imprecision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://use.typekit.net/uzz2drx.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Online Articles - Brian Weatherson</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://brian.weatherson.org"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/bweatherson.bsky.social"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">For Bayesians, Rational Modesty Requires Imprecision</h1>
                  <div>
        <div class="description">
          <p>Gordon Belot has recently developed a novel argument against Bayesianism. He shows that there is an interesting class of problems that, intuitively, no rational belief forming method is likely to get right. But a Bayesian agent’s credence, before the problem starts, that she will get the problem right has to be 1. This is an implausible kind of immodesty on the part of Bayesians. My aim is to show that while this is a good argument against traditional, precise Bayesians, the argument doesn’t neatly extend to imprecise Bayesians. As such, Belot’s argument is a reason to prefer imprecise Bayesianism to precise Bayesianism.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">epistemology</div>
                <div class="quarto-category">games and decisions</div>
                <div class="quarto-category">imprecise probability</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="http://brian.weatherson.org">Brian Weatherson</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Michigan
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 11, 2016</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.3998/ergo.12405314.0002.020">10.3998/ergo.12405314.0002.020</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#the-puzzle" id="toc-the-puzzle" class="nav-link active" data-scroll-target="#the-puzzle"><span class="header-section-number">0.1</span> The Puzzle</a></li>
  <li><a href="#making-the-puzzle-less-precise" id="toc-making-the-puzzle-less-precise" class="nav-link" data-scroll-target="#making-the-puzzle-less-precise"><span class="header-section-number">0.2</span> Making the Puzzle Less Precise</a></li>
  <li><a href="#meeting-the-challenge-imprecisely" id="toc-meeting-the-challenge-imprecisely" class="nav-link" data-scroll-target="#meeting-the-challenge-imprecisely"><span class="header-section-number">0.3</span> Meeting the Challenge, Imprecisely</a></li>
  <li><a href="#objections-and-replies" id="toc-objections-and-replies" class="nav-link" data-scroll-target="#objections-and-replies"><span class="header-section-number">0.4</span> Objections and Replies</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="for-bayesians-rational-modesty-requires-imprecision.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>Gordon <span class="citation" data-cites="Belot2013">Belot (<a href="#ref-Belot2013" role="doc-biblioref">2013</a>)</span> has recently developed a novel argument against Bayesianism. He shows that there is an interesting class of problems that, intuitively, no rational belief forming method is likely to get right. But a Bayesian agent’s credence, before the problem starts, that she will get the problem right has to be 1. This is an implausible kind of <em>immodesty</em> on the part of Bayesians.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> My aim is to show that while this is a good argument against traditional, precise Bayesians, the argument doesn’t neatly extend to imprecise Bayesians. As such, Belot’s argument is a reason to prefer imprecise Bayesianism to precise Bayesianism.</p>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;There is another sense of immodesty that is often discussed in the literature, going back to <span class="citation" data-cites="Lewis1971d">Lewis (<a href="#ref-Lewis1971d" role="doc-biblioref">1971</a>)</span>. This is the idea that some agents think their attitudes are optimal by some standards; these are the immodest ones. And often, it is held that not being self-endorsing in this way is a coherence failure <span class="citation" data-cites="Elga2010-ELGHTD">Elga (<a href="#ref-Elga2010-ELGHTD" role="doc-biblioref">2010</a>)</span>. I don’t think this kind of immodesty is rationally required, for reasons set out by Miriam <span class="citation" data-cites="Schoenfield2014">Schoenfield (<a href="#ref-Schoenfield2014" role="doc-biblioref">2015</a>)</span> and Maria <span class="citation" data-cites="Lasonen-Aarnio2015">Lasonen-Aarnio (<a href="#ref-Lasonen-Aarnio2015" role="doc-biblioref">2015</a>)</span>, but in any case that’s not the kind of modesty that’s at issue in Belot’s argument.</p></li></div><aside>
Published in <a href="https://quod.lib.umich.edu/cgi/t/text/idx/e/ergo/12405314.0002.020/--for-bayesians-rational-modesty-requires-imprecision?rgn=main;view=fulltext">Ergo</a> 2: 20.
</aside>
<p>For present purposes, the precise Bayesian agent has just two defining characteristics. First, their credences in all propositions are given by a particular countably additive probability function. Second, those credences are updated by conditionalisation as new information comes in. These commitments are quite strong in some respects. They say that there is a single probability function that supplies the agent’s credences no matter which question is being investigated, and no matter how little evidence the agent has before the investigation is started. The everyday statistician, even one who is sympathetic to Bayesian approaches, may feel no need to sign up for anything this strong. But many philosophers seem to be interested in varieties of Bayesianism that are just this strong. For instance, there has been extensive discussion in recent epistemology of whether various epistemological approaches, such as dogmatism, can be modeled within the Bayesian framework, with the background assumption being that it counts against those approaches if they cannot.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> In these debates, the issue is not whether the Bayesian approach works in the context of a well-defined question and a substantial evidential background, but whether it does so for all questions in all contexts. Indeed, the assumption is that it does, and epistemological theories inconsistent with it are false. So the precise Bayesian is a figure of some interest, at least in epistemology.</p>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;For dogmatism, see <span class="citation" data-cites="Pryor2000">Pryor (<a href="#ref-Pryor2000" role="doc-biblioref">2000</a>)</span>. The canonical argument that it is inconsistent with Bayesianism is <span class="citation" data-cites="White2006">White (<a href="#ref-White2006" role="doc-biblioref">2006</a>)</span>.</p></li><li id="fn3"><p><sup>3</sup>&nbsp;Note that this formulation leaves it open which side of the biconditional is explanatorily prior. I’m going to defend a view on which the left hand side, i.e., the comparative confidences, are more explanatorily basic than the facts about what is in the agent’s representor. I say a little more about why I take this stand in footnote 7. For much more detail on varieties of imprecise Bayesianism, see <span class="citation" data-cites="Walley1991">Walley (<a href="#ref-Walley1991" role="doc-biblioref">1991</a>)</span>, from whom I take the view that the representor and its members are much less explanatorily important than the comparative judgments the agent makes.</p></li></div><p>The imprecise Bayesian doesn’t have a single probability function for their credences. Rather, they have a representor consisting of a set of probability functions. The agent is more confident in <span class="math inline">\(p\)</span> than <span class="math inline">\(q\)</span> just in case <span class="math inline">\(\Pr(p) &gt; \Pr(q)\)</span> for every <span class="math inline">\(\Pr\)</span> in this representor.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Just like the precise Bayesian, the imprecise Bayesian updates by conditionalisation; their new representor after an update is the result of conditionalising every member of the old representor with the new information. The added flexibility in imprecise Bayesianism will allow us to develop a suitably modest response to Belot’s puzzle.</p>
<section id="the-puzzle" class="level3 page-columns page-full" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="the-puzzle"><span class="header-section-number">0.1</span> The Puzzle</h3>
<p>The set up Belot uses is this. An agent, <em>A</em>, will receive a data stream of 0s and 1s. The data stream will go on indefinitely. I will use <span class="math inline">\(\boldsymbol{x}\)</span> for the (infinite) sequence of data she would (eventually) get, <span class="math inline">\(x_k\)</span> for the <span class="math inline">\(k\)</span>th element of this sequence, and <span class="math inline">\(\boldsymbol{x}_k\)</span> for the sequence consisting of the first <span class="math inline">\(k\)</span> elements of the stream. These variables are, as usual, rigid designators. I’ll also use the capitalised <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{X}_k\)</span> as random variables for the sequence itself, and for the first <span class="math inline">\(k\)</span> elements of the sequence, respectively. So <span class="math inline">\(\boldsymbol{X}= \boldsymbol{x}\)</span> is the substantive and true claim that the sequence that will be received is actually <span class="math inline">\(\boldsymbol{x}\)</span>. And <span class="math inline">\(\boldsymbol{X}_k = \boldsymbol{x}_k\)</span> is the substantive and true claim that the first <span class="math inline">\(k\)</span> elements of that sequence are <span class="math inline">\(\boldsymbol{x}_k\)</span>. Propositions of this form will play a major role below, since they summarise the evidence the agent has after <span class="math inline">\(k\)</span> elements have been revealed. I’ll use <span class="math inline">\(+\)</span> as a sequence concatenation operator, so <span class="math inline">\(\boldsymbol{y}+ \boldsymbol{z}\)</span> is the sequence consisting of all of <span class="math inline">\(\boldsymbol{y}\)</span>, followed by all of <span class="math inline">\(\boldsymbol{z}\)</span>.</p>
<p>Belot is interested in a quite general puzzle, but I’ll focus for most of the paper on a very specific instance of the puzzle. (We’ll return to the more general puzzle in the last section.) We’re going to look at the agent’s evolving credence that <span class="math inline">\(\boldsymbol{X}\)</span> is <em>periodic</em>. Let <span class="math inline">\(p\)</span> be the proposition that <span class="math inline">\(\boldsymbol{X}\)</span> is periodic, since we’ll be returning to that proposition a lot. And let’s start by assuming the agent is a precise Bayesian, to see the challenge Belot develops.</p>
<p>Say that the agent <strong>succeeds</strong> just in case her credence in <span class="math inline">\(p\)</span> eventually gets on the correct side of <span class="math inline">\(\frac{1}{2}\)</span>, and stays there. (The correct side is obviously the high side if <span class="math inline">\(p\)</span> is true, and the low side otherwise.) That is, if <span class="math inline">\(v\)</span> is the truth value function, it succeeds just in case this is true.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="math display">\[\exists n \forall m \geq n: |v(p) - \textit{Cr}(p | \boldsymbol{X}_m = \boldsymbol{x}_m)| &lt; \frac{1}{2}\]</span> The agent <strong>fails</strong> otherwise. Given the assumption that the agent is a classical Bayesian, we can step back from evaluating the agent and evaluate her prior probability function directly. So a prior <span class="math inline">\(\Pr\)</span> succeeds relative to <span class="math inline">\(\boldsymbol{x}\)</span> just in case this is true. <span class="math display">\[\exists n \forall m \geq n: |v(p) - \Pr(p | \boldsymbol{X}_m = \boldsymbol{x}_m)| &lt; \frac{1}{2}\]</span> This is reasonably intuitive; the agent is going to get a lot of data about <span class="math inline">\(\boldsymbol{X}\)</span>, and it is interesting to ask whether that data eventually lets her credence in <span class="math inline">\(p\)</span> get to the right side of <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;Belot lets an agent succeed if <span class="math inline">\(\boldsymbol{X}\)</span> is periodic, and the credence in <span class="math inline">\(p\)</span> never drops below <span class="math inline">\(\frac{1}{2}\)</span>, but I think it’s neater to say that the agent is undecided in this case.</p></li></div><p>Given these notions of success and failure, we can naturally define the success set of a prior (or agent) as the set of sequences it succeeds on, and the failure set as the set of sequences it fails on.</p>
<p>Abusing notation a little, say that <span class="math inline">\(\boldsymbol{x}_i \supset \boldsymbol{x}_k\)</span> iff <span class="math inline">\(\boldsymbol{x}_i\)</span> is a sequence that has <span class="math inline">\(\boldsymbol{x}_k\)</span> as its first <span class="math inline">\(k\)</span> entries. Then we can state the first of Belot’s conditions on a good Bayesian agent/prior. A prior is <strong>open-minded</strong> just in case this condition holds: <span class="math display">\[\forall \boldsymbol{x}_k \exists \boldsymbol{x}_i \supset \boldsymbol{x}_k, \boldsymbol{x}_j \supset \boldsymbol{x}_k: \Pr(p | \boldsymbol{X}_i = \boldsymbol{x}_i) &lt; \frac{1}{2} \wedge \Pr(p | \boldsymbol{X}_j = \boldsymbol{x}_j) &gt; \frac{1}{2}\]</span> That is, no matter what happens, it is possible that the probability of <span class="math inline">\(p\)</span> will fall below , and possible it will rise above . To motivate the first, consider any situation where the sequence to date has looked periodic. (If it had not looked periodic to date, presumably the probability of <span class="math inline">\(p\)</span> should already be low.) Now extend that sequence with a large of random noise. At the end of this, it should no longer be probable that the sequence is periodic. On the other hand, assume the sequence has not looked periodic to date. Extend it by repeating <span class="math inline">\(\boldsymbol{x}_k\)</span> more than <span class="math inline">\(k\)</span> times. At the end of this, it should look probable that the sequence is periodic (at least for large enough <span class="math inline">\(k\)</span>). So open-mindedness looks like a good condition to impose.</p>
<p>The second condition we might impose, though not one Belot names, is <em>modesty</em>. Any function might fail. One natural way it might fail is that it might get, to use a term Belot does use, <em>flummoxed</em>. It could change its mind infinitely often about whether the sequence is periodic. By definition, open-mindedness entails the possibility of being flummoxed. Given the definitions of success and failure, <span class="math inline">\(\Pr\)</span> will fail relative to any <span class="math inline">\(\boldsymbol{x}\)</span> that flummoxes it. So success is not a priori guaranteed. Now for any function we can work out the set of sequences relative to which it fails. It turns out this will be a rather large set. Indeed, the set of sequences on which any open-minded function succeeds is meagre.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Say a function is <strong>modest</strong> if the initial probability it gives to <span class="math inline">\(\boldsymbol{X}\)</span> being in its success set is less than 1. Given how large the failure set is, modesty also seems like a good requirement.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;A meagre subset of a space is any set built up as a countable union of nowhere dense sets.</p></li><li id="fn6"><p><sup>6</sup>&nbsp;Belot goes into much more detail about why modesty is a good requirement to put on a rational prior, but I’m omitting those details since I have very little to add to what Belot says.</p></li></div><p>The argument for modesty is not that it is an immediate consequence of regularity. It does follow from regularity, but in the case we’re considering, regularity is quite implausible. Some sets, even some quite large sets in some sense, will have to be given probability 0. The surprising thing is that a residual set (i.e., the complement of a meagre set) gets probability 0.</p>
<p>It might be thought that modesty here is problematic for the same reason that epistemic modesty is often problematic: it validates Moore-paradoxical thoughts. It’s bad to say <em>p, but there is a probability that not p</em>. It’s even bad, though as <span class="citation" data-cites="Briggs2009">Briggs (<a href="#ref-Briggs2009" role="doc-biblioref">2009</a>)</span> points out, not quite bad for the same reasons, to say <em>Whether I believe p is true or false tomorrow, there will be a probability I’m false</em>. Perhaps modesty is a requirement that someone say something like that, and hence is an improper requirement.</p>
<p>But in fact the requirement of modesty is disanalogous to the ‘requirement’, suggested in the previous paragraph, that agents endorse Moore-paradoxical principles. There isn’t anything wrong with saying <em>Whichever side of one half my credence in p is tomorrow, there is a probability that the truth will be the other side of one half</em>. That’s not Moore-paradoxical. Indeed, unless one is sure that one’s credence in <span class="math inline">\(p\)</span> tomorrow will be 0 or 1, it is something one should endorse.</p>
<p>Or consider a different example. There will be a sequence of 0s and 1s, but this time there will only be three elements, and the agent will only be shown the first of them tomorrow. Let <span class="math inline">\(q\)</span> be the proposition that there are more 1s than 0s in the three-element sequence. Say the agent <strong>succeeds</strong> iff tomorrow, after seeing just one element, her credence in <span class="math inline">\(q\)</span> is the same side of one-half as the truth. And say the agent is <strong>modest</strong> iff, right now, her credence that she succeeds tomorrow is less than one. There is nothing incoherent about being modest. If her credal distribution today is completely flat, giving <span class="math inline">\(\frac{1}{8}\)</span> credence to each of the eight possible sequences, she will be modest, for example.</p>
<p>Now this case is somewhat different to the one Belot started with in a couple of respects. On the one hand, we’re asking about modesty at a particular point, i.e., tomorrow, rather than over a long sequence. On the other hand, we’re asking about whether the agent’s credences will be on the right side of one-half after having seen one-third of the data, rather than, as in the original case, after seeing measure zero of the sequence. The first difference makes it easier to be modest, the second difference makes it harder. So the cases are not perfect analogies, but they are similar enough in respect of modesty to make it plausible that if modesty is coherent in this case, as we’ve shown it is, then it should be coherent in Belot’s case as well.</p>
<p>So that’s the argument that open-mindedness and modesty are good conditions for priors to satisfy. Here’s the worrying result that Belot proves. There are no open-minded modest priors. If <span class="math inline">\(A\)</span> is a classical Bayesian, she will either have to be closed minded or immodest. Neither seems rational, so it seems that being a classical Bayesian is incompatible with being rational. That is, we can’t be precise Bayesians if we accept the following two constraints.</p>
<ul>
<li><p><em>Open-Mindedness</em>: For any initial sequence, there is a continuation after which it seems probable that <span class="math inline">\(\boldsymbol{X}\)</span> is periodic, and a continuation after which it seems probable that <span class="math inline">\(\boldsymbol{X}\)</span> is not periodic.</p></li>
<li><p><em>Modesty</em>: The initial probability that the agent will succeed, i.e., that their credence in <span class="math inline">\(p\)</span> will eventually get to the right side of <span class="math inline">\(\frac{1}{2}\)</span> and stay there, is less than 1.</p></li>
</ul>
<p>Since both open-mindedness and modesty are very plausible constraints, it follows that there is no good way to be a precise Bayesian in the face of this puzzle.</p>
</section>
<section id="making-the-puzzle-less-precise" class="level3 page-columns page-full" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="making-the-puzzle-less-precise"><span class="header-section-number">0.2</span> Making the Puzzle Less Precise</h3>
<p>What happens, though, if the agent is an imprecise Bayesian? Is there a parallel version of Belot’s argument that shows this kind of imprecise Bayesian is necessarily irrational? I’m going to argue that the answer is no.</p>
<p>The first thing we have to do is work out how to redefine the key terms in Belot’s argument once we drop the assumption that the agent is a classical Bayesian. There are several ways of formulating our definitions which are equivalent given that assumption, but not equivalent given that the agent is an imprecise Bayesian. There are three major choice points here.</p>
<ol type="1">
<li><p>What is success?</p></li>
<li><p>What is open-mindedness?</p></li>
<li><p>What is modesty?</p></li>
</ol>
<p>Assume our agent’s credal state is represented by set <span class="math inline">\(S\)</span> of probability functions. Then there are two natural ways to think about success. <span class="math display">\[\begin{aligned}
\forall \Pr \in S:  \exists n \forall m \geq n: |v(p) - \Pr(p | \boldsymbol{X}_m= \boldsymbol{x}_m)| &lt; \frac{1}{2} \\
\exists n \forall \Pr \in S: \forall m \geq n: |v(p) - \Pr(p | \boldsymbol{X}_m= \boldsymbol{x}_m)| &lt; \frac{1}{2}\end{aligned}\]</span> The second is obviously stronger than the first, since it involves moving an existential quantifier out in front of a universal quantifier. And there are some natural cases where an agent could succeed on the first definition, and fail on the second. Here’s one such case.</p>
<p>Let <span class="math inline">\(\Pr_0\)</span> be the <em>fair-coin measure</em>. Acccording to the fair coin measure, if <span class="math inline">\(\boldsymbol{y}\)</span> is any <span class="math inline">\(k\)</span> length sequence of 0s and 1s we have <span class="math inline">\(\Pr_0(\boldsymbol{x}_k = \boldsymbol{y}) = 2^{-k}\)</span>. Intuitively, it thinks the 0s and 1s are generated by flips of a fair coin, and it won’t change its mind about that no matter what happens.</p>
<p>Say a probability function <span class="math inline">\(\Pr\)</span> is <strong>regular periodic</strong> iff it satisfies these two conditions.</p>
<ul>
<li><p><span class="math inline">\(\Pr(p) = 1\)</span>.</p></li>
<li><p>For any periodic sequence <span class="math inline">\(\boldsymbol{y}\)</span>, <span class="math inline">\(\Pr(\boldsymbol{X}= \boldsymbol{y}) &gt; 0\)</span>.</p></li>
</ul>
<p>Intuitively, these functions are certain that <span class="math inline">\(X\)</span> is periodic, and assign positive probability to each possible periodic sequence. Now consider the family of functions we get by taking equal weighted mixtures of <span class="math inline">\(\Pr_0\)</span> with each regular periodic function. Let that family represent the agent’s credence. And assume for now that <span class="math inline">\(\boldsymbol{X}\)</span> is the sequence <span class="math inline">\(\langle 0, 0, 0, \dots \rangle\)</span>. Does the agent succeed?</p>
<p>Well, each <span class="math inline">\(\Pr\)</span> in her representor succeeds. To prove this, it will be helpful to prove a lemma that we’ll again have use for below. For this lemma, let <span class="math inline">\(\Pr_0\)</span> be the fair-coin measure (as already noted), <span class="math inline">\(\Pr_1\)</span> be any measure such that <span class="math inline">\(\Pr_1(p) = 1\)</span>, and <span class="math inline">\(\Pr_2\)</span> be the equal mixture of <span class="math inline">\(\Pr_0\)</span> and <span class="math inline">\(\Pr_1\)</span>.</p>
<p><strong>Lemma 1</strong>: <span class="math inline">\(\Pr_2(p | \boldsymbol{X}_k = \boldsymbol{y}_k) &gt; \frac{1}{2}\)</span> iff <span class="math inline">\(\Pr_1(\boldsymbol{X}_k = \boldsymbol{y}_k) &gt; \Pr_0(\boldsymbol{X}_k = \boldsymbol{y}_k).\)</span></p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Let <span class="math inline">\(\Pr_i(\boldsymbol{X}_k = \boldsymbol{y}_k) = a_i\)</span> for <span class="math inline">\(i \in {0, 1}\)</span>. Recall that <span class="math inline">\(\Pr_0(p) = 0\)</span> and <span class="math inline">\(\Pr_1(p) = 1\)</span>. Then we can quickly get that <span class="math inline">\(\Pr_2(p | \boldsymbol{X}_k = \boldsymbol{y}_k) = \frac{a_1}{a_0 + a_1}\)</span>, from which the lemma immediately follows.&nbsp;◻</p>
</div>
<p>For any <span class="math inline">\(\Pr\)</span> in the agent’s representor, there is some <span class="math inline">\(k\)</span> such that <span class="math inline">\(\Pr(\boldsymbol{X}= \langle 0, 0, 0, \dots \rangle)\)</span> <span class="math inline">\(&gt; 2^{-k}\)</span>. So after at most <span class="math inline">\(k\)</span> 0s have appeared, <span class="math inline">\(\Pr(p)\)</span> will be above <span class="math inline">\(\frac{1}{2}\)</span>, and it isn’t coming back. That means it succeeds. And since <span class="math inline">\(\Pr\)</span> was arbitrary, it follows that all <span class="math inline">\(\Pr\)</span> succeed.</p>
<p>But the agent in a good sense doesn’t succeed. No matter how much data she gets, there will be <span class="math inline">\(\Pr\)</span> in her representor according to which <span class="math inline">\(\Pr(p) &lt; \frac{1}{2}\)</span>. After all, for any <span class="math inline">\(k\)</span>, there are regular periodic <span class="math inline">\(\Pr\)</span> such that the probability of <span class="math inline">\(\boldsymbol{x}_k\)</span> being <span class="math inline">\(k\)</span> 0s is below <span class="math inline">\(\frac{1}{2^k}\)</span>. So if we mix that function with <span class="math inline">\(\Pr_0\)</span>, we get a function where the most probable continuations of this initial sequence are the random sequences provided by the fair coin measure.</p>
<p>In terms of our definitions of success above, the agent satisfies the first, but not the second. Every function in her representor eventually has the probability of <span class="math inline">\(p\)</span> go above <span class="math inline">\(\frac{1}{2}\)</span>. But at any time, there are functions in her representor according to which the probability of <span class="math inline">\(p\)</span> is arbitrarily low.</p>
<p>Here I think we have to make a distinction between different ways of understanding the formalism of imprecise probabilities. (What follows is indebted to <span class="citation" data-cites="Bradley2014">Bradley (<a href="#ref-Bradley2014" role="doc-biblioref">2014</a>)</span>, especially his section 3.1, but I’m disagreeing somewhat with his conclusions, and following more closely the conclusions of <span class="citation" data-cites="Joyce2010">Joyce (<a href="#ref-Joyce2010" role="doc-biblioref">2010</a>)</span> and <span class="citation" data-cites="Schoenfield2012">Schoenfield (<a href="#ref-Schoenfield2012" role="doc-biblioref">2012</a>)</span>.)</p>
<p>One way of thinking about imprecise credences is that each probability function in the representor is something like an advisor, and the agent who is imprecise simply hasn’t settled on which advisor to trust. Call this the <strong>pluralist</strong> interpretation of the formalism. On this interpretation, it is natural to think that what is true of every function is true of the agent.</p>
<p>Another way is to think of the agent’s mind as constituted by, but distinct from, the representors. An analogy to keep in mind here is the way that a parliament is constituted by, but distinct from, its members. Keeping with this analogy, call this the <strong>corporate</strong> interpretation of the formalism. Note that corporate bodies will typically have their own rules for how the views of the members will be translated into being views of the whole. Even if every member of the parliament believes that the national cricket team will win its upcoming game, it doesn’t follow that the parliament believes that; the parliament only believes what it resolves it has believed.</p>
<p>Now I only want to defend the imprecise Bayesian model on the corporate interpretation.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> The pluralist interpretation, it seems to me, faces grave difficulties. For one thing, it has a hard time explaining what’s wrong with the existential claim “There is a precise number <span class="math inline">\(x\)</span> such that <span class="math inline">\(x\)</span> is the probability of <span class="math inline">\(p\)</span>”. Every advisor believes that, so on the pluralist model the agent does too. (Compare the criticisms of “fanatical supervaluationism” in <span class="citation" data-cites="Lewis1993c">Lewis (<a href="#ref-Lewis1993c" role="doc-biblioref">1993</a>)</span>.) More relevant to the discussion here, I am following Belot in thinking we have an argument that each precise Bayesian is unreasonably proud. On the pluralist interpretation, the agent is undecided which of these unreasonable advisors she will follow. But such a state is itself unreasonable; she should have decided not to follow any of them, since they are all provably unreasonable!</p>
<div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;I have an independent metaphysical reason for preferring the corporate interpretation. I think that comparative confidences, things like being at least as confident in <span class="math inline">\(p\)</span> as in <span class="math inline">\(q\)</span>, are metaphysically prior to numerical credences, or even sets of numerical credences. On such a metaphysics, what it is for <span class="math inline">\(\Pr\)</span> to be in the representor just is for every <span class="math inline">\(p, q, r, s\)</span>, if the agent is at least as confident in <span class="math inline">\(p\)</span> given <span class="math inline">\(q\)</span> as in <span class="math inline">\(r\)</span> given <span class="math inline">\(s\)</span>, then <span class="math inline">\(\Pr(p | q) \geq \Pr(r | s)\)</span>. And it seems, though I won’t defend this claim here, that the corporate interpetation fits more naturally with the idea that comparative confidences are primitive.</p></li></div><p>A surprising fact about corporate bodies is that they can be immune to problems that beset each of their members. It would be illegitimate for any one parliamentarian to have law-making power; it is (or at least can be) legitimate for them all to have such power. Indeed, it would be unreasonable for any of them to think that they individually should have law-making powers; that would be unreasonably proud. But it is not unreasonable for them to collectively think that they should collectively have law-making powers. If they are a well-constituted parliament, this is a perfectly reasonable thought. Similarly here, the agent, the corporate body, could avoid being unreasonably proud even though each of the representors is over-confident in its own powers.</p>
<p>Now going back to success and modesty, it seems to me that the first definition of success is appropriate on the pluralist interpretation of the imprecise framework, and the second is appropriate on the corporate interpretation. The first interpretation says that the agent succeeds iff every member succeeds. And the second says that the agent succeeds iff the body of functions, collectively, succeed. Since I’m defending the use of the imprecise framework on the corporate interpretation, it is the second definition of success that is appropriate, and that’s what I will use here.</p>
<p>This understanding isn’t without costs. <span class="citation" data-cites="Bradley2014">Bradley (<a href="#ref-Bradley2014" role="doc-biblioref">2014</a>)</span> argues, in effect, that the best responses to dilation-based arguments against imprecise probabilities (as in <span class="citation" data-cites="White2010">White (<a href="#ref-White2010" role="doc-biblioref">2010</a>)</span>), are only available on the pluralist interpretation. I’m not going to try to solve those problems here, but I will note that the interpretative choice I’m making generates some extra philosophical work elsewhere. Against that, the corporate interpretation has some benefits. It lets us agree with Peter <span class="citation" data-cites="Walley1991">Walley (<a href="#ref-Walley1991" role="doc-biblioref">1991</a>)</span> that there are rational agents who are represented by sets of merely finitely additive probability functions, though no merely finitely additive probability function on its own could represent a rational agent. So the issues between the two interpretations are extensive. For now, I’ll simply note that I’m interested in defending the imprecise Bayesian from Belot’s argument on the corporate interpretation. And with that I’ll return to translating Belot’s puzzle into the imprecise framework, with the second, corporate-friendly, interpretation of success on board.</p>
<p>There are also two natural ways to generalise Belot’s notion of open-mindedness to the imprecise case. We could require that the agent satisfies either the first or second of these conditions. <span class="math display">\[\begin{aligned}
\forall \boldsymbol{x}_k \exists \boldsymbol{x}_i \supset \boldsymbol{x}_k, \boldsymbol{x}_j \supset \boldsymbol{x}_k: \neg(\Pr(p | \boldsymbol{X}_i = \boldsymbol{x}_i) \geq \frac{1}{2}) \wedge \neg(\Pr(p | \boldsymbol{X}_j = \boldsymbol{x}_j) &lt; \frac{1}{2}) \\
\forall \boldsymbol{x}_k \exists \boldsymbol{x}_i \supset \boldsymbol{x}_k, \boldsymbol{x}_j \supset \boldsymbol{x}_k: \Pr(p | \boldsymbol{X}_i = \boldsymbol{x}_i) &lt; \frac{1}{2} \wedge \Pr(p | \boldsymbol{X}_j = \boldsymbol{x}_j) \geq \frac{1}{2}\end{aligned}\]</span> The second is just the same symbols as in Belot’s, and it is what I’ll end up arguing is the right constraint to put on the imprecise Bayesian agent. And it is a considerably more demanding constraint than the first. But the first is perhaps the more natural understanding of <em>open-mindedness</em>. It says that no matter what the initial evidence is, the agent is not guaranteed to settle her credence in <span class="math inline">\(p\)</span> on one side of <span class="math inline">\(\frac{1}{2}\)</span>. That’s a way of being open-minded.</p>
<p>But if the agent satisfies that constraint, she may be open-minded, but she won’t necessarily be responsive to the evidence. Here’s how I’m using the terms ‘open-minded’ and ‘evidence-responsive’. In both clauses, the quantification is intended to be over a salient class of propositions. (The relevant class in the application we’re most interested in is just <span class="math inline">\(\{X\)</span> is periodic, <span class="math inline">\(X\)</span> is not periodic<span class="math inline">\(\}\)</span>.) And I’ll say an agent is ‘confident’ in a proposition iff her credence in it is above <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<dl>
<dt>Open-Minded</dt>
<dd>
<p>Any time an agent is confidence in a proposition, there is some evidence she could get that would make her lose confidence in it.</p>
</dd>
<dt>Evidence-Responsive</dt>
<dd>
<p>For any proposition, there is some evidence the agent could get that would make her confident in it.</p>
</dd>
</dl>
<p>Once we allow imprecise credences, these two notions can come apart. Consider the agent we described above, whose representor consists of equal mixtures of the fair-coin measure and regular periodic functions. They are open-minded; they can always lose confidence that <span class="math inline">\(X\)</span> is periodic or not. But they aren’t evidence-responsive; no matter what the evidence, their credence that <span class="math inline">\(X\)</span> is periodic will never rise above <span class="math inline">\(\frac{1}{2}\)</span>. In fact, their credence that <span class="math inline">\(X\)</span> is periodic will never rise above any positive number.</p>
<p>That suggests open-mindedness is too weak a constraint. If the evidence the agent gets is a string of several hundred 0s, she shouldn’t just lose any initial confidence in <span class="math inline">\(\neg p\)</span>, she should become confident in <span class="math inline">\(p\)</span>. And arguably (though I could imagine a dissent here), if the initial sequence is a seemingly random sequence, the credence in <span class="math inline">\(p\)</span> should drop well below <span class="math inline">\(\frac{1}{2}\)</span>. (The imagined dissent here is from someone who thinks that the noisier the data, the more imprecise credences should get. That’s an interesting view, but perhaps orthogonal to the issues we’re debating here.)</p>
<p>And when we look back at Belot’s motivations for open-mindedness, we see that they are really motivations for being evidence-responsive. One of the distinctive (and I would say problematic) features of precise Bayesianism is that it doesn’t really have a good way of representing a state of indecisiveness or open-mindedness. In the terms we’ve been using here, there’s no difference for the precise Bayesian between being evidence responsive and open minded. The imprecise Bayesian can distinguish these. And in Belot’s puzzle, we should require that the imprecise Bayesian agent is evidence responsive. So we should impose the second, stronger, condition.</p>
<p>The final condition to discuss is modesty. There are three natural candidates here. We could merely require that the agent’s prior probability that <span class="math inline">\(\boldsymbol{x}\)</span> is in her success set is not equal to 1. Or we could require that it be less than 1. Or, even more strongly, we could require that it be less than some number that is less than 1. If her credence that <span class="math inline">\(\boldsymbol{x}\)</span> is in her success set is imprecise over some interval <span class="math inline">\([k, 1]\)</span>, she satisfies the first condition, but not the second or third. If it is imprecise over some interval <span class="math inline">\((k, 1)\)</span>, or <span class="math inline">\([k, 1)\)</span>, she satisfies the first and second conditions, but not the third. In the interests of setting the imprecise Bayesian the hardest possible challenge, though, let’s say that modesty requires the third criteria. Her ex ante credence in success should not just be less than 1, it should be less than some number less than 1.</p>
<p>The aim of the next section is to describe a representor that satisfies open-mindedness and modesty with respect to the question of whether the sequence is periodic. The representor will not represent a state that it is rational for a person to be in; we’ll come back in the last section to the significance of this. My aim is just to show that for the imprecise Bayesian, unlike the precise Bayesian, open-mindedness and modesty are compatible. And the proof of this will be constructive; I’ll build a representor that is, while flawed in some other ways, open-minded and modest.</p>
</section>
<section id="meeting-the-challenge-imprecisely" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="meeting-the-challenge-imprecisely"><span class="header-section-number">0.3</span> Meeting the Challenge, Imprecisely</h3>
<p>Recall that <span class="math inline">\(\Pr_0\)</span> is the <em>fair-coin measure</em>, according to which, if <span class="math inline">\(\boldsymbol{y}\)</span> is any <span class="math inline">\(k\)</span> length sequence of 0s and 1s we have <span class="math inline">\(\Pr_0(\boldsymbol{X}_k = \boldsymbol{y}) = 2^{-k}\)</span>.</p>
<p>Say a finite sequence <span class="math inline">\(\boldsymbol{y}_k\)</span> of length <span class="math inline">\(k\)</span> is <strong>repeating</strong> iff for some <span class="math inline">\(n &gt; 1\)</span>, <span class="math inline">\(\boldsymbol{y}_k\)</span> consists of <span class="math inline">\(n\)</span> repetitions of a sequence of length <span class="math inline">\(k/n\)</span>. For any non-repeating sequence <span class="math inline">\(\boldsymbol{y}_k\)</span> (of length <span class="math inline">\(k\)</span>) let <span class="math inline">\(\boldsymbol{s}_{\boldsymbol{y}_k}\)</span> be the sequence consisting of <span class="math inline">\(\boldsymbol{y}_k\)</span> repeated infinitely often. Let <span class="math inline">\(\Pr_1\)</span> be the function such that, <span class="math display">\[\Pr{}_1(\boldsymbol{X}= \boldsymbol{s}_{\boldsymbol{y}_k}) = \frac{1}{2^{2k}-1}\]</span> Intuitively, we can think of <span class="math inline">\(\Pr_1\)</span> as follows. Consider a measure over representations of periodic sequences. Any periodic sequence can be represented just as a finite sequence, plus the instruction <em>repeat infinitely often</em>, so this is really just a measure over finite sequences. One natural such measure assigns measure <span class="math inline">\(\frac{1}{2^{2k}}\)</span> to each sequence of length <span class="math inline">\(k\)</span>. Of course, several of these representations will be representations of the same sequence. For instance, <span class="math inline">\(\langle 0, 1 \rangle\)</span>, <span class="math inline">\(\langle 0, 1, 0, 1 \rangle\)</span> and <span class="math inline">\(\langle 0, 1, 0, 1, 0, 1 \rangle\)</span> repeated infinitely produce the same sequence. Now the probability of a sequence, according to <span class="math inline">\(\Pr_1\)</span> is just the measure, so defined, of the class of representations of that measure. (It’s a little easier to confirm that the measures sum to 1 than that the probabilities do, which is why I’ve included this little explanation.)</p>
<p>Now define <span class="math inline">\(\Pr_2\)</span> as the equal weight mixture of <span class="math inline">\(\Pr_0\)</span> and <span class="math inline">\(\Pr_1\)</span>, i.e., <span class="math inline">\(\Pr_2(q) = (\Pr_0(q) + \Pr_1(q))/2\)</span>. Since <span class="math inline">\(\Pr_0(p) = 0\)</span>, and <span class="math inline">\(\Pr_1(p) = 1\)</span>, <span class="math inline">\(\Pr_2(p) = \frac{1}{2}\)</span>. There will be several facts about <span class="math inline">\(\Pr_2\)</span> that are useful to have in place for future reference. (Recall I’m using <span class="math inline">\(\boldsymbol{X}\)</span> as a random variable for the sequence the agent will see, <span class="math inline">\(\boldsymbol{x}\)</span> as a rigid designator of that sequence, <span class="math inline">\(\boldsymbol{y}\)</span> and <span class="math inline">\(\boldsymbol{z}\)</span> are variables for arbitrary sequences, and the <span class="math inline">\(k\)</span> subscript to restrict sequences to length <span class="math inline">\(k\)</span>.) The first of these was proven as Lemma&nbsp;1.</p>
<dl>
<dt>Lemma&nbsp;1.</dt>
<dd>
<p><span class="math inline">\(\Pr_2(p | \boldsymbol{X}_k = \boldsymbol{y}_k) &gt; \frac{1}{2} \text{ iff } \Pr_1(\boldsymbol{X}_k = \boldsymbol{y}_k) &gt; \Pr_0(\boldsymbol{X}_k = \boldsymbol{y}_k).\)</span></p>
</dd>
</dl>
<p>Define a new predicate <span class="math inline">\(N\)</span> of finite sequences <span class="math inline">\(\boldsymbol{y}_k\)</span>, to hold just in case <span class="math inline">\(\boldsymbol{y}_k\)</span> could be the initial segment of an infinite sequence of period at most <span class="math inline">\(\frac{k}{2}\)</span>. So <span class="math inline">\(\boldsymbol{y}_k\)</span> must consist of some sequence repeated twice, and anything else in <span class="math inline">\(\boldsymbol{y}_k\)</span> must be consistent with that sequence repeating again (and if necessary again, and again, …). Then we get,</p>
<dl>
<dt>Lemma 2.</dt>
<dd>
<p>For <span class="math inline">\(k \geq 2\)</span>, <span class="math inline">\(\Pr_2(p | \boldsymbol{X}_{2k} = \boldsymbol{y}_{2k}) &gt; \frac{1}{2}\)</span> iff <span class="math inline">\(N\boldsymbol{y}_{2k}\)</span>.</p>
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> By Lemma&nbsp;1, this reduces to the question of the relationship <span class="math inline">\(\Pr_1(\boldsymbol{X}_{2k} = \boldsymbol{y}_{2k}) &gt; \Pr_0(\boldsymbol{X}_{2k} = \boldsymbol{y}_{2k})\)</span>. Moreover, we know that <span class="math inline">\(\Pr_0(\boldsymbol{X}_{2k} = \boldsymbol{y}_{2k}) = 2^{-2k}\)</span>. So the question is whether <span class="math inline">\(\Pr_1(\boldsymbol{X}_{2k} = \boldsymbol{y}_{2k}) &gt; 2^{-2k}\)</span>.</p>
<p>If <span class="math inline">\(N\boldsymbol{y}_{2k}\)</span>, then it is consistent with <span class="math inline">\(\boldsymbol{X}_{2k} = \boldsymbol{y}_{2k}\)</span> that <span class="math inline">\(\boldsymbol{x}\)</span> is a particular periodic sequence with period at most <span class="math inline">\(k\)</span>. Since the probability, according to <span class="math inline">\(\Pr_1\)</span> of any such sequence is greater than <span class="math inline">\(2^{-2k}\)</span>, the right-to-left direction follows.</p>
<p>If <span class="math inline">\(\neg N\boldsymbol{y}_{2k}\)</span>, then the possibilities that get positive probability according to <span class="math inline">\(\Pr_1\)</span> are at most among the following: <span class="math inline">\(\boldsymbol{X}\)</span> consists of the first <span class="math inline">\(k + 1\)</span> digits of <span class="math inline">\(\boldsymbol{y}_{2k}\)</span> repeated endlessly; <span class="math inline">\(\boldsymbol{X}\)</span> consists of the first <span class="math inline">\(k + 2\)</span> digits of <span class="math inline">\(\boldsymbol{y}_{2k}\)</span> repeated endlessly; …; <span class="math inline">\(\boldsymbol{x}\)</span> consists of the first <span class="math inline">\(2k\)</span> digits of <span class="math inline">\(\boldsymbol{y}_{2k}\)</span> repeated endlessly; <span class="math inline">\(\boldsymbol{X}\)</span> is one of the two sequences of period <span class="math inline">\(2k + 1\)</span> starting with <span class="math inline">\(\boldsymbol{y}_{2k}\)</span>, or one of the four sequences of period <span class="math inline">\(2k+2\)</span> starting with <span class="math inline">\(\boldsymbol{y}_{2k}\)</span> or …. So we get the following, starting with the probabilities of each of the possibilities listed in the previous sentence, <span class="math display">\[\begin{aligned}
\Pr{}_1(\boldsymbol{X}_{2k} = \boldsymbol{y}_{2k})
    &amp;\leq \frac{1}{2^{2k+2}-1}
    &amp;+ &amp;\frac{1}{2^{2k+4}-1}
    &amp;+ \dots
    &amp;+ &amp;\frac{1}{2^{4k}-1}
    &amp;+ &amp;\frac{2}{2^{4k+2}-1}
    &amp;+ \dots \\
%
    &amp;&lt; \frac{1}{2^{2k+1}}
    &amp;+ &amp;\frac{1}{2^{2k+3}}
    &amp;+ \dots
    &amp;+ &amp;\frac{1}{2^{4k-1}}
    &amp;+ &amp;\frac{1}{2^{4k}}
    &amp;+ \dots \\
%
    &amp;&lt; \frac{1}{2^{2k}}\end{aligned}\]</span> And from that the left-to-right direction follows.&nbsp;◻</p>
</div>
<dl>
<dt>Lemma 3</dt>
<dd>
<p><span class="math inline">\(\Pr_2\)</span> is open-minded.</p>
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Since any initial sequence <span class="math inline">\(\boldsymbol{y}_k\)</span> that is not <span class="math inline">\(N\)</span> can be easily extended into one that is <span class="math inline">\(N\)</span> (by, e.g., repeating <span class="math inline">\(\boldsymbol{y}_k\)</span>), and one is that is <span class="math inline">\(N\)</span> can be extended into one that is not (by, e.g., having the repeating sequence stop at the very next step), this follows immediately from Lemma&nbsp;2.&nbsp;◻</p>
</div>
<p>Define <span class="math inline">\(f\)</span> to be a function from sequences of length <span class="math inline">\(k \geq 2\)</span> to sequences of length <span class="math inline">\(k+1\)</span> such that <span class="math display">\[f(\boldsymbol{y}_k) = \boldsymbol{y}_k +
    \begin{cases}
        \langle 0 \rangle &amp;\text{if } N\boldsymbol{y}_k \leftrightarrow \Pr{}_1(x_{k+1} = 0 | \boldsymbol{X}_k = \boldsymbol{y}_k) \leq \frac{1}{2} \\
        \langle 1 \rangle &amp;\text{otherwise}
    \end{cases}\]</span> In the normal way, define <span class="math inline">\(f^n(\boldsymbol{y}_k)\)</span> to be the result of applying <span class="math inline">\(f\)</span> <span class="math inline">\(n\)</span> times to <span class="math inline">\(\boldsymbol{y}_k\)</span>. And define <span class="math inline">\(f^\infty(\boldsymbol{y}_k)\)</span> to be the infinite sequence we get by doing this infintely often.</p>
<p>Intuitively, the way <span class="math inline">\(f\)</span> works is that if <span class="math inline">\(\boldsymbol{y}_k\)</span> is already somewhat sequential, then we include the less likely digit, and if it isn’t, then we include the more likely digit. (With ties resolved in favour of including 0 rather than 1.) If we define <span class="math inline">\(p(\boldsymbol{y}_k)\)</span> to be the smallest <span class="math inline">\(n\)</span> such that <span class="math inline">\(\boldsymbol{y}_k\)</span> could be the initial segment of a periodic sequence of length <span class="math inline">\(n\)</span>, then we’ll get that <span class="math inline">\(p(f(\boldsymbol{y}_k)) &gt; p(\boldsymbol{y}_k) \leftrightarrow N\boldsymbol{y}_k\)</span> in all cases, except for the case where <span class="math inline">\(\Pr{}_1(\boldsymbol{x}_k = 0 | \boldsymbol{X}_k = \boldsymbol{y}_k) = \frac{1}{2}\)</span>. That is, if <span class="math inline">\(N\boldsymbol{y}_k\)</span>, then extending <span class="math inline">\(\boldsymbol{y}_k\)</span> in this way will wipe out the possibility of that smallest sequence being extended indefinitely, while if <span class="math inline">\(\neg N\boldsymbol{y}_k\)</span>, then that possibility will still be on the table.</p>
<p>From this, it follows that <span class="math inline">\(f^{\infty}(\boldsymbol{y}_k)\)</span> will flummox <span class="math inline">\(\Pr_2\)</span>, no matter which <span class="math inline">\(\boldsymbol{y}_k\)</span> we start with.</p>
<p>We need one last classification of finite sequences, and then we are done. Say that <span class="math inline">\(O\boldsymbol{y}_k\)</span> just in case some initial segment of <span class="math inline">\(\boldsymbol{y}_k\)</span> of length <span class="math inline">\(r\)</span> could be the initial segment of an infinite period sequence of period less than <span class="math inline">\(\frac{r}{2}\)</span>. This contrasts with <span class="math inline">\(N\)</span> in two ways. First, it requires a sequence that repeats twice, and then starts a third repetition. Second, it does not require that the sequence be ‘live’; there might be subsequent parts of <span class="math inline">\(\boldsymbol{y}_k\)</span> that are not compatible with the sequence repeating. So the sequence <span class="math inline">\(\langle 0, 0, 1, 0, 0, 1\rangle\)</span> satisfies <span class="math inline">\(N\)</span> but not <span class="math inline">\(O\)</span>, while the sequence <span class="math inline">\(\langle 0, 1, 0, 1, 0, 0\rangle\)</span> satisfies <span class="math inline">\(O\)</span> but not <span class="math inline">\(N\)</span>.</p>
<p>There are a countable infinity of finite sequences <span class="math inline">\(\boldsymbol{y}_k\)</span> such that <span class="math inline">\(\neg O \boldsymbol{y}_k\)</span>. Produce some ordering of them, then define <span class="math inline">\(\Pr_i\)</span>, for <span class="math inline">\(i \geq 3\)</span>, to be the probability function such that <span class="math inline">\(\Pr_i(\boldsymbol{X}= f^\infty(\boldsymbol{y}_k)) = 1\)</span>, where <span class="math inline">\(\boldsymbol{y}_k\)</span> is the <span class="math inline">\(i-2\)</span>’th sequence in this order.</p>
<p>Now, consider the set <span class="math inline">\(R\)</span> of all probability functions of the form: <span class="math display">\[\Pr = \sum_{i = 2}^\infty a_i\Pr{}_i\]</span> where each of the <span class="math inline">\(\Pr_i\)</span> are defined as above, each <span class="math inline">\(a_i\)</span> is non-negative, <span class="math inline">\(a_2\)</span> is , and the sum of the <span class="math inline">\(a_i\)</span> from 3 to <span class="math inline">\(\infty\)</span> is also <span class="math inline">\(\frac{1}{2}\)</span>. Intuitively, each function starts by halving the probability <span class="math inline">\(\Pr_2\)</span> gives to each initial (or completed) sequence, and distributing the remaining probability over the countable infinity of flummoxing sequences of the form <span class="math inline">\(f^\infty(\boldsymbol{y}_k)\)</span>, where <span class="math inline">\(\neg O\boldsymbol{y}_k\)</span>.</p>
<p>I’ll now prove that <span class="math inline">\(R\)</span> is open minded.</p>
<dl>
<dt>Lemma 4</dt>
<dd>
If <span class="math inline">\(\neg O \boldsymbol{y}_k\)</span>, then <span class="math inline">\(\neg O f(\boldsymbol{y}_k)\)</span>.
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Since <span class="math inline">\(\neg O \boldsymbol{y}_k\)</span>, the only way that <span class="math inline">\(O f(\boldsymbol{y}_k)\)</span> could be true is if <span class="math inline">\(k = 2r +1\)</span>, and <span class="math inline">\(f(\boldsymbol{y}_k)\)</span> consists of some sequence of length <span class="math inline">\(r\)</span> repeated twice, plus the first digit repeated a third time. But that means that <span class="math inline">\(N\boldsymbol{y}_k\)</span>. And if that’s the case, then the extra digit that is added by <span class="math inline">\(f(\boldsymbol{y}_k)\)</span> will not be the necessary digit to repeat this sequence. So it is impossible that <span class="math inline">\(O f(\boldsymbol{y}_k)\)</span>.&nbsp;◻</p>
</div>
<dl>
<dt>Lemma 5</dt>
<dd>
If <span class="math inline">\(\neg O \boldsymbol{y}_k\)</span>, then <span class="math inline">\(\neg O f^\infty(\boldsymbol{y}_k)\)</span>.
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> This follows trivially from Lemma&nbsp;4.&nbsp;◻</p>
</div>
<dl>
<dt>Theorem 6</dt>
<dd>
<span class="math inline">\(R\)</span> is open-minded.
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Any initial sequence can be extended to a sequence satisfying <span class="math inline">\(O\)</span>. For example, the initial sequence can be repeated in full twice. An immediate consequence of Lemma&nbsp;5 is that for all <span class="math inline">\(i \geq 3, O\boldsymbol{y}_k \rightarrow \Pr_i(\boldsymbol{X}_k = \boldsymbol{y}_k) = 0\)</span>. That means that if <span class="math inline">\(O\boldsymbol{y}_k\)</span>, then for any <span class="math inline">\(\Pr \in R,{ } \Pr(p | \boldsymbol{X}_k = \boldsymbol{y}_k) = \Pr_2(p | \boldsymbol{X}_k = \boldsymbol{y}_k)\)</span>. And now the theorem is an immediate consequence of Lemma&nbsp;3.&nbsp;◻</p>
</div>
<p>Let <span class="math inline">\(F\)</span> be the set of all sequences <span class="math inline">\(f^\infty(\boldsymbol{y}_k)\)</span>, where <span class="math inline">\(\neg O \boldsymbol{y}_k\)</span>.</p>
<dl>
<dt>Lemma 7</dt>
<dd>
If <span class="math inline">\(\boldsymbol{x}\in F\)</span>, then <span class="math inline">\(R\)</span> fails.
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Assume <span class="math inline">\(\boldsymbol{x}\in F\)</span>, so <span class="math inline">\(\boldsymbol{x}\)</span> is not periodic. Then proving the lemma requires showing that for any <span class="math inline">\(i\)</span>, there is a <span class="math inline">\(j \geq i\)</span> such that, according to <span class="math inline">\(R\)</span>, the probability of <span class="math inline">\(p\)</span> given <span class="math inline">\(\boldsymbol{X}_j =\boldsymbol{x}_j\)</span> is not less than <span class="math inline">\(\frac{1}{2}\)</span>. And that requires showing that there is a <span class="math inline">\(\Pr \in R\)</span> such that <span class="math inline">\(\Pr(p | \boldsymbol{X}_j = \boldsymbol{x}_j) \geq \frac{1}{2}\)</span>. This is easy to do. Consider any sequence <span class="math inline">\(\boldsymbol{y}_i\)</span> of length <span class="math inline">\(i\)</span> not identical to <span class="math inline">\(\boldsymbol{x}_i\)</span> such that <span class="math inline">\(\neg O \boldsymbol{y}_i\)</span>. Consider the probability function <span class="math inline">\(\Pr_k \in R\)</span> such that <span class="math inline">\(\Pr_k(\boldsymbol{X}= f^\infty(\boldsymbol{y}_i)) = \frac{1}{2}\)</span>. Once we conditionalise on <span class="math inline">\(\boldsymbol{X}_i = \boldsymbol{x}_i\)</span>, that function will behave just like <span class="math inline">\(\Pr_2\)</span>. And since <span class="math inline">\(\boldsymbol{X}\)</span> flummoxes <span class="math inline">\(\Pr_2\)</span>, that means there is a <span class="math inline">\(\boldsymbol{x}_j\)</span> such that <span class="math inline">\(\Pr(p | \boldsymbol{X}_j = \boldsymbol{x}_j) &gt; \frac{1}{2}\)</span>, and hence <span class="math inline">\(\Pr(p | \boldsymbol{X}_j = \boldsymbol{x}_j) \geq \frac{1}{2}\)</span>.&nbsp;◻</p>
</div>
<dl>
<dt>Lemma 8</dt>
<dd>
For each <span class="math inline">\(\Pr \in R, \Pr(\boldsymbol{x}\in F) = \frac{1}{2}.\)</span>
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> It helps to think of each of the <span class="math inline">\(\Pr \in R\)</span> as mixtures of <span class="math inline">\(\Pr_0\)</span> and <span class="math inline">\(\Pr_1\)</span>, plus a mixture of the <span class="math inline">\(\Pr_i\)</span> for <span class="math inline">\(i \geq 3\)</span>. Now <span class="math inline">\(\Pr_0(\boldsymbol{x}\in F) = 0\)</span>, since for any countable set, <span class="math inline">\(\Pr_0\)</span> says the probability that <span class="math inline">\(\boldsymbol{x}\)</span> is in that set is 0. And <span class="math inline">\(\Pr_1(\boldsymbol{x}\in F) = 0\)</span>, since <span class="math inline">\(\Pr_1\)</span> says that the probability of <span class="math inline">\(\boldsymbol{x}\)</span> being periodic is 1, and none of the members of <span class="math inline">\(F\)</span> are periodic. But for each <span class="math inline">\(\Pr_i\)</span> for <span class="math inline">\(i \geq 3\)</span>, <span class="math inline">\(\Pr_i(\boldsymbol{x}\in F) = 1\)</span>. Indeed, for each such function, there is a particular sequence in <span class="math inline">\(F\)</span> such that the probability that <span class="math inline">\(\boldsymbol{x}\)</span> is that sequence is 1. So for each <span class="math inline">\(\Pr \in R, \Pr(\boldsymbol{x}\in F) = \frac{1}{4} \times 0 + \frac{1}{4} \times 0 + \frac{1}{2} \times 1 = \frac{1}{2}\)</span>.&nbsp;◻</p>
</div>
<dl>
<dt>Theorem 9</dt>
<dd>
According to <span class="math inline">\(R\)</span>, the probability of an agent whose representor is <span class="math inline">\(R\)</span> failing is at least <span class="math inline">\(\frac{1}{2}\)</span>.
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Immediate from Lemma&nbsp;7 and Lemma 8.&nbsp;◻</p>
</div>
<p>So if an agent’s credences are represented by a non-singleton set of probability functions, not a single probability function, it is possible for them to be open-minded and modest. On the other hand, if an agent is represented by a single probability function, as the precise Bayesian desires, then it is impossible to be open-minded and modest. Since being open-minded and modest is desirable, this is a reason to prefer the imprecise Bayesian picture.</p>
</section>
<section id="objections-and-replies" class="level3" data-number="0.4">
<h3 data-number="0.4" class="anchored" data-anchor-id="objections-and-replies"><span class="header-section-number">0.4</span> Objections and Replies</h3>
<p>I’m going to reply to three objections, but since my replies overlap, I’ll group the objections together.</p>
<dl>
<dt>Objection 1</dt>
<dd>
The model here only gives you conditional modesty. Once the initial sequence is <span class="math inline">\(O\)</span>, the representor becomes the singleton of an open-minded probability function, and Belot showed that to be immodest. Ideally, the agent would have a prior that is in some way resiliently modest, whereas this prior is fragilely modest.
</dd>
<dt>Objection 2</dt>
<dd>
This representor is open-minded and modest towards one particular problem, namely whether <span class="math inline">\(\boldsymbol{X}\)</span> is periodic. But Belot was interested in a wider range of problems, indeed in all problems of the form: does <span class="math inline">\(\boldsymbol{x}\)</span> fall into some set that is measurable, dense, and has a dense complement. Ideally, we’d have a prior which is widely open-minded and modest, in the sense that it had an open-minded and modest attitude towards many problems. But this prior is narrowly modest, in the sense that it is open-minded and modest about only one problem.
</dd>
<dt>Objection 3</dt>
<dd>
The representor described here is clearly not a representation of a credal state of anyone rational. Look what it does if the data is a 1 followed by thousands of 0s, or is the first few thousand digits of the binary expansion of <span class="math inline">\(\pi\)</span>, or has a frequency of 0s of 0.2 over all large sub-intervals. No one could adopt this prior, so it doesn’t show anything about the advantages of imprecise Bayesianism.
</dd>
</dl>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Reply.</em> My responses are going to be (1) that we should want more resilient modesty, and though this is a hard technical challenge, it’s possible to see a way forward on it, (2) that we should want somewhat wider open-minded modesty, though how much wider is a hard question, and (3) that the third objection should simply be rejected. Let’s go through those in reverse order, since it’s the response to the third that explains part of what I’m doing in response to the other two.</p>
<p>What we have in section three is a consistency proof. For the imprecise Bayesian, unlike the precise Bayesian, being open-minded is consistent with being modest. That’s good, since it shows that we can’t rule out a rational response to problems like Belot’s. It’s obviously true that the prior in question isn’t rational, but that’s not needed for a consistency proof.</p>
<p>Moreover, we don’t just have a consistency proof, we have a constructive consistency proof - the prior is described in detail. It’s just not going to be possible to do a constructive proof that open-mindedness, modesty and full rationality are consistent. And that’s because to do that would essentially be to solve all of the problems of epistemology ever. Demonstrating a fully rational prior, even for the range of questions Belot considers, is too much to ask.</p>
<p>If there’s a reasonable looking argument that imprecise Bayesians are unlikely to be able to satisfy some set of plausible constraints, then the defender of imprecise Bayesianism is, I think, obliged to show how those constraints can be satisfied. But to ask for a demonstration of how all reasonable constraints can be satisfied at once, in the absence of a decent argument that they cannot be, would clearly be asking too much.</p>
<p>So I don’t care that the prior I described is irrational; it serves its purpose in proving consistency. Now what would be nice is to show that some slightly stronger constraints can be simultaneously satisfied. But we have to be sure that those constraints are in fact reasonable constraints. Here’s one constraint that I think isn’t reasonable: be open-minded towards any proposition of the form <span class="math inline">\({\boldsymbol{X}\in S}\)</span>, where <span class="math inline">\(S\)</span> is a dense set of sequences. Let <span class="math inline">\(S\)</span>, for example, be the set consisting of all sequences of the form <span class="math inline">\(\boldsymbol{y}_k + \boldsymbol{z}\)</span>, where <span class="math inline">\(\boldsymbol{y}_k\)</span> ranges over all finite sequeneces, and <span class="math inline">\(\boldsymbol{z}\)</span> is a particular arbitrary sequence that lacks finite definition in our current language. That set is dense, and indeed measurable. But there’s no evidence that could make it reasonable to take <span class="math inline">\(\boldsymbol{X}\in S\)</span> to be probable. So a prior that wasn’t open-minded towards <span class="math inline">\(\boldsymbol{X}\in S\)</span> could still be perfectly reasonable.</p>
<p>That said, the prior I demonstrated is closed-minded towards several propositions that should be taken seriously. It will never have positive credence that <span class="math inline">\(\boldsymbol{X}\)</span> is eventually periodic without being periodic, or that <span class="math inline">\(\boldsymbol{X}\)</span> is generated by a chance process that gives each data point chance <span class="math inline">\(c \neq \frac{1}{2}\)</span> of being 0. It would be good to have a prior whose open-minded modesty was wider. But before we do that technical work, I think there’s a need to figure out which propositions we should be open-minded about.</p>
<p>I am more worried by the fragility of the modesty of this prior. There’s a reasonable sense in which the prior is open-minded only in virtue of the fact that it has parts which are immodest. At any point where the agent has credence above <span class="math inline">\(\frac{1}{2}\)</span> that <span class="math inline">\(p\)</span>, she has credence 1 that she will succeed.</p>
<p>We could try to complicate the prior a bit more to avoid that. Here’s a sketch of how it could go, with application to one particular initial sequence of data. Consider what happens to <span class="math inline">\(R\)</span> if the initial input is <span class="math inline">\(\langle 0, 1, 0, 0, 1, 0, 0, 1\rangle\)</span>, hereafter <span class="math inline">\(\boldsymbol{y}\)</span>. According to <span class="math inline">\(\Pr_0\)</span>, that initial sequence has probability <span class="math inline">\(\frac{1}{256}\)</span>. According to <span class="math inline">\(\Pr_1\)</span>, it has probability <span class="math inline">\(\frac{1}{63} + \frac{1}{4095} + \frac{1}{65535} \approx \frac{1}{62}\)</span>. So given that initial sequence, <span class="math inline">\(\Pr_2\)</span> says the probability of <span class="math inline">\(p\)</span> is about <span class="math inline">\(\frac{4}{5}\)</span>. And since the sequence is <span class="math inline">\(O\)</span>, it could be the start of the the sequence <span class="math inline">\(\langle 0, 1, 0\rangle\)</span> repeated indefinitely, its probability according to <span class="math inline">\(\Pr_i\)</span> is 0, for <span class="math inline">\(i \geq 3\)</span>. Now consider the set of all probability functions of the form <span class="math inline">\(a\Pr_{R} + b\Pr_{New}\)</span>, where <span class="math inline">\(a + b = 1\)</span>, <span class="math inline">\(b \in (0, \frac{1}{256}), \Pr_{R} \in R\)</span> and <span class="math inline">\(\Pr_{New}\)</span> is the function which gives probability 1 to <span class="math inline">\(\boldsymbol{X}\)</span> being <span class="math inline">\(O(\boldsymbol{y})\)</span>. That prior is open-minded, and even after conditionalising on <span class="math inline">\(\boldsymbol{y}\)</span> satisfies the intermediate of the three modesty conditions described on page - the probability of failure is less than one, though it isn’t less than some number less than one. And this trick could be generalised to satisfy more modesty conditions, and even (though it would take some time to prove this) be unconditionally modest.</p>
<p>But I’m not going to go through those steps here. That’s mostly because I think we already have shown enough to show that imprecise Bayesianism has an advantage over precise Bayesianism. The imprecise Bayesian can, and the precise Bayesian can’t, have an open-minded modest attitude. It would be good to press home that advantage and show that there are other things the imprecise Bayesian can do that the precise Bayesian can’t do, such as having a widely open-minded and resiliently modest prior. But even before such a demonstration takes place, the advantage has been established.&nbsp;◻</p>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Belot2013" class="csl-entry" role="listitem">
Belot, Gordon. 2013. <span>“Bayesian Orgulity.”</span> <em>Philosophy of Science</em> 80 (4): 483–503. <a href="https://doi.org/10.1086/673249">https://doi.org/10.1086/673249</a>.
</div>
<div id="ref-Bradley2014" class="csl-entry" role="listitem">
Bradley, Seamus. 2014. <span>“Imprecise Probabilities.”</span> In <em>The Stanford Encyclopedia of Philosophy</em>, edited by Edward N. Zalta, Winter 2014. <a href="http://plato.stanford.edu/archives/win2014/entries/imprecise-probabilities/" class="uri">http://plato.stanford.edu/archives/win2014/entries/imprecise-probabilities/</a>; Metaphysics Research Lab, Stanford University.
</div>
<div id="ref-Briggs2009" class="csl-entry" role="listitem">
Briggs, Rachael. 2009. <span>“Distorted Reflection.”</span> <em>Philosophical Review</em> 118 (1): 59–85. <a href="https://doi.org/10.1215/00318108-2008-029">https://doi.org/10.1215/00318108-2008-029</a>.
</div>
<div id="ref-Elga2010-ELGHTD" class="csl-entry" role="listitem">
Elga, Adam. 2010. <span>“How to Disagree about How to Disagree.”</span> In <em>Disagreement</em>, edited by Ted Warfield and Richard Feldman, 175–87. Oxford: Oxford University Press.
</div>
<div id="ref-Joyce2010" class="csl-entry" role="listitem">
Joyce, James M. 2010. <span>“A Defence of Imprecise Credences in Inference and Decision Making.”</span> <em>Philosophical Perspectives</em> 24 (1): 281–323. <a href="https://doi.org/10.1111/j.1520-8583.2010.00194.x">https://doi.org/10.1111/j.1520-8583.2010.00194.x</a>.
</div>
<div id="ref-Lasonen-Aarnio2015" class="csl-entry" role="listitem">
Lasonen-Aarnio, Maria. 2015. <span>“New Rational Reflection and Internalism about Rationality.”</span> <em>Oxford Studies in Epistemology</em> 5: 145–71. <a href="https://doi.org/10.1093/acprof:oso/9780198722762.003.0005">https://doi.org/10.1093/acprof:oso/9780198722762.003.0005</a>.
</div>
<div id="ref-Lewis1971d" class="csl-entry" role="listitem">
Lewis, David. 1971. <span>“Immodest Inductive Methods.”</span> <em>Philosophy of Science</em> 38 (1): 54–63. <a href="https://doi.org/10.1086/288339">https://doi.org/10.1086/288339</a>.
</div>
<div id="ref-Lewis1993c" class="csl-entry" role="listitem">
———. 1993. <span>“Many, but Almost One.”</span> In <em>Ontology, Causality, and Mind: Essays on the Philosophy of <span>D. M. Armstrong</span></em>, edited by Keith Campbell, John Bacon, and Lloyd Reinhardt, 23–38. Cambridge: Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511625343.010">https://doi.org/10.1017/CBO9780511625343.010</a>.
</div>
<div id="ref-Pryor2000" class="csl-entry" role="listitem">
Pryor, James. 2000. <span>“The Sceptic and the Dogmatist.”</span> <em>No<span>û</span>s</em> 34 (4): 517–49. <a href="https://doi.org/10.1111/0029-4624.00277">https://doi.org/10.1111/0029-4624.00277</a>.
</div>
<div id="ref-Schoenfield2012" class="csl-entry" role="listitem">
Schoenfield, Miriam. 2012. <span>“Chilling Out on Epistemic Rationality: A Defense of Imprecise Credences (and Other Imprecise Doxastic Attitudes).”</span> <em>Philosophical Studies</em> 158 (2): 197–219. <a href="https://doi.org/10.1007/s11098-012-9886-7">https://doi.org/10.1007/s11098-012-9886-7</a>.
</div>
<div id="ref-Schoenfield2014" class="csl-entry" role="listitem">
———. 2015. <span>“A Dilemma for Calibrationism.”</span> <em>Philosophy and Phenomenological Research</em> 91 (2): 425–55. <a href="https://doi.org/10.1111/phpr.12125">https://doi.org/10.1111/phpr.12125</a>.
</div>
<div id="ref-Walley1991" class="csl-entry" role="listitem">
Walley, Peter. 1991. <em>Statisical Reasoning with Imprecise Probabilities</em>. London: Chapman &amp; Hall.
</div>
<div id="ref-White2006" class="csl-entry" role="listitem">
White, Roger. 2006. <span>“Problems for Dogmatism.”</span> <em>Philosophical Studies</em> 131 (3): 525–57. <a href="https://doi.org/10.1007/s11098-004-7487-9">https://doi.org/10.1007/s11098-004-7487-9</a>.
</div>
<div id="ref-White2010" class="csl-entry" role="listitem">
———. 2010. <span>“Evidential Symmetry and Mushy Credence.”</span> <em>Oxford Studies in Epistemology</em> 3: 161–89.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
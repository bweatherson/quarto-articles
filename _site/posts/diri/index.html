<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.479">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">
<meta name="dcterms.date" content="2011-01-01">
<meta name="description" content="Since interest-relative invariantism (hereafter, IRI) was introduced into contemporary epistemology in the early 2000s, it has been criticised on a number of fronts. This paper responds to six different criticisms of IRI launched by five different authors. And it does so by noting that the best version of IRI is immune to the criticisms they have launched. The ‘best version’ in question notes three things about IRI. First, what matters for knowledge is not strictly the stakes the agent faces in any decision-problem, but really the odds at which she has to bet. Second, IRI is a relatively weak theory; it just says interests sometimes matter. Defenders of IRI have often derived it from much stronger principles about reasoning, and critics have attacked those principles, but much weaker principles would do. Third, and most importantly, interests matter because generate certain kinds of defeaters. It isn’t part of this version of IRI that an agent can know something in virtue of their interests. Rather, the theory says that whether a certain kind of consideration is a defeater to an agent’s putative knowledge that p depends on their interests. This matters for the intuitive plausibility of IRI. Critics have argued, rightly, that interests don’t behave in ways distinctive of grounds of knowledge. But interests do behave like other kinds of defeaters, and this undermines the criticisms of IRI.">

<title>Online Articles - Brian Weatherson - Defending Interest Relative Invariantism</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://use.typekit.net/uzz2drx.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="trad_defn.css">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Online Articles - Brian Weatherson</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://brian.weatherson.org"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/bweatherson.bsky.social"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Defending Interest Relative Invariantism</h1>
                  <div>
        <div class="description">
          <p>Since interest-relative invariantism (hereafter, IRI) was introduced into contemporary epistemology in the early 2000s, it has been criticised on a number of fronts. This paper responds to six different criticisms of IRI launched by five different authors. And it does so by noting that the best version of IRI is immune to the criticisms they have launched. The ‘best version’ in question notes three things about IRI. First, what matters for knowledge is not strictly the <em>stakes</em> the agent faces in any decision-problem, but really the <em>odds</em> at which she has to bet. Second, IRI is a relatively weak theory; it just says interests sometimes matter. Defenders of IRI have often derived it from much stronger principles about reasoning, and critics have attacked those principles, but much weaker principles would do. Third, and most importantly, interests matter because generate certain kinds of <em>defeaters</em>. It isn’t part of this version of IRI that an agent can know something in virtue of their interests. Rather, the theory says that whether a certain kind of consideration is a defeater to an agent’s putative knowledge that <em>p</em> depends on their interests. This matters for the intuitive plausibility of IRI. Critics have argued, rightly, that interests don’t behave in ways distinctive of grounds of knowledge. But interests do behave like other kinds of defeaters, and this undermines the criticisms of IRI.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">epistemology</div>
                <div class="quarto-category">interest-relativity</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="http://brian.weatherson.org">Brian Weatherson</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Michigan
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 1, 2011</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.5840/logos-episteme2011248">10.5840/logos-episteme2011248</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#sect:xphi" id="toc-sect:xphi" class="nav-link active" data-scroll-target="#sect\:xphi"><span class="header-section-number">1</span> Experimental Objections</a></li>
  <li><a href="#knowledge-by-indifference-and-by-wealth" id="toc-knowledge-by-indifference-and-by-wealth" class="nav-link" data-scroll-target="#knowledge-by-indifference-and-by-wealth"><span class="header-section-number">2</span> Knowledge By Indifference and By Wealth</a></li>
  <li><a href="#sect:time" id="toc-sect:time" class="nav-link" data-scroll-target="#sect\:time"><span class="header-section-number">3</span> Temporal Embeddings</a></li>
  <li><a href="#sect:conj" id="toc-sect:conj" class="nav-link" data-scroll-target="#sect\:conj"><span class="header-section-number">4</span> Problematic Conjunctions</a></li>
  <li><a href="#sect:holism" id="toc-sect:holism" class="nav-link" data-scroll-target="#sect\:holism"><span class="header-section-number">5</span> Holism and Defeaters</a></li>
  <li><a href="#non-consequentialist-cases" id="toc-non-consequentialist-cases" class="nav-link" data-scroll-target="#non-consequentialist-cases"><span class="header-section-number">6</span> Non-Consequentialist Cases</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>In recent years a number of authors have defended the interest-relativity of knowledge and justification. Views of this form are floated by John Hawthorne <span class="citation" data-cites="Hawthorne2004">(<a href="#ref-Hawthorne2004" role="doc-biblioref">2004</a>)</span>, and endorsed by Jeremy Fantl and Matthew McGrath <span class="citation" data-cites="Fantl2002 FantlMcGrath2009">(<a href="#ref-Fantl2002" role="doc-biblioref">2002</a>, <a href="#ref-FantlMcGrath2009" role="doc-biblioref">2009</a>)</span>, Jason Stanley <span class="citation" data-cites="Stanley2005-STAKAP">(<a href="#ref-Stanley2005-STAKAP" role="doc-biblioref">2005</a>)</span> and Brian Weatherson <span class="citation" data-cites="Weatherson2005-WEACWD">(<a href="#ref-Weatherson2005-WEACWD" role="doc-biblioref">2005</a>)</span>. The various authors differ quite a lot in how much interest-relativity they allow, but what is common is the defence of interest-relativity.</p>
<aside>
<p>Published in <em>Logos and Episteme</em> 2 (2011): 591-609.</p>
Image from <a href="https://commons.wikimedia.org/w/index.php?curid=60876226">Wikimedia Commons</a>.
</aside>
<p>These views have, quite naturally, drawn a range of criticisms. The primary purpose of this paper is to respond to these criticisms and, as it says on the tin, defend interest-relative invariantism, or IRI for short. But I don’t plan to defend every possible version of IRI, only a particular one. Most of the critics of IRI have assumed that it must have some or all of the following features.</p>
<ol type="1">
<li><p>It is harder to know things in high-stakes situations than in low-stakes situations.</p></li>
<li><p>There is an interest-sensitive constituent of knowledge.</p></li>
<li><p>IRI stands and falls with some principles connecting knowledge and action, such as the principles found in <span class="citation" data-cites="Hawthorne2008-HAWKAA">Hawthorne and Stanley (<a href="#ref-Hawthorne2008-HAWKAA" role="doc-biblioref">2008</a>)</span>.</p></li>
</ol>
<p>My preferred version of IRI has none of these three features.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;It is a tricky exegetical question how many of the three features here must be read into defences of IRI in the literature. My reading is that they do not have to be read in, so it is not overly original of me to defend a version of IRI that does away with all three. But I know many people disagree with that. If they’re right, this paper is more original than I think it is, and so I’m rather happy to be wrong. But I’m going to mostly set these exegetical issues aside, and compare different theories without taking a stand on who originally promulgated them.</p></li></div><p>First, it says that knowledge changes when the <strong>odds</strong> an agent faces change, not when the <strong>stakes</strong> change. More precisely, interests affect belief because whether someone believes <span class="math inline">\(p\)</span> depends <em>inter alia</em> on whether their credence in <span class="math inline">\(p\)</span> is high enough that any bet on <span class="math inline">\(p\)</span> they actually face is a good bet. And interests affect knowledge largely because they affect belief. Raising the stakes of any bet on <span class="math inline">\(p\)</span> does not directly change whether an agent believes <span class="math inline">\(p\)</span>, but changing the odds of the bets on <span class="math inline">\(p\)</span> they face does change it. In practice raising the stakes changes the odds due to the declining marginal utility of material goods. So in practice high-stakes situations are typically long-odds situations. But knowledge is hard in those situations because they are long-odds situations, not because they are high-stakes situations.</p>
<p>So my version of IRI says that knowledge differs between these two cases.</p>
<dl>
<dt>High Cost Map:</dt>
<dd>
<p>Zeno is walking to the Mysterious Bookshop in lower Manhattan. He’s pretty confident that it’s on the corner of Warren Street and West Broadway. But he’s been confused about this in the past, forgetting whether the east-west street is Warren or Murray, and whether the north-south street is Greenwich, West Broadway or Church. In fact he’s right about the location this time, but he isn’t justified in having a credence in his being correct greater than about 0.95. While he’s walking there, he has two options. He could walk to where he thinks the shop is, and if it’s not there walk around for a few minutes to the nearby corners to find where it is. Or he could call up directory assistance, pay $1, and be told where the shop is. Since he’s confident he knows where the shop is, and there’s little cost to spending a few minutes walking around if he’s wrong, he doesn’t do this, and walks directly to the shop.</p>
</dd>
<dt>Low Cost Map:</dt>
<dd>
<p>Just like the previous case, except that Zeno has a new phone with more options. In particular, his new phone has a searchable map, so with a few clicks on the phone he can find where the store is. Using the phone has some very small costs. For example, it distracts him a little, which marginally raises the likelihood of bumping into another pedestrian. But the cost is very small compared to the cost of getting the location wrong. So even though he is very confident about where the shop is, he double checks while walking there.</p>
</dd>
</dl>
<p>I think the Map Cases are like the various cases that have been used to motivate interest-relativity<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> in all important respects. I think Zeno knows where the shop is in High Cost Map, and doesn’t know in Low Cost Map. And he doesn’t know in Low Cost Map because the location of the shop has suddenly become the subject matter of a bet at very long odds. You should think of Zeno’s not checking the location of the shop on his phone-map as a bet on the location of the shop. If he wins the bet, he wins a few seconds of undistracted strolling. If he loses, he has to walk around a few blocks looking for a store. The disutility of the loss seems easily twenty times greater than the utility of the gain, and by hypothesis the probability of winning the bet is no greater than 0.95. So he shouldn’t take the bet. Yet if he knew where the store was, he would be justified in taking the bet. So he doesn’t know where the store is. Now this is not a case where higher <em>stakes</em> defeat knowledge. If anything, the stakes are lower in Low Cost Map. But the relevant odds are longer, and that’s what matters to knowledge.</p>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Such as the Bank Cases in <span class="citation" data-cites="Stanley2005-STAKAP">Stanley (<a href="#ref-Stanley2005-STAKAP" role="doc-biblioref">2005</a>)</span>, or the Train Cases in <span class="citation" data-cites="Fantl2002">Fantl and McGrath (<a href="#ref-Fantl2002" role="doc-biblioref">2002</a>)</span>.</p></li></div><p>Second, on this version of IRI, interests matter because there are interest-sensitive defeaters, not because interests form any kind of new condition on knowledge, alongside truth, justification, belief and so on. In particular, interests matter because there are interest-relative coherence constraints on knowledge. Some coherence constraints, I claim, are not interest-relative. If an agent believes <span class="math inline">\(\neg p\)</span>, that belief defeats her purported knowledge that <span class="math inline">\(p\)</span>, even if the belief that <span class="math inline">\(p\)</span> is true, justified, safe, sensitive and so on. It is tempting to try to posit a further coherence condition.</p>
<dl>
<dt>Practical Coherence</dt>
<dd>
<p>An agent does not know that <span class="math inline">\(p\)</span> if she prefers <span class="math inline">\(\varphi\)</span> to <span class="math inline">\(\psi\)</span> unconditionally, but prefers <span class="math inline">\(\psi\)</span> to <span class="math inline">\(\varphi\)</span> conditional on <span class="math inline">\(p\)</span>.</p>
</dd>
</dl>
<p>But that is too strong. For reasons similar to those gone over at the start of <span class="citation" data-cites="Hawthorne2004">Hawthorne (<a href="#ref-Hawthorne2004" role="doc-biblioref">2004</a>)</span>, it would mean we know nearly nothing. A more plausible condition is:</p>
<dl>
<dt>Relevant Practical Coherence</dt>
<dd>
<p>An agent does not know that <span class="math inline">\(p\)</span> if she prefers <span class="math inline">\(\varphi\)</span> to <span class="math inline">\(\psi\)</span> unconditionally, but prefers <span class="math inline">\(\psi\)</span> to <span class="math inline">\(\varphi\)</span> conditional on <span class="math inline">\(p\)</span>, for any <span class="math inline">\(\varphi, \psi\)</span> that are relevant given her interests.</p>
</dd>
</dl>
<p>When this condition is violated, the agent’s claim to knowledge is defeated. As we’ll see below, defeaters behave rather differently to constituents of knowledge. Some things which could not plausibly be grounds for knowledge could be defeaters to defeaters for knowledge.</p>
<p><strong>Relevant Practical Coherence</strong> suffices, at least among agents who are trying to maximise expected value, to generate an interest-relativity to knowledge. The general structure of the case should be familiar from the existing literature. Let <span class="math inline">\(p\)</span> be a proposition that is true, believed by the agent, and strongly but not quite conclusively supported by their evidence. Let <span class="math inline">\(B\)</span> be a bet that has a small positive return if <span class="math inline">\(p\)</span>, and a huge negative return if <span class="math inline">\(\neg p\)</span>. Assume the agent is now offered the bet, and let <span class="math inline">\(\varphi\)</span> be declining the bet, and <span class="math inline">\(\psi\)</span> be accepting the bet. Conditional on <span class="math inline">\(p\)</span>, the bet wins, so the agent prefers the small positive payout, so prefers <span class="math inline">\(\psi\)</span> to <span class="math inline">\(\varphi\)</span> conditional on <span class="math inline">\(p\)</span>. But the bet has a massively negative expected return, so unconditionally the agent does not want it. That is, unconditionally she prefers <span class="math inline">\(\varphi\)</span> to <span class="math inline">\(\psi\)</span>. Once the bet is offered, the actions <span class="math inline">\(\varphi\)</span> and <span class="math inline">\(\psi\)</span> become relevant given her interests, so by <strong>Relevant Practical Coherence</strong> she no longer knows <span class="math inline">\(p\)</span>. So for such an agent, knowledge is interest-relative.</p>
<p>Cases where knowledge is defeated because if the agent did know <span class="math inline">\(p\)</span>, that would lead to problems elsewhere in their cognitive system, have a few quirky features. In particular, whether the agent knows <span class="math inline">\(p\)</span> can depend on very distant features. Consider the following kind of case.</p>
<blockquote class="blockquote">
<p><strong>Confused Student</strong></p>
<p>Con is systematically disposed to affirm the consequent. That is, if he notices that he believes both <span class="math inline">\(p\)</span> and <span class="math inline">\(q \rightarrow p\)</span>, he’s disposed to either infer <span class="math inline">\(q\)</span>, or if that’s impermissible given his evidence, to ditch his belief in the conjunction of <span class="math inline">\(p\)</span> and <span class="math inline">\(q \rightarrow p\)</span>. Con has completely compelling evidence for both <span class="math inline">\(q \rightarrow p\)</span> and <span class="math inline">\(\neg q\)</span>. He has good but less compelling evidence for <span class="math inline">\(p\)</span>. And this evidence tracks the truth of <span class="math inline">\(p\)</span> in just the right way for knowledge. On the basis of this evidence, Con believes <span class="math inline">\(p\)</span>. Con has not noticed that he believes both <span class="math inline">\(p\)</span> and <span class="math inline">\(q \rightarrow p\)</span>. If he did, he’s unhesitatingly drop his belief that <span class="math inline">\(p\)</span>, since he’d realise the alternatives (given his dispositions) involved dropping belief in a compelling proposition. Two questions:</p>
<ul>
<li><p>Does Con know that <span class="math inline">\(p\)</span>?</p></li>
<li><p>If Con were to think about the logic of conditionals, and reason himself out of the disposition to affirm the consequent, would he know that <span class="math inline">\(p\)</span>?</p></li>
</ul>
</blockquote>
<p>I think the answer to the first question is <em>No</em>, and the answer to the second question is <em>Yes</em>. As it stands, Con’s disposition to affirm the consequent is a doxastic defeater of his putative knowledge that <span class="math inline">\(p\)</span>. Put another way, <span class="math inline">\(p\)</span> doesn’t cohere well enough with the rest of Con’s views for his belief that <span class="math inline">\(p\)</span> to count as knowledge. To be sure, <span class="math inline">\(p\)</span> coheres well enough with those beliefs by objective standards, but it doesn’t cohere at all by Con’s lights. Until he changes those lights, it doesn’t cohere well enough to be knowledge. Moreover (as a referee pointed out), Con’s belief is not safe. Since he could easily have ‘reasoned’ himself out of his belief that <span class="math inline">\(p\)</span>, the belief isn’t safe in the way that knowledge is safe.</p>
<p>I think that beliefs which violate <strong>Relevant Practical Coherence</strong> fail to be knowledge for the same reason that Con’s belief that <span class="math inline">\(p\)</span> fails to be knowledge. In what follows, I’ll make frequent use of this analogy; many of the objections to IRI turn out to be equally strong objections to the view that there are ever defeaters of the type Con suffers from.</p>
<p>This suggests our third point. This version of IRI does not take IRI to be a consequence of more general principles about knowledge and action. It simply says that there exist at least one pair of cases where the only relevant difference between agents in the two cases concerns their interests, but one knows that <span class="math inline">\(p\)</span> and the other does not.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> I happen to think that most of the general principles that philosophers have used to try to derive IRI are false. But since IRI is much weaker than those principles, that is no reason to conclude IRI is false.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;And this is true even though <span class="math inline">\(p\)</span> is not a proposition about their interests, or something that is supported by propositions about their interests, and so on.</p></li><li id="fn4"><p><sup>4</sup>&nbsp;I will consider, and tentatively support, one principle stronger than IRI in the final section. But the key point is that these general principles are not needed to defend IRI.</p></li></div><p>The existence of interest-relativity is then quite a weak claim. There are plenty of stronger claims in the area we could make. I prefer, for instance, a version of IRI where being offered bets like <span class="math inline">\(B\)</span> defeats knowledge that <span class="math inline">\(p\)</span> even if the agent does not have the preferences I ascribed above. (That could be because she isn’t trying to maximise expected value, or because she’s messed up the expected value calculations.) But knowledge could be interest-relative even if I’m wrong about those cases.</p>
<p>So I’ve set out a version of IRI that lacks three features often attributed to IRI. I haven’t argued for that theory here - I do that at much greater length in (Author Paper 1). But I hope I’ve done enough to convince you that the theory is both a version of IRI, and not obviously false. In what follows, I’ll argue that the theory is immune to the various challenges to IRI that have been put forward in the literature. This immunity is, I think, a strong reason to prefer this version of IRI.</p>
<section id="sect:xphi" class="level1 page-columns page-full" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Experimental Objections</h1>
<p>I don’t place as much weight as some philosophers do on the correlation between the verdicts of an epistemological theory and the gut reactions that non-experts have to tricky cases. And I don’t think the best cases for IRI relies on such a correlation holding. The best case for IRI is that it integrates nicely with an independently supported theory of belief, and that it lets us keep a number of plausible principles without drifting into skepticism.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> But still, it is nice to not have one’s theory saying exorbitantly counterintuitive things. Various experimental results, such as the results in <span class="citation" data-cites="May2010">May et al. (<a href="#ref-May2010" role="doc-biblioref">2010</a>)</span> and <span class="citation" data-cites="FeltzZarpentine2010">Feltz and Zarpentine (<a href="#ref-FeltzZarpentine2010" role="doc-biblioref">2010</a>)</span>, might be thought to suggest that IRI does have consequences which are counterintuitive, or which at least run counter to the intuitions of some experimental subjects. I’m going to concentrate on the latter set of results here, though I think that what I say will generalise to related experimental work. In fact, I think the experiments don’t really tell against IRI, because IRI, at least in my preferred version, doesn’t make <em>any</em> unambiguous predictions about the cases at the centre of the experiments. The reason for this is related to my insistence that we concentrate on the odds an agent faces, not the stakes she faces.</p>
<div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;This points are expanded upon greatly in (Author Paper 1).</p></li></div><p>Feltz and Zarpentine gave subjects related vignettes, such as the following pair. (Each subject only received one of the pair.)</p>
<dl>
<dt>High Stakes Bridge</dt>
<dd>
<p>John is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a yawning thousand foot drop. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, ‘I know that my truck will make it across the bridge.’</p>
</dd>
<dt>Low Stakes Bridge</dt>
<dd>
<p>John is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a three foot ditch. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, ‘I know that my truck will make it across the bridge.’ <span class="citation" data-cites="FeltzZarpentine2010">(<a href="#ref-FeltzZarpentine2010" role="doc-biblioref">Feltz and Zarpentine 2010, 696</a>)</span></p>
</dd>
</dl>
<p>Subjects were asked to evaluate John’s thought. And the result was that 27% of the participants said that John does not know that the truck will make it across in <strong>Low Stakes Bridge</strong>, while 36% said he did not know this in <strong>High Stakes Bridge</strong>. Feltz and Zarpentine say that these results should be bad for interest-relativity views. But it is hard to see just why this is so.</p>
<p>Note that the change in the judgments between the cases goes in the direction that IRI seems to predict. The change isn’t trivial, even if due to the smallish sample size it isn’t statistically significant in this sample. But should a view like IRI have predicted a larger change? To figure this out, we need to ask three questions.</p>
<ol type="1">
<li><p>What are the costs of the bridge collapsing in the two cases?</p></li>
<li><p>What are the costs of not taking the bet, i.e., not driving across the bridge?</p></li>
<li><p>What is the rational credence to have in the bridge’s sturdiness given the evidence John has?</p></li>
</ol>
<p>Conditional on the bridge not collapsing, the drivers presumably prefer taking the bridge to not taking it. And the actions of taking the bridge or going around the long way are relevant. So by <strong>Relevant Practical Coherence</strong>, the drivers know the bridge will not collapse in Low Stakes Bridge but not High Stakes Bridge if the following equation is true. (I assume all the other conditions for knowledge are met, and that there are no other salient instances of Relevant Practical Coherence to consider.)</p>
<p><span class="math display">\[\frac{C_H}{G + C_H} &gt; x &gt; \frac{C_L}{G + C_L}\]</span></p>
<p>where <span class="math inline">\(G\)</span> is the gain the driver gets from taking a non-collapsing bridge rather than driving around (or whatever the alternative is), <span class="math inline">\(C_H\)</span> is the cost of being on a collapsing bridge in High Stakes Bridge, <span class="math inline">\(C_L\)</span> is the cost of being on a collapsing bridge in Low Stakes Bridge, and <span class="math inline">\(x\)</span> is the probability that the bridge will collapse. I assume <span class="math inline">\(x\)</span> is constant between the two cases. If that equation holds, then taking the bridge, i.e., acting as if the bridge won’t collapse, maximises expected utility in Low Stakes Bridge but not High Stakes Bridge. So in High Stakes Bridge, adding the proposition that the bridge won’t collapse to the agent’s cognitive system produces incoherence, since the agent won’t (at least rationally) act as if the bridge won’t collapse. So if the equation holds, the agent’s interests in avoiding <span class="math inline">\(C_H\)</span> creates a doxastic defeater in High Stakes Bridge.</p>
<p>But does the equation hold? Or, more relevantly, did the subjects of the experiment believe that the equation hold? None of the four variables has their values clearly entailed by the story, so we have to guess a little as to what the subjects’ views would be.</p>
<p>Feltz and Zarpentine say that the costs in “High Stakes Bridge are very costly—certain death—whereas the costs in Low Stakes Bridge are likely some minor injuries and embarrassment.” <span class="citation" data-cites="FeltzZarpentine2010">(<a href="#ref-FeltzZarpentine2010" role="doc-biblioref">Feltz and Zarpentine 2010, 702</a>)</span> I suspect both of those claims are wrong, or at least not universally believed. A lot more people survive bridge collapses than you may expect, even collapses from a great height.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> And once the road below a truck collapses, all sorts of things can go wrong, even if the next bit of ground is only 3 feet away. (For instance, if the bridge collapses unevenly, the truck could roll, and the driver would probably suffer more than minor injuries.)</p>
<div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;In the West Gate bridge collapse in Melbourne in 1971, a large number of the victims were underneath the bridge; the people on top of the bridge had a non-trivial chance of survival. That bridge was 200 feet above the water, not 1000, but I’m not sure the extra height would matter greatly. Again from a slightly lower height, over 90% of people on the bridge survived the I-35W collapse in Minneapolis in 2007.</p></li></div><p>We aren’t given any information as to the costs of not crossing the bridge. But given that 15 other trucks, with less evidence than John, have decided to cross the bridge, it seems plausible to think they are substantial. If there was an easy way to avoid the bridge, presumably the <em>first</em> truck would have taken it. If <span class="math inline">\(G\)</span> is large enough, and <span class="math inline">\(C_H\)</span> small enough, then the only way for this equation to hold will be for <span class="math inline">\(x\)</span> to be low enough that we’d have independent reason to say that the driver doesn’t know the bridge will hold.</p>
<p>But what is the value of <span class="math inline">\(x\)</span>? John has a lot of information that the bridge will support his truck. If I’ve tested something for sturdiness two or three times, and it has worked, I won’t even think about testing it again. Consider what evidence you need before you’ll happily stand on a particular chair to reach something in the kitchen, or put a heavy television on a stand. Supporting a weight is the kind of thing that either fails the first time, or works fairly reliably. Obviously there could be some strain-induced effects that cause a subsequent failure<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, but John really has a lot of evidence that the bridge will support him.</p>
<div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;As I believe was the case in the I-35W collapse.</p></li></div><p>Given those three answers, it seems to me that it is a reasonable bet to cross the bridge. At the very least, it’s no more of an unreasonable bet than the bet I make every day crossing a busy highway by foot. So I’m not surprised that 64% of the subjects agreed that John knew the bridge would hold him. At the very least, that result is perfectly consistent with IRI, if we make plausible assumptions about how the subjects would answer the three numbered questions above.</p>
<p>And as I’ve stressed, these experiments are only a problem for IRI if the subjects are reliable. I can think of two reasons why they might not be. First, subjects tend to massively discount the costs and likelihoods of traffic related injuries. In most of the country, the risk of death or serious injury through motor vehicle accident is much higher than the risk of death or serious injury through some kind of crime or other attack, yet most people do much less to prevent vehicles harming them than they do to prevent criminals or other attackers harming them.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Second, only 73% of these subjects in <em>this very experiment</em> said that John knows the bridge will support him in <strong>Low Stakes Bridge</strong>. This is rather striking. Unless the subjects endorse an implausible kind of scepticism, something has gone wrong with the experimental design. But if the subjects are implausibly sceptical, then we shouldn’t require our epistemological theory to track their gut reactions. (And if something has gone wrong with the experimental design, then obviously can’t be used as the basis for any objection.) So given the fact that the experiment points broadly in the direction of IRI, and that with some plausible assumptions it is perfectly consistent with that theory, and that the subjects seem unreasonably sceptical to the point of unreliability about epistemology, I don’t think this kind of experimental work threatens IRI.</p>
<div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;See the massive drop in the numbers of students walking or biking to school, reported in <span class="citation" data-cites="Ham2008">Ham, Martin, and Kohl III (<a href="#ref-Ham2008" role="doc-biblioref">2008</a>)</span>, for a sense of how big an issue this is.</p></li></div></section>
<section id="knowledge-by-indifference-and-by-wealth" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Knowledge By Indifference and By Wealth</h1>
<p>Gillian Russell and John Doris <span class="citation" data-cites="RussellDoris2008">(<a href="#ref-RussellDoris2008" role="doc-biblioref">2009</a>)</span> argue that Jason Stanley’s account of knowledge leads to some implausible attributions of knowledge, and if successful their objections would generalise to other forms of IRI. I’m going to argue that Russell and Doris’s objections turn on principles that are <em>prima facie</em> rather plausible, but which ultimately we can reject for independent reasons.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;I think the objections I make here are similar in spirit to those Stanley made in a comments thread on <a href="http://el-prod.baylor.edu/certain_doubts/?p=616">Certain Doubts</a>, though the details are new. The thread is at <a href="http://el-prod.baylor.edu/certain_doubts/?p=616" class="uri">http://el-prod.baylor.edu/certain_doubts/?p=616</a>.</p></li></div><p>Their objection relies on variants of the kind of case Stanley uses heavily in his <span class="citation" data-cites="Stanley2005-STAKAP">(<a href="#ref-Stanley2005-STAKAP" role="doc-biblioref">2005</a>)</span> to motivate a pragmatic constraint on knowledge. Stanley considers the kinds of cases we used to derive IRI from <strong>Relevant Practical Coherence</strong>. So imagine an agent who faces a choice between accepting the status quo, call that <span class="math inline">\(\varphi\)</span>, and taking some giant risk, call that <span class="math inline">\(\psi\)</span>. The giant risk in this case will involve a huge monetary loss if <span class="math inline">\(\neg p\)</span>, and a small non-monetary gain if <span class="math inline">\(p\)</span>. Stanley says, and I agree, that in such a case the agent doesn’t know <span class="math inline">\(p\)</span>, even if their belief in <span class="math inline">\(p\)</span> is true, well supported by evidence, and so on. Moreover, he says, had <span class="math inline">\(\psi\)</span> not been a relevant option, the agent could have known <span class="math inline">\(p\)</span>. I agree, and I think <strong>Relevant Practical Coherence</strong> explains these intuitions well.</p>
<p>Russell and Doris imagine two kinds of variants on Stanley’s case. In one variant the agent doesn’t care about the material loss associated with <span class="math inline">\(\psi \wedge \neg p\)</span>. As I would put it, although their material wealth would decline precipitously in that case, their utility would not, because their utility is not tightly correlated with material wellbeing. Given that, the agent may well prefer <span class="math inline">\(\psi\)</span> to <span class="math inline">\(\varphi\)</span> unconditionally, and so would still know <span class="math inline">\(p\)</span>. Russell and Doris don’t claim this is a problem in itself, but they do think the conjunction of this with the previous paragraph is a problem. As they put it, “you should have reservations ... about what makes the knowledge claim true: not giving a damn, however enviable in other respects, should not be knowledge-making.” <span class="citation" data-cites="RussellDoris2008">(<a href="#ref-RussellDoris2008" role="doc-biblioref">Russell and Doris 2009, 432</a>)</span>.</p>
<p>Their other variant involves an agent with so much money that the material loss is trifling to them. Since the difference in utility between having, say, eight billion dollars and seven billion dollars is not that high, perhaps they will again prefer <span class="math inline">\(\psi\)</span> to <span class="math inline">\(\varphi\)</span> unconditionally, so still know <span class="math inline">\(p\)</span>. But it is, allegedly, counterintuitive to have the knowledge that <span class="math inline">\(p\)</span> turn on the agent’s wealth. As Russell and Doris say, “matters are now even dodgier for practical interest accounts, because <em>money</em> turns out to be knowledge making.” <span class="citation" data-cites="RussellDoris2008">(<a href="#ref-RussellDoris2008" role="doc-biblioref">Russell and Doris 2009, 433</a>)</span> And this isn’t just because wealth can purchase knowledge. As they say, “money may buy the <em>instruments</em> of knowledge ... but here the connection between money and knowledge seems rather too direct.” <span class="citation" data-cites="RussellDoris2008">(<a href="#ref-RussellDoris2008" role="doc-biblioref">Russell and Doris 2009, 433</a>)</span></p>
<p>The first thing to note about this case is that indifference and wealth aren’t really producing knowledge. What they are doing is more like defeating a defeater. Remember that the agent in question had enough evidence, and enough confidence, that they would know <span class="math inline">\(p\)</span> were it not for the practical circumstances. As I said in the introduction, practical considerations enter debates about knowledge in part because they are distinctive kinds of defeaters. It seems that’s what is going on here. And we have, somewhat surprisingly, independent evidence to think that indifference and wealth do matter to defeaters.</p>
<p>Consider two variants on Gilbert Harman’s ‘dead dictator’ example <span class="citation" data-cites="Harman1973">(<a href="#ref-Harman1973" role="doc-biblioref">Harman 1973, 75</a>)</span>. In the original example, an agent reads that the dictator has died through an actually reliable source. But there are many other news sources around, such that if the agent read them, she would lose her belief. Even if the agent doesn’t read those sources, their presence can constitute defeaters to her putative knowledge that the dictator died.</p>
<p>In our first variant on Harman’s example, the agent simply does not care about politics. It’s true that there are many other news sources around that are ready to mislead her about the dictator’s demise. But she has no interest in looking them up, nor is she at all likely to look them up. She mostly cares about literature, and will spend her day reading old novels. In this case, the misleading news sources are too distant, in a sense, to be defeaters. So she still knows the dictator has died. Her indifference towards politics doesn’t generate knowledge - the original reliable report is the knowledge generator - but her indifference means that a would-be defeater doesn’t gain traction.</p>
<p>It might be objected here that the agent doesn’t know the dictator has died because there are misleading reports around saying the dictator is alive, and she is in no position to rebut them. But this is too high a standard for knowledge. There are millions of people in Australia who know that humans are contributing to global warming on purely testimonial grounds. Many, perhaps even most, of these people would not be able to answer a carefully put together argument that humans are not contributing to global warming, such as an argument that picked various outlying statistics to mislead the reader. And such arguments certainly exist; the conservative parts of the media do as much as they can to play them up. But the mere existence of such arguments doesn’t defeat the average person’s testimonial knowledge about anthropogenic global warming. Similarly, the mere existence of misleading reports does not defeat our agent’s knowledge of the dictator’s death, as long as there is no nearby world where she is exposed to the reports. (Thanks here to an anonymous referee.)</p>
<p>In the second variant, the agent cares deeply about politics, and has masses of wealth at hand to ensure that she knows a lot about it. Were she to read the misleading reports that the dictator has survived, then she would simply use some of the very expensive sources she has to get more reliable reports. Again this suffices for the misleading reports not to be defeaters. Even before the rich agent exercises her wealth, the fact that her wealth gives her access to reports that will correct for misleading reports means that the misleading reports are not actually defeaters. So with her wealth she knows things she wouldn’t otherwise know, even before her money goes to work. Again, her money doesn’t generate knowledge – the original reliable report is the knowledge generator – but her wealth means that a would-be defeater doesn’t gain traction.</p>
<p>The same thing is true in Russell and Doris’s examples. The agent has quite a bit of evidence that <span class="math inline">\(p\)</span>. That’s why she knows <span class="math inline">\(p\)</span>. There’s a potential practical defeater for <span class="math inline">\(p\)</span>. But due to either indifference or wealth, the defeater is immunised. Surprisingly perhaps, indifference and/or wealth can be the difference between knowledge and ignorance. But that’s not because they can be in any interesting sense ‘knowledge makers’, any more than I can make a bowl of soup by preventing someone from tossing it out. Rather, they can be things that block defeaters, both when the defeaters are the kind Stanley talks about, and when they are more familiar kinds of defeaters.</p>
</section>
<section id="sect:time" class="level1 page-columns page-full" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Temporal Embeddings</h1>
<p>Michael <span class="citation" data-cites="MBT2009">Blome-Tillmann (<a href="#ref-MBT2009" role="doc-biblioref">2009</a>)</span> has argued that tense-shifted knowledge ascriptions can be used to show that his version of Lewisian contextualism is preferable to IRI. Like Russell and Doris, his argument uses a variant of Stanley’s Bank Cases.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> Let <span class="math inline">\(O\)</span> be that the bank is open Saturday morning. If Hannah has a large debt, she is in a high-stakes situation with respect to <span class="math inline">\(O\)</span>. In Blome-Tillmann’s version of the example, Hannah had in fact incurred a large debt, but on Friday morning the creditor waived this debt. Hannah had no way of anticipating this on Thursday. She has some evidence for <span class="math inline">\(O\)</span>, but not enough for knowledge if she’s in a high-stakes situation. Blome-Tillmann says that this means after Hannah discovers the debt waiver, she could say</p>
<div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;In the interests of space, I won’t repeat those cases yet again here.</p></li></div><ol type="1">
<li>I didn’t know <span class="math inline">\(O\)</span> on Thursday, but on Friday I did.</li>
</ol>
<p>But I’m not sure why this case should be problematic for any version of IRI, and very unsure why it should even look like a <em>reductio</em> of IRI. As Blome-Tillmann notes, it isn’t really a situation where Hannah’s stakes change. She was never actually in a high stakes situation. At most her perception of her stakes change; she thought she was in a high-stakes situation, then realised that she wasn’t. Blome-Tillmann argues that even this change in perceived stakes can be enough to make (1) true if IRI is true. Now actually I agree that this change in perception could be enough to make (1) true, but when we work through the reason that’s so, we’ll see that it isn’t because of anything distinctive, let alone controversial, about IRI.</p>
<p>If Hannah is rational, then given her interests she won’t be ignoring <span class="math inline">\(\neg O\)</span> possibilities on Thursday. She’ll be taking them into account in her plans. Someone who is anticipating <span class="math inline">\(\neg O\)</span> possibilities, and making plans for them, doesn’t know <span class="math inline">\(O\)</span>. That’s not a distinctive claim of IRI. Any theory should say that if a person is worrying about <span class="math inline">\(\neg O\)</span> possibilities, and planning around them, they don’t know <span class="math inline">\(O\)</span>. And that’s simply because knowledge requires a level of confidence that such a person simply does not show. If Hannah is rational, that will describe her on Thursday, but not on Friday. So (1) is true not because Hannah’s practical situation changes between Thursday and Friday, but because her psychological state changes, and psychological states are relevant to knowledge.</p>
<p>What if Hannah is, on Thursday, irrationally ignoring <span class="math inline">\(\neg O\)</span> possibilities, and not planning for them even though her rational self wishes she were planning for them? In that case, it seems she still believes <span class="math inline">\(O\)</span>. After all, she makes the same decisions as she would as if <span class="math inline">\(O\)</span> were sure to be true. But it’s worth remembering that if Hannah does irrationally ignore <span class="math inline">\(\neg O\)</span> possibilities, she is being irrational with respect to <span class="math inline">\(O\)</span>. And it’s very plausible that this irrationality defeats knowledge. That is, you can’t be irrational with respect to a proposition and know it. Irrationality excludes knowledge. In any case, I doubt this is the natural way to read Blome-Tillmann’s example. We naturally read Hannah as being rational, and if she is rational she won’t have the right kind of confidence to count as knowing <span class="math inline">\(O\)</span> on Thursday.</p>
<p>There’s a methodological point here worth stressing. Doing epistemology with imperfect agents often results in facing tough choices, where any way to describe a case feels a little counterintuitive. If we simply hew to intuitions, we risk being led astray by just focussing on the first way a puzzle case is described to us. But once we think through Hannah’s case, we see perfectly good reasons, independent of IRI, to endorse IRI’s prediction about the case.</p>
</section>
<section id="sect:conj" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Problematic Conjunctions</h1>
<p>Blome-Tillmann offers another argument against IRI, that makes heavy use of the notion of having enough evidence to know something. Here is how he puts the argument. (Again I’ve changed the numbering and some terminology for consistency with this paper.)</p>
<blockquote class="blockquote">
<p>Suppose that John and Paul have exactly the same evidence, while John is in a low-stakes situation towards <span class="math inline">\(p\)</span> and Paul in a high-stakes situation towards <span class="math inline">\(p\)</span>. Bearing in mind that IRI is the view that whether one knows <span class="math inline">\(p\)</span> depends on one’s practical situation, IRI entails that one can truly assert:</p>
<ol start="2" type="1">
<li>John and Paul have exactly the same evidence for <span class="math inline">\(p\)</span>, but only John has enough evidence to know <span class="math inline">\(p\)</span>, Paul doesn’t.</li>
</ol>
<p><span class="citation" data-cites="MBT2009">(<a href="#ref-MBT2009" role="doc-biblioref">Blome-Tillmann 2009, 328–29</a>)</span></p>
</blockquote>
<p>And this is meant to be a problem, because (2) is intuitively false.</p>
<p>But IRI doesn’t entail any such thing. We can see this by looking at a simpler example that illustrates the way ‘enough’ works.</p>
<p>George and Ringo both have $6000 in their bank accounts. They both are thinking about buying a new computer, which would cost $2000. Both of them also have rent due tomorrow, and they won’t get any more money before then. George lives in New York, so his rent is $5000. Ringo lives in Syracuse, so his rent is $1000. Clearly, (REC) and (RAC) are true.</p>
<dl>
<dt>REC</dt>
<dd>
Ringo has enough money to buy the computer.
</dd>
<dt>RAC</dt>
<dd>
Ringo can afford the computer.
</dd>
</dl>
<p>And (GEC) is true as well, though there’s at least a reading of (GAC) where it is false.</p>
<dl>
<dt>GEC</dt>
<dd>
George has enough money to buy the computer.
</dd>
<dt>GAC</dt>
<dd>
George can afford the computer.
</dd>
</dl>
<p>Focus for now on (GEC). It is a bad idea for George to buy the computer; he won’t be able to pay his rent. But he has enough money to do so; the computer costs $2000, and he has $6000 in the bank. So (GEC) is true. Admittedly there are things close to (GEC) that aren’t true. He hasn’t got enough money to buy the computer and pay his rent. You might say that he hasn’t got enough money to buy the computer given his other financial obligations. But none of this undermines (GEC).</p>
<p>Now just like George has enough money to buy the computer, Paul has enough evidence to know that <span class="math inline">\(p\)</span>. Paul can’t know that <span class="math inline">\(p\)</span>, just like George can’t buy the computer, because of his practical situation. But that doesn’t mean he doesn’t have enough evidence to know it. He clearly does have enough evidence, since he has the same evidence John has, and John knows that <span class="math inline">\(p\)</span>. So, contra Blome-Tillmann, IRI doesn’t entail this problematic conjunction.</p>
<p>In a footnote attached to this, Blome-Tillmann offers a reformulation of the argument.</p>
<blockquote class="blockquote">
<p>I take it that having enough evidence to ‘know <span class="math inline">\(p\)</span>’ in <span class="math inline">\(C\)</span> just means having evidence such that one is in a position to ‘know <span class="math inline">\(p\)</span>’ in <span class="math inline">\(C\)</span>, rather than having evidence such that one ‘knows <span class="math inline">\(p\)</span>’. Thus, another way to formulate (2) would be as follows: ‘John and Paul have exactly the same evidence for <span class="math inline">\(p\)</span>, but only John is in a position to know <span class="math inline">\(p\)</span>, Paul isn’t.’ <span class="citation" data-cites="MBT2009">(<a href="#ref-MBT2009" role="doc-biblioref">Blome-Tillmann 2009, 329n23</a>)</span></p>
</blockquote>
<p>Now having enough evidence to know <span class="math inline">\(p\)</span> isn’t the same as being in a position to know it, any more than having enough money to buy the computer puts George in a position to buy it. So I think this is more of a new objection than a reformulation of the previous point. But might it be a <em>stronger</em> objection? Might it be that IRI entails (PosK), which is false?</p>
<dl>
<dt>PosK</dt>
<dd>
John and Paul have exactly the same evidence for <span class="math inline">\(p\)</span>, but only John is in a position to know <span class="math inline">\(p\)</span>, Paul isn’t.
</dd>
</dl>
<p>Actually, it isn’t a problem that IRI says that (PosK) is true. In fact, almost any epistemological theory will imply that conjunctions like that are true. In particular, any epistemological theory that allows for the existence of defeaters which do not supervene on the possession of evidence will imply that conjunctions like (PosK) are true. For example, anyone who thinks that whether you can know that a barn-like structure is really a barn depends on whether there are non-barns in the neighbourhood that look like the structure you’re looking at will think that conjunctions like (PosK) are true. Again, it matters a lot that IRI is suggesting that traditional epistemologists did not notice that there are distinctively pragmatic defeaters. Once we see that, we’ll see that conjunctions like (PosK) are not surprising at all.</p>
<p>Consider again Con, and his friend Mod who is disposed to reason by modus ponens and not by affirming the consequent. We could say that Con and Mod have the same evidence for <span class="math inline">\(p\)</span>, but only Mod is in a position to know <span class="math inline">\(p\)</span>. There are only two ways to deny that conjunction. One is to interpret ‘position to know’ so broadly that Con is in a position to know <span class="math inline">\(p\)</span> because he could change his inferential dispositions. But then we might as well say that Paul is in a position to know <span class="math inline">\(p\)</span> because he could get into a different ‘stakes’ situation. Alternatively, we could say that Con’s inferential dispositions count as a kind of evidence against <span class="math inline">\(p\)</span>. But that stretches the notion of evidence beyond a breaking point. Note that we didn’t say Con had any <em>reason</em> to affirm the consequent, just that he does. Someone might adopt, or change, a poor inferential habit because they get new evidence. But they need not do so, and we shouldn’t count their inferential habits as evidence they have.</p>
<p>If that case is not convincing, we can make the same point with a simple Gettier-style case.</p>
<blockquote class="blockquote">
<p><strong>Getting the Job</strong></p>
<p>In world 1, at a particular workplace, someone is about to be promoted. Agnetha knows that Benny is the management’s favourite choice for the promotion. And she also knows that Benny is Swedish. So she comes to believe that the promotion will go to someone Swedish. Unsurprisingly, management does choose Benny, so Agnetha’s belief is true.</p>
<p>World 2 is similar, except there it is Anni-Frid who knows that Benny is the management’s favourite choice for the promotion, that Benny is Swedish. So <em>she</em> comes to believe that the promotion will go to someone Swedish. But in this world Benny quits the workplace just before the promotion is announced, and the management unexpectedly passes over a lot of Danish workers to promote another Swede, namely Björn. So Anni-Frid’s belief that the promotion will go to someone Swedish is true, but not in a way that she could have expected.</p>
</blockquote>
<p>In that story, I think it is clear that Agnetha and Anni-Frid have exactly the same evidence that the job will go to someone Swedish, but only Agnetha is in a position to know this, Anni-Frid is not. The fact that an intermediate step is false in Anni-Frid’s reasoning, but not Agnetha’s, means that Anni-Frid’s putative knowledge is defeated, but Agnetha’s is not. And when that happens, we can have differences in knowledge without differences in evidence. So it isn’t an argument against IRI that it allows differences in knowledge without differences in evidence.</p>
</section>
<section id="sect:holism" class="level1 page-columns page-full" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Holism and Defeaters</h1>
<p>The big lesson of the last few sections is that interests create defeaters. Sometimes an agent can’t know <span class="math inline">\(p\)</span> because adding <span class="math inline">\(p\)</span> to her stock of beliefs would introduce either incoherence or irrationality. The reason is normally that the agent faces some decision where it is, say, bad to do <span class="math inline">\(\varphi\)</span>, but good to do <span class="math inline">\(\varphi\)</span> given <span class="math inline">\(p\)</span>. In that situation, if she adds <span class="math inline">\(p\)</span>, she’ll either incoherently think that it’s bad to do <span class="math inline">\(\varphi\)</span> although it’s good to do it given what is (by her lights) true. Moreover, the IRI theorist says, being incoherent in this way blocks knowledge, so the agent doesn’t know <span class="math inline">\(p\)</span>.</p>
<p>But there are other, more roundabout, ways in which interests can mean that believing <span class="math inline">\(p\)</span> would entail incoherence. One of these is illustrated by an example alleged by Ram Neta to be hard for interest-relative theorists to accommodate.</p>
<blockquote class="blockquote">
<p>Kate needs to get to Main Street by noon: her life depends upon it. She is desperately searching for Main Street when she comes to an intersection and looks up at the perpendicular street signs at that intersection. One street sign says “State Street” and the perpendicular street sign says “Main Street.” Now, it is a matter of complete indifference to Kate whether she is on State Street–nothing whatsoever depends upon it. <span class="citation" data-cites="Neta2007">(<a href="#ref-Neta2007" role="doc-biblioref">Neta 2007, 182</a>)</span></p>
</blockquote>
<p>Let’s assume for now that Kate is rational; dropping this assumption introduces mostly irrelevant complications. That is, we will assume Kate is an expected utility maximiser. Kate will not believe she’s on Main Street. She would only have that belief if she took it to be settled that she’s on Main, and hence not worthy of spending further effort investigating. But presumably she won’t do that. The rational thing for her to do is to get confirming (or, if relevant, confounding) evidence for the appearance that she’s on Main. If it were settled that she was on Main, the rational thing to do would be to try to relax, and be grateful that she had found Main Street. Since she has different attitudes about what to do <em>simpliciter</em> and conditional on being on Main Street, she doesn’t believe she’s on Main Street.</p>
<p>So far so good, but what about her attitude towards the proposition that she’s on State Street? She has enough evidence for that proposition that her credence in it should be rather high. And no practical issues turn on whether she is on State. So she believes she is on State, right?</p>
<p>Not so fast! Believing that she’s on State has more connections to her cognitive system than just producing actions. Note in particular that street signs are hardly basic epistemic sources. They are the kind of evidence we should be ‘conservative’ about in the sense of <span class="citation" data-cites="Pryor2004-PRYWWW">Pryor (<a href="#ref-Pryor2004-PRYWWW" role="doc-biblioref">2004</a>)</span>. We should only use them if we antecedently believe they are correct. So for Kate to believe she’s on State, she’d have to believe the street signs she can see are correct. If not, she’d incoherently be relying on a source she doesn’t trust, even though it is not a basic source.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> But if she believes the street signs are correct, she’d believe she was on Main, and that would lead to practical incoherence. So there’s no way to coherently add the belief that she’s on State Street to her stock of beliefs. So she doesn’t know, and can’t know, that she’s either on State or on Main. This is, in a roundabout way, due to the high stakes Kate faces.</p>
<div class="no-row-height column-margin column-container"><li id="fn11"><p><sup>11</sup>&nbsp;The caveats here about basic sources are to cancel any suggestion that Kate has to antecedently believe that any source is reliable before she uses it. As <span class="citation" data-cites="Pryor2000-PRYTSA">Pryor (<a href="#ref-Pryor2000-PRYTSA" role="doc-biblioref">2000</a>)</span> notes, that view is problematic. The view that we only get knowledge from a street sign if we antecedently have reason to trust it is not so implausible.</p></li></div><p>Neta thinks that the best way for the interest-relative theorist to handle this case is to say that the high stakes associated with the proposition that Kate is on Main Street imply that certain methods of belief formation do not produce knowledge. And he argues, plausibly, that such a restriction will lead to implausibly sceptical results. But that’s not the only way for the interest-relative theorist to go. What they could, and I think should, say is that Kate can’t know she’s on State Street because the only grounds for that belief are intimately connected to a proposition that, in virtue of her interests, she needs very large amounts of evidence to believe.</p>
</section>
<section id="non-consequentialist-cases" class="level1 page-columns page-full" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Non-Consequentialist Cases</h1>
<p>None of the replies yet have leaned heavily on the last of the three points from the introduction, the fact that IRI is an existential claim. This reply will make heavy use of that fact.</p>
<p>If an agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. But when agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is not so clear that orthodox decision theory is the appropriate way to model the agents. That’s relevant to cases like this one, which Jessica Brown has argued are problematic for the epistemological theories John Hawthorne and Jason Stanley have recently been defending.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn12"><p><sup>12</sup>&nbsp;The target here is not directly the interest-relativity of their theories, but more general principles about the role of knowledge in action and assertion. But it’s important to see how IRI handles the cases that Brown discusses, since these cases are among the strongest challenges that have been raised to IRI.</p></li></div><blockquote class="blockquote">
<p>A student is spending the day shadowing a surgeon. In the morning he observes her in clinic examining patient A who has a diseased left kidney. The decision is taken to remove it that afternoon. Later, the student observes the surgeon in theatre where patient A is lying anaesthetised on the operating table. The operation hasn’t started as the surgeon is consulting the patient’s notes. The student is puzzled and asks one of the nurses what’s going on:</p>
<p><strong>Student</strong>: I don’t understand. Why is she looking at the patient’s records? She was in clinic with the patient this morning. Doesn’t she even know which kidney it is?</p>
<p><strong>Nurse</strong>: Of course, she knows which kidney it is. But, imagine what it would be like if she removed the wrong kidney. She shouldn’t operate before checking the patient’s records. <span class="citation" data-cites="Brown2008-BROKAP">(<a href="#ref-Brown2008-BROKAP" role="doc-biblioref">Brown 2008, 1144–45</a>)</span></p>
</blockquote>
<p>It is tempting, but I think mistaken, to represent the payoff table associated with the surgeon’s choice as follows. Let <strong>Left</strong> mean the left kidney is diseased, and <strong>Right</strong> mean the right kidney is diseased.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><strong>Left</strong></td>
<td style="text-align: center;"><strong>Right</strong></td>
</tr>
<tr class="even">
<td><strong>Remove left kidney</strong></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
</tr>
<tr class="odd">
<td><strong>Remove right kidney</strong></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td><strong>Check notes</strong></td>
<td style="text-align: center;"><span class="math inline">\(1-\varepsilon\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1-\varepsilon\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Here <span class="math inline">\(\varepsilon\)</span> is the trivial but non-zero cost of checking the chart. Given this table, we might reason that since the surgeon knows that she’s in the left column, and removing the left kidney is the best option in that column, she should remove the left kidney rather than checking the notes.</p>
<p>But that reasoning assumes that the surgeon does not have any obligations over and above her duty to maximise expected utility. And that’s very implausible, since consequentialism is a fairly implausible theory of medical ethics.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn13"><p><sup>13</sup>&nbsp;I’m not saying that consequentialism is wrong as a theory of medical ethics. But if it is right, so many intuitions about medical ethics are going to be mistaken that such intuitions have no evidential force. And Brown’s argument relies on intuitions about this case having evidential value. So I think for her argument to work, we have to suppose non-consequentialism about medical ethics.</p></li></div><p>It’s not clear exactly what obligation the surgeon has. Perhaps it is an obligation to not just know which kidney to remove, but to know this on the basis of evidence she has obtained while in the operating theatre. Or perhaps it is an obligation to make her belief about which kidney to remove as sensitive as possible to various possible scenarios. Before she checked the chart, this counterfactual was false: <em>Had she misremembered which kidney was to be removed, she would have a true belief about which kidney was to be removed.</em> Checking the chart makes that counterfactual true, and so makes her belief that the left kidney is to be removed a little more sensitive to counterfactual possibilities.</p>
<p>However we spell out the obligation, it is plausible given what the nurse says that the surgeon has some such obligation. And it is plausible that the ‘cost’ of violating this obligation, call it <span class="math inline">\(\delta\)</span>, is greater than the cost of checking the notes. So here is the decision table the surgeon faces.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><strong>Left</strong></td>
<td style="text-align: center;"><strong>Right</strong></td>
</tr>
<tr class="even">
<td><strong>Remove left kidney</strong></td>
<td style="text-align: center;"><span class="math inline">\(1-\delta\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1-\delta\)</span></td>
</tr>
<tr class="odd">
<td><strong>Remove right kidney</strong></td>
<td style="text-align: center;"><span class="math inline">\(-1-\delta\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1-\delta\)</span></td>
</tr>
<tr class="even">
<td><strong>Check notes</strong></td>
<td style="text-align: center;"><span class="math inline">\(1-\varepsilon\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1-\varepsilon\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>And it isn’t surprising, or a problem for an interest-relative theory of knowledge, that the surgeon should check the notes, even if she believes <em>and knows</em> that the left kidney is the diseased one. This is not to say that the surgeon does know that the left kidney is diseased, just that the version of IRI being defended here is neutral on that question.</p>
<p>There is a very general point here. It suffices to derive IRI that we defend principles like the following:</p>
<ul>
<li><p>Whenever maximising expected value is called for, one should maximise expected value conditional on everything one knows.</p></li>
<li><p>Maximising expected value is called for often enough that there exist the kinds of pairs of cases IRI claims exist. That’s because in some cases, changing the options facing an agent will make it the case that which live option is best differs from which live option is best given <span class="math inline">\(p\)</span>, even though the agent antecedently knew <span class="math inline">\(p\)</span>.</p></li>
</ul>
<p>But that doesn’t imply that maximising expected value is always called for. Especially in a medical case, it is hard to square an injunction like “Do No Harm!” with a view that one should maximise expected value, since maximising expected value requires treating harms and benefits symmetrically. What would be a problem for the version of IRI defended here was a case with the following four characteristics.</p>
<ul>
<li><p>Maximising expected value is called for in the case.</p></li>
<li><p>Conditional on <span class="math inline">\(p\)</span>, the action with the highest expected value is <span class="math inline">\(\varphi\)</span>.</p></li>
<li><p>It would be wrong to do <span class="math inline">\(\varphi\)</span>.</p></li>
<li><p>The agent knows <span class="math inline">\(p\)</span>.</p></li>
</ul>
<p>It is tempting for the proponent of IRI to resist any attempted counterexample by claiming it is not really a case of knowledge. That might be the right thing to say in Brown’s case. But IRI defenders should remember that it is often a good move to deny that the first condition holds. Consequentialism is not an obviously correct theory of decision making in morally fraught situations; purported counterexamples that rely on it can therefore be resisted.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-MBT2009" class="csl-entry" role="listitem">
Blome-Tillmann, Michael. 2009. <span>“Contextualism, Subject-Sensitive Invariantism, and the Interaction of <span>‘Knowledge’</span>-Ascriptions with Modal and Temporal Operators.”</span> <em>Philosophy and Phenomenological Research</em> 79 (2): 315–31. <a href="https://doi.org/10.1111/j.1933-1592.2009.00280.x">https://doi.org/10.1111/j.1933-1592.2009.00280.x</a>.
</div>
<div id="ref-Brown2008-BROKAP" class="csl-entry" role="listitem">
Brown, Jessica. 2008. <span>“Knowledge and Practical Reason.”</span> <em>Philosophy Compass</em> 3 (6): 1135–52. <a href="https://doi.org/10.1111/j.1747-9991.2008.00176.x">https://doi.org/10.1111/j.1747-9991.2008.00176.x</a>.
</div>
<div id="ref-Fantl2002" class="csl-entry" role="listitem">
Fantl, Jeremy, and Matthew McGrath. 2002. <span>“Evidence, Pragmatics, and Justification.”</span> <em>Philosophical Review</em> 111: 67–94. <a href="https://doi.org/10.2307/3182570">https://doi.org/10.2307/3182570</a>.
</div>
<div id="ref-FantlMcGrath2009" class="csl-entry" role="listitem">
———. 2009. <em>Knowledge in an Uncertain World</em>. Oxford: Oxford University Press.
</div>
<div id="ref-FeltzZarpentine2010" class="csl-entry" role="listitem">
Feltz, Adam, and Chris Zarpentine. 2010. <span>“Do You Know More When It Matters Less?”</span> <em>Philosophical Psychology</em> 23 (5): 683–706. <a href="https://doi.org/10.1080/09515089.2010.514572">https://doi.org/10.1080/09515089.2010.514572</a>.
</div>
<div id="ref-Ham2008" class="csl-entry" role="listitem">
Ham, Sandra A., Sarah Martin, and Harold W. Kohl III. 2008. <span>“Changes in the Percentage of Students Who Walk or Bike to School-United States, 1969 and 2001.”</span> <em>Journal of Physical Activity and Health</em> 5 (2): 205–15. <a href="https://doi.org/10.1123/jpah.5.2.205">https://doi.org/10.1123/jpah.5.2.205</a>.
</div>
<div id="ref-Harman1973" class="csl-entry" role="listitem">
Harman, Gilbert. 1973. <em>Thought</em>. Princeton: Princeton University Press.
</div>
<div id="ref-Hawthorne2004" class="csl-entry" role="listitem">
Hawthorne, John. 2004. <em>Knowledge and Lotteries</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Hawthorne2008-HAWKAA" class="csl-entry" role="listitem">
Hawthorne, John, and Jason Stanley. 2008. <span>“<span class="nocase">Knowledge and Action</span>.”</span> <em>Journal of Philosophy</em> 105 (10): 571–90. <a href="https://doi.org/10.5840/jphil20081051022">https://doi.org/10.5840/jphil20081051022</a>.
</div>
<div id="ref-May2010" class="csl-entry" role="listitem">
May, Joshua, Walter Sinnott-Armstrong, Jay G. Hull, and Aaron Zimmerman. 2010. <span>“Practical Interests, Relevant Alternatives, and Knowledge Attributions: An Empirical Study.”</span> <em>Review of Philosophy and Psychology</em> 1 (2): 265–73. <a href="https://doi.org/10.1007/s13164-009-0014-3">https://doi.org/10.1007/s13164-009-0014-3</a>.
</div>
<div id="ref-Neta2007" class="csl-entry" role="listitem">
Neta, Ram. 2007. <span>“Anti-Intellectualism and the Knowledge-Action Principle.”</span> <em>Philosophy and Phenomenological Research</em> 75 (1): 180–87. <a href="https://doi.org/10.1111/j.1933-1592.2007.00069.x">https://doi.org/10.1111/j.1933-1592.2007.00069.x</a>.
</div>
<div id="ref-Pryor2000-PRYTSA" class="csl-entry" role="listitem">
Pryor, James. 2000. <span>“<span class="nocase">The Skeptic and the Dogmatist</span>.”</span> <em>No<span>û</span>s</em> 34 (4): 517–49. <a href="https://doi.org/10.1111/0029-4624.00277">https://doi.org/10.1111/0029-4624.00277</a>.
</div>
<div id="ref-Pryor2004-PRYWWW" class="csl-entry" role="listitem">
———. 2004. <span>“<span class="nocase">What’s Wrong with Moore’s Argument?</span>”</span> <em>Philosophical Issues</em> 14 (1): 349–78. <a href="https://doi.org/10.1111/j.1533-6077.2004.00034.x">https://doi.org/10.1111/j.1533-6077.2004.00034.x</a>.
</div>
<div id="ref-RussellDoris2008" class="csl-entry" role="listitem">
Russell, Gillian, and John M. Doris. 2009. <span>“Knowledge by Indifference.”</span> <em>Australasian Journal of Philosophy</em> 86 (3): 429–37. <a href="https://doi.org/10.1080/00048400802001996">https://doi.org/10.1080/00048400802001996</a>.
</div>
<div id="ref-Stanley2005-STAKAP" class="csl-entry" role="listitem">
Stanley, Jason. 2005. <em><span class="nocase">Knowledge and Practical Interests</span></em>. Oxford University Press.
</div>
<div id="ref-Weatherson2005-WEACWD" class="csl-entry" role="listitem">
Weatherson, Brian. 2005. <span>“<span>Can We Do Without Pragmatic Encroachment?</span>”</span> <em>Philosophical Perspectives</em> 19 (1): 417–43. <a href="https://doi.org/10.1111/j.1520-8583.2005.00068.x">https://doi.org/10.1111/j.1520-8583.2005.00068.x</a>.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{weatherson2011,
  author = {Weatherson, Brian},
  title = {Defending {Interest} {Relative} {Invariantism}},
  volume = {2},
  pages = {591-609},
  date = {2011},
  doi = {10.5840/logos-episteme2011248},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
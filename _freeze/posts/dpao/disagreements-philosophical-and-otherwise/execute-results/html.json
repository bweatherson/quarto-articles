{
  "hash": "e7b06e1ef5e9308ab540ae14bd67cf5c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Disagreements, Philosophical and Otherwise\"\ndescription: |\n  The Equal Weight View of disagreement says that if an agent sees that an epistemic peer disagrees with her about p, the agent should change her credence in p to half way between her initial credence, and the peer's credence. But it is hard to believe the Equal Weight View for a surprising reason; not everyone believes it. And that means that if one did believe it, one would be required to lower one's belief in it in light of this peer disagreement. Brian Weatherson explores the options for how a proponent of the Equal Weight View might respond to this difficulty, and how this challenge fits into broader arguments against the Equal Weight View.\ndate: June 10 2013\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ndoi: \"10.1093/acprof:oso/9780199698370.003.0004\"\ncategories:\n  - epistemology\n  - disagreement\ncitation_url: 10.1093/acprof:oso/9780199698370.003.0004\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: disagreement.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 4\n    number_sections: true\n---\n\n\nThis paper started life as a short note I wrote around New Year 2007\nwhile in Minneapolis. It was originally intended as a blog post. That\nmight explain, if not altogether excuse, the flippant tone in places.\nBut it got a little long for a post, so I made it into the format of a\npaper and posted it to my website. The paper has received a lot of\nattention, so it seems like it will be helpful to see it in print. Since\na number of people have responded to the argument as stated, I've\ndecided to just reprint the article warts and all, and make a few\ncomments at the end about how I see its argument in the context of the\nsubsequent debate.\n\n<aside>\nPublished in _The Epistemology of Disagreement: New Essays_, edited by David Christensen and Jennifer Lackey, OUP 54-75\n</aside>\n\n::: {.center}\n**Disagreeing about Disagreement (2007)**\n:::\n\nI argue with my friends a lot. That is, I offer them reasons to believe\nall sorts of philosophical conclusions. Sadly, despite the quality of my\narguments, and despite their apparent intelligence, they don't always\nagree. They keep insisting on principles in the face of my wittier and\nwittier counterexamples, and they keep offering their own dull alleged\ncounterexamples to my clever principles. What is a philosopher to do in\nthese circumstances? (And I don't mean get better friends.)\n\nOne popular answer these days is that I should, to some extent, defer to\nmy friends. If I look at a batch of reasons and conclude $p$, and my\nequally talented friend reaches an incompatible conclusion $q$, I should\nrevise my opinion so I'm now undecided between $p$ and $q$. I should, in\nthe preferred lingo, assign equal weight to my view as to theirs. This\nis despite the fact that I've looked at their reasons for concluding $q$\nand found them wanting. If I hadn't, I would have already concluded $q$.\nThe mere fact that a friend (from now on I'll leave off the qualifier\n'equally talented and informed', since all my friends satisfy that)\nreaches a contrary opinion should be reason to move me. Such a position\nis defended by Richard Feldman\n[-@Feldman2005-FELRTE; -@Feldman2006-FELEPA], David Christensen\n[-@Christensen2007-CHREOD] and Adam Elga [-@Elga2007-ELGRAD].\n\nThis equal weight view, hereafter EW, is itself a philosophical\nposition. And while some of my friends believe it, some of my friends do\nnot. (Nor, I should add for your benefit, do I.) This raises an odd\nlittle dilemma. If EW is correct, then the fact that my friends disagree\nabout it means that I shouldn't be particularly confident that it is\ntrue, since EW says that I shouldn't be too confident about any position\non which my friends disagree. But, as I'll argue below, to consistently\nimplement EW, I have to be maximally confident that it is true. So to\naccept EW, I have to inconsistently both be very confident that it is\ntrue and not very confident that it is true. This seems like a problem,\nand a reason to not accept EW. We can state this argument formally as\nfollows, using the notion of a peer and an expert. Some people are peers\nif they are equally philosophically talented and informed as each other,\nand one is more expert than another if they are more informed and\ntalented than the other.\n\n1.  There are peers who disagree about EW, and there is no one who is an\n    expert relative to them who endorses EW.\n\n2.  If 1 is true, then according to EW, my credence in EW should be less\n    than 1.\n\n3.  If my credence in EW is less than 1, then the advice that EW offers\n    in a wide range of cases is incoherent.\n\n4.  So, the advice EW offers in a wide range of cases is incoherent.\n\nThe first three sections of this paper will be used to defend the first\nthree premises. The final section will look at the philosophical\nconsequences of the conclusion.\n\n### Peers and EW\n\nThomas Kelly [-@Kelly2005-KELTES] has argued against EW and in favour of\nthe view that a peer with the irrational view should defer to a peer\nwith the rational view. Elga helpfully dubs this the 'right reasons'\nview. Ralph Wedgwood [-@Wedgwood2007-WEDNON Ch. 11] has argued against\nEW and in favour of the view that one should have a modest 'egocentric\nbias', i.e. a bias towards one's own beliefs. On the other hand, as\nmentioned above, Elga, Christensen and Feldman endorse versions of EW.\nSo it certainly looks like there are very talented and informed\nphilosophers on either side of this debate.\n\nNow I suppose that if we were taking EW completely seriously, we would\nat this stage of the investigation look very closely at whether these\nfive really are epistemic peers. We could pull out their grad school\ntranscripts, look at the citation rates for their papers, get reference\nletters from expert colleagues, maybe bring one or two of them in for\njob-style interviews, and so on. But this all seems somewhat\ninappropriate for a scholarly journal. Not to mention a little\ntactless.[^1] So I'll just stipulate that they seem to be peers in the\nsense relevant for EW, and address one worry a reader may have about my\nargument.\n\nAn objector might say, \"Sure it seems antecedently that Kelly and\nWedgwood are the peers of the folks who endorse EW. But take a look at\nthe arguments for EW that have been offered. They look like good\narguments, don't they? Doesn't the fact that Kelly and Wedgwood don't\naccept these arguments mean that, however talented they might be in\ngeneral, they obviously have a blind spot when it comes to the\nepistemology of disagreement? If so, we shouldn't treat them as experts\non this question.\" There is something right about this. People can be\nexperts in one area, or even many areas, while their opinions are\nsystematically wrong in another. But the objector's line is unavailable\nto defenders of EW.\n\nIndeed, these defenders have been quick to distance themselves from the\nobjector. Here, for instance, is Elga's formulation of the EW view, a\nformulation we'll return to below.\n\n> Your probability in a given disputed claim should equal your prior\n> conditional probability in that claim. Prior to what? Prior to your\n> thinking through the claim, and finding out what your advisor thinks\n> of it. Conditional on what? On whatever you have learned about the\n> circumstances of how you and your advisor have evaluated the claim.\n> [@Elga2007-ELGRAD 490]\n\nThe fact that Kelly and Wedgwood come to different conclusions can't be\nenough reason to declare that they are not peers. As Elga stresses, what\nmatters is the prior judgment of their acuity. And Elga is right to\nstress this. If we declared anyone who doesn't accept reasoning that we\nfind compelling not a peer, then the EW view will be trivial. After all,\nthe EW view only gets its force from cases as described in the\nintroduction, where our friends reject reasoning we accept, and accept\nreasons we reject. If that makes them not a peer, the EW view never\napplies. So we can't argue that anyone who rejects EW is thereby less of\nan expert in the relevant sense than someone who accepts it, merely in\nvirtue of their rejection of EW. So it seems we should accept premise 1.\n\n### Circumstances of Evaluation\n\nElga worries about the following kind of case. Let $p$ be that the sum\nof a certain series of numbers, all of them integers, is 50. Let $q$ be\nthat the sum of those numbers is $400e$. My friend and I both add the\nnumbers, and I conclude $p$ while he concludes $q$. It seems that there\nis no reason to defer to my friend. I know, after all, that he has made\nsome kind of mistake. The response, say defenders of EW, is that\ndeference is context-sensitive. If I know, for example, that my friend\nis drunk, then I shouldn't defer to him. More generally, as Elga puts\nit, how much I should defer should depend on what I know about the\ncircumstances.\n\nNow this is relevant because one of the relevant circumstances might be\nthat my friend has come to a view that I regard as insane. That's what\nhappens in the case of the sums. Since my prior probability that my\nfriend is right given that he has an insane seeming view is very low, my\nposterior probability that my friend is right should also, according to\nElga, be low. Could we say that, although antecedently we regard\nWedgwood and Kelly as peers of those they disagree with, that the\ncircumstance of their disagreement is such that we should disregard\ntheir views?\n\nIt is hard to see how this would be defensible. It is true that a\nproponent of EW will regard Kelly and Wedgwood as wrong. But we can't\nsay that we should disregard the views of all those we regard as\nmistaken. That leads to trivialising EW, for reasons given above. The\nclaim has to be that their views are so outrageous, that we wouldn't\ndefer to anyone with views that outrageous. And this seems highly\nimplausible. But that's the only reason that premise 2 could be false.\nSo we should accept premise 2.\n\n### A Story about Disagreement\n\nThe tricky part of the argument is proving premise 3. To do this, I'll\nuse a story involving four friends, Apollo, Telemachus, Adam and Tom.\nThe day before our story takes place, Adam has convinced Apollo that he\nshould believe EW, and organise his life around it. Now Apollo and\nTelemachus are on their way to Fenway Park to watch the Red Sox play the\nIndians. There have been rumours flying around all day about whether the\nRed Sox injured star player, David Ortiz, will be healthy enough to\nplay. Apollo and Telemachus have heard all the competing reports, and\nare comparing their credences that Ortiz will play. (Call the\nproposition that he will play $p$.) Apollo's credence in $p$ is 0.7, and\nTelemachus's is 0.3. In fact, 0.7 is the rational credence in p given\ntheir shared evidence, and Apollo truly believes that it is.[^2] And, as\nit turns out, the Red Sox have decided but not announced that Ortiz will\nplay, so $p$ is true.\n\nDespite these facts, Apollo lowers his credence in $p$. In accord with\nhis newfound belief in EW, he changes his credence in $p$ to 0.5. Apollo\nis sure, after all, that when it comes to baseball Telemachus is an\nepistemic peer. At this point Tom arrives, and with a slight disregard\nfor the important baseball game at hand, starts trying to convince them\nof the right reasons view on disagreement. Apollo is not convinced, but\nTelemachus thinks it sounds right. As he puts it, the view merely says\nthat the rational person believes what the rational person believes. And\nwho could disagree with that?\n\nApollo is not convinced, and starts telling them the virtues of EW. But\na little way in, Tom cuts him off with a question. \"How probable,\" he\nasks Apollo, \"does something have to be before you'll assert it?\"\n\nApollo says that it has to be fairly probable, though just what the\nthreshold is depends on just what issues are at stake. But he agrees\nthat it has to be fairly high, well above 0.5 at least.\n\n\"Well,\" says Tom, \"in that case you shouldn't be defending EW in public.\nBecause you think that Telemachus and I are the epistemic peers of you\nand Adam. And we think EW is false. So even by EW's own lights, the\nprobability you assign to EW should be 0.5. And that's not a high enough\nprobability to assert it.\" Tom's speech requires that Apollo regard he\nand Telemachus as Apollo's epistemic peers with regard to this question.\nBy premises 1 and 2, Apollo should do this, and we'll assume that he\ndoes.\n\nSo Apollo agrees with all this, and agrees that he shouldn't assert EW\nany more. But he still plans to use it, i.e. to have a credence in $p$\nof 0.5 rather than 0.7. But now Telemachus and Tom press on him the\nfollowing analogy.\n\nImagine that there were two competing experts, each of whom gave\ndiffering views about the probability of $q$. One of the experts, call\nher Emma, said that the probability of $q$, given the evidence, is 0.5.\nThe other expert, call her Rae, said that the probability of $q$, given\nthe evidence, is 0.7. Assuming that Apollo has the same evidence as the\nexperts, but he regards the experts as experts at evaluating evidence,\nwhat should his credence in $q$ be? It seems plausible that it should be\na weighted average of what Emma says and what Rae says. In particular,\nit should be 0.5 only if Apollo is maximally confident that Emma is the\nexpert to trust, and not at all confident that Rae is the expert to\ntrust.\n\nThe situation is parallel to the one Apollo actually faces. EW says that\nhis credence in $p$ should be 0.5. The right reason view says that his\ncredence in $p$ should be 0.7. Apollo is aware of both of these facts.\nSo his credence in $p$ should be 0.5 iff he is certain that EW is the\ntheory to trust, just as his credence in $q$ should be 0.5 iff he is\ncertain that Emma is the expert to trust. Indeed, a credence of 0.5 in\n$p$ is incoherent unless Apollo is certain EW is the theory to trust.\nBut Apollo is not at all certain of this. His credence in EW, as is\nrequired by EW itself, is 0.5. So as long as Apollo keeps his credence\nin p at 0.5, he is being incoherent. But EW says to keep his credence in\np at 0.5. So EW advises him to be incoherent. That is, EW offers\nincoherent advice. We can state this more carefully in an argument.\n\n5.  EW says that Apollo's credence in p should be 0.5.\n\n6.  If 5, then EW offers incoherent advice unless it also says that\n    Apollo's credence in EW should be 1.\n\n7.  EW says that Apollo's credence in EW should be 0.5.\n\n8.  So, EW offers incoherent advice.\n\nSince Apollo's case is easily generalisable, we can infer that in a\nlarge number of cases, EW offers advice that is incoherent. Line 7 in\nthis argument is hard to assail given premises 1 and 2 of the master\nargument. But I can imagine objections to each of the other lines.\n\n*Objection*: Line 6 is false. Apollo can coherently have one credence in\np while being unsure about whether it is the rational credence to have.\nIn particular, he can coherently have his credence in p be 0.5, while he\nis unsure whether his credence in p should be 0.5 or 0.7. In general\nthere is no requirement for agents who are not omniscient to have their\ncredences match their judgments of what their credences should be.\n\n*Replies*: I have two replies to this, the first dialectical and the\nsecond substantive.\n\nThe dialectical reply is that if the objector's position on coherence is\naccepted, then a lot of the motivation for EW fades away. A core idea\nbehind EW is that Apollo was unsure before the conversation started\nwhether he or Telemachus would have the most rational reaction to the\nevidence, and hearing what each of them say does not provide him with\nmore evidence. (See the 'bootstrapping' argument in @Elga2007-ELGRAD for\na more formal statement of this idea.) So Apollo should have equal\ncredence in the rationality of his judgment and of Telemachus's\njudgment.\n\nBut if the objector is correct, Apollo can do that without changing his\nview on EW one bit. He can, indeed should, have his credence in $p$ be\n0.7, while being uncertain whether his credence in p should be 0.7 (as\nhe thinks) or 0.3 (as Telemachus thinks). Without some principle\nconnecting what Apollo should think about what he should think to what\nApollo should think, it is hard to see why this is not the uniquely\nrational reaction to Apollo's circumstances. In other words, if this is\nan objection to my argument against EW, it is just as good an objection\nto a core argument for EW.\n\nThe substantive argument is that the objector's position requires\nviolating some very weak principles concerning rationality and\nhigher-order beliefs. The objector is right that, for instance, in order\nto justifiably believe that $p$ (to degree $d$), one need not know, or\neven believe, that one is justified in believing $p$ (to that degree).\nIf nothing else, the anti-luminosity arguments in @Williamson2000-WILKAI\nshow that to be the case. But there are weaker principles that are more\nplausible, and which the objector's position has us violate. In\nparticular, there is the view that we can't both be justified in\nbelieving that $p$ (to degree $d$), while we know we are not justified\nin believing that we are justified in believing $p$ (to that degree). In\nsymbols, if we let $Jp$ mean that the agent is justified in believing\n$p$, and box and diamond to be epistemic modals, we have the principle\n**MJ** (for Might be Justified).\n\nMJ\n\n:   $Jp \\rightarrow \\diamond JJp$\n\nThis seems like a much more plausible principle, since if we know we\naren't justified in believing we're justified in believing $p$, it seems\nlike we should at least suspend judgment in $p$. That is, we shouldn't\nbelieve $p$. That is, we aren't justified in believing $p$. But the\nobjector's position violates principle **MJ**, or at least a\nprobabilistic version of it, as we'll now show.\n\nWe aim to prove that the objector is committed to Apollo being justified\nin believing $p$ to degree 0.5, while he knows he is not justified in\nbelieving he is justified in believing $p$ to degree 0.5. The first part\nis trivial, it's just a restatement of the objector's view, so it is the\nsecond part that we must be concerned with.\n\nNow, either EW is true, or it isn't true. If it is true, then Apollo is\nnot justified in having a greater credence in it than 0.5. But his only\njustification for believing p to degree 0.5 is EW. He's only justified\nin believing he's justified in believing $p$ if he can justify his use\nof EW in it. But you can't justify a premise in which your rational\ncredence is 0.5. So Apollo isn't justified in believing he is justified\nin believing $p$. If EW isn't true, then Apollo isn't even justified in\nbelieving $p$ to degree 0.5. And he knows this, since he knows EW is his\nonly justification for lowering his credence in $p$ that far. So he\ncertainly isn't justified in believing he is justified in believing $p$\nto degree 0.5 Moreover, every premise in this argument has been a\npremise that Apollo knows to obtain, and he is capable of following all\nthe reasoning. So he knows that he isn't justified in believing he is\njustified in believing $p$ to degree 0.5, as required.\n\nThe two replies I've offered to the objector complement one another. If\nsomeone accepts **MJ**, then they'll regard the objector's position as\nincoherent, since we've just shown that **MJ** is inconsistent with that\nposition. If, on the other hand, someone rejects **MJ** and everything\nlike it, then they have little reason to accept EW in the first place.\nThey should just accept that Apollo's credence in p should be, as per\nhypothesis the evidence suggests, 0.7. The fact that an epistemic peer\ndisagrees, in the face of the same evidence, might give Apollo reason to\ndoubt that this is in fact that uniquely rational response to the\nevidence. But, unless we accept a principle like **MJ**, that's\nconsistent with Apollo retaining the rational response to the evidence,\nnamely a credence of 0.7 in p. So it is hard to see how someone could\naccept the objector's argument, while also being motivated to accept EW.\nIn any case, I think **MJ** is plausible enough on its own to undermine\nthe objector's position.[^3]\n\n*Objection*: Line 5 is false. Once we've seen that the credence of EW is\n0.5, then Apollo's credence in first-order claims such as p should, as\nthe analogy with q suggests, be a weighted average of what EW says it\nshould be, and what the right reason view says it should be. So, even by\nEW's own lights, Apollo's credence in p should be 0.6.\n\n*Replies*: Again I have a dialectical reply, and a substantive reply.\n\nThe dialectical reply is that once we make this move, we really have\nvery little motivation to accept EW. There is, I'll grant, some\nintuitive plausibility to the view that when faced with a disagreeing\npeer, we should think the right response is half way between our\ncompeting views. But there is no intuitive plausibility whatsoever to\nthe view that in such a situation, we should naturally move to a\nposition three-quarters of the way between the two competing views, as\nthis objector suggests. Much of the argument for EW, especially in\nChristensen, turns on intuitions about cases, and the objector would\nhave us give all of that up. Without those intuitions, however, EW falls\nin a heap.\n\nThe substantive reply is that the idea behind the objection can't be\ncoherently sustained. The idea is that we should first apply EW to\nphilosophical questions to work out the probability of different\ntheories of disagreement, and then apply those probabilities to\nfirst-order disagreements. The hope is that in doing so we'll reach a\nstable point at which EW can be coherently applied. But there is no such\nstable point. Consider the following series of questions.\n\nQ1\n\n:   Is EW true?\n\nTwo participants say yes, two say no. We have a dispute, leading to our\nnext question.\n\nQ2\n\n:   What is the right reaction to the disagreement over Q1?\n\nEW answers this by saying our credence in EW should be 0.5. But that's\nnot what the right reason proponents say. They don't believe EW, so they\nhave no reason to move their credence in EW away from 0. So we have\nanother dispute, and we can ask\n\nQ3\n\n:   What is the right reaction to the disagreement over Q2?\n\nEW presumably says that we should again split the difference. Our\ncredence in EW might now be 0.25, half-way between the 0.5 it was after\nconsidering Q2, and what the right reasons folks say. But, again, those\nwho don't buy EW will disagree, and won't be moved to adjust their\ncredence in EW. So again there's a dispute, and again we can ask\n\nQ4\n\n:   What is the right reaction to the disagreement over Q3?\n\nThis could go on for a while. The only 'stable point' in the sequence is\nwhen we assign a credence of 0 to EW. That's to say, the only way to\ncoherently defend the idea behind the objection is to assign credence 0\nto EW. But that's to give up on EW. As with the previous objection, we\ncan't hold on to EW and object to the argument.\n\n### Summing Up\n\nThe story I've told here is a little idealised, but otherwise common\nenough. We often have disagreements both about first-order questions,\nand about how to resolve this disagreement. In these cases, there is no\ncoherent way to assign equal weight to all prima facie rational views\nboth about the first order question and the second order,\nepistemological, question. The only way to coherently apply EW to all\nfirst order questions is to put our foot down, and say that despite the\napparent intelligence of our philosophical interlocutors, we're not\nletting them dim our credence in EW. But if we are prepared to put our\nfoot down here, why not about some first-order question or other? It\ncertainly isn't because we have more reason to believe an\nepistemological theory like EW than we have to believe first order\ntheories about which there is substantive disagreement. So perhaps we\nshould hold on to those theories, and let go of EW.\n\n::: {.center}\n**Afterthoughts (2010)**\n:::\n\nI now think that the kind of argument I presented in the 2007 paper is\nnot really an argument against EW as such, but an argument against one\npossible motivation for EW. I also think that alternate motivations for\nEW are no good, so I still think it is an important argument. But I\nthink it's role in the dialectic is a little more complicated than I\nappreciated back then.\n\nMuch of my thinking about disagreement problems revolves around the\nfollowing table. The idea behind the table, and much of the related\nargument, is due to Thomas Kelly [-@Kelly2010-KELPDA]. In the table, $S$\nand $T$ antecedently had good reasons to take themselves to be epistemic\npeers, and they know that their judgments about $p$ are both based on\n$E$. In fact, $E$ is excellent evidence for $p$, but only $S$ judges\nthat $p$; $T$ judges that $\\neg p$. Now let's look at what seems to be\nthe available evidence for and against $p$.\n\n::: {.center}\n  ------------------------- ------------------------------\n    **Evidence for $p$**       **Evidence against $p$**\n   $S$'s judgment that $p$   $T$'s judgment that $\\neg p$\n             $E$            \n  ------------------------- ------------------------------\n:::\n\nNow that doesn't look to me like a table where the evidence is equally\nbalanced for and against $p$. Even granting that the judgments are\nevidence over and above $E$, and granting that how much weight we should\ngive to judgments should track our *ex ante* judgments of their\nreliability rather than our *ex post* judgments of their reliability,\nboth of which strike me as false but necessary premises for EW, it\n*still* looks like there is more evidence for $p$ than against $p$.[^4]\nThere is strictly more evidence for $p$ than against it, since $E$\nexists. If we want to conclude that $S$ should regard $p$ and $\\neg p$\nas equally well supported for someone in her circumstance, we have to\nshow that the table is somehow wrong. I know of three possible moves the\nEW defender could make here.\n\nDavid Christensen [-@Christensen2010-CHRDQB], as I read him, says that\nthe table is wrong because when we are representing the evidence $S$\nhas, we should not include her own judgment. There's something plausible\nto this. Pretend for a second that $T$ doesn't exist, so it's clearly\nrational for $S$ to judge that $p$. It would still be wrong of $S$ to\nsay, \"Since $E$ is true, $p$. And I judged that $p$, so that's another\nreason to believe that $p$, because I'm smart.\" By hypothesis, $S$ is\nsmart, and that smart people judge things is reason to believe those\nthings are true. But this doesn't work when the judgment is one's own.\nThis is something that needs explaining in a full theory of the\nepistemic significance of judgment, but let's just take it as a given\nfor now.[^5] Now the table, or at least the table as is relevant to $S$,\nlooks as follows.\n\n::: {.center}\n  ---------------------- ------------------------------\n   **Evidence for $p$**     **Evidence against $p$**\n           $E$            $T$'s judgment that $\\neg p$\n  ---------------------- ------------------------------\n:::\n\nBut I don't think this does enough to support EW, or really anything\nlike it. First, it won't be true in general that the two sides of this\ntable balance. In many cases, $E$ is strong evidence for $p$, and $T$'s\njudgment won't be particularly strong evidence against $p$. In fact, I'd\nsay the kind of case where $E$ is much better evidence for $p$ than\n$T$'s judgment is against $p$ is the statistically normal kind. Or, at\nleast, it is the normal kind of case modulo the assumption that $S$ and\n$T$ have the same evidence. In cases where that isn't true, learning\nthat $T$ thinks $\\neg p$ is good evidence that $T$ has evidence against\n$p$ that you don't have, and you should adjust accordingly. But by\nhypothesis, $S$ knows that isn't the case here. So I don't see why this\nshould push us even close to taking $p$ and $\\neg p$ to be equally well\nsupported.\n\nThe other difficulty for defending EW by this approach is that it seems\nto undermine the original motivations for the view. As Christensen\nnotes, the table above is specifically for $S$. Here's what the table\nlooks like for $T$.\n\n::: {.center}\n  ------------------------- --------------------------\n    **Evidence for $p$**     **Evidence against $p$**\n   $S$'s judgment that $p$  \n             $E$            \n  ------------------------- --------------------------\n:::\n\nIt's no contest! So $T$ should firmly believe $p$. But that isn't the\nintuition anyone gets, as far as I can tell, in any of the cases\nmotivating EW. And the big motivation for EW comes from intuitions about\ncases. Once we acknowledge that these intuitions are unreliable, as we'd\nhave to do if we were defending EW this way, we seem to lack any reason\nto accept EW.\n\nThe second approach to blocking the table is to say that $T$'s judgment\nis an undercutting defeater for the support $E$ provides for $p$. This\nlooks superficially promising. Having a smart person say that your\nevidence supports something other than you thought it did seems like it\ncould be an undercutting defeater, since it is a reason to think the\nevidence supports something else, and hence doesn't support what you\nthought it does. And, of course, if $E$ is undercut, then the table just\nhas one line on it, and the two sides look equal.\n\nBut it doesn't seem like it can work in general, for a reason that\n@Kelly2010-KELPDA makes clear. We haven't said what $E$ is so far. Let's\nstart with a case where $E$ consists of the judgments of a million other\nvery smart people that $p$. Then no one, not even the EW theorist, will\nthink that $T$'s judgment undercuts the support $E$ provides to $p$.\nIndeed, even if $E$ just consists of one other person's judgment, it\nwon't be undercut by $T$'s judgment. The natural thought for an\nEW-friendly person to have in that case is that since there are two\npeople who think $p$, and one who thinks $\\neg p$, then $S$'s credence\nin $p$ should be $\\frac{2}{3}$. But that's impossible if $E$, i.e.,\nthe third person's judgment, is undercut by $T$'s judgment. It's true\nthat $T$'s judgment will partially *rebut* the judgments that $S$, and\nthe third party, make. It will move the probability of $p$, at least\naccording to EW, from 1 to $\\frac{2}{3}$. But that evidence won't be\nin any way *undercut*.\n\nAnd as Kelly points out, evidence is pretty fungible. Whatever support\n$p$ gets from other people's judgments, it could get very similar\nsupport from something other than a judgment. We get roughly the same\nevidence for $p$ by learning that a smart person predicts $p$ as\nlearning that a successful computer model predicts $p$. So the following\nargument looks sound to me.\n\n1.  When $E$ consists of other people's judgments, the support it\n    provides to $p$ is not undercut by $T$'s judgment.\n\n2.  If the evidence provided by other people's judgments is not undercut\n    by $T$'s judgment, then some non-judgmental evidence is not undercut\n    by $T$'s judgment.\n\n3.  So, not all non-judgmental evidence is not undercut by $T$'s\n    judgment.\n\nSo it isn't true in general that the table is wrong because $E$ has been\ndefeated by an undercutting defeater.\n\nThere's another problem with the defeat model in cases where the initial\njudgments are not full beliefs. Change the case so $E$ provides\nbasically no support to either $p$ or $\\neg p$. In fact, $E$ is just\nirrelevant to $p$, and the agent's have nothing to base either a firm or\na probabilistic judgment about $p$ on. For this reason, $S$ declines to\nform a judgment, but $T$ forms a firm judgment that $p$. Moreover,\nalthough both $S$ and $T$ are peers, that's because they are both\nequally poor at making judgments about cases like $p$. Here's the table\nthen:\n\n::: {.center}\n  ------------------------- --------------------------\n    **Evidence for $p$**     **Evidence against $p$**\n   $T$'s judgment that $p$  \n  ------------------------- --------------------------\n:::\n\nSince $E$ is irrelevant, it doesn't appear, either before or after we\nthink about defeaters. And since $T$ is not very competent, that's not\ngreat evidence for $p$. But EW says that $S$ should 'split the\ndifference' between her initial agnositicism, and $T$'s firm belief in\n$p$. I don't see how that could be justified by $S$'s evidence.\n\nSo that move doesn't work either, and we're left with the third option\nfor upsetting the table. This move is, I think, the most promising of\nthe lot. It is to say that $S$'s own judgment *screens off* the evidence\nthat $E$ provides. So the table is misleading, because it 'double\ncounts' evidence.\n\nThe idea of screening I'm using here, at least on behalf of EW, comes\nfrom Reichenbach's *The Direction of Time*, and in particular from his\nwork on deriving a principle that lets us infer events have a common\ncause. The notion was originally introduced in probabilistic terms. We\nsay that $C$ screens off the positive correlation between $B$ and $A$ if\nthe following two conditions are met.\n\n1.  $A$ and $B$ are positively correlated probabilistically, i.e.\n    $Pr(A | B) > Pr(A)$.\n\n2.  Given $C$, $A$ and $B$ are probabilistically independent,\\\n    i.e. $Pr(A | B \\wedge C) = Pr(A | C)$.\n\nI'm interested in an evidential version of screening. If we have a\nprobabilistic analysis of evidential support, the version of screening\nI'm going to offer here is identical to the Reichenbachian version just\nprovided. But I want to stay neutral on whether we should think of\nevidence probabilistically.[^6] When I say that $C$ screens off the\nevidential support that $B$ provides to $A$, I mean the following. (Both\nthese clauses, as well as the statement that $C$ screens off $B$ from\n$A$, are made relative to an evidential background. I'll leave that as\ntacit in what follows.)\n\n1.  $B$ is evidence that $A$.\n\n2.  $B \\wedge C$ is no better evidence that $A$ than $C$ is.[^7]\n\nHere is one stylised example of where screening helps conceptualise\nthings. Detective Det is trying to figure out whether suspect Sus\ncommitted a certain crime. Let $A$ be that Sus is guilty, $B$ be that\nSus's was seen near the crime scene near the time the crime was\ncommitted, and $C$ be that Sus was at the crime scene when the crime was\ncommitted. Then both clauses are satisfied. $B$ is evidence for $A$;\nthat's why we look for witnesses who place the suspect near the crime\nscene. But given the further evidence $C$, then $B$ is neither here nor\nthere with respect to $A$. We're only interested in finding if Sus was\nnear the crime scene because we want to know whether he was at the crime\nscene. If we know that he was there, then learning he was seen near\nthere doesn't move the investigation along. So both clauses of the\ndefinition of screening are satisfied.\n\nWhen there is screened evidence, there is the potential for double\ncounting. It would be wrong to say that if we know $B \\wedge C$ we have\ntwo pieces of evidence against Sus. Similarly, if a judgment screens off\nthe evidence it is based on, then the table 'double counts' the evidence\nfor $p$. Removing the double counting, by removing $E$, makes the table\nsymmetrical. And that's just what EW needs.\n\nSo the hypothesis that judgments screen the evidence they are based on,\nor JSE for short, can help EW respond to the argument from this table.\nBut JSE is vulnerable to regress arguments. I now think that the\nargument in 'Disagreeing about Disagreement' is a version of the regress\nargument against JSE. So really it's an argument against the most\npromising response to a particularly threatening argument against EW.\n\nUnfortunately for EW, those regress arguments are actually quite good.\nTo see ths, let's say an agent makes a judgment on the basis of $E$, and\nlet $J$ be the proposition that that judgment was made. JSE says that\n$E$ is now screened off, and the agent's evidence is just $J$. But with\nthat evidence, the agent presumably makes a new judgment. Let $J^\\prime$\nbe the proposition that that judgment was made. We might ask now, does\n$J^\\prime$ sit alongside $J$ as extra evidence, is it screened off by\n$J$, or does it screen off $J$? The picture behind JSE, the picture that\nsays that judgments on the basis of some evidence screen that evidence,\nsuggest that $J^\\prime$ should in turn screen $J$. But now it seems we\nhave a regress on our hands. By the same token, $J^{\\prime \\prime}$, the\nproposition concerning the new judgment made on the basis of $J^\\prime$,\nshould screen off $J^\\prime$, and the proposition\n$J^{\\prime \\prime \\prime}$ about the fourth judgment made, should screen\noff $J^{\\prime \\prime}$, and so on. The poor agent has no unscreened\nevidence left! Something has gone horribly wrong.\n\nI think this regress is ultimately fatal for JSE. But to see this, we\nneed to work through the possible responses that a defender of JSE could\nmake. There are really just two moves that seem viable. One is to say\nthat the regress does not get going, because $J$ is better evidence than\n$J^\\prime$, and perhaps screens it. The other is to say that the regress\nis not vicious, because all these judgments should agree in their\ncontent. I'll end the paper by addressing these two responses.\n\nThe first way to avoid the regress is to say that there is something\nspecial about the first level. So although $J$ screens $E$, it isn't the\ncase that $J^\\prime$ screens $J$. That way, the regress doesn't start.\nThis kind of move is structurally like the move Adam Elga\n[-@Elga2010-ELGHTD] has recently suggested. He argues that we should\nadjust our views about first-order matters in (partial) deference to our\npeers, but we shouldn't adjust our views about the right response to\ndisagreement in this way.\n\nIt's hard to see what could motivate such a position, either about\ndisagreement or about screening. It's true that we need some kind of\nstopping point to avoid these regresses. But the most natural stopping\npoint is the very first level. Consider a toy example. It's common\nknowledge that there are two apples and two oranges in the basket, and\nno other fruit. (And that no apple is an orange.) Two people disagree\nabout how many pieces of fruit there are in the basket. $A$ thinks there\nare four, $B$ thinks there are five, and both of them are equally\nconfident. Two other people, $C$ and $D$, disagree about what $A$ and\n$B$ should do in the face of this disagreement. All four people regard\neach other as peers. Let's say $C$'s position is the correct one\n(whatever that is) and $D$'s position is incorrect. Elga's position is\nthat $A$ should partially defer to $B$, but $C$ should not defer to $D$.\nThis is, intuitively, just back to front. $A$ has evidence that\nimmediately and obviously entails the correctness of her position. $C$\nis making a complicated judgment about a philosophical question where\nthere are plausible and intricate arguments on each side. The position\n$C$ is in is much more like the kind of case where experience suggests a\nmeasure of modesty and deference can lead us away from foolish errors.\nIf anyone should be sticking to their guns here, it is $A$, not $C$.\n\nThe same thing happens when it comes to screening. Let's say that $A$\nhas some evidence that (a) she has made some mistakes on simple sums in\nthe past, but (b) tends to massively over-estimate the likelihood that\nshe's made a mistake on any given puzzle. What should she do? One\noption, in my view the correct one, is that she should believe that\nthere are four pieces of fruit in the basket, because that's what the\nevidence obviously entails. Another option is that she should be not\nvery confident there are four pieces of fruit in the basket, because she\nmakes mistakes on these kinds of sums. Yet another option is that she\nshould be pretty confident (if not completely certain) that there are\nfour pieces of fruit in the basket, because if she were not very\nconfident about this, this would just be a manifestation of her\nover-estimation of her tendency to err. The 'solution' to the regress\nwe're considering here says that the second of these three reactions is\nthe uniquely rational reaction. The idea behind the solution is that we\nshould respond to the evidence provided by first-order judgments, and\ncorrect that judgment for our known biases, but that we shouldn't in\nturn correct for the flaws in our self-correcting routine. I don't see\nwhat could motivate such a position. Either we just rationally respond\nto the evidence, and in this case just believe there are four pieces of\nfruit in the basket, or we keep correcting for errors we make in any\njudgment. It's true that the latter plan leads either to regress or to\nthe kind of ratificationism we're about to critically examine. But\nthat's not because the disjunction is false, it's because the first\ndisjunct is true.\n\nA more promising way to avoid the regress is suggested by some other\nwork of Elga's, in this case a paper he co-wrote with Andy Egan\n[@Egan2005-EGAICB]. Their idea, as I understand them, is that for any\nrational agent, any judgment they make must be such that when they add\nthe fact that they made that judgment to their evidence (or, perhaps\nbetter given JSE, replace their evidence with the fact that they made\nthat judgment), the rational judgment to make given the new evidence has\nthe same content as the original judgment. So if you're rational, and\nyou come to believe that $p$ is likely true, then the rational thing to\nbelieve given you've made that judgment is that $p$ is likely true.\n\nNote that this isn't as strong a requirement as it may first seem. The\nrequirement is not that any time an agent makes a judgment, rationality\nrequires that they say on reflection that it is the correct judgments.\nRather, the requirement is that the only judgments rational agents make\nare those judgments that, on reflection, she would reflectively endorse.\nWe can think of this as a kind of ratifiability constraint on judgment,\nlike the ratifiability constraint on decision making that Richard\nJeffrey uses to handle Newcomb cases @JeffreyLogicOfDecision.\n\nTo be a little more precise, a judgment is ratifiable for agent $S$ just\nin case the rational judgment for $S$ to make conditional on her having\nmade that judgment has the same content as the original judgment. The\nthought then is that we avoid the regress by saying rational agents\nalways make ratifiable judgments. If the agent does do that, there isn't\nmuch of a problem with the regress; once she gets to the first level,\nshe has a stable view, even once she reflects on it.\n\nIt seems to me that this assumption, that only ratifiable judgments are\nrational, is what drives most of the arguments in Egan and Elga's paper\non self-confidence, so I don't think this is a straw-man move. Indeed,\nas the comparison to Jeffrey suggests, it has some motivation behind it.\nNevertheless it is false. I'll first note one puzzling feature of the\nview, then one clearly false implication of the view.\n\nThe puzzling feature is that in some cases there may be nothing we can\nrationally do which is ratifiable. One way this can happen involves a\nslight modification of Egan and Elga's example of the\ndirectionaly-challenged driver. Imagine that when I'm trying to decide\nwhether $p$, for any $p$ in a certain field, I know (a) that whatever\njudgment I make will usually be wrong, and (b) if I conclude my\ndeliberations without making a judgment, then $p$ is usually true. If we\nalso assume JSE, then it follows there is no way for me to end\ndeliberation. If I make a judgment, I will have to retract it because of\n(a). But if I think of ending deliberation, then because of (b) I'll\nhave excellent evidence that $p$, and it would be irrational to ignore\nthis evidence. (Nicholas @Silins2005 has used the idea that failing to\nmake a judgment can be irrational in a number of places, and those\narguments motivated this example.)\n\nThis is puzzling, but not obviously false. It is plausible that there\nare some epistemic dilemmas, where any position an agent takes is going\nto be irrational. (By that, I mean it is at least as plausible that\nthere are epistemic dilemmas as that there are moral dilemmas, and I\nthink the plausibility of moral dilemmas is reasonably high.) That a\ncase like the one I've described in the previous paragraph is a dilemma\nis perhaps odd, but no reason to reject the theory.\n\nThe real problem, I think, for the ratifiability proposal is that there\nare cases where unratifiable judgments are clearly preferable to\nratifiable judgments. Assume that I'm a reasonably good judge of what's\nlikely to happen in baseball games, but I'm a little over-confident. And\nI know I'm over-confident. So the rational credence, given some\nevidence, is usually a little closer to $\\frac{1}{2}$ than I admit.\nAt risk of being arbitrarily precise, let's say that if $p$ concerns a\nbaseball game, and my credence in $p$ is $x$, the rational credence in\n$p$, call it $y$, for someone with no other information than this is\ngiven by:\n\n$$y = x + \\frac{sin(2\\pi x)}{50}$$\n\nTo give you a graphical sense of how that looks, the dark line in this\ngraph is $y$, and the lighter diagonal line is $y = x$.\n\n::: {.center}\n![image](sinewave.JPG)\n:::\n\nNote that the two lines intersect at three points:\n$(0, 0), (\\frac{1}{2}, \\frac{1}{2})$ and $(1, 1)$. So if my\ncredence in $p$ is either 0, $\\frac{1}{2}$ or 1, then my judgment is\nratifiable. Otherwise, it is not. So the ratifiability constraint says\nthat for any $p$ about a baseball game, my credence in $p$ should be\neither 0, $\\frac{1}{2}$ or 1. But that's crazy. It's easy to imagine\nthat I know (a) that in a particular game, the home team is much\nstronger than the away team, (b) that the stronger team usually, but far\nfrom always, wins baseball games, and (c) I'm systematically a little\nover-confident about my judgments about baseball games, in the way just\ndescribed. In such a case, my credence that the home team will win\nshould be high, but less than 1. That's just what the ratificationist\ndenies is possible.\n\nThis kind of case proves that it isn't always rational to have\nratifiable credences. It would take us too far afield to discuss this in\ndetail, but it is interesting to think about the comparison between the\nkind of case I just discussed, and the objections to backwards induction\nreasoning in decision problems that have been made by Pettit and Sugden\n[-@Pettit1989-PETTBI], and by Stalnaker\n[-@Stalnaker1996; -@Stalnaker1998; -@Stalnaker1999]. The backwards\ninduction reasoning they criticise is, I think, a development of the\nidea that decisions should be ratifiable. And the clearest examples of\nwhen that reasoning fails concern cases where there is a unique\nratifiable decision, and it is guaranteed to be one of the worst\npossible outcomes. The example I described in the last few paragraphs\nhas, quite intentionally, a similar structure.\n\nThe upshot of all this is that I think these regress arguments work.\nThey aren't, I think, directly an argument against EW. What they are is\nan argument against the most promising way the EW theorist has for\narguing that the table I started with misstates $S$'s epistemic\nsituation. Given that the regress argument against JSE works though, I\ndon't see any way of rescuing EW from this argument.\n\n[^1]: Though if EW is correct, shouldn't the scholarly journals be full\n    of just this information?\n\n[^2]: This is obviously somewhat of an idealisation, since there won't\n    usually be a unique precise rational response to the evidence. But I\n    don't think this idealisation hurts the argument to follow. I should\n    note that the evidence here *excludes* their statements of their\n    credences, so I really mean the evidence that they brought to bear\n    on the debate over whether $p$.\n\n[^3]: Added in 2010: I still think there's a dilemma here for EW, but\n    I'm less convinced than I used to be that **MJ** is correct.\n\n[^4]: By *ex ante* and *ex post* I mean before and after we learn about\n    $S$ and $T$'s use of $E$ to make a judgment about $p$. I think that\n    should change how reliable we take $S$ and $T$ to be, and that this\n    should matter to what use, if any, we put their judgments, but it is\n    crucial to EW that we ignore this evidence. Or, at least, it is\n    crucial to EW that $S$ and $T$ ignore this evidence.\n\n[^5]: My explanation is that evidence screens any judgments made on the\n    basis of that evidence, in the sense of screening to be described\n    below.\n\n[^6]: In general I'm sceptical of always treating evidence\n    probabilistically. Some of my reasons for scepticism are in\n    @Weatherson2007.\n\n[^7]: Branden Fitelson pointed out to me that the probabilistic version\n    entails one extra condition, namely that $\\neg B \\wedge C$ is no\n    worse evidence for $A$ than $C$ is. But I think that extra condition\n    is irrelevant to disagreement debates, so I'm leaving it out.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
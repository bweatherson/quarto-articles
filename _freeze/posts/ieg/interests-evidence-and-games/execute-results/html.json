{
  "hash": "02681defd4f3fc478a78e86b67038063",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Interests, Evidence and Games\"\ndescription: |\n Pragmatic encroachment theories have a problem with evidence. On the one hand, the arguments that knowledge is interest-relative look like they will generalise to show that evidence too is interest-relative. On the other hand, our best story of how interests affect knowledge presupposes an interest-invariant notion of evidence. The aim of this paper is to sketch a theory of evidence that is interest-relative, but which allows that 'best story' to go through with minimal changes. The core idea is that the evidence someone has is just what evidence a radical interpreter says they have. And a radical interpreter is playing a kind of game with the person they are interpreting. The cases that pose problems for pragmatic encroachment theorists generate fascinating games between the interpreter and the interpretee. They are games with multiple equilibria. To resolve them we need to detour into the theory of equilibrium selection. I'll argue that the theory we need is the theory of **risk-dominant equilibria**. That theory will tell us how the interpreter will play the game, which in turn will tell us what evidence the person has. The evidence will be interest-relative, because what the equilibrium of the game is will be interest-relative. But it will not undermine the story we tell about how interests usually affect knowledge.\ndate: June 29 2018\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ndoi: \"10.1017/epi.2018.26\"\ncategories:\n  - epistemology\n  - interest-relativity\n  - games and decisions\ncitation_url: https://doi.org/10.1017/epi.2018.26\njournal:\n    title: \"Episteme\"\n    publisher: \"Cambridge University Press\"\nvolume: 15\nnumber: 3\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: stag.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    number_sections: true\n---\n\n\n\nPragmatic encroachment theories have a problem with evidence. On the one\nhand, the arguments that knowledge is interest-relative look like they\nwill generalise to show that evidence too is interest-relative. On the\nother hand, our best story of how interests affect knowledge presupposes\nan interest-invariant notion of evidence.\n\n<aside>\nPublished in _Episteme_ 15: 329-344.\n\nImage by [Paul Wordingham](https://www.flickr.com/photos/13422316@N00/) via [Creative Commons](https://search.creativecommons.org/photos/457692bc-4ca5-4f3d-a964-d0840e034e3a).\n</aside>\n\nThe aim of this paper is to sketch a theory of evidence that is\ninterest-relative, but which allows that 'best story' to go through with\nminimal changes. The core idea is that the evidence someone has is just\nwhat evidence a radical interpreter says they have. And a radical\ninterpreter is playing a kind of game with the person they are\ninterpreting. The cases that pose problems for pragmatic encroachment\ntheorists generate fascinating games between the interpreter and the\ninterpretee. They are games with multiple equilibria. To resolve them we\nneed to detour into the theory of equilibrium selection. I'll argue that\nthe theory we need is the theory of **risk-dominant equilibria**. That\ntheory will tell us how the interpreter will play the game, which in\nturn will tell us what evidence the person has. The evidence will be\ninterest-relative, because what the equilibrium of the game is will be\ninterest-relative. But it will not undermine the story we tell about how\ninterests usually affect knowledge.\n\n### Encroachment, Reduction and Explanation {#encroachmentreductionandexplanation}\n\nI will start with an argument for a familiar disjunctive conclusion:\neither knowledge is interest-relative, or scepticism is true. The\nargument will resemble arguments to the same disjunctive conclusion in\n@Hawthorne2004 and @FantlMcGrath2009. Indeed, it is inspired by those\ndiscussions. But it uses less controversial premises than previous\nversions.\n\nThe argument starts by considering a game, one I'll call the red-blue\ngame. Here are the rules of the game.\n\n1.  Two sentences will be written on the board, one in red, one in blue.\n\n2.  The player will make two choices.\n\n3.  First, they will pick a colour, red or blue.\n\n4.  Second, they say whether the sentence in that colour is true or\n    false.\n\n5.  If they are right, they win. If not, they lose.\n\n6.  If they win, they get \\$50, and if they lose, they get nothing.\n\nOur player is Parveen. She is an epistemologist who works on pragmatic\nencroachment, and (as will become important in a minute), she has\nfrequently cited both *Knowledge and Lotteries*  [@Hawthorne2004], and\n*Knowledge and Practical Interests*  [@Stanley2005]. She knows the rules\nof the game, and no other relevant facts about the game. When the game\nstarts, the following two sentences are written on the board, the first\nin red, the second in blue.\n\n1.  Two plus two equals four.\n\n2.  *Knowledge and Lotteries* was published before *Knowledge and\n    Practical Interests*.\n\nIntuitively, there is a unique rational play in this game: Red-True.\nThat is, Parveen announces that she will evaluate the truth value of the\nred sentence, and then announce that it's true. That's a sure \\$50.\n\nOn the other hand, in normal circumstances, we would say that Parveen\ndoes know that *Knowledge and Lotteries* was published before *Knowledge\nand Practical Interests*. After all, she has looked up their publication\ndates many times in checking over her papers.\n\nThere is a puzzle in reconciling these intuitions. The pragmatic\nencroachment theorist has a solution to these puzzles. In normal\ncircumstances, Parveen does know that *Knowledge and Lotteries* was\npublished before *Knowledge and Practical Interests*. But these are not\nnormal circumstances. Right now, it matters whether her reason to\nbelieve that *Knowledge and Lotteries* was published before *Knowledge\nand Practical Interests* is as strong as her reason to believe that two\nplus two equals four. And (unless something very weird is happening),\nthat isn't true for Parveen. So she knows that red-true will win, she\ndoesn't know any other play will win, so she should play Red-True.\n\nIf we reject pragmatic encroachment, and we are not sceptics, we should\nsay that Parveen does know that *Knowledge and Lotteries* was published\nbefore *Knowledge and Practical Interests*. And then it is a mystery why\nplaying Red-True is more rational than playing Blue-True. After all,\nParveen knows the rules of the game, and she knows (by hypothesis) the\nblue sentence is true, so if she can do even basic logical reasoning in\na knowledge preserving way, she knows she will get as good a result as\npossible by playing Blue-True. So it is a bit of a mystery why it would\nbe anything other than maximally rational to play Blue-True.\n\nOne way we might try to resolve this mystery is by saying that although\nParveen knows that Blue-True will win \\$50, she super-knows that\nRed-True will win \\$50. What do we mean here by *super knowledge*? Think\nof this as a placeholder for certainty, or knowledge that one knows, or\nanything other epistemic state that you think might be relevant to her\npractical decision making. Perhaps the fact that she super-knows what\ntwo plus two is, but doesn't super-know when the epistemology books were\npublished, could be the explanation for why Red-True is the unique\nrational play.[^1]\n\nBut no such explanation can work, because Parveen doesn't super-know\nthat playing Red-True will win \\$50. She super-knows that two plus two\nis four. But we have not assumed that she super-knows the rules of the\ngame. So she doesn't super-know that Red-True will win, she just knows\nit. And she also, by hypothesis, knows that Blue-True will win. So\nlooking at any kind of super-knowledge can't break the intuitive\nasymmetry between Red-True and Blue-True.\n\nPut another way, if Parveen knows that *Knowledge and Lotteries* was\npublished before *Knowledge and Practical Interests*, then she knows\nthat she is playing the following game.\n\n1.  Two sentences will be written on the board, one in red, one in blue.\n\n2.  The player chooses to play either Blue-True, Blue-False, Red-True,\n    or Red-False.\n\n3.  If they play Blue-True, they win \\$50.\n\n4.  If they play Blue-False, they win nothing.\n\n5.  If they play Red-True, they win \\$50 if the red sentence is true,\n    and nothing otherwise.\n\n6.  If they play Red-False, they win \\$50 if the red sentence is false,\n    and nothing otherwise.\n\nAnd is is rational to play Blue-True in that game. (It might also be\nrational to pay Red-True depending on what the red sentence is, but it\nis always rational to play Blue-True.) Yet it is not rational to play\nBlue-True in the original game. So Parveen does not know, when she plays\nthe original game, that *Knowledge and Lotteries* was published before\n*Knowledge and Practical Interests*.\n\nSo to avoid pragmatic encroachment here we must deny that Parveen ever\nknew that *Knowledge and Lotteries* was published before *Knowledge and\nPractical Interests*. On its own, that's not a sceptical conclusion:\nlots of people don't know that. But once we go down that path, it looks\nlike not much knowledge will be left. After all, we can repeat the game\nwith any number of different things in the place of the blue sentence.\nIf we adopt the constraint that Parveen only knows *p*, right now, if it\nis rationally permissible for her to play Blue-True when *p* is the blue\nsentence, no matter what the red sentence is, then either we have to say\nvery unintuitive things about rational plays of the game, or we have to\nsay she knows very little.\n\nSo we've got the conclusion that either pragmatic encroachment is true,\nor scepticism is true. Since I'm not a sceptic, I'm happy to conclude\nthat pragmatic encroachment is true. But note that we've done this\nwithout any reference to high stakes situations. The stakes in Parveen's\ngame are just \\$50. That's not nothing, but it's not 'high stakes' in\nthe way that phrase is normally used.\n\nThe version of pragmatic encroachment we get is that what matters for\nknowledge are not the stakes involved in any bet on *p*, but the\nodds.[^2] Parveen loses knowledge because she is being asked, in effect,\nto make a super long odds bet on a fact about publication schedules. She\nis in no position to rationally make a bet at those odds. So she doesn't\nknow the fact about publication schedules.\n\nAnd that's the general principle: agents only know a proposition if they\nare in a position to rationally bet on that proposition at the odds\ncurrently being offered to them. In practice, high stakes situations\ntend to feature bets at long odds, so in practice much knowledge\ndissipates in high stakes cases. But the explanation of the dissipation\nis the odds the agent faces, not the stakes.\n\nMore precisely, I endorse these principles as constraints on knowledge:\n\n-   If the agent knows that *p*, then for any question they have an\n    interest in, the answer to that question is identical to the answer\n    to that question conditional on *p*.\n\n-   When an agent is considering the choice between two options, the\n    question of which option has a higher expected utility given their\n    evidence is a question they have an interest in.\n\nThose principles are meant to not merely be extensionally adequate. They\nare meant to explain why agents lose knowledge when considering some\nsets of options, like in the Red-Blue game. In some sense, they are\nmeant to be part of reductive explanations. These reductive explanations\ntake as primitive inputs facts about the agent's evidence, and facts\nabout evidential probability. I'm going to set aside worries about the\nmetaphysics of evidential probability, and just focus on evidence.\nBecause it turns out that there is a real problem in getting a plausible\ntheory of evidence that can function as an input to that reductive\nexplanation.\n\n### The Problems with Evidence {#theproblemswithevidence}\n\nGo back to the red-blue game. Consider a version of the game where:\n\n-   The red sentence is that two plus two equals four.\n\n-   The blue sentence is something that, if known, would be part of the\n    agent's evidence.\n\nI'm going to argue that there are cases where the only rational play is\nRed-True, but the blue sentence is something we want to say that,\nordinarily, the subject knows. And I'll argue that this is a problem for\nthe kind of reductive explanation I just sketched. If pragmatic effects\nmatter to what the evidence is, we can't take the evidence as a fixed\ninput into an explanation of how and when pragmatic effects matter.\n\nLet's have Parveen play the game again. She's going to be playing the\ngame in a restaurant, one in Ann Arbor where she lives. Just before the\ngame starts, she notices an old friend, Rahul, across the room. Rahul is\nsomeone she knows well, and can ordinarily recognise, but she had no\nidea he was in town. She thought Rahul was living in Italy. Still, we\nwould ordinarily say that she now knows Rahul is in the restaurant;\nindeed that he is in the restaurant. It would be perfectly acceptable\nfor her to say to someone else, \"I saw Rahul here\", for example. Now the\ngame starts.\n\n-   The red sentence is *Two plus two equals four*.\n\n-   The blue sentence is *Rahul is in this restaurant*.\n\nNow we have a problem. On the one hand, there is only one rational play\nhere: Red-True. If you haven't seen someone for a long time, then you\ncan't be completely certain it's them when you spot them across a\nrestaurant. It would be foolish to be as confident that it's Rahul as\nthat two and two make four. It looks like this is a case where pragmatic\neffects defeat knowledge.\n\nOn the other hand, our story for why Parveen loses knowledge here has\nrun into problems. I wanted to tell a story roughly like the following.\nShe can't play Blue-True when the probability of the blue sentence,\ngiven her evidence, is less than the probability of the red sentence,\ngiven her evidence. That explanation can only go through if the blue\nsentence is itself not part of her evidence, since the probability of\nanything given itself is one. So we need a story about how it is that it\nis not part of Parveen's evidence that Rahul is not in the restaurant.\n\nThat story can't be the one that presupposes facts about what is in\nParveen's evidence. So it can't use facts about the probability of some\nproposition given her evidence; at least not in any simple way. If we\ncan independently identify Parveen's evidence, then we can go back to\nusing evidential probability. But until we've done that, we're stuck.\n\nThere are two options here that seem possible for the pragmatic\nencroachment theorist, but not particularly attractive.\n\nOne is to say that propositions like *Rahul is in this restaurant* are\nnever part of Parveen's evidence. Perhaps her evidence just consists of\nthings like *I am being appeared to Rahul-like*. Such an approach is\nproblematic for two reasons. The first is that it is subject to all the\nusual objections to psychological theories of evidence\n [@Williamson2007-WILTPO-17]. The second is that we can re-run the\nargument with the blue sentence being some claim about Parveen's\npsychological state, and still get the result that the only rational\nplay is Red-True. A retreat to a psychological conception of evidence\nwill only help with this problem if agents are infallible judges of\ntheir own psychological states, and that is not in general true\n [@Schwitzgebel2008].\n\nAnother option is to deny that a reductive explanation is needed here.\nPerhaps pragmatic effects, like the particular sentences that are chosen\nfor this instance of the Red-Blue game, mean that Parveen's evidence no\nlonger includes facts about Rahul, but this isn't something we can give\na reductive account of. We shouldn't assume that everything will have a\nsimple reductive explanation, so this isn't so bad in theory. The\nproblem in practice is that without a reductive explanation, we don't\nhave a predictive theory of when pragmatic effects matter. And that\nseems to be a bad thing. For instance, the following theory is\ncompletely consistent with Parveen's case as described.\n\n1.  E=K; i.e., one's evidence is all and only what one knows.\n\n2.  Someone does not know *p* if the evidential probability of *p* is\n    not close enough to one for current purposes.\n\n3.  Since it is part of Parveen's evidence that Rahul is in the\n    restaurant, the probability that he is there is one, so it is close\n    enough to one for current purposes.\n\n4.  So this is not a case where pragmatic effects change what she knows.\n\nThat theory seems to me to be badly mistaken, since it goes on to\npredict that it is rationally permissible to play Blue-True. But we need\na pragmatic account that says that it is mistaken, and says something\nabout which alternative situations would not threaten Parveen's\nknowledge. We don't yet, as far as I can see, have such an account. The\naim of the rest of this paper is to provide one.[^3]\n\n### A Simple, but Unsatisfying, Solution {#asimplebutunsatisfyingsolution}\n\nLet's take a step back and look at the puzzle more abstractly. We have\nan agent *S*, who has some option *O*, and it really matters whether or\nnot the value of *O*, i.e., $V(O)$ is at least $x$. It is\nuncontroversial that the agent's evidence includes some background $K$,\nand controversial whether it includes some contested proposition $p$. It\nis also uncontroversial that $V(O | p) \\geq x$, and we're assuming that\nfor any proposition $q$ that is in the agent's evidence,\n$V(O | q) = V(O)$. That is, we're assuming the relevant values are\nconditional on evidence. We can capture that last assumption with one\nbig assumption that probably isn't true, but is a harmless idealisation\nfor these purposes. Say there is a prior value function $V^-$, with a\nsimilar metaphysical status to the mythical, mystical prior probability\nfunction. Then for any choice $C$, $V(C) = V^-(C | E)$, where $E$ is the\nevidence the agent has.\n\nNow we're in a position to state a simple, but unsatisfying, solution.\nLet $p$ be the proposition that the agent might or might not know, and\nthe question of whether $V(O) \\geq x$ be the only salient one that $p$\nis relevant to. Then the agent knows $p$ only if the following is true:\n\n> $\\frac{V^-(O | K) + V^-(O | K \\wedge p)}{2} \\geq x$\n\nThat is, we work out the value of $O$ with and without the evidence $p$,\nand if the average is greater than $x$, good enough!\n\nThat solves the problem of Parveen and Rahul. Parveen's evidence may or\nmay not include that Rahul is in the restaurant. If it does, then\nBlue-True has a value of \\$50. If it does not, then Blue-True's value is\nsomewhat lower. Even if the evidence includes that someone who looks a\nlot like Rahul is in the restaurant, the value of Blue-True might only\nbe \\$45. Averaging them out, the value is less than \\$50. But you'd only\nplay Blue-True if it was worthwhile it play it instead of Red-True,\nwhich is worth \\$50. So you shouldn't play Blue-True.\n\nGreat! Well, great except for two monumental problems. The first problem\nis that what we've said here really only helps with very simple cases,\nwhere there is a single decision problem that a single contested\nproposition is relevant to. We need some way to generalise the case to\nless constrained situations. The second (and bigger) problem is that the\nsolution is completely ad hoc. Why should we use the arithmetic mean of\nthese two things rather than any other formula that would have implied\nthe intuitively correct result in the Parveen-Rahul case? Pragmatic\nencroachment starts with a very elegant, very intuitive, principle: you\nonly know the things you can reasonable take to be settled for the\npurposes of current deliberation. And that deliberation should be driven\nby the aim of maximising expected utility. It does not look like any\nsuch elegant, intuitive, principles will lead to some theorem about\naveraging out the value of an option with and without new evidence.\n\nHappily, the two problems have a common solution. But the solution\nrequires a detour into some technical work. It's time for some game\ntheory.\n\n### Gamifying the Problem {#gamifyingtheproblem}\n\nWe can usefully think of some philosophical problems as games, and hence\nsubjects for study using game theoretic techniques. This is especially\nwhen the problems involve interactions of rational agents. Here, for\nexample, is the game table for Newcomb's problem, with the human who is\nusually the focus of the problem as Row, and the demon as Column.[^4]\n\n::: {.center}\n                    Predict 1 Box   Predict 2 Boxes\n  ---------------- --------------- -----------------\n      Choose 1 Box     1000, 1            0,0\n    Choose 2 Boxes     1001, 0           1, 1\n:::\n\nThis game has a unique Nash equilbrium; the bottom right corner.[^5] And\nthat's one way of motivating the view that (a) the game is possible, and\n(b) the rational move for the human is to choose two boxes.\n\nLet's look at a more complicated game. I'll call it The Interpretation\nGame. The game has two players. Just like in Newcomb's problem, one of\nthem is a human, the other is a philosophical invention. But in this\ncase the invention is not a demon, but The Radical Interpreter.[^6] To\nknow the payouts for the players, we need to know their value function.\nMore colloquially, we need to know their goals.\n\n-   The Radical Interpreter assigns mental states to Human in such a way\n    as to predict Human's actions given Human rationality. We'll assume\n    here that evidence is a mental state, so saying what evidence Human\n    has is among Radical Interpreter's tasks. (Indeed, in the game play\n    to come, it will be their primary task.)\n\n-   Human acts so as to maximise the expected utility of their action,\n    conditional on the evidence that they have. Human doesn't always\n    know what evidence they have; it depends on what The Radical\n    Interpreter says.\n\nThe result is that the game is a coordination game. The Radical\nInterpreter wants to assign evidence in a way that predicts rational\nHuman action, and Human wants to do what's rational given that\nassignment of evidence. Coordination games typically have multiple\nequilibria, and this one is no exception.\n\nLet's make all that (marginally) more concrete. Human is offered a bet\non *p*. If the bet wins, it wins 1 util; if the bet loses, it loses 100\nutils. Human's only choice is to Take or Decline the bet. The\nproposition *p*, the subject of the bet, is like the claim that Rahul is\nin the restaurant. It is something that is arguably part of Human's\nevidence. Unfortunately, it is also arguable that it is not part of\nHuman's evidence. We will let $K$ be the rest of Human's evidence (apart\nfrom $p$, and things entailed by $K \\cup \\{p\\}$), and stipulate that\n$\\Pr(p | K) = 0.9$. Each party now faces a choice.\n\n-   The Radical Interpreter has to choose whether *p* is part of Human's\n    evidence or not.\n\n-   Human has to decide whether to Take or Decline the bet.\n\nThe Radical Interpreter achieves their goal if human takes the bet iff\n*p* is part of their evidence. If *p* is part of the evidence, then The\nRadical Interpreter thinks that the bet has positive expected utility,\nso Human will take it. And if *p* is not part of the evidence, then The\nRadical Interpreter thinks that the bet has negative expected utility,\nso Human will decline it. Either way, The Radical Interpreter wants\nHuman's action to coordinate with theirs. And Human, of course, wants to\nmaximise expected utility. So we get the following table for the game.\n\n::: {.center}\n                     $p \\in E$   $p \\notin E$\n  ----------------- ----------- --------------\n       Take the bet    1, 1        -9.1, 0\n    Decline the bet    0, 0          0, 1\n:::\n\nWe have, in effect, already covered The Radical Interpreter's payouts.\nThey win in the top-left and lower-right quadrants, and lose otherwise.\nHuman's payouts are only a little trickier. In the bottom row, they are\nguaranteed 0, since the bet is declined. In the top-left, the bet is a\nsure winner; their evidence entails it wins. So they get a payout of 1.\nIn the top-right, the bet wins with probability 0.9, so the expected\nreturn[^7] of taking it is $1 \\times 0.9 - 100 \\times 0.1 = -9.1$.\n\nThere are two Nash equilibria for the game - I've bolded them below.\n\n::: {.center}\n                     $p \\in E$   $p \\notin E$\n  ----------------- ----------- --------------\n       Take the bet  **1, 1**      -9.1, 0\n    Decline the bet    0, 0        **0, 1**\n:::\n\nThe mathematical result that there are two equilibria to this game\nshould not come as a surprise. In discussing games like this earlier, we\nsaid that general principles connecting evidence, knowledge and action\nare not predictive; they are consistent both with *p* being part of the\nevidence, and with it not being part of the evidence. The general\nprinciples we had stated rule out, in effect, non-equilibrium solutions\nto games like this one. But they are not predictive in cases where there\nare multiple equilibria.\n\nTo make more progress, we need to turn to more contested areas of game\ntheory. In particular, we need to look at some work on equilibrium\nchoice. We'll introduce this material via a game that is inspired by an\nexample of Rousseau's.\n\n### Equilibrium Selection Principles {#equilibriumselectionprinciples}\n\nAt an almost maximal level of abstraction, a two player, two option each\ngame looks like this.\n\n::: {.center}\n                $a$                  $b$\n  ----- -------------------- --------------------\n    $A$  $r_{11}$, $c_{11}$   $r_{12}$, $c_{12}$\n    $B$  $r_{21}$, $c_{21}$   $r_{22}$, $c_{22}$\n:::\n\nWe're going to focus on games that have the following eight properties:\n\n-   $r_{11} > r_{21}$\n\n-   $r_{22} > r_{12}$\n\n-   $c_{11} > c_{12}$\n\n-   $c_{22} > c_{21}$\n\n-   $r_{11} > r_{22}$\n\n-   $c_{11} \\geq c_{22}$\n\n-   $\\frac{r_{21}+r_{22}}{2} > \\frac{r_{11}+r_{12}}{2}$\n\n-   $\\frac{c_{12}+c_{22}}{2} \\geq \\frac{c_{11}+c_{21}}{2}$\n\nThe first four clauses say that the game has two (strict) Nash\nequilibria: $Aa$ and $Bb$. The fifth and sixth clauses say that the $Aa$\nequilibria is **Pareto-optimal**: no one prefers the other equilibria to\nit. In fact it says something a bit stronger: one of the players\nstrictly prefers the $Aa$ equilibria, and the other player does not\nprefer $Bb$. The seventh and eighth clauses say that the $Bb$ equilibria\nis **risk-optimal**. Risk-optimality is a somewhat complicated notion in\ngeneral; see @HarsanyiSelten1988 for more details. But for our purposes,\nwe can focus on a simple characterisation of it. Neither player would\nprefer playing $A$/$a$ to playing $B$/$b$ if they thought it was a coin\nflip which equilibrium the other player was aiming for.\n\nI'm going to offer an argument from Hans Carlsson and Eric van Damme\n[-@CarlssonVanDamme1993] for the idea that in these games, rational\nplayers will end up at $Bb$. The game that Human and The Radical\nInterpreter are playing fits these eight conditions, and The Radical\nInterpreter is perfectly rational, so this will imply that in that game,\nThe Radical Interpreter will say that $p \\notin E$, which is what we\naimed to show.\n\nGames satisfying these eight inequalities are sometimes called *Stag\nHunt* games. There is some flexibility, and some vagueness, in which of\nthe eight inequalities need to be strict, but that level of detail isn't\nimportant here. The name comes from a thought experiment in Rousseau's\n*Discourse on Inequality*.\n\n> [T]hey were perfect strangers to foresight, and were so far from\n> troubling themselves about the distant future, that they hardly\n> thought of the morrow. If a deer was to be taken, every one saw that,\n> in order to succeed, he must abide faithfully by his post: but if a\n> hare happened to come within the reach of any one of them, it is not\n> to be doubted that he pursued it without scruple, and, having seized\n> his prey, cared very little, if by so doing he caused his companions\n> to miss theirs.  [@Rousseau1913 209--10]\n\nIt is rather interesting to think through which real-life situations are\nbest modeled as Stag Hunts, especially in situations where people have\nthought that the right model was a version of Prisoners' Dilemma. This\nkind of thought is one way in to appreciating the virtues of Rousseau's\npolitical outlook, and especially the idea that social coordination\nmight not require anything like the heavy regulatory presence that, say,\nHobbes thought was needed. But that's a story for another day. What\nwe're going to be interested in is why Rousseau was right to think that\na 'stranger to foresight', who is just focussing on this game, should\ntake the rabbit.\n\nTo make matters a little easier, we'll focus on a very particular\ninstance of Stag Hunt, as shown here. (From here I'm following Carlsson\nand van Damme very closely; this is their example, with just the\nlabelling slightly altered.)\n\n::: {.center}\n         $a$    $b$\n  ----- ------ ------\n    $A$  4, 4   0, 3\n    $B$  3, 0   3, 3\n:::\n\nAt first glance it might seem like $Aa$ is the right choice; it produces\nthe best outcome. This isn't like Prisoners Dilemma, where the best\ncollective outcome is dominated. In fact $Aa$ is the best outcome for\neach individual. But it is risky, and Carlsson and van Damme show how to\nturn that risk into an argument for choosing $Bb$.\n\nEmbed this game in what they call a *global game*. We'll start the game\nwith each player knowing just that they will play a game with the\nfollowing payout table, with $x$ to be selected at random from a flat\ndistribution over $[-1, 5]$.\n\n::: {.center}\n         $a$    $b$\n  ----- ------ ------\n    $A$  4, 4   0, x\n    $B$  x, 0   x, x\n:::\n\nBefore they play the game, each player will get a noisy signal about the\nvalue of $x$. There will be signals $s_R$ and $s_C$ chosen\n(independently) from a flat distribution over $[x - 0.25, x + 0.25]$,\nand shown to Row and Column respectively. So each player will know the\nvalue of $x$ to within $\\frac{1}{4}$, and know that the other player\nknows it to within $\\frac{1}{4}$ as well. But this is a margin of error\nmodel, and in those models there is very little that is common\nknowledge. That, they argue, makes a huge difference.\n\nIn particular, they prove that iterated deletion of strictly dominated\nstrategies (almost) removes all but one strategy pair.[^8] Each player\nwill play $A$/$a$ if the signal is greater than 2, and $B$/$b$\notherwise.[^9] Surprisingly, this shows that players should play the\nrisk-optimal strategy even when they know the other strategy is\nPareto-optimal. When a player gets a signal in $(2, 3.75)$, then they\nknow that $x < 4$, so $Bb$ is the Pareto-optimal equilibrium. But the\nlogic of the global game suggests the risk-dominant equilibrium is what\nto play.\n\nCarlsson and van Damme go on to show that many of the details of this\ncase don't matter. As long as (a) there is a margin of error in each\nside's estimation of the payoffs, and (b) every choice is a dominant\noption in some version of the global game, then iterated deletion of\nstrongly dominant strategies will lead to each player making the\nrisk-dominant choice.\n\nI conclude from that that risk-dominant choices are rational in these\ngames. There is a limit assumption involved here; what's true for games\nwith arbitrarily small margins of error is true for games with no margin\nof error. (We'll come back to that assumption below.) And since The\nRadical Interpreter is rational, they will play the strategy that is not\neliminated by deleting dominant strategies. That is, they will play the\nrisk-dominant strategy.\n\nIn game with Human, the rational (i.e., risk-dominant) strategy for The\nRadical Interpreter is to say that $p \\notin E$. And in the case of\nParveen and Rahul, rational (i.e., risk-dominant) strategy for The\nRadical Interpreter is to say that it is not part of Parveen's evidence\nthat Rahul is in the restaurant. And this is an interest-relative theory\nof evidence; had Parveen been playing a different game, The Radical\nInterpreter would have said that it is part of Parveen's evidence that\nRahul was in the restaurant.\n\nAnd from this point we can say all the things we wanted to say about the\ncase. If it is part of Parveen's evidence that Rahul is in the\nrestaurant, then she knows this. Conversely, if she knows it, then The\nRadical Interpreter would have said it is part of her evidence, so it is\npart of her evidence. Parveen will perform the action that maximises\nexpected utility given her evidence. And she will lose knowledge when\nthat disposition makes her do things that would be known to be\nsub-optimal if she didn't lose knowledge.\n\nIn short, this model gives us a way to keep what was good about the\npragmatic encroachment theory, while also allowing that evidence can be\ninterest-relative. It does require a slightly more complex theory of\nrationality than we had previously used. Rather than just say that\nagents maximise evidential expected utility, we have to say that they\nplay risk-dominant strategies in coordination games. But it turns out\nthat this is little more than saying that they maximise evidential\nexpected utility, and they expect others (at least perfectly rational\nabstract others) to do the same, and they expect those others to expect\nthey will maximise expected utility, and so on.\n\n### Objections and Replies {#objectionsandreplies}\n\nWe'll end the body of the paper with some objections that might be\nraised to this model. And then the appendix will contain proofs of a\ncouple of the formal claims.\n\n*Objection*: The formal result of the previous section only goes through\nif we assume that the agents do not know precisely what the payoffs are\nin the game. We shouldn't assume that what holds for arbitrarily small\nmargins of error will hold in the limit, i.e., when they do know the\npayoffs.\n\n*Reply*: If pushed, I would defend the use limit assumptions like this\nto resolve hard cases like Stag Hunt. But I don't need that assumption\nhere, What we really need is that Parveen doesn't know precisely the\nprobability of Rahul being in the restaurant given the rest of her\nevidence. Given that evidence is not luminous, as @Williamson2000 shows,\nthis is a reasonable assumption. So the margin of error assumption that\nCarlsson and van Damme make is not, in our case, an assumption that\nmerely makes the math easier; it is built into the case.\n\n*Objection*: Even if Parveen doesn't know the payoffs precisely, The\nRadical Interpreter does. They are an idealisation, so they can be taken\nto be ideal.\n\n*Reply*: It turns out that Carlsson and van Damme's result doesn't\nrequire that both parties are ignorant of the precise values of the\npayoffs. As long as one party doesn't know the exact value of the\npayoff, the argument goes through. I prove this in Appendix Two.\n\n*Objection*: The formal argument requires that in the 'global game'\nthere are values for $x$ that make $A$ the dominant choice. These cases\nserve as a base step for an inductive argument that follows. But in\nParveen's case, there is no such setting for $x$, so the inductive\nargument can't get going.\n\n*Reply*: What matters is that there are values of $x$ such that $A$ is\nthe strictly dominant choice, and Human (or Parveen) doesn't know that\nthey know that they know, etc., that those values are not actual. And\nthat's true in our case. For all Human (or Parveen) knows that they know\nthat they know that they know..., the proposition in question is not\npart of their evidence under a maximally expansive verdict on The\nRadical Interpreter's part. So the relevant cases are there in the\nmodel, even if for some high value of $n$ they are known$^n$ not to\nobtain.\n\n*Objection*: This model is much more complex than the simple motivation\nfor pragmatic encroachment.\n\n*Reply*: Sadly, this is true. I would like to have a simpler model, but\nI don't know how to create one. The argument I gave earlier that our\nsimple principles underdetermine what to say in cases like Parveen and\nRahul's seems fairly compelling. So more complexity will be needed, one\nway or another. I think paying this price in complexity is worth it\noverall, but I can see how some people might think otherwise.\n\n*Objection*: Change the case involving Human so that the bet loses 15\nutils if *p* is false, rather than 100. Now the risk-dominant\nequilibrium is that Human takes the bet, and The Radical Interpreter\nsays that *p* is part of Human's evidence. But note that if it was\nclearly true that *p* was not part of Human's evidence, then this would\nstill be too risky a situation for them to know *p*. So whether it is\npossible that *p* is part of Human's evidence matters.\n\n*Reply*: This is all true, and it shows that the view I'm putting\nforward is incompatible with some programs in epistemology. In\nparticular, it is incompatible with E=K, since the what it takes to be\nevidence on this story is slightly different from what it takes to be\nknowledge. I don't think E=K is so intuitively obvious that this refutes\nthe theory, but it is potentially a cost that I have to give it up.\n\n*Objection*: Carlsson and van Damme discuss one kind of global game. But\nthere are other global games that have different equilibria. For\ninstance, changing the method by which the noisy signal is selected\nwould change the equilibrium of the global game. So this kind of\nargument can't show that the risk-dominant equilibrium is the one true\nsolution.\n\n*Reply*: This is somewhat true. There are other ways of embedding the\ngame involving Human and The Radical Interpreter in global games that\nlead to different outcomes. They are usually somewhat artificial; e.g.,\nby having the signal be systematically biased in one way. But what\nreally matters is the game where the error in Human's knowledge of the\npayoffs is determined by their actual epistemic limitations. I think\nthat will lead to something like the model we have here. But it is\npossible that the final result will differ a bit from what I have here,\nor (more likely) have some indeterminacy about just how interests\ninteract with evidence and knowledge. The precise details are ultimately\nless important to me than whether we can provide a motivated story of\nhow interests affect knowledge and evidence that does not presuppose we\nknow what the agent's evidence is. And the method I've outlined here\nshows that we can do that, even if we end up tinkering a bit with the\ndetails.\n\n### Appendix One: Carlsson and van Damme's Game {#appendixone:carlssonandvandammesgame .unnumbered}\n\nTwo players, Row (or R) and Column (or C) will a version of the\nfollowing game.\n\n::: {.center}\n         $a$    $b$\n  ----- ------ ------\n    $A$  4, 4   0, x\n    $B$  x, 0   x, x\n:::\n\nThey won't be told what $x$ is, but they will get a noisy signal of $x$,\ndrawn from an even distribution over $[x - 0.25, x + 0.25]$. Call these\nsignals $s_R$ and $s_C$. Each player must then choose $A$, getting\neither 4 or 0 depending on the other player's choice, or choose $B$,\ngetting $x$ for sure.\n\nBefore getting the signal, the players must choose a strategy. A\nstrategy is a function from signals to choices. Since the higher the\nsignal is, the better it is to play $B$, we can equate strategies with\n'tipping points', where the player plays $B$ if the signal is above the\ntipping point, and $A$ below the tipping point. Strictly speaking, a\ntipping point will pick out not a strategy but an equivalence class of\nstrategies, which differ in how they act if the signal is the tipping\npoint. But since that happens with probability 0, the strategies in the\nequivalence class have the same expected return, and so we won't aim to\ndistinguish them.\n\nAlso, strictly speaking, there are strategies that are not tipping\npoints, because they map signals onto probabilities of playing $A$,\nwhere the probability decreases as $A$ rises. I won't discuss these\ndirectly, but it isn't too hard to see how these are shown to be\nsuboptimal using the argument that is about to come. It eases exposition\nto focus on the pure strategies, and to equate these with tipping\npoints. And since my primary aim here is to explain why the result\nholds, not to simply repeat an already existing proof, I'll mostly\nignore these mixed strategies.\n\nCall the tipping points for Row and Column respectively $T_R$ and $T_C$.\nSince the game is symmetric, we'll just have to show that in conditions\nof common knowledge of rationality, $T_R = 2$. It follows by symmetry\nthat $T_C = 2$ as well. And the only rule we'll use is iterated deletion\nof strictly dominated strategies. That is, we'll assume players won't\nplay strategies where another strategy does better no matter what the\nopponent chooses, and they won't play strategies where another strategy\ndoes better provided the other player does not play a dominated\nstrategy, and they won't play strategies where another strategy does\nbetter provided the other player does not play a strategy ruled out by\nthese first two conditions, and so on.\n\nThe return to a strategy is uncertain, even given the other player's\nstrategy. But given the strategies of each player, we can work out an\nexpected return for each player. And that's what we'll assume is the\nreturn to a strategy pair.\n\nNote first that $T_R = 4.25$ strictly dominates any strategy where\n$T_R  = y > 4.25$. If $s_R \\in (4.25, y)$, then $T_R$ is guaranteed to\nreturn above 4, and the alternative strategy is guaranteed to return 4.\nIn all other cases, the strategies have the same return. And there is\nsome chance that $s_R \\in (4.25, y)$. So we can delete all strategies\n$T_R  = y > 4.25$, and similarly all strategies $T_C = y > 4.25$. By\nsimilar reasoning, we can rule out $T_R < -0.25$ and $T_C < -0.25$.\n\nIf $s_R \\in [-0.75, 4.75]$, then it is equally likely that $x$ is above\n$s_R$ as it is below it. Indeed, the posterior distribution of $x$ is\nflat over $[s_R - 0.25, s_R + 0.25]$. From this it follows that the\nexpected return of playing $B$ after seeing signal $s_R$ is just $s_R$.\n\nNow comes the important step. Assume that we know that $T_C \\leq y > 2$.\nNow consider the expected return of playing $A$ given various values for\n$s_R > 2$. Given that the lower $T_C$ is, the higher the expected return\nis of playing $A$, we'll just work on the simple case where $T_C = y$,\nrealizing that this is an upper bound on the expected return of $A$\ngiven $T_C \\leq y$. The expected return of $A$ is 4 times the\nprobability that Column will play $a$, i.e., 4 times the probability\nthat $s_C < T_C$. Given all the symmetries that have been built into the\npuzzle, we know that the probability that $s_C < s_R$ is 0.5. So the\nexpected return of playing $A$ is at most 2 if $s_R \\geq y$. But the\nexpected return of playing $B$ is, as we showed in the last paragraph,\n$s_R$, which is greater than 2. So it is better to play $B$ than $A$ if\n$s_R \\geq y$. And the difference is substantial, so even if $s_R$ is\nepsilon less than that $y$, it will still be better to play $B$. (This\nis hand-wavy of course, but we'll make it rigorous in just a second.)\n\nSo if $T_C \\leq y > 2$ we can prove that $T_R$ should be lower still,\nbecause given that assumption it is better to play $B$ even if the\nsignal is just less than $y$. Repeating this reasoning over and over\nagain pushes us to it being better to play $B$ than $A$ as long as\n$s_R > 2$. And the same kind of reasoning from the opposite end pushes\nus to it being better to play $A$ than $B$ as long as $s_R < 2$. So we\nget $s_R = 2$ as the uniquely rational solution to the game.\n\nLet's make that a touch more rigorous. Assume that $T_C = y$, and $s_r$\nis slightly less than $y$. In particular, we'll assume that\n$z = y - s_R$ is in $(0, 0.5)$. Then the probability that $s_C < y$ is\n$0.5 + 2z - 2z^2$. So the expected return of playing $A$ is\n$2 + 8z - 8z^2$. And the expected return of playing $B$ is, again,\n$s_R$. These will be equal when the following is true. (The working out\nis a tedious but trivial application of the quadratic formula, plus some\nrearranging.)\n\n$$s_R = y + \\frac{\\sqrt{145-32y} - 9}{16}$$ So if we know that\n$T_C \\geq y$, we know that $T_R \\geq y + \\frac{\\sqrt{145-32y} - 9}{16}$,\nwhich will be less than $y$ if $y > 2$. And then by symmetry, we know\nthat $T_C$ must be at most as large as that as well. And then we can use\nthat fact to derive a further upper bound on $T_R$ and hence on $T_C$,\nand so on. And this will continue until we push both down to 2. It does\nrequire quite a number of steps of iterated deletion. Here is the upper\nbound on the threshold after $n$ rounds of deletion of dominated\nstrategies. (These numbers are precise for the first two rounds, then\njust to three significant figures after that.)\n\n::: {.center}\n   Round   Upper Bound on Threshold\n  ------- --------------------------\n     1              4.250\n     2              3.875\n     3              3.599\n     4              3.378\n     5              3.195\n     6              3.041\n     7              2.910\n     8              2.798\n     9              2.701\n    10              2.617\n:::\n\nThat is, $T_R = 4.25$ dominates any strategy with a tipping point above\n4.25. And $T_R = 3.875$ dominates any strategy with a higher tipping\npoint than that, assuming $T_C \\leq 4.25$. And $T_R \\approx 3.599$\ndominates any strategy with a higher tipping point than that, assuming\n$T_C \\leq 3.875$. And so on.\n\nAnd similar reasoning shows that at each stage not only are all\nstrategies with higher tipping points dominated, but so are strategies\nthat assign positive probability (whether it is 1 or less than 1), to\nplaying $A$ when the signal is above the 'tipping point'. So this kind\nof reasoning rules out all mixed strategies (except those that respond\nprobabilistically to $s_R = 2$).\n\nSo we've shown what was intended, namely that iterated deletion of\ndominated strategies will rule out all strategies except the\nrisk-optimal equilibrium. We needed the possibility that $x$ is greater\nthan the maximal return for $A$ to get the iterated dominance going. And\nwe needed the signal to have an error bar to it, so that each round of\niteration removes more strategies. But that's all we needed; the\nparticular values we chose are irrelevant to the proof.\n\n### Appendix Two: The Modified Game {#appendixtwo:themodifiedgame .unnumbered}\n\nThe aim of this section is to prove something that Carllson and van\nDamme did not prove, namely that the analysis of the previous appendix\ngoes through with very little change if one party gets a perfect signal,\nwhile the other gets a noisy signal. That is, we're going to consider\nthe game that is just like the game of appendix one, but it is common\nknowledge that the signal Column gets, $s_C$, equals $x$.\n\nSince the game is no longer symmetric, we can't appeal to the symmetry\nof the game as we frequently did in the previous appendix. But this only\nslows the proof down, it doesn't stop it.\n\nWe can actually rule out slightly more at the first step in this game\nthan in the previous game. Since Column could not be wrong about $x$,\nColumn knows that if $s_C > 4$ then playing $b$ dominates playing $a$.\nSo one round of deleting dominated strategies rules out $T_C > 4$, as\nwell as ruling out $T_R > 4.25$.\n\nAt any stage, if we know $T_C \\leq y > 2$, then $T_R = y$ dominates\n$T_R > y$. That's because if $s_R \\geq y$, and $T_C \\leq y$, then the\nprobability that Column will play $a$ (given Row's signal) is less than\n0.5. After all, the signal is just as likely to be above $x$ as below it\n(as long as the signal isn't too close to the extremes). So if $s_R$ is\nat or above $T_C$, then it is at least 0.5 likely that $s_C = x$ is at\nor above $T_C$. So the expected return of playing $A$ is at most 2. But\nthe expected return of playing $B$ equals the signal, which is greater\nthan 2. So if Row knows $T_C \\leq y > 2$, Row also knows it is better to\nplay $B$ if $s_R \\geq y$. And that just means that $T_R \\leq y$.\n\nAssume now that it is common knowledge that $T_R \\leq y$, for some\n$y > 2$. And assume that $x = s_C$ is just a little less than $y$. In\nparticular, define $z = y -x$, and assume $z \\in (0, 0.25)$. We want to\nwork out the upper bound on the expected return to Column of playing\n$a$. (The return of playing $b$ is known, it is $x$.) The will be\nhighest when $T_R$ is lowest, so assume $T_R \\leq y$. Then the\nprobability that Row plays $A$ is $(1 + 2z)/2$. So the expected return\nof playing $a$ is $2 + 4z$, i.e., $2 + 4(y - x)$. That will be greater\nthan $x$ only when\n\n$$x < \\frac{2 + 4y}{5}$$ And so if it is common knowledge that\n$T_R \\leq y$, then it is best for Column to play $b$ unless\n$x < \\frac{2 + 4y}{5}$. That is, if it is common knowledge that\n$T_R \\leq y$, then $T_C$ must be at most $\\frac{2 + 4y}{5}$.\n\nSo now we proceed in a zig-zag fashion. At one stage, we show that $T_R$\nmust be as low as $T_C$. At the next, we show that if it has been proven\nthat $T_R$ takes a particular value greater than 2, then $T_C$ must be\nlower still. And this process will eventually rule out all values for\n$T_R$ and $T_C$ greater than 2.\n\nThis case is crucial to the story of the paper because The Radical\nInterpreter probably does not have an error bar in their estimation of\nthe game they are playing. But it turns out the argument for\nrisk-dominant equilibria being the unique solution to interpretation\ngames is consistent with that. As long as one player has a margin of\nerror, each player should play the risk-dominant equilibria.\n\n[^1]: That we need some kind of super-knowledge for action, and not mere\n    knowledge, is a popular, and natural, explanation of the case. For\n    versions of this explanation, obviously with more details than I've\n    given here, see for example Jessica @Brown2008 and Jennifer\n    @Lackey2010.\n\n[^2]: Jessica @Brown2008 [176] shows that pragmatic encroachment\n    theories that rely just on the stakes involved are subject to\n    serious counterexample. Katherine @Rubin2015 argues that if we have\n    a 'global' version of pragmatic encroachment, where all our\n    epistemic notions are interest-relative, then it is implausible that\n    it is the stakes the subject faces that matter for knowledge. Since\n    I'm defending such a global version of pragmatic encroachment,\n    Rubin's arguments show that it is important that I'm relying on\n    odds, not stakes. Baron @Reed2014 argues that if it is stakes alone\n    that matter to pragmatic encroachment, then agents who the pragmatic\n    encroachment theorist takes to be perfectly rational would be\n    subject to a Dutch Book.\n\n[^3]: You can read this paper as a reply to the challenge posed by\n    @IchikawaEtAl2012. They note that there are challenges facing the\n    pragmatic encroachment theorist whether they make evidence\n    interest-relative, or interest-invariant. I'm going to show how to\n    have an interest-relative theory of evidence, and keep what was\n    desirable about pragmatic encroachment theories.\n\n[^4]: In these games, Row chooses a row, and Column chooses a column,\n    and that determines the cell that is the outcome of the game. The\n    cells include two numbers. The first is Row's payout, and the second\n    is Column's. The games are non-competitive; the players are simply\n    trying to maximise their own returns, not maximise the difference\n    between their return and the other player's return.\n\n[^5]: A Nash equilibrium is an outcome of the game where every player\n    does as well as they can given the moves of the other players.\n    Equivalently, it is an outcome where no player can improve their\n    payout by unilaterally defecting from the equilibrium.\n\n[^6]: The Radical Interpreter feels like they should be a humanesque\n    character in *Alice in Wonderland* or *The Phantom Tollbooth*, but\n    for now they are resolutely abstract.\n\n[^7]: I am making a large, if orthodox, assumption here: that the\n    payouts that we use for equilibrium analysis should be expected\n    returns, not actual returns. I think that's the right thing to do,\n    since it is usually impossible to say what the actual return of a\n    game is. Even when we say that the payout is a certain number of\n    dollars, we are really saying that the return is a certain kind of\n    gamble. Maybe the value of the currency will deprecate quickly, and\n    the dollars are not that valuable. Maybe the revolution will come\n    and wealth will be a liability. Almost all games have probabilistic\n    payouts, and this game is no different.\n\n[^8]: A sketch of the proof is in Appendix One.\n\n[^9]: Strictly speaking, we can't rule out various mixed strategies when\n    the signal is precisely 2, but this makes little difference, since\n    that occurs with probability 0.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
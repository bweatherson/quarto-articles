{
  "hash": "ee5e3443a08a233d039a73c512115512",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"For Bayesians, Rational Modesty Requires Imprecision\"\ndescription: |\n  Gordon Belot has recently developed a novel argument against Bayesianism. He shows that there is an interesting class of problems that, intuitively, no rational belief forming method is likely to get right. But a Bayesian agent's credence, before the problem starts, that she will get the problem right has to be 1. This is an implausible kind of immodesty on the part of Bayesians. My aim is to show that while this is a good argument against traditional, precise Bayesians, the argument doesn't neatly extend to imprecise Bayesians. As such, Belot's argument is a reason to prefer imprecise Bayesianism to precise Bayesianism.\ndate: March 11 2016\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ndoi: \"10.3998/ergo.12405314.0002.020\"\ncategories:\n  - epistemology\n  - games and decisions\n  - imprecise probability\ncitation_url: https://doi.org/10.3998/ergo.12405314.0002.020\njournal:\n    title: \"Ergo\"\n    publisher: \"Michigan Publishing\"\nvolume: 2\nnumber: 20\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: mingwing.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    number_sections: true\n---\n\n\n\nGordon @Belot2013 has recently developed a novel argument against\nBayesianism. He shows that there is an interesting class of problems\nthat, intuitively, no rational belief forming method is likely to get\nright. But a Bayesian agent's credence, before the problem starts, that\nshe will get the problem right has to be 1. This is an implausible kind\nof *immodesty* on the part of Bayesians.[^1] My aim is to show that\nwhile this is a good argument against traditional, precise Bayesians,\nthe argument doesn't neatly extend to imprecise Bayesians. As such,\nBelot's argument is a reason to prefer imprecise Bayesianism to precise\nBayesianism.\n\n<aside>\nPublished in [Ergo](https://quod.lib.umich.edu/cgi/t/text/idx/e/ergo/12405314.0002.020/--for-bayesians-rational-modesty-requires-imprecision?rgn=main;view=fulltext) 2: 20.\n</aside>\n\nFor present purposes, the precise Bayesian agent has just two defining\ncharacteristics. First, their credences in all propositions are given by\na particular countably additive probability function. Second, those\ncredences are updated by conditionalisation as new information comes in.\nThese commitments are quite strong in some respects. They say that there\nis a single probability function that supplies the agent's credences no\nmatter which question is being investigated, and no matter how little\nevidence the agent has before the investigation is started. The everyday\nstatistician, even one who is sympathetic to Bayesian approaches, may\nfeel no need to sign up for anything this strong. But many philosophers\nseem to be interested in varieties of Bayesianism that are just this\nstrong. For instance, there has been extensive discussion in recent\nepistemology of whether various epistemological approaches, such as\ndogmatism, can be modeled within the Bayesian framework, with the\nbackground assumption being that it counts against those approaches if\nthey cannot.[^2] In these debates, the issue is not whether the Bayesian\napproach works in the context of a well-defined question and a\nsubstantial evidential background, but whether it does so for all\nquestions in all contexts. Indeed, the assumption is that it does, and\nepistemological theories inconsistent with it are false. So the precise\nBayesian is a figure of some interest, at least in epistemology.\n\nThe imprecise Bayesian doesn't have a single probability function for\ntheir credences. Rather, they have a representor consisting of a set of\nprobability functions. The agent is more confident in $p$ than $q$ just\nin case $\\Pr(p) > \\Pr(q)$ for every $\\Pr$ in this representor.[^3] Just\nlike the precise Bayesian, the imprecise Bayesian updates by\nconditionalisation; their new representor after an update is the result\nof conditionalising every member of the old representor with the new\ninformation. The added flexibility in imprecise Bayesianism will allow\nus to develop a suitably modest response to Belot's puzzle.\n\n### The Puzzle\n\nThe set up Belot uses is this. An agent, *A*, will receive a data stream\nof 0s and 1s. The data stream will go on indefinitely. I will use\n$\\boldsymbol{x}$ for the (infinite) sequence of data she would\n(eventually) get, $x_k$ for the $k$th element of this sequence, and\n$\\boldsymbol{x}_k$ for the sequence consisting of the first $k$ elements\nof the stream. These variables are, as usual, rigid designators. I'll\nalso use the capitalised $\\boldsymbol{X}$ and $\\boldsymbol{X}_k$ as\nrandom variables for the sequence itself, and for the first $k$ elements\nof the sequence, respectively. So $\\boldsymbol{X}= \\boldsymbol{x}$ is\nthe substantive and true claim that the sequence that will be received\nis actually $\\boldsymbol{x}$. And $\\boldsymbol{X}_k = \\boldsymbol{x}_k$\nis the substantive and true claim that the first $k$ elements of that\nsequence are $\\boldsymbol{x}_k$. Propositions of this form will play a\nmajor role below, since they summarise the evidence the agent has after\n$k$ elements have been revealed. I'll use $+$ as a sequence\nconcatenation operator, so $\\boldsymbol{y}+ \\boldsymbol{z}$ is the\nsequence consisting of all of $\\boldsymbol{y}$, followed by all of\n$\\boldsymbol{z}$.\n\nBelot is interested in a quite general puzzle, but I'll focus for most\nof the paper on a very specific instance of the puzzle. (We'll return to\nthe more general puzzle in the last section.) We're going to look at the\nagent's evolving credence that $\\boldsymbol{X}$ is *periodic*. Let $p$\nbe the proposition that $\\boldsymbol{X}$ is periodic, since we'll be\nreturning to that proposition a lot. And let's start by assuming the\nagent is a precise Bayesian, to see the challenge Belot develops.\n\nSay that the agent **succeeds** just in case her credence in $p$\neventually gets on the correct side of $\\frac{1}{2}$, and stays\nthere. (The correct side is obviously the high side if $p$ is true, and\nthe low side otherwise.) That is, if $v$ is the truth value function, it\nsucceeds just in case this is true.[^4]\n$$\\exists n \\forall m \\geq n: |v(p) - \\textit{Cr}(p | \\boldsymbol{X}_m = \\boldsymbol{x}_m)| < \\frac{1}{2}$$\nThe agent **fails** otherwise. Given the assumption that the agent is a\nclassical Bayesian, we can step back from evaluating the agent and\nevaluate her prior probability function directly. So a prior $\\Pr$\nsucceeds relative to $\\boldsymbol{x}$ just in case this is true.\n$$\\exists n \\forall m \\geq n: |v(p) - \\Pr(p | \\boldsymbol{X}_m = \\boldsymbol{x}_m)| < \\frac{1}{2}$$\nThis is reasonably intuitive; the agent is going to get a lot of data\nabout $\\boldsymbol{X}$, and it is interesting to ask whether that data\neventually lets her credence in $p$ get to the right side of\n$\\frac{1}{2}$.\n\nGiven these notions of success and failure, we can naturally define the\nsuccess set of a prior (or agent) as the set of sequences it succeeds\non, and the failure set as the set of sequences it fails on.\n\nAbusing notation a little, say that\n$\\boldsymbol{x}_i \\supset \\boldsymbol{x}_k$ iff $\\boldsymbol{x}_i$ is a\nsequence that has $\\boldsymbol{x}_k$ as its first $k$ entries. Then we\ncan state the first of Belot's conditions on a good Bayesian\nagent/prior. A prior is **open-minded** just in case this condition\nholds:\n$$\\forall \\boldsymbol{x}_k \\exists \\boldsymbol{x}_i \\supset \\boldsymbol{x}_k, \\boldsymbol{x}_j \\supset \\boldsymbol{x}_k: \\Pr(p | \\boldsymbol{X}_i = \\boldsymbol{x}_i) < \\frac{1}{2} \\wedge \\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) > \\frac{1}{2}$$\nThat is, no matter what happens, it is possible that the probability of\n$p$ will fall below , and possible it will rise above . To motivate the\nfirst, consider any situation where the sequence to date has looked\nperiodic. (If it had not looked periodic to date, presumably the\nprobability of $p$ should already be low.) Now extend that sequence with\na large of random noise. At the end of this, it should no longer be\nprobable that the sequence is periodic. On the other hand, assume the\nsequence has not looked periodic to date. Extend it by repeating\n$\\boldsymbol{x}_k$ more than $k$ times. At the end of this, it should\nlook probable that the sequence is periodic (at least for large enough\n$k$). So open-mindedness looks like a good condition to impose.\n\nThe second condition we might impose, though not one Belot names, is\n*modesty*. Any function might fail. One natural way it might fail is\nthat it might get, to use a term Belot does use, *flummoxed*. It could\nchange its mind infinitely often about whether the sequence is periodic.\nBy definition, open-mindedness entails the possibility of being\nflummoxed. Given the definitions of success and failure, $\\Pr$ will fail\nrelative to any $\\boldsymbol{x}$ that flummoxes it. So success is not a\npriori guaranteed. Now for any function we can work out the set of\nsequences relative to which it fails. It turns out this will be a rather\nlarge set. Indeed, the set of sequences on which any open-minded\nfunction succeeds is meagre.[^5] Say a function is **modest** if the\ninitial probability it gives to $\\boldsymbol{X}$ being in its success\nset is less than 1. Given how large the failure set is, modesty also\nseems like a good requirement.[^6]\n\nThe argument for modesty is not that it is an immediate consequence of\nregularity. It does follow from regularity, but in the case we're\nconsidering, regularity is quite implausible. Some sets, even some quite\nlarge sets in some sense, will have to be given probability 0. The\nsurprising thing is that a residual set (i.e., the complement of a\nmeagre set) gets probability 0.\n\nIt might be thought that modesty here is problematic for the same reason\nthat epistemic modesty is often problematic: it validates\nMoore-paradoxical thoughts. It's bad to say *p, but there is a\nprobability that not p*. It's even bad, though as @Briggs2009 points\nout, not quite bad for the same reasons, to say *Whether I believe p is\ntrue or false tomorrow, there will be a probability I'm false*. Perhaps\nmodesty is a requirement that someone say something like that, and hence\nis an improper requirement.\n\nBut in fact the requirement of modesty is disanalogous to the\n'requirement', suggested in the previous paragraph, that agents endorse\nMoore-paradoxical principles. There isn't anything wrong with saying\n*Whichever side of one half my credence in p is tomorrow, there is a\nprobability that the truth will be the other side of one half*. That's\nnot Moore-paradoxical. Indeed, unless one is sure that one's credence in\n$p$ tomorrow will be 0 or 1, it is something one should endorse.\n\nOr consider a different example. There will be a sequence of 0s and 1s,\nbut this time there will only be three elements, and the agent will only\nbe shown the first of them tomorrow. Let $q$ be the proposition that\nthere are more 1s than 0s in the three-element sequence. Say the agent\n**succeeds** iff tomorrow, after seeing just one element, her credence\nin $q$ is the same side of one-half as the truth. And say the agent is\n**modest** iff, right now, her credence that she succeeds tomorrow is\nless than one. There is nothing incoherent about being modest. If her\ncredal distribution today is completely flat, giving $\\frac{1}{8}$\ncredence to each of the eight possible sequences, she will be modest,\nfor example.\n\nNow this case is somewhat different to the one Belot started with in a\ncouple of respects. On the one hand, we're asking about modesty at a\nparticular point, i.e., tomorrow, rather than over a long sequence. On\nthe other hand, we're asking about whether the agent's credences will be\non the right side of one-half after having seen one-third of the data,\nrather than, as in the original case, after seeing measure zero of the\nsequence. The first difference makes it easier to be modest, the second\ndifference makes it harder. So the cases are not perfect analogies, but\nthey are similar enough in respect of modesty to make it plausible that\nif modesty is coherent in this case, as we've shown it is, then it\nshould be coherent in Belot's case as well.\n\nSo that's the argument that open-mindedness and modesty are good\nconditions for priors to satisfy. Here's the worrying result that Belot\nproves. There are no open-minded modest priors. If $A$ is a classical\nBayesian, she will either have to be closed minded or immodest. Neither\nseems rational, so it seems that being a classical Bayesian is\nincompatible with being rational. That is, we can't be precise Bayesians\nif we accept the following two constraints.\n\n-   *Open-Mindedness*: For any initial sequence, there is a continuation\n    after which it seems probable that $\\boldsymbol{X}$ is periodic, and\n    a continuation after which it seems probable that $\\boldsymbol{X}$\n    is not periodic.\n\n-   *Modesty*: The initial probability that the agent will succeed,\n    i.e., that their credence in $p$ will eventually get to the right\n    side of $\\frac{1}{2}$ and stay there, is less than 1.\n\nSince both open-mindedness and modesty are very plausible constraints,\nit follows that there is no good way to be a precise Bayesian in the\nface of this puzzle.\n\n### Making the Puzzle Less Precise\n\nWhat happens, though, if the agent is an imprecise Bayesian? Is there a\nparallel version of Belot's argument that shows this kind of imprecise\nBayesian is necessarily irrational? I'm going to argue that the answer\nis no.\n\nThe first thing we have to do is work out how to redefine the key terms\nin Belot's argument once we drop the assumption that the agent is a\nclassical Bayesian. There are several ways of formulating our\ndefinitions which are equivalent given that assumption, but not\nequivalent given that the agent is an imprecise Bayesian. There are\nthree major choice points here.\n\n1.  What is success?\n\n2.  What is open-mindedness?\n\n3.  What is modesty?\n\nAssume our agent's credal state is represented by set $S$ of probability\nfunctions. Then there are two natural ways to think about success.\n$$\\begin{aligned}\n\\forall \\Pr \\in S:  \\exists n \\forall m \\geq n: |v(p) - \\Pr(p | \\boldsymbol{X}_m= \\boldsymbol{x}_m)| < \\frac{1}{2} \\\\\n\\exists n \\forall \\Pr \\in S: \\forall m \\geq n: |v(p) - \\Pr(p | \\boldsymbol{X}_m= \\boldsymbol{x}_m)| < \\frac{1}{2}\\end{aligned}$$\nThe second is obviously stronger than the first, since it involves\nmoving an existential quantifier out in front of a universal quantifier.\nAnd there are some natural cases where an agent could succeed on the\nfirst definition, and fail on the second. Here's one such case.\n\nLet $\\Pr_0$ be the *fair-coin measure*. Acccording to the fair coin\nmeasure, if $\\boldsymbol{y}$ is any $k$ length sequence of 0s and 1s we\nhave $\\Pr_0(\\boldsymbol{x}_k = \\boldsymbol{y}) = 2^{-k}$. Intuitively,\nit thinks the 0s and 1s are generated by flips of a fair coin, and it\nwon't change its mind about that no matter what happens.\n\nSay a probability function $\\Pr$ is **regular periodic** iff it\nsatisfies these two conditions.\n\n-   $\\Pr(p) = 1$.\n\n-   For any periodic sequence $\\boldsymbol{y}$,\n    $\\Pr(\\boldsymbol{X}= \\boldsymbol{y}) > 0$.\n\nIntuitively, these functions are certain that $X$ is periodic, and\nassign positive probability to each possible periodic sequence. Now\nconsider the family of functions we get by taking equal weighted\nmixtures of $\\Pr_0$ with each regular periodic function. Let that family\nrepresent the agent's credence. And assume for now that $\\boldsymbol{X}$\nis the sequence $\\langle 0, 0, 0, \\dots \\rangle$. Does the agent\nsucceed?\n\nWell, each $\\Pr$ in her representor succeeds. To prove this, it will be\nhelpful to prove a lemma that we'll again have use for below. For this\nlemma, let $\\Pr_0$ be the fair-coin measure (as already noted), $\\Pr_1$\nbe any measure such that $\\Pr_1(p) = 1$, and $\\Pr_2$ be the equal\nmixture of $\\Pr_0$ and $\\Pr_1$.\n\n**Lemma 1**: $\\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\frac{1}{2}$ iff\n$\\Pr_1(\\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\Pr_0(\\boldsymbol{X}_k = \\boldsymbol{y}_k).$\n\n::: {.proof}\n*Proof.* Let $\\Pr_i(\\boldsymbol{X}_k = \\boldsymbol{y}_k) = a_i$ for\n$i \\in {0, 1}$. Recall that $\\Pr_0(p) = 0$ and $\\Pr_1(p) = 1$. Then we\ncan quickly get that\n$\\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) = \\frac{a_1}{a_0 + a_1}$,\nfrom which the lemma immediately follows. ◻\n:::\n\nFor any $\\Pr$ in the agent's representor, there is some $k$ such that\n$\\Pr(\\boldsymbol{X}= \\langle 0, 0, 0, \\dots \\rangle)$ $> 2^{-k}$. So\nafter at most $k$ 0s have appeared, $\\Pr(p)$ will be above\n$\\frac{1}{2}$, and it isn't coming back. That means it succeeds. And\nsince $\\Pr$ was arbitrary, it follows that all $\\Pr$ succeed.\n\nBut the agent in a good sense doesn't succeed. No matter how much data\nshe gets, there will be $\\Pr$ in her representor according to which\n$\\Pr(p) < \\frac{1}{2}$. After all, for any $k$, there are regular\nperiodic $\\Pr$ such that the probability of $\\boldsymbol{x}_k$ being $k$\n0s is below $\\frac{1}{2^k}$. So if we mix that function with\n$\\Pr_0$, we get a function where the most probable continuations of this\ninitial sequence are the random sequences provided by the fair coin\nmeasure.\n\nIn terms of our definitions of success above, the agent satisfies the\nfirst, but not the second. Every function in her representor eventually\nhas the probability of $p$ go above $\\frac{1}{2}$. But at any time,\nthere are functions in her representor according to which the\nprobability of $p$ is arbitrarily low.\n\nHere I think we have to make a distinction between different ways of\nunderstanding the formalism of imprecise probabilities. (What follows is\nindebted to @Bradley2014, especially his section 3.1, but I'm\ndisagreeing somewhat with his conclusions, and following more closely\nthe conclusions of @Joyce2010 and @Schoenfield2012.)\n\nOne way of thinking about imprecise credences is that each probability\nfunction in the representor is something like an advisor, and the agent\nwho is imprecise simply hasn't settled on which advisor to trust. Call\nthis the **pluralist** interpretation of the formalism. On this\ninterpretation, it is natural to think that what is true of every\nfunction is true of the agent.\n\nAnother way is to think of the agent's mind as constituted by, but\ndistinct from, the representors. An analogy to keep in mind here is the\nway that a parliament is constituted by, but distinct from, its members.\nKeeping with this analogy, call this the **corporate** interpretation of\nthe formalism. Note that corporate bodies will typically have their own\nrules for how the views of the members will be translated into being\nviews of the whole. Even if every member of the parliament believes that\nthe national cricket team will win its upcoming game, it doesn't follow\nthat the parliament believes that; the parliament only believes what it\nresolves it has believed.\n\nNow I only want to defend the imprecise Bayesian model on the corporate\ninterpretation.[^7] The pluralist interpretation, it seems to me, faces\ngrave difficulties. For one thing, it has a hard time explaining what's\nwrong with the existential claim \"There is a precise number $x$ such\nthat $x$ is the probability of $p$\". Every advisor believes that, so on\nthe pluralist model the agent does too. (Compare the criticisms of\n\"fanatical supervaluationism\" in @Lewis1993c.) More relevant to the\ndiscussion here, I am following Belot in thinking we have an argument\nthat each precise Bayesian is unreasonably proud. On the pluralist\ninterpretation, the agent is undecided which of these unreasonable\nadvisors she will follow. But such a state is itself unreasonable; she\nshould have decided not to follow any of them, since they are all\nprovably unreasonable!\n\nA surprising fact about corporate bodies is that they can be immune to\nproblems that beset each of their members. It would be illegitimate for\nany one parliamentarian to have law-making power; it is (or at least can\nbe) legitimate for them all to have such power. Indeed, it would be\nunreasonable for any of them to think that they individually should have\nlaw-making powers; that would be unreasonably proud. But it is not\nunreasonable for them to collectively think that they should\ncollectively have law-making powers. If they are a well-constituted\nparliament, this is a perfectly reasonable thought. Similarly here, the\nagent, the corporate body, could avoid being unreasonably proud even\nthough each of the representors is over-confident in its own powers.\n\nNow going back to success and modesty, it seems to me that the first\ndefinition of success is appropriate on the pluralist interpretation of\nthe imprecise framework, and the second is appropriate on the corporate\ninterpretation. The first interpretation says that the agent succeeds\niff every member succeeds. And the second says that the agent succeeds\niff the body of functions, collectively, succeed. Since I'm defending\nthe use of the imprecise framework on the corporate interpretation, it\nis the second definition of success that is appropriate, and that's what\nI will use here.\n\nThis understanding isn't without costs. @Bradley2014 argues, in effect,\nthat the best responses to dilation-based arguments against imprecise\nprobabilities (as in @White2010), are only available on the pluralist\ninterpretation. I'm not going to try to solve those problems here, but I\nwill note that the interpretative choice I'm making generates some extra\nphilosophical work elsewhere. Against that, the corporate interpretation\nhas some benefits. It lets us agree with Peter @Walley1991 that there\nare rational agents who are represented by sets of merely finitely\nadditive probability functions, though no merely finitely additive\nprobability function on its own could represent a rational agent. So the\nissues between the two interpretations are extensive. For now, I'll\nsimply note that I'm interested in defending the imprecise Bayesian from\nBelot's argument on the corporate interpretation. And with that I'll\nreturn to translating Belot's puzzle into the imprecise framework, with\nthe second, corporate-friendly, interpretation of success on board.\n\nThere are also two natural ways to generalise Belot's notion of\nopen-mindedness to the imprecise case. We could require that the agent\nsatisfies either the first or second of these conditions.\n$$\\begin{aligned}\n\\forall \\boldsymbol{x}_k \\exists \\boldsymbol{x}_i \\supset \\boldsymbol{x}_k, \\boldsymbol{x}_j \\supset \\boldsymbol{x}_k: \\neg(\\Pr(p | \\boldsymbol{X}_i = \\boldsymbol{x}_i) \\geq \\frac{1}{2}) \\wedge \\neg(\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) < \\frac{1}{2}) \\\\\n\\forall \\boldsymbol{x}_k \\exists \\boldsymbol{x}_i \\supset \\boldsymbol{x}_k, \\boldsymbol{x}_j \\supset \\boldsymbol{x}_k: \\Pr(p | \\boldsymbol{X}_i = \\boldsymbol{x}_i) < \\frac{1}{2} \\wedge \\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) \\geq \\frac{1}{2}\\end{aligned}$$\nThe second is just the same symbols as in Belot's, and it is what I'll\nend up arguing is the right constraint to put on the imprecise Bayesian\nagent. And it is a considerably more demanding constraint than the\nfirst. But the first is perhaps the more natural understanding of\n*open-mindedness*. It says that no matter what the initial evidence is,\nthe agent is not guaranteed to settle her credence in $p$ on one side of\n$\\frac{1}{2}$. That's a way of being open-minded.\n\nBut if the agent satisfies that constraint, she may be open-minded, but\nshe won't necessarily be responsive to the evidence. Here's how I'm\nusing the terms 'open-minded' and 'evidence-responsive'. In both\nclauses, the quantification is intended to be over a salient class of\npropositions. (The relevant class in the application we're most\ninterested in is just $\\{X$ is periodic, $X$ is not periodic$\\}$.) And\nI'll say an agent is 'confident' in a proposition iff her credence in it\nis above $\\frac{1}{2}$.\n\nOpen-Minded\n\n:   Any time an agent is confidence in a proposition, there is some\n    evidence she could get that would make her lose confidence in it.\n\nEvidence-Responsive\n\n:   For any proposition, there is some evidence the agent could get that\n    would make her confident in it.\n\nOnce we allow imprecise credences, these two notions can come apart.\nConsider the agent we described above, whose representor consists of\nequal mixtures of the fair-coin measure and regular periodic functions.\nThey are open-minded; they can always lose confidence that $X$ is\nperiodic or not. But they aren't evidence-responsive; no matter what the\nevidence, their credence that $X$ is periodic will never rise above\n$\\frac{1}{2}$. In fact, their credence that $X$ is periodic will\nnever rise above any positive number.\n\nThat suggests open-mindedness is too weak a constraint. If the evidence\nthe agent gets is a string of several hundred 0s, she shouldn't just\nlose any initial confidence in $\\neg p$, she should become confident in\n$p$. And arguably (though I could imagine a dissent here), if the\ninitial sequence is a seemingly random sequence, the credence in $p$\nshould drop well below $\\frac{1}{2}$. (The imagined dissent here is\nfrom someone who thinks that the noisier the data, the more imprecise\ncredences should get. That's an interesting view, but perhaps orthogonal\nto the issues we're debating here.)\n\nAnd when we look back at Belot's motivations for open-mindedness, we see\nthat they are really motivations for being evidence-responsive. One of\nthe distinctive (and I would say problematic) features of precise\nBayesianism is that it doesn't really have a good way of representing a\nstate of indecisiveness or open-mindedness. In the terms we've been\nusing here, there's no difference for the precise Bayesian between being\nevidence responsive and open minded. The imprecise Bayesian can\ndistinguish these. And in Belot's puzzle, we should require that the\nimprecise Bayesian agent is evidence responsive. So we should impose the\nsecond, stronger, condition.\n\nThe final condition to discuss is modesty. There are three natural candidates here. We could\nmerely require that the agent's prior probability that $\\boldsymbol{x}$\nis in her success set is not equal to 1. Or we could require that it be\nless than 1. Or, even more strongly, we could require that it be less\nthan some number that is less than 1. If her credence that\n$\\boldsymbol{x}$ is in her success set is imprecise over some interval\n$[k, 1]$, she satisfies the first condition, but not the second or\nthird. If it is imprecise over some interval $(k, 1)$, or $[k, 1)$, she\nsatisfies the first and second conditions, but not the third. In the\ninterests of setting the imprecise Bayesian the hardest possible\nchallenge, though, let's say that modesty requires the third criteria.\nHer ex ante credence in success should not just be less than 1, it\nshould be less than some number less than 1.\n\nThe aim of the next section is to describe a representor that satisfies\nopen-mindedness and modesty with respect to the question of whether the\nsequence is periodic. The representor will not represent a state that it\nis rational for a person to be in; we'll come back in the last section\nto the significance of this. My aim is just to show that for the\nimprecise Bayesian, unlike the precise Bayesian, open-mindedness and\nmodesty are compatible. And the proof of this will be constructive; I'll\nbuild a representor that is, while flawed in some other ways,\nopen-minded and modest.\n\n### Meeting the Challenge, Imprecisely\n\nRecall that $\\Pr_0$ is the *fair-coin measure*, according to which, if\n$\\boldsymbol{y}$ is any $k$ length sequence of 0s and 1s we have\n$\\Pr_0(\\boldsymbol{X}_k = \\boldsymbol{y}) = 2^{-k}$.\n\nSay a finite sequence $\\boldsymbol{y}_k$ of length $k$ is **repeating**\niff for some $n > 1$, $\\boldsymbol{y}_k$ consists of $n$ repetitions of\na sequence of length $k/n$. For any non-repeating sequence\n$\\boldsymbol{y}_k$ (of length $k$) let\n$\\boldsymbol{s}_{\\boldsymbol{y}_k}$ be the sequence consisting of\n$\\boldsymbol{y}_k$ repeated infinitely often. Let $\\Pr_1$ be the\nfunction such that,\n$$\\Pr{}_1(\\boldsymbol{X}= \\boldsymbol{s}_{\\boldsymbol{y}_k}) = \\frac{1}{2^{2k}-1}$$\nIntuitively, we can think of $\\Pr_1$ as follows. Consider a measure over\nrepresentations of periodic sequences. Any periodic sequence can be\nrepresented just as a finite sequence, plus the instruction *repeat\ninfinitely often*, so this is really just a measure over finite\nsequences. One natural such measure assigns measure\n$\\frac{1}{2^{2k}}$ to each sequence of length $k$. Of course,\nseveral of these representations will be representations of the same\nsequence. For instance, $\\langle 0, 1 \\rangle$,\n$\\langle 0, 1, 0, 1 \\rangle$ and $\\langle 0, 1, 0, 1, 0, 1 \\rangle$\nrepeated infinitely produce the same sequence. Now the probability of a\nsequence, according to $\\Pr_1$ is just the measure, so defined, of the\nclass of representations of that measure. (It's a little easier to\nconfirm that the measures sum to 1 than that the probabilities do, which\nis why I've included this little explanation.)\n\nNow define $\\Pr_2$ as the equal weight mixture of $\\Pr_0$ and $\\Pr_1$,\ni.e., $\\Pr_2(q) = (\\Pr_0(q) + \\Pr_1(q))/2$. Since $\\Pr_0(p) = 0$, and\n$\\Pr_1(p) = 1$, $\\Pr_2(p) = \\frac{1}{2}$. There will be several\nfacts about $\\Pr_2$ that are useful to have in place for future\nreference. (Recall I'm using $\\boldsymbol{X}$ as a random variable for\nthe sequence the agent will see, $\\boldsymbol{x}$ as a rigid designator\nof that sequence, $\\boldsymbol{y}$ and $\\boldsymbol{z}$ are variables\nfor arbitrary sequences, and the $k$ subscript to restrict sequences to\nlength $k$.) The first of these was proven as\nLemma 1.\n\nLemma 1.\n\n:   $\\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\frac{1}{2} \\text{ iff } \\Pr_1(\\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\Pr_0(\\boldsymbol{X}_k = \\boldsymbol{y}_k).$\n\nDefine a new predicate $N$ of finite sequences $\\boldsymbol{y}_k$, to\nhold just in case $\\boldsymbol{y}_k$ could be the initial segment of an\ninfinite sequence of period at most $\\frac{k}{2}$. So\n$\\boldsymbol{y}_k$ must consist of some sequence repeated twice, and\nanything else in $\\boldsymbol{y}_k$ must be consistent with that\nsequence repeating again (and if necessary again, and again, ...). Then\nwe get,\n\nLemma 2.\n\n: For $k \\geq 2$, $\\Pr_2(p | \\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) > \\frac{1}{2}$ iff $N\\boldsymbol{y}_{2k}$.\n\n::: {.proof}\n*Proof.* By Lemma 1, this reduces to the question of the relationship\n$\\Pr_1(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) > \\Pr_0(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k})$.\nMoreover, we know that\n$\\Pr_0(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) = 2^{-2k}$. So the\nquestion is whether\n$\\Pr_1(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) > 2^{-2k}$.\n\nIf $N\\boldsymbol{y}_{2k}$, then it is consistent with\n$\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}$ that $\\boldsymbol{x}$ is a\nparticular periodic sequence with period at most $k$. Since the\nprobability, according to $\\Pr_1$ of any such sequence is greater than\n$2^{-2k}$, the right-to-left direction follows.\n\nIf $\\neg N\\boldsymbol{y}_{2k}$, then the possibilities that get positive\nprobability according to $\\Pr_1$ are at most among the following:\n$\\boldsymbol{X}$ consists of the first $k + 1$ digits of\n$\\boldsymbol{y}_{2k}$ repeated endlessly; $\\boldsymbol{X}$ consists of\nthe first $k + 2$ digits of $\\boldsymbol{y}_{2k}$ repeated endlessly;\n...; $\\boldsymbol{x}$ consists of the first $2k$ digits of\n$\\boldsymbol{y}_{2k}$ repeated endlessly; $\\boldsymbol{X}$ is one of the\ntwo sequences of period $2k + 1$ starting with $\\boldsymbol{y}_{2k}$, or\none of the four sequences of period $2k+2$ starting with\n$\\boldsymbol{y}_{2k}$ or .... So we get the following, starting with the\nprobabilities of each of the possibilities listed in the previous\nsentence, $$\\begin{aligned}\n\\Pr{}_1(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) \n    &\\leq \\frac{1}{2^{2k+2}-1} \n    &+ &\\frac{1}{2^{2k+4}-1} \n    &+ \\dots \n    &+ &\\frac{1}{2^{4k}-1} \n    &+ &\\frac{2}{2^{4k+2}-1} \n    &+ \\dots \\\\\n%\n    &< \\frac{1}{2^{2k+1}} \n    &+ &\\frac{1}{2^{2k+3}} \n    &+ \\dots\n    &+ &\\frac{1}{2^{4k-1}}\n    &+ &\\frac{1}{2^{4k}}\n    &+ \\dots \\\\\n%\n    &< \\frac{1}{2^{2k}}\\end{aligned}$$ And from that the left-to-right\ndirection follows. ◻\n:::\n\nLemma 3\n\n:    $\\Pr_2$ is open-minded.\n\n::: {.proof}\n*Proof.* Since any initial sequence $\\boldsymbol{y}_k$ that is not $N$\ncan be easily extended into one that is $N$ (by, e.g., repeating\n$\\boldsymbol{y}_k$), and one is that is $N$ can be extended into one\nthat is not (by, e.g., having the repeating sequence stop at the very\nnext step), this follows immediately from\nLemma 2. ◻\n:::\n\nDefine $f$ to be a function from sequences of length $k \\geq 2$ to\nsequences of length $k+1$ such that\n$$f(\\boldsymbol{y}_k) = \\boldsymbol{y}_k + \n    \\begin{cases} \n        \\langle 0 \\rangle &\\text{if } N\\boldsymbol{y}_k \\leftrightarrow \\Pr{}_1(x_{k+1} = 0 | \\boldsymbol{X}_k = \\boldsymbol{y}_k) \\leq \\frac{1}{2} \\\\\n        \\langle 1 \\rangle &\\text{otherwise}\n    \\end{cases}$$ In the normal way, define $f^n(\\boldsymbol{y}_k)$ to\nbe the result of applying $f$ $n$ times to $\\boldsymbol{y}_k$. And\ndefine $f^\\infty(\\boldsymbol{y}_k)$ to be the infinite sequence we get\nby doing this infintely often.\n\nIntuitively, the way $f$ works is that if $\\boldsymbol{y}_k$ is already\nsomewhat sequential, then we include the less likely digit, and if it\nisn't, then we include the more likely digit. (With ties resolved in\nfavour of including 0 rather than 1.) If we define $p(\\boldsymbol{y}_k)$\nto be the smallest $n$ such that $\\boldsymbol{y}_k$ could be the initial\nsegment of a periodic sequence of length $n$, then we'll get that\n$p(f(\\boldsymbol{y}_k)) > p(\\boldsymbol{y}_k) \\leftrightarrow N\\boldsymbol{y}_k$\nin all cases, except for the case where\n$\\Pr{}_1(\\boldsymbol{x}_k = 0 | \\boldsymbol{X}_k = \\boldsymbol{y}_k) = \\frac{1}{2}$.\nThat is, if $N\\boldsymbol{y}_k$, then extending $\\boldsymbol{y}_k$ in\nthis way will wipe out the possibility of that smallest sequence being\nextended indefinitely, while if $\\neg N\\boldsymbol{y}_k$, then that\npossibility will still be on the table.\n\nFrom this, it follows that $f^{\\infty}(\\boldsymbol{y}_k)$ will flummox\n$\\Pr_2$, no matter which $\\boldsymbol{y}_k$ we start with.\n\nWe need one last classification of finite sequences, and then we are\ndone. Say that $O\\boldsymbol{y}_k$ just in case some initial segment of\n$\\boldsymbol{y}_k$ of length $r$ could be the initial segment of an\ninfinite period sequence of period less than $\\frac{r}{2}$. This\ncontrasts with $N$ in two ways. First, it requires a sequence that\nrepeats twice, and then starts a third repetition. Second, it does not\nrequire that the sequence be 'live'; there might be subsequent parts of\n$\\boldsymbol{y}_k$ that are not compatible with the sequence repeating.\nSo the sequence $\\langle 0, 0, 1, 0, 0, 1\\rangle$ satisfies $N$ but not\n$O$, while the sequence $\\langle 0, 1, 0, 1, 0, 0\\rangle$ satisfies $O$\nbut not $N$.\n\nThere are a countable infinity of finite sequences $\\boldsymbol{y}_k$\nsuch that $\\neg O \\boldsymbol{y}_k$. Produce some ordering of them, then\ndefine $\\Pr_i$, for $i \\geq 3$, to be the probability function such that\n$\\Pr_i(\\boldsymbol{X}= f^\\infty(\\boldsymbol{y}_k)) = 1$, where\n$\\boldsymbol{y}_k$ is the $i-2$'th sequence in this order.\n\nNow, consider the set $R$ of all probability functions of the form:\n$$\\Pr = \\sum_{i = 2}^\\infty a_i\\Pr{}_i$$ where each of the $\\Pr_i$ are\ndefined as above, each $a_i$ is non-negative, $a_2$ is , and the sum of\nthe $a_i$ from 3 to $\\infty$ is also $\\frac{1}{2}$. Intuitively,\neach function starts by halving the probability $\\Pr_2$ gives to each\ninitial (or completed) sequence, and distributing the remaining\nprobability over the countable infinity of flummoxing sequences of the\nform $f^\\infty(\\boldsymbol{y}_k)$, where $\\neg O\\boldsymbol{y}_k$.\n\nI'll now prove that $R$ is open minded.\n\nLemma 4\n:    If $\\neg O \\boldsymbol{y}_k$, then $\\neg O f(\\boldsymbol{y}_k)$.\n\n::: {.proof}\n*Proof.* Since $\\neg O \\boldsymbol{y}_k$, the only way that\n$O f(\\boldsymbol{y}_k)$ could be true is if $k = 2r +1$, and\n$f(\\boldsymbol{y}_k)$ consists of some sequence of length $r$ repeated\ntwice, plus the first digit repeated a third time. But that means that\n$N\\boldsymbol{y}_k$. And if that's the case, then the extra digit that\nis added by $f(\\boldsymbol{y}_k)$ will not be the necessary digit to\nrepeat this sequence. So it is impossible that\n$O f(\\boldsymbol{y}_k)$. ◻\n:::\n\nLemma 5\n:    If $\\neg O \\boldsymbol{y}_k$, then $\\neg O f^\\infty(\\boldsymbol{y}_k)$.\n\n::: {.proof}\n*Proof.* This follows trivially from  Lemma 4. ◻\n:::\n\nTheorem 6\n:    $R$ is open-minded.\n\n::: {.proof}\n*Proof.* Any initial sequence can be extended to a sequence satisfying\n$O$. For example, the initial sequence can be repeated in full twice. An\nimmediate consequence of\nLemma 5 is that for all\n$i \\geq 3,  O\\boldsymbol{y}_k \\rightarrow \\Pr_i(\\boldsymbol{X}_k = \\boldsymbol{y}_k) = 0$.\nThat means that if $O\\boldsymbol{y}_k$, then for any\n$\\Pr \\in R,{ } \\Pr(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) = \\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k)$.\nAnd now the theorem is an immediate consequence of\nLemma 3. ◻\n:::\n\nLet $F$ be the set of all sequences $f^\\infty(\\boldsymbol{y}_k)$, where\n$\\neg O \\boldsymbol{y}_k$.\n\nLemma 7\n:    If $\\boldsymbol{x}\\in F$, then $R$ fails.\n\n::: {.proof}\n*Proof.* Assume $\\boldsymbol{x}\\in F$, so $\\boldsymbol{x}$ is not\nperiodic. Then proving the lemma requires showing that for any $i$,\nthere is a $j \\geq i$ such that, according to $R$, the probability of\n$p$ given $\\boldsymbol{X}_j =\\boldsymbol{x}_j$ is not less than\n$\\frac{1}{2}$. And that requires showing that there is a $\\Pr \\in R$\nsuch that\n$\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) \\geq \\frac{1}{2}$.\nThis is easy to do. Consider any sequence $\\boldsymbol{y}_i$ of length\n$i$ not identical to $\\boldsymbol{x}_i$ such that\n$\\neg O \\boldsymbol{y}_i$. Consider the probability function\n$\\Pr_k \\in R$ such that\n$\\Pr_k(\\boldsymbol{X}= f^\\infty(\\boldsymbol{y}_i)) = \\frac{1}{2}$.\nOnce we conditionalise on $\\boldsymbol{X}_i = \\boldsymbol{x}_i$, that\nfunction will behave just like $\\Pr_2$. And since $\\boldsymbol{X}$\nflummoxes $\\Pr_2$, that means there is a $\\boldsymbol{x}_j$ such that\n$\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) > \\frac{1}{2}$, and\nhence\n$\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) \\geq \\frac{1}{2}$. ◻\n:::\n\nLemma 8\n:    For each $\\Pr \\in R, \\Pr(\\boldsymbol{x}\\in F) = \\frac{1}{2}.$\n\n::: {.proof}\n*Proof.* It helps to think of each of the $\\Pr \\in R$ as mixtures of\n$\\Pr_0$ and $\\Pr_1$, plus a mixture of the $\\Pr_i$ for $i \\geq 3$. Now\n$\\Pr_0(\\boldsymbol{x}\\in F) = 0$, since for any countable set, $\\Pr_0$\nsays the probability that $\\boldsymbol{x}$ is in that set is 0. And\n$\\Pr_1(\\boldsymbol{x}\\in F) = 0$, since $\\Pr_1$ says that the\nprobability of $\\boldsymbol{x}$ being periodic is 1, and none of the\nmembers of $F$ are periodic. But for each $\\Pr_i$ for $i \\geq 3$,\n$\\Pr_i(\\boldsymbol{x}\\in F) = 1$. Indeed, for each such function, there\nis a particular sequence in $F$ such that the probability that\n$\\boldsymbol{x}$ is that sequence is 1. So for each\n$\\Pr \\in R, \\Pr(\\boldsymbol{x}\\in F) = \\frac{1}{4} \\times 0 + \\frac{1}{4} \\times 0 + \\frac{1}{2} \\times 1 = \\frac{1}{2}$. ◻\n:::\n\nTheorem 9\n:    According to $R$, the\nprobability of an agent whose representor is $R$ failing is at least\n$\\frac{1}{2}$.\n\n::: {.proof}\n*Proof.* Immediate from Lemma 7 and Lemma 8. ◻\n:::\n\nSo if an agent's credences are represented by a non-singleton set of\nprobability functions, not a single probability function, it is possible\nfor them to be open-minded and modest. On the other hand, if an agent is\nrepresented by a single probability function, as the precise Bayesian\ndesires, then it is impossible to be open-minded and modest. Since being\nopen-minded and modest is desirable, this is a reason to prefer the\nimprecise Bayesian picture.\n\n### Objections and Replies\n\nI'm going to reply to three objections, but since my replies overlap,\nI'll group the objections together.\n\nObjection 1\n:    The model here only gives you conditional modesty. Once the initial\nsequence is $O$, the representor becomes the singleton of an open-minded\nprobability function, and Belot showed that to be immodest. Ideally, the\nagent would have a prior that is in some way resiliently modest, whereas\nthis prior is fragilely modest.\n\nObjection 2\n:    This representor is open-minded and modest towards one particular\nproblem, namely whether $\\boldsymbol{X}$ is periodic. But Belot was\ninterested in a wider range of problems, indeed in all problems of the\nform: does $\\boldsymbol{x}$ fall into some set that is measurable,\ndense, and has a dense complement. Ideally, we'd have a prior which is\nwidely open-minded and modest, in the sense that it had an open-minded\nand modest attitude towards many problems. But this prior is narrowly\nmodest, in the sense that it is open-minded and modest about only one\nproblem.\n\nObjection 3\n:     The representor described here is clearly not a representation of a\ncredal state of anyone rational. Look what it does if the data is a 1\nfollowed by thousands of 0s, or is the first few thousand digits of the\nbinary expansion of $\\pi$, or has a frequency of 0s of 0.2 over all\nlarge sub-intervals. No one could adopt this prior, so it doesn't show\nanything about the advantages of imprecise Bayesianism.\n\n::: {.proof}\n*Reply.* My responses are going to be (1) that we should want more\nresilient modesty, and though this is a hard technical challenge, it's\npossible to see a way forward on it, (2) that we should want somewhat\nwider open-minded modesty, though how much wider is a hard question, and\n(3) that the third objection should simply be rejected. Let's go through\nthose in reverse order, since it's the response to the third that\nexplains part of what I'm doing in response to the other two.\n\nWhat we have in section three is a consistency proof. For the imprecise\nBayesian, unlike the precise Bayesian, being open-minded is consistent\nwith being modest. That's good, since it shows that we can't rule out a\nrational response to problems like Belot's. It's obviously true that the\nprior in question isn't rational, but that's not needed for a\nconsistency proof.\n\nMoreover, we don't just have a consistency proof, we have a constructive\nconsistency proof - the prior is described in detail. It's just not\ngoing to be possible to do a constructive proof that open-mindedness,\nmodesty and full rationality are consistent. And that's because to do\nthat would essentially be to solve all of the problems of epistemology\never. Demonstrating a fully rational prior, even for the range of\nquestions Belot considers, is too much to ask.\n\nIf there's a reasonable looking argument that imprecise Bayesians are\nunlikely to be able to satisfy some set of plausible constraints, then\nthe defender of imprecise Bayesianism is, I think, obliged to show how\nthose constraints can be satisfied. But to ask for a demonstration of\nhow all reasonable constraints can be satisfied at once, in the absence\nof a decent argument that they cannot be, would clearly be asking too\nmuch.\n\nSo I don't care that the prior I described is irrational; it serves its\npurpose in proving consistency. Now what would be nice is to show that\nsome slightly stronger constraints can be simultaneously satisfied. But\nwe have to be sure that those constraints are in fact reasonable\nconstraints. Here's one constraint that I think isn't reasonable: be\nopen-minded towards any proposition of the form ${\\boldsymbol{X}\\in S}$,\nwhere $S$ is a dense set of sequences. Let $S$, for example, be the set\nconsisting of all sequences of the form\n$\\boldsymbol{y}_k + \\boldsymbol{z}$, where $\\boldsymbol{y}_k$ ranges\nover all finite sequeneces, and $\\boldsymbol{z}$ is a particular\narbitrary sequence that lacks finite definition in our current language.\nThat set is dense, and indeed measurable. But there's no evidence that\ncould make it reasonable to take $\\boldsymbol{X}\\in S$ to be probable.\nSo a prior that wasn't open-minded towards $\\boldsymbol{X}\\in S$ could\nstill be perfectly reasonable.\n\nThat said, the prior I demonstrated is closed-minded towards several\npropositions that should be taken seriously. It will never have positive\ncredence that $\\boldsymbol{X}$ is eventually periodic without being\nperiodic, or that $\\boldsymbol{X}$ is generated by a chance process that\ngives each data point chance $c \\neq \\frac{1}{2}$ of being 0. It\nwould be good to have a prior whose open-minded modesty was wider. But\nbefore we do that technical work, I think there's a need to figure out\nwhich propositions we should be open-minded about.\n\nI am more worried by the fragility of the modesty of this prior. There's\na reasonable sense in which the prior is open-minded only in virtue of\nthe fact that it has parts which are immodest. At any point where the\nagent has credence above $\\frac{1}{2}$ that $p$, she has credence 1\nthat she will succeed.\n\nWe could try to complicate the prior a bit more to avoid that. Here's a\nsketch of how it could go, with application to one particular initial\nsequence of data. Consider what happens to $R$ if the initial input is\n$\\langle 0, 1, 0, 0, 1, 0, 0, 1\\rangle$, hereafter $\\boldsymbol{y}$.\nAccording to $\\Pr_0$, that initial sequence has probability\n$\\frac{1}{256}$. According to $\\Pr_1$, it has probability\n$\\frac{1}{63} + \\frac{1}{4095} + \\frac{1}{65535} \\approx \\frac{1}{62}$.\nSo given that initial sequence, $\\Pr_2$ says the probability of $p$ is\nabout $\\frac{4}{5}$. And since the sequence is $O$, it could be the\nstart of the the sequence $\\langle 0, 1, 0\\rangle$ repeated\nindefinitely, its probability according to $\\Pr_i$ is 0, for $i \\geq 3$.\nNow consider the set of all probability functions of the form\n$a\\Pr_{R} + b\\Pr_{New}$, where $a + b = 1$,\n$b \\in (0, \\frac{1}{256}), \\Pr_{R} \\in R$ and $\\Pr_{New}$ is the\nfunction which gives probability 1 to $\\boldsymbol{X}$ being\n$O(\\boldsymbol{y})$. That prior is open-minded, and even after\nconditionalising on $\\boldsymbol{y}$ satisfies the intermediate of the\nthree modesty conditions described on page - the probability of failure\nis less than one, though it isn't less than some number less than one.\nAnd this trick could be generalised to satisfy more modesty conditions,\nand even (though it would take some time to prove this) be\nunconditionally modest.\n\nBut I'm not going to go through those steps here. That's mostly because\nI think we already have shown enough to show that imprecise Bayesianism\nhas an advantage over precise Bayesianism. The imprecise Bayesian can,\nand the precise Bayesian can't, have an open-minded modest attitude. It\nwould be good to press home that advantage and show that there are other\nthings the imprecise Bayesian can do that the precise Bayesian can't do,\nsuch as having a widely open-minded and resiliently modest prior. But\neven before such a demonstration takes place, the advantage has been\nestablished. ◻\n:::\n\n[^1]: There is another sense of immodesty that is often discussed in the\n    literature, going back to @Lewis1971d. This is the idea that some\n    agents think their attitudes are optimal by some standards; these\n    are the immodest ones. And often, it is held that not being\n    self-endorsing in this way is a coherence failure @Elga2010-ELGHTD.\n    I don't think this kind of immodesty is rationally required, for\n    reasons set out by Miriam @Schoenfield2014 and Maria\n    @Lasonen-Aarnio2015, but in any case that's not the kind of modesty\n    that's at issue in Belot's argument.\n\n[^2]: For dogmatism, see @Pryor2000. The canonical argument that it is\n    inconsistent with Bayesianism is @White2006.\n\n[^3]: Note that this formulation leaves it open which side of the\n    biconditional is explanatorily prior. I'm going to defend a view on\n    which the left hand side, i.e., the comparative confidences, are\n    more explanatorily basic than the facts about what is in the agent's\n    representor. I say a little more about why I take this stand in\n    footnote 7. For much more detail on varieties of\n    imprecise Bayesianism, see @Walley1991, from whom I take the view\n    that the representor and its members are much less explanatorily\n    important than the comparative judgments the agent makes.\n\n[^4]: Belot lets an agent succeed if $\\boldsymbol{X}$ is periodic, and\n    the credence in $p$ never drops below $\\frac{1}{2}$, but I think\n    it's neater to say that the agent is undecided in this case.\n\n[^5]: A meagre subset of a space is any set built up as a countable\n    union of nowhere dense sets.\n\n[^6]: Belot goes into much more detail about why modesty is a good\n    requirement to put on a rational prior, but I'm omitting those\n    details since I have very little to add to what Belot says.\n\n[^7]: I have an independent\n    metaphysical reason for preferring the corporate interpretation. I\n    think that comparative confidences, things like being at least as\n    confident in $p$ as in $q$, are metaphysically prior to numerical\n    credences, or even sets of numerical credences. On such a\n    metaphysics, what it is for $\\Pr$ to be in the representor just is\n    for every $p, q, r, s$, if the agent is at least as confident in $p$\n    given $q$ as in $r$ given $s$, then $\\Pr(p | q) \\geq \\Pr(r | s)$.\n    And it seems, though I won't defend this claim here, that the\n    corporate interpetation fits more naturally with the idea that\n    comparative confidences are primitive.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
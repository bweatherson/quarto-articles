{
  "hash": "ddda2595d6ef6d9b99718041261c8663",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Games, Beliefs and Credences\"\ndescription: |\n  In previous work I've defended an interest-relative theory of belief. This paper continues the defence. I have four aims. First, to offer a new kind of reason for being unsatisfied with the simple Lockean reduction of belief to credence. Second, to defend the legitimacy of appealing to credences in a theory of belief. Third, to illustrate the importance of theoretical, as well as practical, interests in an interest-relative account of belief. And finally, to have another try at extending my basic account of belief to cover propositions that are practically and theoretically irrelevant to the agent.\ndate: March 1 2016\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ndoi: \"10.1111/phpr.12088\"\ncategories:\n  - epistemology\n  - interest-relativity\n  - games and decisions\ncitation_url: https://doi.org/10.1111/phpr.12088\njournal:\n    title: \"Philosophy and Phenomenological Research\"\n    publisher: \"Wiley\"\nvolume: 92\nnumber: 2\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: dalicat.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    number_sections: true\n---\n\n\n\nIn previous work\n[@Weatherson2005-WEACWD; @Weatherson2011-WEADIR; @Weatherson2012-WEAKBI]\nI've defended an interest-relative theory of belief. This paper\ncontinues the defence. I have four aims.\n\n<aside>\nPublished in _Philosophy and Phenomenological Research_ 92: 209-236.\n</aside>\n\n1.  To offer a new kind of reason for being unsatisfied with the simple\n    Lockean reduction of belief to credence.\n\n2.  To defend the legitimacy of appealing to credences in a theory of\n    belief.\n\n3.  To illustrate the importance of theoretical, as well as practical,\n    interests in an interest-relative account of belief.\n\n4.  To have another try at extending my basic account of belief to cover\n    propositions that are practically and theoretically irrelevant to\n    the agent.\n\nYou're probably familiar with the following dialectic. We want there to\nbe some systematic connection between credences and beliefs. At first\nblush, saying that a person believes $p$ and has a very low credence in\n$p$ isn't just an accusation of irrationality, it is literally\nincoherent. The simplest such connection would be a reduction of beliefs\nto credences. But the simplest reductions don't work.\n\n<aside>\nImage via [Meagan](https://www.flickr.com/photos/7796992@N08) via [Creative Commons](https://search.creativecommons.org/photos/326f0c20-275b-479b-98e0-aa7945751de6).\n</aside>\n\nIf we identify beliefs with credence 1, and take credences to support\nbetting dispositions, then a rational agent will have very few beliefs.\nThere are lots of things that an agent, we would normally say, believes\neven though she wouldn't bet on them at absurd odds. Note that this\nargument doesn't rely on *reducing* credences to betting dispositions;\nas long as credences support the betting dispositions, the argument goes\nthrough.\n\nA simple retreat is to the so-called **Lockean thesis**, which holds\nthat to believe that $p$ is to have credence in $p$ greater than some\nthreshold $t$, where $t < 1$. Just how the threshold is determined could\nbe a matter of some discretion. Perhaps it is a function of the agent's\nsituation, or of the person ascribing beliefs to the agent, or to the\nperson evaluating that ascription. Never mind these complexities;\nassuming all such things are held fixed, the Lockean thesis says that\nthere is a threshold $t$ such that everything with credence above $t$ is\nbelieved.\n\nThere's a simple objection to the Lockean thesis. Given some very weak\nassumptions about the world, it implies that there are plenty of\nquadruples $\\langle S, A, B, A \\wedge B \\rangle$ such that\n\n-   $S$ is a rational agent.\n\n-   $A, B$ and $A \\wedge B$ are propositions.\n\n-   $S$ believes $A$ and believes $B$.\n\n-   $S$ does not believe $A \\wedge B$.\n\n-   $S$ knows that she has all these states, and consciously\n    reflectively endorses them.\n\nNow one might think, indeed I do think, that such quadruples do not\nexist at all. But set that objection aside. If the Lockean is correct,\nthese quadruples should be everywhere. That's because for any\n$t \\in (0, 1)$ you care to pick, quadruples of the form\n$\\langle S, C, D, C \\wedge D \\rangle$ are very very common.\n\n-   $S$ is a rational agent.\n\n-   $C, D$ and $C \\wedge D$ are propositions.\n\n-   $S$'s credence in $C$ is greater than $t$, and her credence in $D$\n    is greater than $t$.\n\n-   $S$'s credence in $C \\wedge D$ is less than $t$.\n\n-   $S$ knows that she has all these states, and reflectively endorses\n    them.\n\nThe best arguments for the existence of quadruples\n$\\langle S, A, B, A \\wedge B \\rangle$ are non-constructive existence\nproofs. David @Christensen2005 for instance, argues from the existence\nof the preface paradox to the existence of these quadruples. I've\nexpressed some reservations about that argument in the past\n[@Weatherson2005-WEACWD]. But what I want to stress here is that even if\nthese existence proofs work, they don't really prove what the Lockean\nneeds. They don't show that quadruples satisfying the constraints we\nassociated with $\\langle S, A, B, A \\wedge B \\rangle$ are just as common\nas quadruples satisfying the constraints we associated with\n$\\langle S, C, D, C \\wedge D \\rangle$, for any $t$. But if the Lockean\nwere correct, they should be exactly as common.\n\nThis kind of consideration pushes some of us, well me in any case,\ntowards an interest-relative account of belief. But I'm going to set\nthat move aside to start by investigating a different objection. This\nobjection holds that the Lockean thesis could not be true, because\ncredence 1 is not *sufficient* for belief. That is, the Lockean is\ncommitted to the thesis known as *regularity*; that everything left open\nby belief gets a positive credence. I think regularity is false. That's\nhardly news, there are plenty of good arguments against it, though most\nof these involve cases with some idealisations. Timothy\n@Williamson2007-WILHPI has a compelling argument against regularity\nturning on reflections about a case involving infinite coin flips.[^1]\nI'm going to offer a 'finite' argument against regularity, which I hope\nis of independent interest, and from that conclude the Lockean is\nmistaken. There is a worry that my argument against the Lockean also\nundermines my preferred positive view, and I'll suggest an independently\nmotivated patch. I'll then turn to Richard Holton's attack on the very\nnotion of credence, which obviously would have repercussions for\nattempts to understand beliefs in terms of credences were it to succeed.\nI think it doesn't succeed, but it does show there are important and\nunderappreciated constraints on a theory of belief. I'll conclude with a\ncomparison between my preferred interest-relative account of belief, and\na recent account suggested by Jacob Ross and Mark Schroeder. The short\nversion of the comparison is that I think there's less difference\nbetween the views than Ross and Schroeder think, though naturally I\nthink what differences there are favour my view.\n\n### Playing Games with a Lockean\n\nI'm going to raise problems for Lockeans, and for defenders of\nregularity in general, by discussing a simple game. The game itself is a\nnice illustration of how a number of distinct solution concepts in game\ntheory come apart. (Indeed, the use I'll make of it isn't a million\nmiles from the use that @KohlbergMertens1986 make of it.) To set the\nproblem up, I need to say a few words about how I think of game theory.\nThis won't be at all original - most of what I say is taken from\nimportant works by Robert @Stalnaker1994\n[@Stalnaker1996; @Stalnaker1998; @Stalnaker1999]. But it is different to\nwhat I used to think, and perhaps to what some other people think too,\nso I'll set it out slowly.[^2]\n\nStart with a simple decision problem, where the agent has a choice\nbetween two acts $A_1$ and $A_2$, and there are two possible states of\nthe world, $S_1$ and $S_2$, and the agent knows the payouts for each\nact-state pair are given by the following able.\n\n::: {.center}\n  ------- ------- -------\n           $S_1$   $S_2$\n    $A_1$    4       0\n    $A_2$    1       1\n  ------- ------- -------\n:::\n\nWhat to do? I hope you share the intuition that it is radically\nunderdetermined by the information I've given you so far. If $S_2$ is\nmuch more probable than $S_1$, then $A_2$ should be chosen; otherwise\n$A_1$ should be chosen. But I haven't said anything about the relative\nprobability of those two states. Now compare that to a simple game. Row\nhas two choices, which I'll call $A_1$ and $A_2$. Column also has two\nchoices, which I'll call $S_1$ and $S_2$. It is common knowledge that\neach player is rational, and that the payouts for the pairs of choices\nare given in the following table. (As always, Row's payouts are given\nfirst.)\n\n::: {.center}\n  ------- ------- -------\n           $S_1$   $S_2$\n    $A_1$  4, 0    0, 1\n    $A_2$  1, 0    1, 1\n  ------- ------- -------\n:::\n\nWhat should Row do? This one is easy. Column gets 1 for sure if she\nplays $S_2$, and 0 for sure if she plays $S_1$. So she'll play $S_2$.\nAnd given that she's playing $S_2$, it is best for Row to play $A_2$.\n\nYou probably noticed that the game is just a version of the decision\nproblem that we discussed a couple of paragraphs ago. The relevant\nstates of the world are choices of Column. But that's fine; we didn't\nsay in setting out the decision problem what constituted the states\n$S_1$ and $S_2$. And note that we solved the problem without explicitly\nsaying anything about probabilities. What we added was some information\nabout Column's payouts, and the fact that Column is rational. From there\nwe deduced something about Column's play, namely that she would play\n$S_2$. And from that we concluded what Row should do.\n\nThere's something quite general about this example. What's distinctive\nabout game theory isn't that it involves any special kinds of decision\nmaking. Once we get the probabilities of each move by the other player,\nwhat's left is (mostly) expected utility maximisation. (We'll come back\nto whether the 'mostly' qualification is needed below.) The distinctive\nthing about game theory is that the probabilities aren't specified in\nthe setup of the game; rather, they are solved for. Apart from special\ncases, such as where one option strictly dominates another, we can't say\nmuch about a decision problem with unspecified probabilities. But we can\nand do say a lot about games where the setup of the game doesn't specify\nthe probabilities, because we can solve for them given the other\ninformation we have.\n\nThis way of thinking about games makes the description of game theory as\n'interactive epistemology' [@Aumann1999] rather apt. The theorist's work\nis to solve for what a rational agent should think other rational agents\nin the game should do. From this perspective, it isn't surprising that\ngame theory will make heavy use of equilibrium concepts. In solving a\ngame, we must deploy a theory of rationality, and attribute that theory\nto rational actors in the game itself. In effect, we are treating\nrationality as something of an unknown, but one that occurs in every\nequation we have to work with. Not surprisingly, there are going to be\nmultiple solutions to the puzzles we face.\n\nThis way of thinking lends itself to an epistemological interpretation\nof one of the most puzzling concepts in game theory, the mixed strategy.\nArguably the core solution concept in game theory is the Nash\nequilibrium. As you probably know, a set of moves is a Nash equilibrium\nif no player can improve their outcome by deviating from the\nequilibrium, conditional on no other player deviating. In many simple\ngames, the only Nash equilibria involve mixed strategies. Here's one\nsimple example.\n\n::: {.center}\n  ------- ------- -------\n           $S_1$   $S_2$\n    $A_1$  0, 1    10, 0\n    $A_2$  9, 0    -1, 1\n  ------- ------- -------\n:::\n\nThis game is reminiscent of some puzzles that have been much discussed\nin the decision theory literature, namely asymmetric Death in Damascus\npuzzles. Here Column wants herself and Row to make the 'same' choice,\ni.e., $A_1$ and $S_1$ or $A_2$ and $S_2$. She gets 1 if they do, 0\notherwise. And Row wants them to make different choices, and gets 10 if\nthey do. Row also dislikes playing $A_2$, and this costs her 1 whatever\nelse happens. It isn't too hard to prove that the only Nash equilibrium\nfor this game is that Row plays a mixed strategy playing both $A_1$ and\n$A_2$ with probability , while Column plays the mixed strategy that\ngives $S_1$ probability , and $S_2$ with probability .\n\nNow what is a mixed strategy? It is easy enough to take away form the\nstandard game theory textbooks a **metaphysical** interpretation of what\na mixed strategy is. Here, for instance, is the paragraph introducing\nmixed strategies in Dixit and Skeath's *Games of Strategy*.\n\n> When players choose to act unsystematically, they pick from among\n> their pure strategies in some random way ...We call a random mixture\n> between these two pure strategies a mixed strategy. [@DixitSkeath2004\n> 186]\n\nDixit and Skeath are saying that it is definitive of a mixed strategy\nthat players use some kind of randomisation device to pick their plays\non any particular run of a game. That is, the probabilities in a mixed\nstrategy must be in the world; they must go into the players' choice of\nplay. That's one way, the paradigm way really, that we can think of\nmixed strategies metaphysically.\n\nBut the understanding of game theory as interactive epistemology\nnaturally suggests an **epistemological** interpretation of mixed\nstrategies.\n\n> One could easily ...[model players] ...turning the choice over to a\n> randomizing device, but while it might be harmless to permit this,\n> players satisfying the cognitive idealizations that game theory and\n> decision theory make could have no motive for playing a mixed\n> strategy. So how are we to understand Nash equilibrium in model\n> theoretic terms as a solution concept? We should follow the suggestion\n> of Bayesian game theorists, interpreting mixed strategy profiles as\n> representations, not of players' choices, but of their beliefs.\n> [@Stalnaker1994 57-8]\n\nOne nice advantage of the epistemological interpretation, as noted by\nBinmore [-@Binmore2007 185] is that we don't require players to have\n$n$-sided dice in their satchels, for every $n$, every time they play a\ngame.[^3] But another advantage is that it lets us make sense of the\ndifference between playing a pure strategy and playing a mixed strategy\nwhere one of the 'parts' of the mixture is played with probability one.\n\nWith that in mind, consider the below game, which I'll call Red-Green.\nI've said something different about this game in earlier work\n[@Weatherson2012-WEAGAT]. But I now think that to understand what's\ngoing on, we need to think about mixed strategies where one element of\nthe mixture has probability one.\n\nInformally, in this game $A$ and $B$ must each play either a green or\nred card. I will capitalise $A$'s moves, i.e., $A$ can play GREEN or\nRED, and italicise $B$'s moves, i.e., $B$ can play *green* or *red*. If\ntwo green cards, or one green card and one red card are played, each\nplayer gets \\$1. If two red cards are played, each gets nothing. Each\ncares just about their own wealth, so getting \\$1 is worth 1 util. All\nof this is common knowledge. More formally, here is the game table, with\n$A$ on the row and $B$ on the column.\n\n::: {.center}\n  ------- --------- -------\n           *green*   *red*\n    GREEN   1, 1     1, 1\n      RED   1, 1     0, 0\n  ------- --------- -------\n:::\n\nWhen I write game tables like this, and I think this is the usual way\ngame tables are to be interpreted [@Weatherson2012-WEAKBI], I mean that\nthe players know that these are the payouts, that the players know the\nother players to be rational, and these pieces of knowledge are common\nknowledge to at least as many iterations as needed to solve the game.\nWith that in mind, let's think about how the agents should approach this\ngame.\n\nI'm going to make one big simplifying assumption at first. We'll relax\nthis later, but it will help the discussion to start with this\nassumption. This assumption is that the doctrine of **Uniqueness**\napplies here; there is precisely one rational credence to have in any\nsalient proposition about how the game will play. Some philosophers\nthink that Uniqueness always holds [@White2005-WHIEP]. I join with those\nsuch as @North2010 and @Schoenfield2013 who don't. But it does seem like\nUniqueness might *often* hold; there might often be a right answer to a\nparticular problem. Anyway, I'm going to start by assuming that it does\nhold here.\n\nThe first thing to note about the game is that it is symmetric. So the\nprobability of $A$ playing GREEN should be the same as the probability\nof $B$ playing *green*, since $A$ and $B$ face exactly the same problem.\nCall this common probability $x$. If $x < 1$, we get a quick\ncontradiction. The expected value, to Row, of GREEN, is 1. Indeed, the\nknown value of GREEN is 1. If the probability of *green* is $x$, then\nthe expected value of RED is $x$. So if $x < 1$, and Row is rational,\nshe'll definitely play GREEN. But that's inconsistent with the claim\nthat $x < 1$, since that means that it isn't definite that Row will play\nGREEN.\n\nSo we can conclude that $x = 1$. Does that mean we can know that Row\nwill play GREEN? No. Assume we could conclude that. Whatever reason we\nwould have for concluding that would be a reason for any rational person\nto conclude that Column will play *green*. Since any rational person can\nconclude this, Row can conclude it. So Row knows that she'll get 1\nwhether she plays GREEN or RED. But then she should be indifferent\nbetween playing GREEN and RED. And if we know she's indifferent between\nplaying GREEN and RED, and our only evidence for what she'll play is\nthat she's a rational player who'll maximise her returns, then we can't\nbe in a position to know she'll play GREEN.\n\nI think the arguments of the last two paragraphs are sound. We'll turn\nto an objection presently, but let's note how bizarre is the conclusion\nwe've reached. One argument has shown that it could not be more probable\nthat Row will play GREEN. A second argument has shown that we can't know\nthat Row will play GREEN. It reminds me of examples involving blindspots\n[@Sorensen1988]. Consider this case:\n\n(B)  Brian does not know (B).\n\nThat's true, right? Assume it's false, so I do know (B). Knowledge is\nfactive, so (B) is true. But that contradicts the assumption that it's\nfalse. So it's true. But I obviously don't know that it's true; that's\nwhat this very true proposition says.[^4]\n\nNow I'm not going to rest anything on this case, because there are so\nmany tricky things one can say about blindspots, and about the paradoxes\ngenerally. It does suggest that there are other finite cases where one\ncan properly have maximal credence in a true proposition without\nknowledge.[^5] And, assuming that we shouldn't believe things we know we\ndon't know, that means we can have maximal credence in things we don't\nbelieve. All I want to point out is that this phenomena of maximal\ncredence without knowledge, and presumably without full belief, isn't a\nquirky feature of self-reference, or of games, or of puzzles about\ninfinity; it comes up in a wide range of cases.\n\nFor the rest of this section I want to reply to one objection, and\nweaken an assumption I made earlier. The objection is that I'm wrong to\nassume that agents will only maximise expected utility. They may have\ntie-breaker rules, and those rules might undermine the arguments I gave\nabove. The assumption is that there's a uniquely rational credence to\nhave in any given situation.\n\nI argued that if we knew that $A$ would play GREEN, we could show that\n$A$ had no reason to play GREEN. But actually what we showed was that\nthe expected utility of playing GREEN would be the same as playing RED.\nPerhaps $A$ has a reason to play GREEN, namely that GREEN weakly\ndominates RED. After all, there's one possibility on the table where\nGREEN does better than RED, and none where RED does better. And perhaps\nthat's a reason, even if it isn't a reason that expected utility\nconsiderations are sensitive to.\n\nNow I don't want to insist on expected utility maximisation as the only\nrule for rational decision making. Sometimes, I think some kind of\ntie-breaker procedure is part of rationality. In the papers by Stalnaker\nI mentioned above, he often appeals to this kind of weak dominance\nreasoning to resolve various hard cases. But I don't think weak\ndominance provides a reason to play GREEN in this particular case. When\nStalnaker says that agents should use weak dominance reasoning, it is\nalways in the context of games where the agents' attitude towards the\ngame matrix is different to their attitude towards each other. One case\nthat Stalnaker discusses in detail is where the game table is common\nknowledge, but there is merely common (justified, true) belief in common\nrationality. Given such a difference in attitudes, it does seem there's\na good sense in which the most salient departure from equilibrium will\nbe one in which the players end up somewhere else on the table. And\ngiven that, weak dominance reasoning seems appropriate.\n\nBut that's not what we've got here. Assuming that rationality requires\nplaying GREEN/*green*, the players know we'll end up in the top left\ncorner of the table. There's no chance that we'll end up elsewhere. Or,\nperhaps better, there is just as much chance we'll end up 'off the\ntable', as that we'll end up in a non-equilibrium point on the table. To\nmake this more vivid, consider the 'possibility' that $B$ will play\n*blue*, and if $B$ plays *blue*, $A$ will receive 2 if she plays RED,\nand -1 if she plays GREEN. Well hold on, you might think, didn't I say\nthat *green* and *red* were the only options, and this was common\nknowledge? Well, yes, I did, but if the exercise is to consider what\nwould happen if something the agent knows to be true doesn't obtain,\nthen the possibility that one agent will play blue certainly seems like\none worth considering. It is, after all, a metaphysical possibility. And\nif we take it seriously, then it isn't true that under *any* possible\nplay of the game, GREEN does better than RED.\n\nWe can put this as a dilemma. Assume, for *reductio*, that GREEN/*green*\nis the only rational play. Then if we restrict our attention to\npossibilities that are epistemically open to $A$, then GREEN does just\nas well as RED; they both get 1 in every possibility. If we allow\npossibilities that are epistemically closed to $A$, then the possibility\nwhere $B$ plays *blue* is just as relevant as the possibility that $B$\nis irrational. After all, we stipulated that this is a case where\nrationality is common knowledge. In neither case does the weak dominance\nreasoning get any purchase.\n\nWith that in mind, we can see why we don't need the assumption of\nUniqueness. Let's play through how a failure of Uniqueness could\nundermine the argument. Assume, again for **reductio**, that we have\ncredence $\\varepsilon > 0$ that $A$ will play RED. Since $A$ maximises\nexpected utility, that means $A$ must have credence 1 that $B$ will play\n*green*. But this is already odd. Even if you think people can have\ndifferent reactions to the same evidence, it is odd to think that one\nrational agent could regard a possibility as infinitely less likely than\nanother, given isomorphic evidence. And that's not all of the problems.\nEven if $A$ has credence 1 that $B$ will play *green*, it isn't obvious\nthat playing RED is rational. After all, relative to the space of\nepistemic possibilities, GREEN weakly dominates RED. Remember that we're\nno longer assuming that it can be known what $A$ or $B$ will play. So\neven without Uniqueness, there are two reasons to think that it is wrong\nto have credence $\\varepsilon > 0$ that $A$ will play RED. So we've\nstill shown that credence 1 doesn't imply knowledge, and since the proof\nis known to us, and full belief is incompatible with knowing that you\ncan't know, this is a case where credence 1 doesn't imply full belief.\nSo whether $A$ plays GREEN, like whether the coin will ever land tails,\nis a case the Lockean cannot get right. No matter where they set the\nthreshold for belief our credence is above that threshold, but we don't\nbelieve.\n\nSo I think this case is a real problem for a Lockean view about the\nrelationship between credence and belief. If *A* is rational, she can\nhave credence 1 that *B* will play *green*, but won't believe that *B*\nwill play *green*. But now you might worry that my own account of the\nrelationship between belief and credence is in just as much trouble.\nAfter all, I said that to believe $p$ is, roughly, to have the same\nattitudes towards all salient questions as you have conditional on $p$.\nAnd it's hard to identify a question that rational *A* would answer\ndifferently upon conditionalising on the proposition that *B* plays\n*green*.\n\nI think what went wrong in my earlier view was that I'd too quickly\nequated updating with conditionalisation. The two can come apart. Here's\nan example from @Gillies2010 that makes the point well.[^6]\n\n> I have lost my marbles. I know that just one of them -- Red or Yellow\n> -- is in the box. But I don't know which. I find myself saying things\n> like ...\"If Yellow isn't in the box, the Red must be.\" (4:13)\n\nAs Gillies goes on to point out, this isn't really a problem for the\nRamsey test view of conditionals.\n\n> The Ramsey test -- the schoolyard version, anyway -- is a test for\n> when an indicative conditional is acceptable given your beliefs. It\n> says that (if *p*)(*q*) is acceptable in belief state *B* iff *q* is\n> acceptable in the derived or subordinate state\n> *B*-plus-the-information-that-*p*. (4:27)\n\nAnd he notes that this can explain what goes on with the marbles\nconditional. Add the information that Yellow isn't in the box, and it\nisn't just true, but must be true, that Red is in the box.\n\nNote though that while we can explain this conditional using the Ramsey\ntest, we can't explain it using any version of the idea that\nprobabilities of conditionals are conditional probabilities. The\nprobability that Red must be in the box is 0. The probability that\nYellow isn't in the box is not 0. So conditional on Yellow not being in\nthe box, the probability that Red must be in the box is still 0. Yet the\nconditional is perfectly assertable.\n\nThere is, and this is Gillies's key point, something about the behaviour\nof modals in the consequents of conditionals that we can't capture using\nconditional probabilities, or indeed many other standard tools. And what\ngoes for consequents of conditionals goes for updated beliefs too. Learn\nthat Yellow isn't in the box, and you'll conclude that Red must be. But\nthat learning can't go via conditionalisation; just conditionalise on\nthe new information and the probability that Red must be in the box goes\nfrom 0 to 0.\n\nNow it's a hard problem to say exactly how this alternative to updating\nby conditionalisation should work. But very roughly, the idea is that at\nleast some of the time, we update by eliminating worlds from the space\nof possibilities. This affects dramatically the probability of\npropositions whose truth is sensitive to which worlds are in the space\nof possibiilties.\n\nFor example, in the game I've been discussing, we should believe that\nrational *B* might play *red*. Indeed, the probability of that is, I\nthink, 1. And whether or not *B* might play red is highly salient; it\nmatters to the probability of whether *A* will play GREEN or RED.\nConditionalising on something that has probability 1, such as that *B*\nwill play *green*, can hardly change that probability. But updating on\nthe proposition that *B* will play *green* can make a difference. We can\nsee that by simply noting that the conditional *If B plays green, she\nmight play red* is incoherent.\n\nSo I conclude that a theory of belief like mine can handle the puzzle\nthis game poses, as long as it distinguishes between conditionalising\nand updating, in just the way Gillies suggests. To believe that *p* is\nto be disposed to not change any attitude towards a salient question on\nupdating that *p*. (Plus some bells and whistles to deal with\npropositions that are not relevant to salient questions. We'll return to\nthem below.) Updating often goes by conditionalisation, so we can often\nsay that belief means having attitudes that match unconditionally and\nconditionally on *p*. But not all updating works that way, and the\ntheory of belief needs to acknowledge this.\n\n### Holton on Credence\n\nWhile I don't agree with the Lockeans, I do endorse a lot of similar\ntheses to them about the relationship between belief and credence. These\ntheses include that both beliefs and credences exist and that the two\nare constitutively (as opposed to merely causally) connected. I differ\nfrom the Lockeans in holding that both belief and credence have\nimportant explanatory roles, and that the connection between the two\ngoes via the interests of the agent. As with most work in this area, my\nviews start off from considerations of cases much like DeRose's famous\nbank cases.[^7] Here's another contribution to the genre. I know it's an\novercrowded field, but I wanted a case that (a) is pretty realistic, and\n(b) doesn't involve the attribution (either to oneself or others) of a\npropositional attitude. In the example, X and Y are parents of a child,\nZ.\n\n> Y: This salad you bought is very good. Does it have nuts in it?\\\n> X: No. The nuttiness you're tasting is probably from the beans.\\\n> Y: Oh, so we could pack it for Z's lunch tomorrow.\\\n> X: Hang on, I better check about the nuts. Z's pre-school is very\n> fussy about nuts. One of the children there might have an allergy, and\n> it would be awful to get in trouble over her lunch.\n\nHere's what I think is going on in that exchange.[^8] At $t_2$ (I'll use\n$t_i$ for the time of the $i$'th utterance in the exchange), X believes\nthat the salad has no nuts in it. Indeed, the one word sentence \"No\"\nexpresses that belief. But by $t_4$, X has lost that belief. It would be\nfine to pack the salad for lunch if it has no nuts, but X isn't willing\nto do this for the simple reason that X no longer believes that it has\nno nuts. Moreover, this change of belief was, or at least could have\nbeen for all we've said so far, rational on X's part.\n\nThere's something a little puzzling about that. Jacob Ross and Mark\nSchroeder [-@SchroederRoss2012] voice a common intuition when they say\nthat beliefs should only change when new evidence comes in. Indeed, they\nuse this intuition as a key argument against my view of belief. But X\ndoesn't get any evidence that bears on the nuttiness of the salad. Yet X\nrationally changes beliefs. So I just conclude that sometimes we can\nchange beliefs without new evidence coming in; sometimes our interests,\nbroadly construed, change, and that is enough to change beliefs.\n\nWe'll come back to Ross and Schroeder's arguments in the next section,\nbecause first I want to concede something to the view that only evidence\nchanges beliefs. That view is false, but there might be a true view in\nthe area. And that's the view that only change in evidence can change\n*credences*. But that view only makes sense if there are such things as\ncredences, and that's something that Richard @Holton2013 has recently\nlaunched an intriguing argument against.\n\nHolton's broader project is a much more sweeping attack on the Lockean\nthesis than I have proposed. Actually, it is a pair of more sweeping\nattacks. One of the pair is that the Lockeans identify something that\nexists, namely belief, with something that doesn't, namely high\ncredence. I would not, could not, sign up for that critique. But I am\nmuch more sympathetic to the other attack in the pair, namely that\ncredences and beliefs have very different dynamics.\n\nCredences are, by their nature, exceedingly unstable. Whether an agent's\ncredence that $p$ is above or below any number *x* is liable to change\naccording to any number of possible changes in evidence. But, at least\nif the agent is rational, beliefs are not so susceptible to change.\nHolton thinks that rational agents, or at least rational humans,\nfrequently instantiate the following pattern. They form a belief that\n$p$, on excellent grounds. They later get some evidence that $\\neg p$.\nThe evidence is strong enough that, had they had it to begin with, they\nwould have remained uncertain about $p$. But they do not decide to\nreopen the investigation into whether $p$. They hold on to their belief\nthat $p$, the matter having been previously decided.\n\nSuch an attitude might look like unprincipled dogmatism. But it need not\nbe, I think, as long as four criteria are met. (I think Holton agrees\nwith these criteria.) One is that the agent's willingness to reopen the\nquestion of whether $p$ must increase. She must be more willing, in the\nlight of yet more evidence against $p$, to consider whether $p$ is\nreally true. A second is that, should the agent (irrationally) reopen\nthe question of whether $p$, she should not use the fact that she\npreviously closed that question as evidence. Once the genie is out of\nthe box, only reasoning about $p$ can get it back in. A third is that\nthe costs of the inquiry must be high enough to warrant putting it off.\nIf simply turning one's head fifteen degrees to the left will lead to\nacquiring evidence that definitively settles whether $p$, it is a little\ndogmatic to refuse to do so in the face of evidence against one's\npreviously formed opinion that $p$. And finally, the costs of being\nwrong about $p$ must not be too high. X, in our little dialogue above,\nwould be terribly dogmatic if they didn't reopen the question of whether\nthe salad had nuts in it, on being informed that this information was\nbeing used in a high stakes inquiry.\n\nSo beliefs should have a kind of resilience. Credences, if they exist,\nshould not have this kind of resilience. So this suggests that a simple\nreduction of belief to credence, as the Lockeans suggest, cannot be\nright. You might worry that things are worse, that no reduction of\nbelief to credence can be compatible with the difference in resilience\nbetween belief and credence. We'll return to that point, because first I\nwant to look at Holton's stronger claim: that there are no such things\nas credences.\n\nHolton acknowledges, as of course he must, that we have probabilistic\ntruth-directed attitudes. We can imagine a person, call her Paula, who\nthinks it's likely that Richard III murdered his nephews, for instance.\nBut Holton offers several reasons for thinking that in these\nprobabilistic truth-directed attitudes, the probability goes in the\ncontent, not in the attitude. That is, we should interpret Paula as\nbelieving the probabilistic claim, Richard III probably murdered his\nnephews, and not as having some graded attitude towards the simple\nproposition *Richard III murdered his nephews*. More precisely, Holton\nthinks we should understand Paula's *explicit* attitudes that way, and\nthat independent of having reason to think that agents explicitly have\nprobabilistic attitudes, there's no good way to make sense of the claim\nthat they implicitly have probabilistic attitudes. So there's no such\nthing as credences, as usually understood. Or, at least, there's no good\nsense to be made of the claim that there are credences.\n\nIn response, I want to make six points.\n\n1.  Holton is right about cases like Paula's, and the possibility of\n    iterating terms like *probably* provides independent support for\n    this view.\n\n2.  Beliefs like the one Paula has are odd; they seem to have very\n    strange truth conditions.\n\n3.  Our theory of mind needs some mechanism for explaining the\n    relationship between confidence and action.\n\n4.  The 'explanatory gap' here could be filled by positing a binary\n    attitude *is more confident that*.\n\n5.  This binary attitude can do all the work that graded attitudes were\n    posited to do, and in a (historically sensitive) way saves the\n    credences story.\n\n6.  Credences (or at least confidences) can have a key role within a\n    Holton-like story about graded belief. They can both explain why\n    agents reconsider some beliefs, and provide a standard of\n    correctness for decisions to reconsider.\n\nLet's take those in order.\n\nI'm not going to rehearse Holton's argument for the 'content view': that\nin cases like Paula's the content of her attitude, and not the attitude\nitself, is probabilistic. But I do want to offer one extra consideration\nin its favour. (I'm indebted here to work in progress by my colleague\nSarah @MossPragmaticsEpistemicModals, though I'm not sure she'd approve\nof this conclusion.) As well as Paula, we can imagine a person Pip who\nisn't sure that Paula is right, but thinks she's probably right. That\nis, Pip thinks that Richard III probably probably murdered his nephews.\nIt's easy to make sense of Pip on the content view. Modalities in\npropositions iterate smoothly; that's what they are designed to do. But\nit's much harder to iterate attitudes. The possibility of cases like Pip\nsuggests Holton must be right about Paula's case.\n\nBut Paula's case is odd. Beliefs have truth conditions. What are the\ntruth conditions for Paula's belief? On the one hand, it seems they must\nbe sensitive to her evidence. If she later acquires conclusive evidence\nthat Richard III was framed, she won't think her earlier self had a\nfalse belief. But if we put the evidence into the content of the belief,\nwe get the strange result that her belief can't be preserved by uttering\nthe same words to herself over again. That is, if the content of Paula's\nbelief is *Given the evidence I have now, Richard III likely murdered\nhis nephews*, she can't have the very same belief tomorrow by retaining\nthe thought *Richard III likely murdered his nephews*. And she can't\nhave a belief with the same content as anyone else by the two of them\nboth thinking *Richard III likely murdered his nephews*. Those seem like\nunhappy conclusions, especially in the midst of a project that wants to\nemphasise the resiliency of belief. So perhaps we should say, following\n@Stephenson2007 or @MacFarlane2011, that the truth conditions of the\nbelief are agent-relative. Or, if we're unhappy with the MacFarlane\nstory, we might be pushed towards a kind of expressivism (perhaps a la\n@Yalcin2011), which isn't quite like either the content view or the\nattitude view that Holton discusses. I'm personally partial to the\nrelativist view, but I don't want to argue for that here, just note that\nthe content view raises some interesting problems, and that natural\nsolutions to them could in a way blur the boundaries between the content\nand attitude views.\n\nAs Holton notes in his discussion of Brutus, when our confidence in a\nproposition changes, our actions will change. Paula gets a little\nevidence that Richard III was framed, and her actions may change. Of\ncourse, not much of what we do in everyday life is sensitive to facts\nabout English royal history, but there may be some effects. Maybe she'll\nbe less inclined to speak up if the topic of the princes' murder comes\nup, or she'll take a slightly more jaundiced view of Shakespeare's play\n(compare @Friend2003.) Holton says that these falling confidences need\nnot have all the precise structure of credences. In particular, they may\nnot have the topology of the interval $[0, 1]$. But lots of credence\nlovers think that's too demanding. There's a long tradition of thinking\nthat credences need not all be comparable.[^9] What's important is that\nthe relative confidences exist, and that they have a robust relationship\nto action.\n\nThere's an old fashioned way of doing this. The idea is implicit in\n@RamseyTruthProb, and made central in @DeFinetti1964. Take the binary\nattitude *is more confident that p than q* as primitive. As Holton\nnotes, surface structure of our attitude reports suggest that this\nattitude, unlike the graded attitude of credence, is part of folk\npsychology. Lay down some constraints on this attitude. To get enough\nconstraints that the binary relation determines a unique probability\nfunction, the constraints will have to be very tight. In particular,\nyou'll need some kind of Archimedean principle, and a principle of\nuniversal comparability. Those aren't very plausible, especially the\nsecond. But even weaker constraints will get you something interesting.\nIn particular, it isn't hard to lay down enough constraints that there\nis a unique set $S$ of probability functions such that the agent is more\nconfident that $p$ than $q$ just in case $\\Pr( p) > \\Pr(q)$ for all\n$\\Pr \\in S$. (For much more detail, see for instance @Walley1991.)\n\nIn that way, we can derive credences from the relative confidences of a\nreasonably coherent agent. But we can do with even less coherence than\nthat I think. A throwaway remark from @Ramsey1929 provides a key clue.\nWhat is it to have credence $\\frac{2}{3}$ in $p$? Don't say it's a\nbetting disposition; mental states and behavioural dispositions aren't\nthat tightly linked. Here's Ramsey's idea. To have credence\n$\\frac{2}{3}$ in $p$ is to be exactly as confident in $p$ as in\n$q \\vee r$, where $q, r$ and $s$ are taken to be exclusive and\nexhaustive, and one has equal confidence in all three. It's easy to see\nhow to extend that to a definition of credence $\\frac{m}{n}$ for any\ninteger $m, n$. It's a little trickier to say precisely what, say,\ncredence $\\frac{1}{\\pi}$ is, but rational credences are probably\ncredences enough to explain action. And just like that, we have a way of\ntalking about credences, i.e., graded attitudes, without positing\nanything more than a binary attitude *more confident than*.\n\nPerhaps Holton could argue that we only have unary attitudes, not binary\nattitudes like *more confident than*. If Maury is more confident that\nOswald shot JFK than that Richard III murdered his nephews, that means\nhe really believes the proposition *It is more likely that Oswald shot\nJFK than that Richard III murdered his nephews*. But such a view seems\nforced at best, and isn't motivated by Holton's other arguments for the\n'content view'. This attitude of *more confident than* isn't iterable.\nIt isn't subject to the particular kind of reasoning errors that Holton\ntakes to be evidence for the content view in the probabilistic case. It\nis an attitude we ordinarily report as a binary attitude in normal\nspeech. In short, it looks like a genuine binary attitude.\n\nGiven that the binary attitude exists, and that we can define numerical\n(at least rational) credences in terms of it, I'd say that's enough to\nsay that credences exist. In a sense, credences will be epiphenomenal.\nWhat does the explanatory work is the binary relation *more confident\nthat*. Maury might stay away from a showing of *Richard III* because he\nis less confident that it is historically accurate than he used to be.\nWe can work out from Maury's other relative confidences what his\ncredence in Richard III's guilt is and was. Or, at least, we can work\nout bounds on these. But those numbers aren't in a fundamental sense\nexplanatory, and neither are the complicated sets of relative\nconfidences that constitute the numbers. What's *really* explanatory are\nrelative confidences. But it's a harmless enough mode of speech to talk\nas if credences are explanatory; they are easier to talk about than the\nunderlying relative confidences.\n\n### The Power of Theoretical Interests\n\nSo I think we should accept that credences exist. And we can just about\nreduce beliefs to credences. In previous work I argued that we could do\nsuch a reduction. I'm not altogether sure whether the amendments to that\nview I'm proposing here means it no longer should count as a reductive\nview; we'll come back to that question in the conclusion.\n\nThe view I defended in previous work is that the reduction comes through\nthe relationship between conditional and unconditional attitudes. Very\nroughly, to believe that *p* is simply to have the same attitudes,\ntowards all salient questions, unconditionally as you have conditional\non *p*. In a syrupy slogan, belief means never having to say you've\nconditionalised. For reasons I mentioned in section 1, I now think that\nwas inaccurate; I should have said that belief means never having to say\nyou've updated, or at least that you've updated your view on any salient\nquestion.\n\nThe restriction to salient questions is important. Consider any *p* that\nI normally take for granted, but such that I wouldn't bet on it at\ninsane odds. I prefer declining such a bet to taking it. But conditional\non *p*, I prefer taking the bet. So that means I don't believe any such\n*p*. But just about any *p* satisfies that description, for at least\nsome 'insane' odds. So I believe almost nothing. That would be a\n*reductio* of the position. I respond by saying that the choice of\nwhether to take an insane bet is not normally salient.\n\nBut now there's a worry that I've let in too much. For many *p*, there\nis no salient decision that they even bear on. What I would do\nconditional on *p*, conditional on $\\neg p$, and unconditionally is\nexactly the same, over the space of salient choices. (And this isn't a\ncase where updating and conditionalising come apart; I'll leave this\nproviso mostly implicit from now on.) So with the restriction in place,\nI believe *p* and $\\neg p$. That seems like a *reductio* of the view\ntoo. I probably do have inconsistent beliefs, but not in virtue of *p*\nbeing irrelevant to me right now. I've changed my mind a little about\nwhat the right way to avoid this problem is, in part because of some\narguments by Jacob Ross and Mark Schroeder.\n\nThey have what looks like, on the surface, a rather different view to\nmine. They say that to believe *p* is to have a **default reasoning\ndisposition** to use *p* in reasoning. Here's how they describe their\nown view.\n\n> What we should expect, therefore, is that for some propositions we\n> would have a *defeasible* or *default* disposition to treat them as\n> true in our reasoning--a disposition that can be overridden under\n> circumstances where the cost of mistakenly acting as if these\n> propositions are true is particularly salient. And this expectation is\n> confirmed by our experience. We do indeed seem to treat some uncertain\n> propositions as true in our reasoning; we do indeed seem to treat them\n> as true automatically, without first weighing the costs and benefits\n> of so treating them; and yet in contexts such as High where the costs\n> of mistakenly treating them as true is salient, our natural tendency\n> to treat these propositions as true often seems to be overridden, and\n> instead we treat them as merely probable.\n>\n> But if we concede that we have such defeasible dispositions to treat\n> particular propositions as true in our reasoning, then a hypothesis\n> naturally arises, namely, that beliefs consist in or involve such\n> dispositions. More precisely, at least part of the functional role of\n> belief is that believing that *p* defeasibly disposes the believer to\n> treat *p* as true in her reasoning. Let us call this hypothesis the\n> *reasoning disposition account* of belief. [@SchroederRoss2012 9-10]\n\nThere are, relative to what I'm interested in, three striking\ncharacteristics of Ross and Schroeder's view.\n\n1.  Whether you believe *p* is sensitive to how you reason; that is,\n    your theoretical interests matter.\n\n2.  How you would reason about some questions that are not live is\n    relevant to whether you believe *p*.\n\n3.  Dispositions can be masked, so you can believe *p* even though you\n    don't actually use *p* in reasoning now.\n\nI think they take all three of these points to be reasons to favour\ntheir view over mine. As I see it, we agree on point 1 (and I always had\nthe resources to agree with them), I can accommodate point 2 with a\nmodification to my theory, and point 3 is a cost of their theory, not a\nbenefit. Let's take those points in order.\n\nThere are lots of reasons to dislike what Ross and Schroeder call\n*Pragmatic Credal Reductionism* (PCR). This is, more or less, the view\nthat the salient questions, in the sense relevant above, are just those\nwhich are practically relevant to the agent. So to believe $p$ just is\nto have the same attitude towards all practically relevant questions\nunconditionally as conditional on $p$. There are at least three reasons\nto resist this view.\n\nOne reason comes from the discussions of Ned Block's example Blockhead\nÂ [@Block1978]. As Braddon-Mitchell and Jackson point out, the lesson to\ntake from that example is that beliefs are constituted in part by their\nrelations to other mental states Â [@DBMJackson2007 114ff]. There's a\nquick attempted refutation of PCR via the Blockhead case which doesn't\nquite work. We might worry that if all that matters to belief given PCR\nis how it relates to action, PCR will have the implausible consequence\nthat Blockhead has a rich set of beliefs. That isn't right; PCR is\ncompatible with the view that Blockhead doesn't have credences, and\nhence doesn't have credences that constitute beliefs. But the Blockhead\nexample's value isn't exhausted by its use in quick refutations.[^10]\nThe lesson is that beliefs are, by their nature, interactive. It seems\nto me that PCR doesn't really appreciate that lesson.\n\nAnother reason comes from recent work by Jessica @Brown2013. Compare\nthese two situations.\n\n1.  *S* is in circumstances *C*, and has to decide whether to do *X*.\n\n2.  *S* is in completely different circumstances to *C*, but is\n    seriously engaged in planning for future contingencies. She's\n    currently trying to decide whether in circumstances *C* to do *X*.\n\nIntuitively, *S* can bring exactly the same evidence, knowledge and\nbeliefs to bear on the two problems. If *C* is a particularly high\nstakes situation, say it is a situation where one has to decide what to\nfeed someone with a severe peanut allergy, then a lot of things that can\nordinarily be taken for granted cannot, in this case, be taken for\ngranted. And that's true whether *S* is actually in *C*, or she is just\nplanning for the possibility that she finds herself in *C*.\n\nSo I conclude that both practical and theoretical interests matter for\nwhat we can take for granted in inquiry. The things we can take for\ngranted into a theoretical inquiry into what to do in high stakes\ncontexts as restricted, just as they are when we are in a high stakes\ncontext, and must make a practical decision. Since the latter\nrestriction on what we can take for granted is explained by (and\npossibly constituted by) a restriction on what we actually believe in\nthose contexts, we should similarly conclude that agents simply believe\nless when they are reasoning about high stakes contexts, whatever their\nactual context.\n\nA third reason to dislike PCR comes from the 'Renzi' example in Ross and\nSchroeder's paper. I'll run through a somewhat more abstract version of\nthe case, because I don't think the details are particularly important.\nStart with a standard decision problem. The agent knows that X is better\nto do if *p*, and Y is better to do if $\\neg p$. The agent should then\ngo through calculating the relative gains to doing X or Y in the\nsituations they are better, and the probability of *p*. But the agent\nimagined doesn't do that. Rather, the agent divides the possibility\nspace in four, taking the salient possibilities to be\n$p \\wedge q, p \\wedge \\neg q, \\neg p \\wedge q$ and\n$\\neg p \\wedge \\neg q$, and then calculates the expected utility of X\nand Y accordingly. This is a bad bit of reasoning on the agent's part.\nIn the cases we are interested in, *q* is exceedingly likely. Moreover,\nthe expected utility of each act doesn't change a lot depending on *q*'s\ntruth value. So it is fairly obvious that we'll end up making the same\ndecision whether we take the 'small worlds' in our decision model to be\njust the world where *p*, and the world where $\\neg p$, or the four\nworlds this agent uses. But the agent does use these four, and the\nquestion is what to say about them.\n\nRoss and Schroeder say that such an agent should not be counted as\nbelieving that *q*. If they are consciously calculating the probability\nthat *q*, and taking $\\neg q$ possibilities into account when\ncalculating expected utilities, they regard *q* as an open question. And\nregarding *q* as open in this way is incompatible with believing it. I\nagree with all this.\n\nThey also think that PCR implies that the agent *does* believe *q*. The\nreason is that conditionalising on *q* doesn't change the agent's\nbeliefs about any practical question. I think that's right too, at least\non a natural understanding of what 'practical' is.\n\nMy response to all these worries is to say that whether someone believes\nthat *p* depends not just on how conditionalising (or more generally\nupdating) on *p* would affect someone's action, but on how it would\naffect their reasoning. That is, just as we learned from the Blockhead\nexample, to believe that *p* requires having a mental state that is\nconnected to the rest of one's cognitive life in roughly the way a\nbelief that *p* should be connected. Let's go through both the last two\ncases to see how this works on my theory.\n\nOne of the things that happens when the stakes go up is that\nconditionalising on very probable things can change the outcome of\ninteresting decisions. Make the probability that some nice food is\npeanut-free be high, but short of one. Conditional on it being\npeanut-free, it's a good thing to give to a peanut-allergic guest. But\nunconditionally, it's a bad thing to give to such a guest, because the\nniceness of the food doesn't outweigh the risk of killing them. And\nthat's true whether the guest is actually there, or you're just thinking\nabout what to do should such a guest arrive in the future. In general,\nthe same questions will be relevant whether you're in *C* trying to\ndecide whether to do *X*, or simply trying to decide whether to *X* in\n*C*. In one case they will be practically relevant questions, in the\nother they will be theoretically relevant questions. But this feels a\nlot like a distinction without a difference, since the agent should have\nsimilar beliefs in the two cases.\n\nThe same response works for Ross and Schroeder's case. The agent was\ntrying to work out the expected utility of X and Y by working out the\nutility of each action in each of four 'small worlds', then working out\nthe probability of each of these. Conditional on *q*, the probability of\ntwo of them ($p \\wedge \\neg q, \\neg p \\wedge \\neg q$), will be 0.\nUnconditionally, this probability won't be 0. So the agent has a\ndifferent view on some question they have taken an interest in\nunconditionally to their view conditional on *q*. So they don't believe\n*q*. The agent shouldn't care about that question, and conditional on\neach question they should care about, they have the same attitude\nunconditionally and conditional on *q*. But they do care about these\nprobabilistic questions, so they don't believe *q*.\n\nSo I think that Ross and Schroeder and I agree on point 1; something\nbeyond practical interests is relevant to belief.\n\nThey have another case that I think does suggest a needed revision to my\ntheory. I'm going to modify their case a little to change the focus a\nlittle, and to avoid puzzles about vagueness. (What follows is a version\nof their example about DalÄ±Ì's moustache, purged of any worries about\nvagueness, and without the focus on consistency. I don't think the\nproblem they true to press on me, that my theory allows excessive\ninconsistency of belief among rational agents, really sticks. Everyone\nwill have to make qualifications to consistency to deal with the preface\nparadox, and for reasons I went over in Â [@Weatherson2005-WEACWD], I\nthink the qualifications I make are the best ones to make.)\n\nLet *D* be the proposition that the number of games the Detroit Tigers\nwon in 1976 (in the MLB regular season) is not a multiple of 3. At most\ntimes, *D* is completely irrelevant to anything I care about, either\npractically or theoretically. My attitudes towards any relevant question\nare the same unconditionally as conditional on *D*. So there's a worry\nthat I'll count as believing *D*, and believing $\\neg D$, by default.\n\nIn earlier work, I added a clause meant to help with cases like this. I\nsaid that for determining whether an agent believes that *p*, we should\ntreat the question of whether *p*'s probability is above or below 0.5 as\nsalient, even if the agent doesn't care about it. Obviously this won't\nhelp with this particular case. The probability of *D* is around , and\nis certainly above 0.5. My 'fix' avoids the consequence that I\nimplausibly count as believing $\\neg D$. But I still count, almost as\nimplausibly, as believing *D*. This needs to be fixed.\n\nHere's my proposed change. For an agent to count as believing *p*, it\nmust be possible for *p* to do some work for them in reasoning. Here's\nwhat I mean by work. Consider a very abstract set up of a decision\nproblem, as follows.\n\n::: {.center}\n  --- ----- -----\n       *p*   *q*\n    X   4     1\n    Y   3     2\n  --- ----- -----\n:::\n\nThat table encodes a lot of information. It encodes that $p \\vee q$ is\ntrue; otherwise there are some columns missing. It encodes that the only\nlive choices are X or Y; otherwise there are rows missing. It encodes\nthat doing X is better than doing Y if *p*, and worse if *q*.\n\nFor any agent, and any decision problem, there is a table like this that\nthey would be disposed to use to resolve that problem. Or, perhaps,\nthere are a series of tables and there is no fact about which of them\nthey would be most disposed to use.\n\nGiven all that terminology, here's my extra constraint on belief. To\nbelieve that *p*, there must be some decision problem such that some\ntable the agent would be disposed to use to solve it encodes that *p*.\nIf there is no such problem, the agent does not believe that *p*. For\nanything that I intuitively believe, this is an easy condition to\nsatisfy. Let the problem be whether to take a bet that pays 1 if *p*,\nand loses 1 otherwise. Here's the table I'd be disposed to use to solve\nthe problem.\n\n::: {.center}\n  ------------- -----\n                 *p*\n       Take bet   1\n    Decline bet   0\n  ------------- -----\n:::\n\nThis table encodes that *p*, so it is sufficient to count as believing\nthat *p*. And it doesn't matter that this bet isn't on the table. I'm\ndisposed to use this table, so that's all that matters.\n\nBut might there be problems in the other direction. What about an agent\nwho, if offered such a bet on *D*, would use such a simple table? I\nsimply say that they believe that *D*. I would not use any such table.\nI'd use this table.\n\n::: {.center}\n  ------------- ----- ----------\n                 *D*   $\\neg D$\n       Take bet   1      --1\n    Decline bet   0       0\n  ------------- ----- ----------\n:::\n\nNow given the probability of *D*, I'd still end up taking the bet; it\nhas an expected return of . (Well, actually I'd probably decline the bet\nbecause being offered the bet would change the probability of *D* for\nreasons made clear in Â @RunyonGuysDolls [14--15]. But that hardly\nundermines the point I'm making.) But this isn't some analytic fact\nabout me, or even I think some respect in which I'm obeying the dictates\nof rationality. It's simply a fact that I wouldn't take *D* for granted\nin any inquiry. And that's what my non-belief that *D* consists in.\n\nThis way of responding to the Tigers example helps respond to a nice\nobservation that Ross and Schroeder make about correctness. A belief\nthat *p* is, in some sense, *incorrect* if $\\neg p$. It isn't altogether\nclear how to capture this sense given a simple reduction of beliefs to\ncredences. I propose to capture it using this idea that decision tables\nencode propositions. A table is incorrect if it encodes something that's\nfalse. To believe something is, *inter alia*, to be disposed to use a\ntable that encodes it. So if it is false, it involves a disposition to\ndo something incorrect.\n\nIt also helps capture Holton's observation that beliefs should be\nresilient. If someone is disposed to use decision tables that encode\nthat *p*, that disposition should be fairly resilient. And to the extent\nthat it is resilient, they will satisfy all the other clauses in my\npreferred account of belief. So anyone who believes *p* should have a\nresilient belief that *p*.\n\nThe last point is where I think my biggest disagreement with Ross and\nSchroeder lies. They think it is very important that a theory of belief\nvindicate a principle they call **Stability**.\n\n> **Stability**: A fully rational agent does not change her beliefs\n> purely in virtue of an evidentially irrelevant change in her credences\n> or preferences. (20)\n\nHere's the kind of case that is meant to motivate Stability, and show\nthat views like mine are in tension with it.\n\n> Suppose Stella is extremely confident that steel is stronger than\n> Styrofoam, but she's not so confident that she'd bet her life on this\n> proposition for the prospect of winning a penny. PCR implies,\n> implausibly, that if Stella were offered such a bet, she'd cease to\n> believe that steel is stronger than Styrofoam, since her credence\n> would cease to rationalize acting as if this proposition is true. (22)\n\nRoss and Schroeder's own view is that if Stella has a defeasible\ndisposition to treat as true the proposition that steel is stronger than\nStyrofoam, that's enough for her to believe it. And that can be true if\nthe disposition is not only defeasible, but actually defeated in the\ncircumstances Stella is in. This all strikes me as just as implausible\nas the failure of Stability. Let's go over its costs.\n\nThe following propositions are clearly not mutually consistent, so one\nof them must be given up. We're assuming that Stella is facing, and\nknows she is facing, a bet that pays a penny if steel is stronger than\nStyrofoam, and costs her life if steel is not stronger than Styrofoam.\n\n1.  Stella believes that steel is stronger than Styrofoam.\n\n2.  Stella believes that if steel is stronger than Styrofoam, she'll win\n    a penny and lose nothing by taking the bet.\n\n3.  If 1 and 2 are true, and Stella considers the question of whether\n    she'll win a penny and lose nothing by taking the bet, she'll\n    believe that she'll win a penny and lose nothing by taking the bet.\n\n4.  Stella prefers winning a penny and losing nothing to getting\n    nothing.\n\n5.  If Stella believes that she'll win a penny and lose nothing by\n    taking the bet, and prefers winning a penny and losing nothing to\n    getting nothing, she'll take the bet.\n\n6.  Stella won't take the bet.\n\nIt's part of the setup of the problem that 2 and 4 are true. And it's\ncommon ground that 6 is true, at least assuming that Stella is rational.\nSo we're left with 1, 3 and 5 as the possible candidates for falsehood.\n\nRoss and Schroeder say that it's implausible to reject 1. After all,\nStella believed it a few minutes ago, and hasn't received any evidence\nto the contrary. And I guess rejecting 1 isn't the most intuitive\nphilosophical conclusion I've ever drawn. But compare the alternatives!\n\nIf we reject 3, we must say that Stella will simply refuse to infer *r*\nfrom *p*, *q* and $(p \\wedge q) \\rightarrow r$. Now it is notoriously\nhard to come up with a general principle for closure of beliefs. But it\nis hard to see why this particular instance would fail. And in any case,\nit's hard to see why Stella wouldn't have a general, defeasible,\ndisposition to conclude *r* in this case, so by Ross and Schroeder's own\nlights, it seems 3 should be acceptable.\n\nThat leaves 5. It seems on Ross and Schroeder's view, Stella simply must\nviolate a very basic principle of means-end reasoning. She desires\nsomething, she believes that taking the bet will get that thing, and\ncome with no added costs. Yet, she refuses to take the bet. And she's\nrational to do so! At this stage, I think I've lost what's meant to be\nbelief-like about their notion of belief. I certainly think attributing\nthis kind of practical incoherence to Stella is much less plausible than\nattributing a failure of Stability to her.\n\nPut another way, I don't think presenting Stability on its own as a\ndesideratum of a theory is exactly playing fair. The salient question\nisn't whether we should accept or reject Stability. The salient question\nis whether giving up Stability is a fair price to pay for saving basic\ntenets of means-end rationality. And I think that it is. Perhaps there\nwill be some way of understanding cases like Stella's so that we don't\nhave to choose between theories of belief that violate Stability\nconstraints, and theories of belief that violate coherence constraints.\nBut I don't see one on offer, and I'm not sure what such a theory could\nlook like.\n\nI have one more argument against Stability, but it does rest on somewhat\ncontentious premises. There's often a difference between the best\n*methodology* in an area, and the correct *epistemology* of that area.\nWhen that happens, it's possible that there is a good methodological\nrule saying that if such-and-such happens, re-open a certain inquiry.\nBut that rule need not be epistemologically significant. That is, it\nneed not be the case that the happening of such-and-such provides\nevidence against the conclusion of the inquiry. It just provides a\nreason that a good researcher will re-open the inquiry. And, as we've\nstated above, an open inquiry is incompatible with belief.\n\nHere's one way that might happen. Like other non-conciliationists about\ndisagreement, e.g., Â @Kelly2010-KELPDA, I hold that disagreement by\npeers with the same evidence as you doesn't provide *evidence* that you\nare wrong. But it might provide an excellent reason to re-open an\ninquiry. We shouldn't draw conclusions about the methodological\nsignificance of disagreement from the epistemology of disagreement. So\nlearning that your peers all disagree with a conclusion might be a\nreason to re-open inquiry into that conclusion, and hence lose belief in\nthe conclusion, without providing evidence that the conclusion is false.\nThis example rests on a very contentious claim about the epistemology of\ndisagreement. But any gap that opens up between methodology and\nepistemology will allow such an example to be constructed, and hence\nprovide an independent reason to reject Stability.\n\n### Conclusion\n\nYou might well worry that the view here is too *complex* to really be a\ntheory of belief. Belief is a simple state; why all the epicycles? This\nis a good question, and I'm not sure I have a sufficiently good answer\nto it.\n\nAt heart, the theory I've offered here is simple. To believe *p* is to\ntake *p* for granted, to take it as given, to take it as a settled\nquestion. But one doesn't take a question as settled in a vacuum. I will\ntake some questions as settled in some circumstances and not others.\nIt's here that the complexities enter in.\n\nTo believe *p*, it isn't necessary that we take it as settled in all\ncontexts. That would mean that anything one believes one would bet on at\nany odds. But it isn't sufficient to take it as settled in some context\nor other. If I'm facing a tricky bet on *p*, the fact that I'd take *p*\nas settled in some other context doesn't mean that I believe *p*. After\nall, I might even decline the bet, although I desire the reward for\nwinning the bet, and believe that if *p* I will win. And we can't just\nfocus on the actual circumstances. Five minutes ago, I neither took it\nas settled or as open that the Cubs haven't won the World Series for\nquite a while. I simply wasn't thinking about that proposition, and\ndidn't really take it to be one thing or another.\n\nThis is why things get so complex. To believe *p* is to hold a fairly\nsimple attitude towards *p* in some relevant circumstances. But which\ncircumstances? That's what's hard to say, and it's why the theory is so\nmessy. And I think we have an argument that it must be a little hard to\nsay, namely an argument by exhaustion of all the possible simple things\nto say. The previous paragraph starts such an argument.\n\nI'd be a little surprised if the account here is the best or last word\non the matter though. It does feel a little disjunctive, as if there is\na simpler reduction to be had. But I think it's better than what came\nbefore, so I'm putting it forward.\n\nThe previous version of the theory I put forward was clearly reductive;\nbeliefs were reduced to credences and preferences. This version is not\nquite as clearly reductive. Which decision tables the agent is disposed\nto use, and which propositions those tables encode, are not obviously\nfacts about credences and preferences. So it feels like I've given up on\nthe reductive project.\n\nI'm not altogether happy about this; reduction is a good aim to have.\nBut if reduction of belief to other states fails, I'd think this kind of\nreason is why it is going to fail. Facts about how an agent\nconceptualises a problem, how she sets up the decision table, are\ndistinct from facts about which values she writes into the table. This\nis the deepest reason why the Lockean theory is false. Belief is not the\ndifference between one column in the decision table getting probability\n0.98 rather than 0.97; it is the difference between one column being\nexcluded rather than included. If that difference can't be accounted for\nin terms of actual credences and preferences, the reductionist project\nwill fail.\n\n[^1]: If there's any gap in Williamson's argument, it is I think at the\n    point where he concludes that any two infinite sequences of coin\n    flips have the same probability of landing all heads. I think that\n    the defender of non-numerical, comparative approaches to probability\n    can deny that with some plausibility. Perhaps the two sequences of\n    coin flips have *incomparable* probabilities of landing all heads.\n    But this leads us into complications that are irrlevant to this\n    paper, especially since I think it turns out there is a sound\n    Williamsonian argument against the Lockean who lets different\n    sequences have incomparable probabilities. For a more pessimistic\n    take on Williamson's argument, see @Weintraub2008.\n\n[^2]: I'm grateful to the participants in a game theory seminar at ArchÃ©\n    in 2011, especially Josh Dever and Levi Spectre, for very helpful\n    discussions that helped me see through my previous confusions.\n\n[^3]: Actually, I guess it is worse than if some games have the only\n    equilibria involving mixed strategies with irrational probabilities.\n    And it might be noted that Binmore's introduction of mixed\n    strategies, on page 44 of his [-@Binmore2007], sounds much more like\n    the metaphysical interpretation. But I think the later discussion is\n    meant to indicate that this is just a heuristic introduction; the\n    epistemological interpretation is the correct one.\n\n[^4]: It's received wisdom in philosophy that one can never properly say\n    something of the form *p, but I don't know that p*. This is used as\n    a data point in views as far removed from each other as those\n    defended in @Heal1994 and @Williamson1996-WILKAA. But I don't feel\n    the force of this alleged datum at all, and (B) is just one reason.\n    For a different kind of case that makes the same point, see\n    @MaitraWeatherson.\n\n[^5]: As an aside, the existence of these cases is why I get so\n    irritated when epistemologists try to theorise about 'Gettier Cases'\n    as a class. What does (B) have in common with inferences from a\n    justified false belief, or with otherwise sound reasoning that is\n    ever so close to issuing in a false conclusion due to relatively bad\n    luck? As far as I can tell, the class of justified true beliefs that\n    aren't knowledge is a disjunctive mess, and this should matter for\n    thinking about the nature of knowledge. For further examples, see\n    @WilliamsonLofoten and @Nagel2013-Williamson.\n\n[^6]: A similar example is in Kratzer [-@Kratzer2012 94].\n\n[^7]: The idea of using allergies to illustrate the kind of case we're\n    interested in is due to @SchroederRoss2012, and I'm grateful for the\n    idea. It makes the intuitions much more vivid. The kind of cases\n    we're considering play a big role in, *inter alia*, @DeRose1992\n    [@Cohen1999] and @Fantl2002.\n\n[^8]: What I say here obviously has some similarities to a view put\n    forward by Jennifer @Nagel2008, but I ultimately end up drawing\n    rather different conclusions to the ones she draws.\n\n[^9]: Notable members of the tradition include @Levi1974, @Jeffrey1983\n    and @vanFraassen1989.\n\n[^10]: The point I'm making here is relevant I think to recent debates\n    about the proper way to formalise counterexamples in philosophy, as\n    in Â [@Williamson2007-WILTPO-17; @IchikawaJarvis2009; @Malmgren2011].\n    I worry that too much of that debate is focussed on the role that\n    examples play in one-step refutations. But there's more, much more,\n    to a good example than that.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
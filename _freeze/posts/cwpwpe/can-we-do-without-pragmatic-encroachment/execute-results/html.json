{
  "hash": "8db1a3b034f35cea29eb42e840bd3fc3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Can We Do Without Pragmatic Encroachment?\"\ndescription: |\n  I argue that interests primarily affect the relationship between credence and belief. A view is set out and defended where evidence and rational credence are not interest-relative, but belief, rational belief, and knowledge are.\ndate: December 13 2005\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ndoi: \"10.1111/j.1520-8583.2005.00068.x\"\ncategories:\n  - epistemology\n  - interest-relativity\n  - philosophy of mind\ncitation_url: https://doi.org/10.1111/j.1520-8583.2005.00068.x\njournal:\n    title: \"Philosophical Perspectives\"\n    publisher: \"Wiley\"\nvolume: 19\nnumber: 1\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: train.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    number_sections: true\n---\n\n\n\n### Introduction\n\nRecently several authors have defended claims suggesting that there is a\ncloser connection between practical interests and epistemic\njustification than has traditionally been countenanced. Jeremy Fantl and\nMatthew McGrath [-@Fantl2002] argue that there is a \"pragmatic necessary\ncondition on epistemic justification\" (77), namely the following.\n\n<aside>\nPublished in *Philosophical Perspectives* 19: 417-43.\n</aside>\n\n(PC)\n\n:   _S_ is justified in believing that _p_ only if _S_ is rational to\n    prefer as if _p_. (77)\n\nAnd John @Hawthorne2004 and Jason @Stanley2005-STAKAP have argued that\nwhat it takes to turn true belief into knowledge is sensitive to the\npractical environment the subject is in. These authors seem to be\nsuggesting there is, to use Jonathan Kvanvig's phrase \"pragmatic\nencroachment\" in epistemology. In this paper I'll argue that their\narguments do not quite show this is true, and that concepts of\nepistemological justification need not be pragmatically sensitive. The\naim here isn't to show that (PC) is false, but rather that it shouldn't\nbe described as a pragmatic condition on *justification*. Rather, it is\nbest thought of as a pragmatic condition on *belief*. There are two ways\nto spell out the view I'm taking here. These are both massive\nsimplifications, but they are close enough to the truth to show the kind\nof picture I'm aiming for.\n\n<aside>\nThanks to Michael Almeida, Tamar SzaboÃÅ Gendler, Peter Gerdes, Jon Kvanvig, Barry Lam, Ishani Maitra, Robert Stalnaker, Jason Stanley, Matthew Weiner for helpful discussions, and especially to Matthew McGrath for correcting many mistakes in an earlier draft of this paper.\n</aside>\n\nFirst, imagine a philosopher who holds a very simplified version of\nfunctionalism about belief, call it (B).\n\n(B\\)  \n\n:   _S_ believes that _p_ iff _S_ prefers as if _p_\n\nOur philosopher one day starts thinking about justification, and decides\nthat we can get a principle out of (B) by adding normative operators to\nboth sides, inferring (JB).\n\n(JB)\n\n:   _S_ is justified in believing that _p_ only if _S_ is justified to\n    prefer as if _p_\n\nNow it would be a mistake to treat (JB) as a pragmatic condition on\n*justification* (rather than belief) if it was derived from (B) by this\nsimple means. And if our philosopher goes on to infer (PC) from (JB), by\nreplacing 'justified' with 'rational', and inferring the conditional\nfrom the biconditional, we still don't get a pragmatic condition on\n*justification*.\n\nSecond, Fantl and McGrath focus their efforts on attacking the following\nprinciple.\n\nEvidentialism\n\n:   For any two subjects _S_ and S$^\\prime$, necessarily, if _S_ and\n    S$^\\prime$ have the same evidence for/against _p_, then _S_ is\n    justified in believing that _p_ iff S$^\\prime$ is, too.\n\nI agree, evidentialism is false. And I agree that there are\ncounterexamples to evidentialism from subjects who are in different\npractical situations. What I don't agree is that we learn much about the\nrole of pragmatic factors in *epistemology* properly defined from these\ncounterexamples to evidentialism. Evidentialism follows from the\nfollowing three principles.\n\nProbabilistic Evidentialism\n\n:   For any two subjects _S_ and S$^\\prime$, and any degree of belief\n    $\\alpha$ necessarily, if _S_ and S$^\\prime$ have the same evidence\n    for/against _p_, then _S_ is justified in believing that _p_ to\n    degree $\\alpha$ iff S$^\\prime$ is, too.\n\nThreshold View\n\n:   For any two subjects _S_ and S$^\\prime$, and any degree of belief\n    $\\alpha$, if _S_ and S$^\\prime$ both believe _p_ to degree $\\alpha$,\n    then _S_ believes that _p_ iff S$^\\prime$ does too.\n\nProbabilistic Justification\n\n:   For any $S, S$ is justified in believing _p_ iff there is some\n    degree of belief $\\alpha$ such that _S_ is justified in believing\n    _p_ to degree $\\alpha$, and in S's situation, believing _p_ to\n    degree $\\alpha$ suffices for believing _p_.\n\n(Degrees of belief here are meant to be the subjective correlates of\nKeynesian probabilities. See @Keynes1921 for more details. They need\nnot, and usually will not, be numerical values. The Threshold View is\nso-called because given some other plausible premises it implies that\n$S$ believes that _p_ iff S's degree of belief in _p_ is above a\nthreshold.)\n\nI endorse Probabilistic Justification, and for present purposes at least\nI endorse Probabilistic Evidentialism. The reason I think Evidentialism\nfails is because the Threshold View is false. It is plausible that\nProbabilistic Justification and Probabilistic Evidentialism are\nepistemological principles, while the Threshold View is a principle from\nphilosophy of mind. So this matches up with the earlier contention that\nthe failure of Evidentialism tells us something interesting about the\nrole of pragmatics in philosophy of mind, rather than something about\nthe role of pragmatics in epistemology.\n\nAs noted, Hawthorne and Stanley are both more interested in knowledge\nthan justification. So my discussion of their views will inevitably be\nsomewhat distorting. I think what I say about justification here should\ncarry over to a theory of knowledge, but space prevents a serious\nexamination of that question. The primary bit of 'translation' I have to\ndo to make their works relevant to a discussion of justification is to\ninterpret their defences of the principle (KP) below as implying some\nsupport for (JP), which is obviously similar to (PC).\n\n(KP)\n\n:   If _S_ knows that _p_, then _S_ is justified in using _p_ as a\n    premise in practical reasoning.\n\n(JP)\n\n:   If _S_ justifiably believes that _p_, then _S_ is justified in using\n    _p_ as a premise in practical reasoning.\n\nI think (JP) is just as plausible as (KP). In any case it is\nindependently plausible whether or not Hawthorne and Stanley are\ncommitted to it. So I'll credit recognition of (JP)'s importance to a\ntheory of justification to them, and hope that in doing so I'm not\nirrepairably damaging the public record.\n\nThe overall plan here is to use some philosophy of mind, specifically\nfunctionalist analyses of belief to respond to some arguments in\nepistemology. But, as you can see from the role the Threshold View plays\nin the above argument, our starting point will be the question what is\nthe relation between the credences decision theory deals with, and our\ntraditional notion of a belief? I'll offer an analysis of this relation\nthat supports my above claim that we should work with a pragmatic notion\nof belief rather than a pragmatic notion of justification. The analysis\nI offer has a hole in it concerning propositions that are not relevant\nto our current plans, and I'll fix the hold in section 3. Sections 4 and\n5 concern the role that closure principles play in my theory, in\nparticular the relationship between having probabilistically coherent\ndegrees of belief and logically coherent beliefs. In this context, a\nclosure principle is a principle that says probabilistic coherence\nimplies logical coherence, at least in a certain domain. (It's called a\nclosure principle because we usually discuss it by working out\nproperties of probabilistically coherent agents, and show that their\nbeliefs are closed under entailment in the relevant domain.) In section\n4 I'll defend the theory against the objection, most commonly heard from\nthose wielding the preface paradox, that we need not endorse as strong a\nclosure principle as I do. In section 5 I'll defend the theory against\nthose who would endorse an even stronger closure principle than is\ndefended here. Once we've got a handle on the relationship between\ndegrees of belief and belief *tout court*, we'll use that to examine the\narguments for pragmatic encroachment. In section 6 I'll argue that we\ncan explain the intuitions behind the cases that seem to support\npragmatic encroachment, while actually keeping all of the pragmatic\nfactors in our theory of belief. In section 7 I'll discuss how to\nendorse principles like (PC) and (JP) (as far as they can be endorsed)\nwhile keeping a non-pragmatic theory of probabilistic justification. The\ninteresting cases here are ones where agents have mistaken and/or\nirrational beliefs about their practical environment, and intuitions in\nthose cases are cloudy. But it seems the most natural path in these\ncases is to keep a pragmatically sensitive notion of belief, and a\npragmatically insensitive notion of justification.\n\n### Belief and Degree of Belief\n\nTraditional epistemology deals with beliefs and their justification.\nBayesian epistemology deals with degrees of belief and their\njustification. In some sense they are both talking about the same thing,\nnamely epistemic justification. Two questions naturally arise. Do we\nreally have two subject matters here (degrees of belief and belief *tout\ncourt*) or two descriptions of the one subject matter? If just one\nsubject matter, what relationship is there between the two modes of\ndescription of this subject matter?\n\nThe answer to the first question is I think rather easy. There is no\nevidence to believe that the mind contains two representational systems,\none to represent things as being probable or improbable and the other to\nrepresent things as being true or false. The mind probably does contain\na vast plurality of representational systems, but they don't divide up\nthe doxastic duties this way. If there are distinct visual and auditory\nrepresentational systems, they don't divide up duties between degrees of\nbelief and belief *tout court*, for example. If there were two distinct\nsystems, then we should imagine that they could vary independently, at\nleast as much as is allowed by constitutive rationality. But such\nvariation is hard to fathom. So I'll infer that the one representational\nsystem accounts for our credences and our categorical beliefs. (It\nfollows from this that the question @Bovens1999 ask, namely what beliefs\n*should* an agent have given her degrees of belief, doesn't have a\nnon-trivial answer. If fixing the degrees of belief in an environment\nfixes all her doxastic attitudes, as I think it does, then there is no\nfurther question of what she should believe given these are her degrees\nof belief.)\n\nThe second question is much harder. It is tempting to say that $S$\nbelieves that _p_ iff S's credence in _p_ is greater than some salient\nnumber $r$, where $r$ is made salient either by the context of belief\nascription, or the context that _S_ is in. I'm following Mark\n@Kaplan1996 in calling this the threshold view. There are two well-known\nproblems with the threshold view, both of which seem fatal to me.\n\nAs Robert Stalnaker [-@Stalnaker1984 91] emphasised, any number $r$ is\nbound to seem arbitrary. Unless these numbers are made salient by the\nenvironment, there is no special difference between believing _p_ to\ndegree 0.9786 and believing it to degree 0.9875. But if $r$ is 0.98755,\nthis will be *the difference* between believing _p_ and not believing\nit, which is an important difference. The usual response to this, as\nfound in [@Foley1993 Ch. 4] and @Hunter1996 is to say that the boundary\nis vague. But it's not clear how this helps. On an epistemic theory of\nvagueness, there is still a number such that degrees of belief above\nthat count, and degrees below that do not, and any such number is bound\nto seem unimportant. On supervaluational theories, the same is true.\nThere won't be a *determinate* number, to be sure, but there will a\nnumber, and that seems false. My preferred degree of belief theory of\nvagueness, as set out in @Weatherson2005-WEATTT has the same\nconsequence. Hunter defends a version of the threshold view combined\nwith a theory of vagueness based around fuzzy logic, which seems to be\nthe only theory that could avoid the arbitrariness objection. But as\n@Williamson1994-WILV showed, there are deep and probably insurmountable\ndifficulties with that position. So I think the vagueness response to\nthe arbitrariness objection is (a) the only prima facie plausible\nresponse and (b) unsuccessful.\n\nThe second problem concerns conjunction. It is also set out clearly by\nStalnaker.\n\n> Reasoning in this way from accepted premises to their deductive\n> consequences ($P$, also $Q$, therefore $R$) does seem perfectly\n> straightforward. Someone may object to one of the premises, or to the\n> validity of the argument, but one could not intelligibly agree that\n> the premises are each acceptable and the argument valid, while\n> objecting to the acceptability of the conclusion. [@Stalnaker1984 92]\n\nIf categorical belief is having a credence above the threshold, then one\ncan coherently do exactly this. Let $x$ be a number between $r$ and than\n$r$ ^$\\nicefrac{1}{2}$^, such that for an atom of type U has probability\n$x$ of decaying within a time $t$, for some $t$ and U. Assume our agent\nknows this fact, and is faced with two (isolated) atoms of U. Let _p_ be\nthat the first decays within $t$, and $q$ be that the second decays\nwithin $t$. She should, given her evidence, believe _p_ to degree $x, q$\nto degree $x$, and $p \\wedge q$ to degree $x ^2$. If she believed\n$p \\wedge q$ to a degree greater than $r$, she'd have to either have\ncredences that were not supported by her evidence, or credences that\nwere incoherent. (Or, most likely, both.) So this theory violates the\nplatitude. This is a well-known argument, so there are many responses to\nit, most of them involving something like appeal to the preface paradox.\nI'll argue in section 4 that the preface paradox doesn't in fact offer\nthe threshold view proponent much support here. But even before we get\nto there, we should note that the arbitrariness objection gives us\nsufficient reason to reject the threshold view.\n\nA better move is to start with the functionalist idea that to believe\nthat _p_ is to treat _p_ as true for the purposes of practical\nreasoning. To believe _p_ is to have preferences that make sense, by\nyour own lights, in a world where _p_ is true. So, if you prefer A to B\nand believe that _p_, you prefer A to B given _p_. For reasons that will\nbecome apparent below, we'll work in this paper with a notion of\npreference where *conditional* preferences are primary.[^1] So the core\ninsight we'll work with is the following:\n\n> If you prefer A to B given $q$, and you believe that _p_, then you\n> prefer A to B given $p \\wedge q$\n\nThe bold suggestion here is that if that is true for all the A, B and\n*q* that matter, then you believe _p_. Put formally, where *Bel*(_p_)\nmeans that the agent believes that _p_, and A $\\geq _q$ B means that the\nagent thinks A is at least as good as B given $q$, we have the following\n\n1.  *Bel*(_p_) $\\leftrightarrow \\forall$A$\\forall$B$\\forall q$ (A\n    $\\geq _q$ B $\\leftrightarrow$ A $\\geq _{p \\wedge q}$ B)\n\nIn words, an agent believes that _p_ iff conditionalising on _p_ doesn't\nchange any conditional preferences over things that matter.[^2] The\nleft-to-right direction of this seems trivial, and the right-to-left\ndirection seems to be a plausible way to operationalise the\nfunctionalist insight that belief is a functional state. There is some\nwork to be done if (1) is to be interpreted as a truth though.\n\nIf we interpret the quantifiers in (1) as unrestricted, then we get the\n(false) conclusion that just about no one believes no contingent\npropositions. To prove this, consider a bet that wins iff the statue in\nfront of me waves back at me due to random quantum effects when I wave\nat it. If I take the bet and win, I get to live forever in paradise. If\nI take the bet and lose, I lose a penny. Letting A be that I take the\nbet, B be that I decline the bet, $q$ be a known tautology (so my\npreferences given $q$ are my preferences *tout court*) and _p_ be that\nthe statue does not wave back, we have that I prefer A to B, but not A\nto B given _p_. So by this standard I don't believe that _p_. This is\nfalse -- right now I believe that statues won't wave back at me when I\nwave at them.\n\nThis seems like a problem. But the solution to it is not to give up on\nfunctionalism, but to insist on its pragmatic foundations. The\nquantifiers in (1) should be restricted, with the restrictions motivated\npragmatically. What is crucial to the theory is to say what the\nrestrictions on A and B are, and what the restrictions on $q$ are. We'll\ndeal with these in order.\n\nFor better or worse, I don't right now have the option taking that bet\nand hence spending eternity in paradise if the statue waves back at me.\nTaking or declining such unavailable bets are not open choices. For any\noption that is open to me, assuming that statues do not in fact wave\ndoes not change its utility. That's to say, I've already factored in the\nnon-waving behaviour of statues into my decision-making calculus. That's\nto say, I believe statues don't wave.\n\nAn action A is a live option for the agent if it is really possible for\nthe agent to perform A. An action A is a salient option if it is an\noption the agent takes seriously in deliberation. Most of the time\ngambling large sums of money on internet gambling sites over my phone is\na live option, but not a salient option. I know this option is\nsuboptimal, and I don't have to recompute every time whether I should do\nit. Whenever I'm making a decision, I don't have to add in to the list\nof choices *bet thousands of dollars on internet gambling sites*, and\nthen rerule that out every time. I just don't consider that option, and\nproperly so. If I have a propensity to daydream, then becoming the\ncentrefielder for the Boston Red Sox might be a salient option to me,\nbut it certainly isn't a live option. We'll say the two initial\nquantifiers range over the options that are live and salient options for\nthe agent.\n\nNote that we *don't* say that the quantifiers range over the options\nthat are live and salient for the person making the belief ascription.\nThat would lead us to a form of contextualism for which we have little\nevidence. We also don't say that an option becomes salient for the agent\niff they *should* be considering it. At this stage we are just saying\nwhat the agent does believe, not what they should believe, so we don't\nhave any clauses involving normative concepts.\n\nNow we'll look at the restrictions on the quantifier over propositions.\nSay a proposition is *relevant* if the agent is disposed to take\nseriously the question of whether it is true (whether or not she is\ncurrently considering that question) and conditionalising on that\nproposition or its negation changes some of the agents *unconditional*\npreferences over live, salient options.[^3] The first clause is designed\nto rule out wild hypotheses that the agent does not take at all\nseriously. If $q$ is not such a proposition, if the agent is disposed to\ntake it seriously, then it is relevant if there are live, salient A and\nB such that A $\\geq _q$ B $\\leftrightarrow$ A $\\geq$ B is false. Say a\nproposition is *salient* if the agent is currently considering whether\nit is true. Finally, say a proposition is *active* relative to _p_ iff\nit is a (possibly degenerate) conjunction of propositions such that each\nconjunct is either relevant or salient, and such that the conjunction is\nconsistent with _p_. (By a degenerate conjunction I mean a conjunction\nwith just one conjunct. The consistency requirement is there because it\nmight be hard in some cases to make sense of preferences given\ninconsistencies.) Then the propositional quantifier in (1) ranges over\nactive propositions.\n\nWe will expand and clarify this in the next section, but our current\nsolution to the relationship between beliefs and degrees of belief is\nthat degrees of belief determine an agent's preferences, and she\nbelieves that _p_ iff the claim (1) about her preferences is true when\nthe quantifiers over options are restricted to live, salient actions,\nand the quantifier over propositions is restricted to salient\npropositions. The simple view would be to say that the agent believes\nthat _p_ iff conditioning on _p_ changes none of her preferences. The\nmore complicated view here is that the agent believes that _p_ iff\nconditioning on _p_ changes none of her conditional preferences over\nlive, salient options, where the conditions are also active relative to\n_p_.\n\n### Impractical Propositions\n\nThe theory sketched in the previous paragraph seems to me right in the\nvast majority of cases. It fits in well with a broadly functionalist\nview of the mind, and as we'll see it handles some otherwise difficult\ncases with aplomb. But it needs to be supplemented a little to handle\nbeliefs about propositions that are practically irrelevant. I'll\nillustrate the problem, then note how I prefer to solve it.\n\nI don't know what Julius Caeser had for breakfast the morning he crossed\nthe Rubicon. But I think he would have had *some* breakfast. It is hard\nto be a good general without a good morning meal after all. Let _p_ be\nthe proposition that he had breakfast that morning. I believe _p_. But\nthis makes remarkably little difference to my practical choices in most\nsituations. True, I wouldn't have written this paragraph as I did\nwithout this belief, but it is rare that I have to write about Caeser's\ndietary habits. In general whether _p_ is true makes no practical\ndifference to me. This makes it hard to give a pragmatic account of\nwhether I believe that _p_. Let's apply (1) to see whether I really\nbelieve that _p_.\n\n1.  *Bel*(_p_) $\\leftrightarrow \\forall$A$\\forall$B$\\forall q$ (A\n    $\\geq _q$ B $\\leftrightarrow$ A $\\geq _{p \\wedge q}$ B)\n\nSince _p_ makes no practical difference to any choice I have to make,\nthe right hand side is true. So the left hand side is true, as desired.\nThe problem is that the right hand side of (2) is also true here.\n\n2.  *Bel*($\\neg p$) $\\leftrightarrow \\forall$A$\\forall$B$\\forall q$ (A\n    $\\geq _q$ B $\\leftrightarrow$ A $\\geq _{\\neg p \\wedge q}$ B)\n\nAdding the assumption that Caeser had no breakfast that morning doesn't\nchange any of my practical choices either. So I now seem to\n*inconsistently* believe both _p_ and $\\neg p$. I have some inconsistent\nbeliefs, I'm sure, but those aren't among them. We need to clarify what\n(1) claims.\n\nTo do so, I supplement the theory sketched in section 2 with the\nfollowing principles.\n\n-   A proposition _p_ is *eligible* *for belief* if it satisfies\n    $\\forall$A$\\forall$B$\\forall q$ (A $\\geq _q$ B $\\leftrightarrow$ A\n    $\\geq _{p \\wedge q}$ B), where the first two quantifiers range over\n    the open, salient actions in the sense described in section 2.\n\n-   For any proposition _p_, and any proposition $q$ that is relevant or\n    salient, among the actions that are (by stipulation!) open and\n    salient with respect to _p_ are *believing that p*, *believing that\n    q*, *not believing that p* and *not believing that q*\n\n-   For any proposition, the subject prefers believing it to not\n    believing it iff (a) it is eligible for belief and (b) the agents\n    degree of belief in the proposition is greater than\n    $\\nicefrac{1}{2}$.\n\n-   The previous stipulation holds both unconditionally and conditional\n    on _p_, for any _p_.\n\n-   The agent believes that _p_ iff $\\forall$A$\\forall$B$\\forall q$ (A\n    $\\geq _q$ B $\\leftrightarrow$ A $\\geq _{p \\wedge q}$ B), where the\n    first two quantifiers range over all actions that are either open\n    and salient *tout court* (i.e. in the sense of section 2) or open\n    and salient with respect to _p_ (as described above).\n\nThis all looks moderately complicated, but I'll explain how it works in\nsome detail as we go along. One simple consequence is that an agent only\nbelieves that _p_ iff their degree of belief in _p_ is greater than\n$\\nicefrac{1}{2}$. Since my degree of belief in Caeser's foodless\nmorning is not greater than $\\nicefrac{1}{2}$, in fact it is\nconsiderably less, I don't believe $\\neg p$. On the other hand, since my\ndegree of belief in _p_ is considerably greater than $\\nicefrac{1}{2}$,\nI prefer to believe it than disbelieve it, so I believe it.\n\nThere are many possible objections to this position, which I'll address\nsequentially.\n\n*Objection*: Even if I have a high degree of belief in _p_, I might\nprefer to not believe _p_ because I think that belief in _p_ is bad for\nsome other reason. Perhaps, if _p_ is a proposition about my brilliance,\nit might be immodest to believe that _p_.\n\n*Reply*: Any of these kinds of considerations should be put into the\ncredences. If it is immodest to believe that you are a great\nphilosopher, it is equally immodest to believe to a high degree that you\nare a great philosopher.\n\n*Objection*: Belief that _p_ is not an action in the ordinary sense of\nthe term.\n\n*Reply*: True, which is why this is described as a supplement to the\noriginal theory, rather than just cashing out its consequences.\n\n*Objection*: It is impossible to choose to believe or not believe\nsomething, so we shouldn't be applying these kinds of criteria.\n\n*Reply*: I'm not as convinced of the impossibility of belief by choice\nas others are, but I won't push that for present purposes. Let's grant\nthat beliefs are always involuntary. So these 'actions' aren't open\nactions in any interesting sense, and the theory is section 2 was really\nincomplete. As I said, this is a supplement to the theory in section 2.\n\nThis doesn't prevent us using principles of constitutive rationality,\nsuch as we prefer to believe _p_ iff our credence in _p_ is over\n$\\nicefrac{1}{2}$. Indeed, on most occasions where we use constitutive\nrationality to infer that a person has some mental state, the mental\nstate we attribute to them is one they could not fail to have. But\nfunctionalists are committed to constitutive rationality [@Lewis1994b].\nSo my approach here is consistent with a broadly functionalist outlook.\n\n*Objection*: This just looks like a roundabout way of stipulating that\nto believe that _p_, your degree of belief in _p_ has to be greater than\n$\\nicefrac{1}{2}$. Why not just add that as an extra clause than going\nthrough these little understood detours about preferences about beliefs?\n\n*Reply*: There are three reasons for doing things this way rather than\nadding such a clause.\n\nFirst, it's nice to have a systematic theory rather than a theory with\nan ad hoc clause like that.\n\nSecond, the effect of this constraint is much more than to restrict\nbelief to propositions whose credence is greater than $\\nicefrac{1}{2}$.\nConsider a case where _p_ and $q$ and their conjunction are all salient,\n_p_ and $q$ are probabilistically independent, and the agent's credence\nin each is 0.7. Assume also that $p, q$ and $p \\wedge q$ are completely\nirrelevant to any practical deliberation the agent must make. Then the\ncriteria above imply that the agent does not believe that _p_ or that\n$q$. The reason is that the agent's credence in $p \\wedge q$ is 0.49, so\nshe prefers to not believe $p \\wedge q$. But conditional on _p_, her\ncredence in $p \\wedge q$ is 0.7, so she prefers to believe it. So\nconditionalising on _p_ does change her preferences with respect to\nbelieving $p \\wedge q$, so she doesn't believe _p_. So the effect of\nthese stipulations rules out much more than just belief in propositions\nwhose credence is below $\\nicefrac{1}{2}$.\n\nThis suggests the third, and most important point. The problem with the\nthreshold view was that it led to violations of closure. Given the\ntheory as stated, we can prove the following theorem. Whenever _p_ and\n$q$ and their conjunction are all open or salient, and both are\nbelieved, and the agent is probabilistically coherent, the agent also\nbelieves $p \\wedge q$. This is a quite restricted closure principle, but\nthis is no reason to deny that it is *true*, as it fails to be true on\nthe threshold view.\n\nThe proof of this theorem is a little complicated, but worth working\nthrough. First we'll prove that if the agent believes _p_, believes $q$,\nand _p_ and $q$ are both salient, then the agent prefers believing\n$p \\wedge q$ to not believing it, if $p \\wedge q$ is eligible for\nbelief. In what follows *Pr*($x | y$) is the agent's conditional degree\nof belief in $x$ given $y$. Since the agent is coherent, we'll assume\nthis is a probability function (hence the name).\n\n1.  Since the agent believes that $q$, they prefer believing that $q$ to\n    not believing that $q$ (by the criteria for belief)\n\n2.  So the agent prefers believing that $q$ to not believing that $q$\n    given _p_ (From 1 and the fact that they believe that _p_, and that\n    $q$ is salient)\n\n3.  So *Pr*($q | p$) $> \\nicefrac{1}{2}$ (from 2)\n\n4.  *Pr*($q | p$) = *Pr*($p \\wedge q | p$) (by probability calculus)\n\n5.  So *Pr*($p \\wedge q | p$) $> \\nicefrac{1}{2}$ (from 3, 4)\n\n6.  So, if $p \\wedge q$ is eligible for belief, then the agent prefers\n    believing that $p \\wedge q$ to not believing it, given _p_ (from 5)\n\n7.  So, if $p \\wedge q$ is eligible for belief, the agent prefers\n    believing that $p \\wedge q$ to not believing it (from 6, and the\n    fact that they believe that _p_, and $p \\wedge q$ is salient)\n\nSo whenever, $p, q$ and $p \\wedge q$ are salient, and the agent believes\neach conjunct, the agent prefers believing the conjunction $p \\wedge q$\nto not believing it, if $p \\wedge q$ is eligible. Now we have to prove\nthat $p \\wedge q$ is eligible for belief, to prove that it is actually\nbelieved. That is, we have to prove that (5) follows from (4) and (3),\nwhere the initial quantifiers range over actions that are open and\nsalient *tout court*.\n\n1.  $\\forall$A$\\forall$B$\\forall r$ (A $\\geq_r$ B $\\leftrightarrow$ A\n    $\\geq _p  \\wedge r$ B)\n\n2.  $\\forall$A$\\forall$B$\\forall r$ (A $\\geq_r$ B $\\leftrightarrow$ A\n    $\\geq _q  \\wedge r$ B)\n\n3.  $\\forall$A$\\forall$B$\\forall r$ (A $\\geq_r$ B $\\leftrightarrow$ A\n    $\\geq _{p \\wedge q \\wedge r}$ B)\n\nAssume that (5) isn't true. That is, there are A, B and _S_ such that\n$\\neg$(A $\\geq_s$ B $\\leftrightarrow$ A $\\geq _{p \\wedge q \\wedge s}$B).\nBy hypothesis _S_ is active, and consistent with $p \\wedge q$. So it is\nthe conjunction of relevant, salient propositions. Since $q$ is salient,\nthis means $q \\wedge s$ is also active. Since _S_ is consistent with\n$p \\wedge q$, it follows that $q \\wedge s$ is consistent with _p_. So\n$q \\wedge s$ is a possible substitution instance for $r$ in (3). Since\n(3) is true, it follows that A $\\geq _{q \\wedge s}$ B $\\leftrightarrow$\nA $\\geq _{p \\wedge q \\wedge s}$ B. By similar reasoning, it follows that\n$s$ is a permissible substitution instance in (4), giving us A $\\geq_s$\nB $\\leftrightarrow$ A $\\geq _{q \\wedge s}$ B. Putting the last two\nbiconditionals together we get A $\\geq_s$ B $\\leftrightarrow$ A\n$\\geq _{p \\wedge q \\wedge s}$B, contradicting our hypothesis that there\nis a counterexample to (5). So whenever (3) and (4) are true, (5) is\ntrue as well, assuming $p, q$ and $p \\wedge q$ are all salient.\n\n### Defending Closure\n\nSo on my account of the connection between degrees of belief and belief\n*tout court*, probabilistic coherence implies logical coherence amongst\nsalient propositions. The last qualification is necessary. It is\npossible for a probabilistically coherent agent to not believe the\n*non*-salient consequences of things they believe, and even for a\nprobabilistically coherent agent to have inconsistent beliefs as long as\nnot all the members of the inconsistent set are active. Some people\nargue that even this weak a closure principle is implausible. David\n@Christensen2005, for example, argues that the preface paradox provides\na reason for doubting that beliefs must be closed under entailment, or\neven must be consistent. Here is his description of the case.\n\n> We are to suppose that an apparently rational person has written a\n> long non-fiction book---say, on history. The body of the book, as is\n> typical, contains a large number of assertions. The author is highly\n> confident in each of these assertions; moreover, she has no hesitation\n> in making them unqualifiedly, and would describe herself (and be\n> described by others) as believing each of the book's many claims. But\n> she knows enough about the difficulties of historical scholarship to\n> realize that it is almost inevitable that at least a few of the claims\n> she makes in the book are mistaken. She modestly acknowledges this in\n> her preface, by saying that she believes the book will be found to\n> contain some errors, and she graciously invites those who discover the\n> errors to set her straight. [@Christensen2005 33-4]\n\nChristensen thinks such an author might be rational in every one of her\nbeliefs, even though these are all inconsistent. Although he does not\nsay this, nothing in his discussion suggests that he is using the\nirrelevance of some of the propositions in the author's defence. So here\nis an argument that we should abandon closure amongst relevant beliefs.\n\nChristensen's discussion, like other discussions of the preface paradox,\nmakes frequent use of the fact that examples like these are quite\ncommon. We don't have to go to fake barn country to find a\ncounterexample to closure. But it seems to me that we need two quite\nstrong idealisations in order to get a real counterexample here.\n\nThe first of these is discussed in forthcoming work by Ishani Maitra\n[@MaitraANG], and is briefly mentioned by Christensen in setting out the\nproblem. We only have a counterexample to closure if the author\n*believes* every thing she writes in her book. (Indeed, we only have a\ncounterexample if she reasonably believes every one of them. But we'll\nassume a rational author who only believes what she ought to believe.)\nThis seems unlikely to be true to me. An author of a historical book is\nlike a detective who, when asked to put forward her best guess about\nwhat explains the evidence, says \"If I had to guess, I'd say ...\" and\nthen launches into spelling out her hypothesis. It seems clear that she\nneed not *believe* the truth of her hypothesis. If she did that, she\ncould not later learn it was true, because you can't learn the truth of\nsomething you already believe. And she wouldn't put any effort into\ninvestigating alternative suspects. But she can come to learn her\nhypothesis was true, and it would be rational to investigate other\nsuspects. It seems to me (following here Maitra's discussion) that we\nshould understand scholarly assertions as being governed by the same\nkind of rules that govern detectives making the kind of speech being\ncontemplated here. And those rules don't require that the speaker\nbelieve the things they say without qualification. The picture is that\nthe little prelude the detective explicitly says is implicit in all\nscholarly work.\n\nThere are three objections I know to this picture, none of them\nparticularly conclusive. First, Christensen says that the author doesn't\nqualify their assertions. But neither does our detective qualify most\nindividual sentences. Second, Christensen says that most people would\ndescribe our author as believing her assertions. But it is also natural\nto describe our detective as believing the things she says in her\nspeech. It's natural to say things like \"She thinks it was the butler,\nwith the lead pipe,\" in reporting her hypothesis. Third, Timothy\n@Williamson2000-WILKAI has argued that if speakers don't believe what\nthey say, we won't have an explanation of why Moore's paradoxical\nsentences, like \"The butler did it, but I don't believe the butler did\nit,\" are always defective. Whatever the explanation of the\nparadoxicality of these sentences might be, the alleged requirement that\nspeakers believe what they say can't be it. For our detective cannot\nproperly say \"The butler did it, but I don't believe the butler did it\"\nin setting out her hypothesis, even though *believing* the butler did it\nis not necessary for her to say \"The butler did it\" in setting out just\nthat hypothesis.\n\nIt is plausible that for *some* kinds of books, the author should only\nsay things they believe. This is probably true for travel guides, for\nexample. Interestingly, casual observation suggests that authors of such\nbooks are much less likely to write modest prefaces. This makes some\nsense if those books can only include statements their authors believe,\nand the authors believe the conjunctions of what they believe.\n\nThe second idealisation is stressed by Simon @Evnine1999 in his paper\n\"Believing Conjunctions\". The following situation does not involve me\nbelieving anything inconsistent.\n\n-   I believe that what Manny just said, whatever it was, is false.\n\n-   Manny just said that the stands at Fenway Park are green.\n\n-   I believe that the stands at Fenway Park are green.\n\nIf we read the first claim *de dicto*, that I believe that Manny just\nsaid something false, then there is no inconsistency. (Unless I also\nbelieve that what Manny just said was that the stands in Fenway Park are\ngreen.) But if we read it *de re*, that the thing Manny just said is one\nof the things I believe to be false, then the situation does involve me\nbeing inconsistent. The same is true when the author believes that one\nof the things she says in her book is mistaken. If we understand what\nshe says *de dicto*, there is no contradiction in her beliefs. It has to\nbe understood *de re* before we get a logical problem. And the fact is\nthat most authors do not have *de re* attitudes towards the claims made\nin their book. Most authors don't even remember everything that's in\ntheir books. (I'm not sure I remember how this section started, let\nalone this paper.) Some may argue that authors don't even have the\ncapacity to consider a proposition as long and complicated as the\nconjunction of all the claims in their book. Christensen considers this\nobjection, but says it isn't a serious problem.\n\n> It is undoubtedly true that ordinary humans cannot entertain\n> book-length conjunctions. But surely, agents who do not share this\n> fairly *superficial* limitation are easily conceived. And it seems\n> just as wrong to say of such agents that they are rationally required\n> to believe in the inerrancy of the books they write. (38: my emphasis)\n\nI'm not sure this is undoubtedly true; it isn't clear that propositions\n(as opposed to their representations) have lengths. And humans can\nbelieve propositions that *can* be represented by sentences as long as\nbooks. But even without that point, Christensen is right that there is\nan idealisation here, since ordinary humans do not know exactly what is\nin a given book, and hence don't have *de re* attitudes towards the\npropositions expressed in the book.\n\nI'm actually rather suspicious of the intuition that Christensen is\npushing here, that idealising in this way doesn't change intuitions\nabout the case. The preface paradox gets a lot of its (apparent) force\nfrom intuitions about what attitude we should have towards real books.\nOnce we make it clear that the real life cases are not relevant to the\nparadox, I find the intuitions become rather murky. But I won't press\nthis point.\n\nA more important point is that we believers in closure don't think that\nauthors should think their books are inerrant. Rather, following\n@Stalnaker1984, we think that authors shouldn't unqualifiedly *believe*\nthe individual statements in their book if they don't believe the\nconjunction of those statements. Rather, their attitude towards those\npropositions (or at least some of them) should be that they are probably\ntrue. (As Stalnaker puts it, they accept the story without believing\nit.) Proponents of the preface paradox know that this is a possible\nresponse, and tend to argue that it is impractical. Here is Christensen\non this point.\n\n> It is clear that our everyday binary way of talking about beliefs has\n> immense practical advantages over a system which insisted on some more\n> fine-grained reporting of degrees of confidence ... At a minimum,\n> talking about people as believing, disbelieving, or withholding belief\n> has at least as much point as do many of the imprecise ways we have of\n> talking about things that can be described more precisely. (96)\n\nRichard Foley makes a similar point.\n\n> There are *deep* reasons for wanting an epistemology of beliefs,\n> reasons that epistemologies of degrees of belief by their very nature\n> cannot possibly accommodate. [@Foley1993 170, my emphasis]\n\nIt's easy to make too much of this point. It's a lot easier to triage\npropositions into TRUE, FALSE and NOT SURE and work with those\ncategories than it is to work assign precise numerical probabilities to\neach proposition. But these are not the only options. Foley's discussion\nsubsequent to the above quote sometimes suggests they are, especially\nwhen he contrasts the triage with \"indicat\\[ing\\] as accurately as I can\nmy degree of confidence in each assertion that I defend.\" (171) But\nreally it isn't *much* harder to add two more categories, PROBABLY TRUE\nand PROBABLY FALSE to those three, and work with that five-way division\nrather than a three-way division. It's not clear that humans as they are\nactually constructed have a *strong* preference for the three-way over\nthe five-way division, and even if they do, I'm not sure in what sense\nthis is a 'deep' fact about them.\n\nOnce we have the five-way division, it is clear what authors should do\nif they want to respect closure. For any conjunction that they don't\nbelieve (i.e. classify as true), they should not believe one of the\nconjuncts. But of course they can classify every conjunct as probably\ntrue, even if they think the conjunction is false, or even certainly\nfalse. Still, might it not be considered something of an idealisation to\nsay rational authors must make this five-way distinction amongst\npropositions they consider? Yes, but it's no more of an idealisation\nthan we need to set up the preface paradox in the first place. To use\nthe preface paradox to find an example of someone who reasonably\nviolates closure, we need to insist on the following three constraints.\n\n1.  They are part of a research community where only asserting\n    propositions you believe is compatible with active scholarship;\n\n2.  They know exactly what is in their book, so they are able to believe\n    that one of the propositions in the book is mistaken, where this is\n    understood *de re*; but\n\n3.  They are unable to effectively function if they have to effect a\n    five-way, rather than a three-way, division amongst the propositions\n    they consider.\n\nPut more graphically, to motivate the preface paradox we have to think\nthat our inability to have *de re* thoughts about the contents of books\nis a \"superficial constraint\", but our preference for working with a\nthree-way rather than a five-way division is a \"deep\" fact about our\ncognitive system. Maybe each of these attitudes could be plausible taken\non its own (though I'm sceptical of that) but the conjunction seems just\nabsurd.\n\nI'm not entirely sure an agent subject to exactly these constraints is\neven fully conceivable. (Such an agent is negatively conceivable, in\nDavid Chalmers's terminology, but I rather doubt they are positively\nconceivable.) But even if they are a genuine possibility, why the norms\napplicable to an agent satisfying that very gerrymandered set of\nconstraints should be considered relevant norms for our state is far\nfrom clear. I'd go so far as to say it's clear that the applicability\n(or otherwise) of a given norm to such an odd agent is no reason\nwhatsoever to say it applies to us. But since the preface paradox only\nprovides a reason for just these kinds of agents to violate closure, we\nhave no reason for ordinary humans to violate closure. So I see no\nreason here to say that we can have probabilistic coherence without\nlogical coherence, as proponents of the threshold view insist we can\nhave, but which I say we can't have *at least when the propositions\ninvolved are salient*. The more pressing question, given the failure of\nthe preface paradox argument, is why I don't endorse a much stronger\nclosure principle, one that drops the restriction to salient\npropositions. The next section will discuss that point.\n\nI've used Christensen's book as a stalking horse in this section,\nbecause it is the clearest and best statement of the preface paradox.\nSince Christensen is a paradox-mongerer and I'm a paradox-denier, it\nmight be thought we have a deep disagreement about the relevant\nepistemological issues. But actually I think our overall views are\nfairly close despite this. I favour an epistemological outlook I call\n\"Probability First\", the view that getting the epistemology of partial\nbelief right is of the first importance, and everything else should flow\nfrom that. Christensen's view, reduced to a slogan, is \"Probability\nFirst and Last\". This section has been basically about the difference\nbetween those two slogans. It's an important dispute, but it's worth\nbearing in mind that it's a factional squabble within the Probability\nParty, not an outbreak of partisan warfare.\n\n### Too Little Closure?\n\nIn the previous section I defended the view that a coherent agent has\nbeliefs that are deductively cogent with respect to salient\npropositions. Here I want to defend the importance of the qualification.\nLet's start with what I take to be the most important argument for\nclosure, the passage from Stalnaker's *Inquiry* that I quoted above.\n\n> Reasoning in this way from accepted premises to their deductive\n> consequences ($P$, also $Q$, therefore $R$) does seem perfectly\n> straightforward. Someone may object to one of the premises, or to the\n> validity of the argument, but one could not intelligibly agree that\n> the premises are each acceptable and the argument valid, while\n> objecting to the acceptability of the conclusion. [@Stalnaker1984 92]\n\nStalnaker's wording here is typically careful. The relevant question\nisn't whether we can accept _p_, accept $q$, accept _p_ and $q$ entail\n$r$, and reject $r$. As Christensen [-@Christensen2005 Ch. 4] notes,\nthis is impossible even on the threshold view, as long as the threshold\nis above 2/3. The real question is whether we can accept _p_, accept\n$q$, accept _p_ and $q$ entail $r$, and *fail* to accept $r$. And this\nis always a live possibility on any threshold view, though it seems\nabsurd at first that this could be coherent.\n\nBut it's important to note how *active* the verbs in Stalnaker's\ndescription are. When faced with a valid argument we have to *object* to\none of the premises, or the validity of the argument. What we can't do\nis *agree* to the premises and the validity of the argument, while\n*objecting* to the conclusion. I agree. If we are really *agreeing* to\nsome propositions, and *objecting* to others, then all those\npropositions are salient. And in that case closure, deductive coherence,\nis mandatory. This doesn't tell us what we have to do if we haven't\npreviously made the propositions salient in the first place.\n\nThe position I endorse here is very similar in its conclusions to that\nendorsed by Gilbert Harman in *Change in View*. There Harman endorses\nthe following principle. (At least he endorses it as true -- he doesn't\nseem to think it is particularly explanatory because it is a special\ncase of a more general interesting principle.)\n\nRecognized Logical Implication Principle\n\n:   One has reason to believe _p_ if one *recognizes* that _p_ is\n    logically implied by one's view. [@Harman1986 17]\n\nThis seems right to me, both what it says and its implicature that the\nreason in question is not a conclusive reason. My main objection to\nthose who use the preface paradox to argue against closure is that they\ngive us a mistaken picture of what we have *to do* epistemically. When I\nhave inconsistent beliefs, or I don't believe some consequence of my\nbeliefs, that is something I have a reason to deal with at some stage,\nsomething I have to do. When we say that we have things to do, we don't\nmean that we have to do them *right now*, or instead of everything else.\nMy current list of things to do includes cleaning my bathroom, yet here\nI am writing this paper, and (given the relevant deadlines) rightly so.\nWe can have the job of cleaning up our epistemic house as something to\ndo while recognising that we can quite rightly do other things first.\nBut it's a serious mistake to infer from the permissibility of doing\nother things that cleaning up our epistemic house (or our bathroom)\nisn't something to be done. The bathroom won't clean itself after all,\nand eventually this becomes a problem.\n\nThere is a possible complication when it comes to tasks that are very\nlow priority. My attic is to be cleaned, or at least it could be\ncleaner, but there are no imaginable circumstances under which something\nelse wouldn't be higher priority. Given that, should we really leave\n*clean the attic* on the list of things to be done? Similarly, there\nmight be implications I haven't followed through that it couldn't\npossibly be worth my time to sort out. Are they things to be done? I\nthink it's worthwhile recording them as such, because otherwise we might\nmiss opportunities to deal with them in the process of doing something\nelse. I don't need to put off anything else in order to clean the attic,\nbut if I'm up there for independent reasons I should bring down some of\nthe garbage. Similarly, I don't need to follow through implications\nmostly irrelevant to my interests, but if those propositions come up for\nindependent reasons, I should deal with the fact that some things I\nbelieve imply something I don't believe. Having it be the case that all\nimplications from things we believe to things we don't believe\nconstitute jobs to do (possibly in the loose sense that cleaning my\nattic is something to do) has the right implications for what epistemic\nduties we do and don't have.\n\nWhile waxing metaphorical, it seems time to pull out a rather helpful\nmetaphor that Gilbert @Ryle1949 develops in *The Concept of Mind* at a\npoint where he's covering what we'd now call the inference/implication\ndistinction. (This is a large theme of chapter 9, see particularly pages\n292-309.) Ryle's point in these passages, as it frequently is throughout\nthe book, is to stress that minds are fundamentally active, and the\nactivity of a mind cannot be easily recovered from its end state.\nAlthough Ryle doesn't use this language, his point is that we shouldn't\nconfuse the difficult activity of drawing inferences with the smoothness\nand precision of a logical implication. The language Ryle does use is\nmore picturesque. He compares the easy work a farmer does when\nsauntering down a path from the hard work he did when building the path.\nA good argument, in philosophy or mathematics or elsewhere, is like a\nwell made path that permits sauntering from the start to finish without\nundue strain. But from that it doesn't follow that the task of coming up\nwith that argument, of building that path in Ryle's metaphor, was easy\nwork. The easiest paths to walk are often the hardest to build.\nPath-building, smoothing out our beliefs so they are consistent and\nclosed under implication, is hard work, even when the finished results\nlook clean and straightforward. Its work that we shouldn't do unless we\nneed to. But making sure our beliefs are closed under entailment even\nwith respect to irrelevant propositions is suspiciously like the\nactivity of buildings paths between points without first checking you\nneed to walk between them.\n\nFor a less metaphorical reason for doubting the wisdom of this unchecked\ncommitment to closure, we might notice the difficulties theorists tend\nto get into all sorts of difficulties. Consider, for example, the view\nput forward by Mark Kaplan in *Decision Theory as Philosophy*. Here is\nhis definition of belief.\n\n> You count as believing P just if, were your sole aim to assert the\n> truth (as it pertains to P), and you only options were to assert that\n> P, assert that $\\neg$P or make neither assertion, you would prefer to\n> assert that P. (109)\n\nKaplan notes that conditional definitions like this are prone to Shope's\nconditional fallacy. If my sole aim were to assert the truth, I might\nhave different beliefs to what I now have. He addresses one version of\nthis objection (namely that it appears to imply that everyone believes\ntheir sole desire is to assert the truth) but as we'll see presently he\ncan't avoid all versions of it.\n\nThese arguments are making me thirsty. I'd like a beer. Or at least I\nthink I would. But wait! On Kaplan's theory I can't think that I'd like\na beer, for if my sole aim were to assert the truth as it pertains to my\nbeer-desires, I wouldn't have beer desires. And then I'd prefer to\nassert that I wouldn't like a beer, I'd merely like to assert the truth\nas it pertains to my beer desires.\n\nEven bracketing this concern, Kaplan ends up being committed to the view\nthat I can (coherently!) believe that _p_ even while regarding _p_ as\nhighly improbable. This looks like a refutation of the view to me, but\nKaplan accepts it with some equanimity. He has two primary reasons for\nsaying we should live with this. First, he says that it only looks like\nan absurd consequence if we are committed to the Threshold View. To this\nall I can say is that *I* don't believe the Threshold View, but it still\nseems absurd to me. Second, he says that any view is going to have to be\nrevisionary to some extent, because our ordinary concept of belief is\nnot \"coherent\" (142). His view is that, \"Our ordinary notion of belief\nboth construes belief as a state of confidence short of certainty and\ntakes consistency of belief to be something that is at least possible\nand, perhaps, even desirable\" and this is impossible. I think the view\nhere interprets belief as a state less than confidence and allows for as\nmuch consistency as the folk view does (i.e. consistency amongst salient\npropositions), so this defence is unsuccessful as well.\n\nNone of the arguments here in favour of our restrictions on closure are\ncompletely conclusive. In part the argument at this stage rests on the\nlack of a plausible rival theory that doesn't interpret belief as\ncertainty but implements a stronger closure principle. It's possible\nthat tomorrow someone will come up with a theory that does just this.\nUntil then, we'll stick with the account here, and see what its\nepistemological implications might be.\n\n### Examples of Pragmatic Encroachment\n\nFantl and McGrath's case for pragmatic encroachment starts with cases\nlike the following. (The following case is not quite theirs, but is\nsimilar enough to suit their plan, and easier to explain in my\nframework.)\n\n> *Local and Express*\n>\n> There are two kinds of trains that run from the city to the suburbs:\n> the local, which stops at all stations, and the express, which skips\n> the first eight stations. Harry and Louise want to go to the fifth\n> station, so they shouldn't catch the Express. Though if they do it\n> isn't too hard to catch a local back the other way, so it isn't\n> usually a large cost. Unfortunately, the trains are not always clearly\n> labelled. They see a particular train about to leave. If it's a local\n> they are better off catching it, if it is an express they should wait\n> for the next local, which they can see is already boarding passengers\n> and will leave in a few minutes. While running towards the train, they\n> hear a fellow passenger say \"It's a local.\" This gives them good, but\n> far from overwhelming, reason to believe that the train is a local.\n> Passengers get this kind of thing wrong fairly frequently, but they\n> don't have time to get more information. So each of them face a\n> gamble, which they can take by getting on the train. If the train is a\n> local, they will get home a few minutes early. If it is an express\n> they will get home a few minutes later. For Louise, this is a low\n> stakes gamble, as nothing much turns on whether she is a few minutes\n> early or late, but she does have a weak preference for arriving\n> earlier rather than later. But for Harry it is a high stakes gamble,\n> because if he is late he won't make the start of his daughter's soccer\n> game, which will highly upset her. There is no large payoff for Harry\n> arriving early.\n\nWhat should each of them do? What should each of them believe?\n\nThe first question is relatively easy. Louise should catch the train,\nand Harry should wait for the next. For each of them that's the utility\nmaximising thing to do. The second one is harder. Fantl and McGrath\nsuggest that, despite being in the same epistemic position with respect\nto everything except their interests, Louise is justified in believing\nthe train is a local and Harry is not. I agree. (If you don't think the\nparticular case fits this pattern, feel free to modify it so the\ndifference in interests grounds a difference in what they are justified\nin believing.) Does this show that our notion of epistemic justification\nhas to be pragmatically sensitive? I'll argue that it does not.\n\nThe fundamental assumption I'm making is that what is primarily subject\nto epistemic evaluation are degrees of belief, or what are more commonly\ncalled states of confidence in ordinary language. When we think about\nthings this way, we see that Louise and Harry are justified in adopting\n*the very same degrees of belief*. Both of them should be confident, but\nnot absolutely certain, that the train is a local. We don't have even\nthe appearance of a counterxample to Probabilistic Evidentialism here.\nIf we like putting this in numerical terms, we could say that each of\nthem is justified in assigning a probability of around 0.9 to the\nproposition *That train is a local*.[^4] So as long as we adopt a\nProbability First epistemology, where we in the first instance evaluate\nthe probabilities that agents assign to propositions, Harry and Louise\nare evaluated alike iff they do the same thing.\n\nHow then can we say that Louise alone is justified in believing that the\ntrain is a local? Because that state of confidence they are justified in\nadopting, the state of being fairly confident but not absolutely certain\nthat the train is a local, counts as believing that the train is a local\ngiven Louise's context but not Harry's context. Once Louise hears the\nother passenger's comment, conditionalising on *That's a local* doesn't\nchange any of her preferences over open, salient actions, including such\n'actions' as believing or disbelieving propositions. But conditional on\nthe train being a local, Harry prefers catching the train, which he\nactually does not prefer.\n\nIn cases like this, interests matter not because they affect the degree\nof confidence that an agent can reasonably have in a proposition's\ntruth. (That is, not because they matter to epistemology.) Rather,\ninterests matter because they affect whether those reasonable degrees of\nconfidence amount to belief. (That is, because they matter to philosophy\nof mind.) There is no reason here to let pragmatic concerns into\nepistemology.\n\n### Justification and Practical Reasoning\n\nThe discussion in the last section obviously didn't show that there is\nno encroachment of pragmatics into epistemology. There are, in\nparticular, two kinds of concerns one might have about the prospects for\nextending my style of argument to block all attempts at pragmatic\nencroachment. The biggest concern is that it might turn out to be\nimpossible to defend a Probability First epistemology, particularly if\nwe do not allow ourselves pragmatic concerns. For instance, it is\ncrucial to this project that we have a notion of evidence that is not\ndefined in terms of traditional epistemic concepts (e.g. as knowledge),\nor in terms of interests. This is an enormous project, and I'm not going\nto attempt to tackle it here. The second concern is that we won't be\nable to generalise the discussion of that example to explain the\nplausibility of (JP) without conceding something to the defenders of\npragmatic encroachment.\n\n(JP)\n\n:   If _S_ justifiably believes that _p_, then _S_ is justified in using\n    _p_ as a premise in practical reasoning.\n\nAnd that's what we will look at in this section. To start, we need to\nclarify exactly what (JP) means. Much of this discussion will be\nindebted to Fantl and McGrath's discussion of various ways of making\n(JP) more precise. To see some of the complications at issue, consider a\nsimple case of a bet on a reasonably well established historical\nproposition. The agent has a lot of evidence that supports _p_, and is\noffered a bet that returns \\$1 if _p_ is true, and loses \\$500 if _p_ is\nfalse. Since her evidence doesn't support *that* much confidence in _p_,\nshe properly declines the bet. One might try to reason intuitively as\nfollows. Assume that she justifiably believed that _p_. Then she'd be in\na position to make the following argument.\n\n> _p_\\\n> If _p_, then I should take the bet\\\n> So, I should take the bet\n\nSince she isn't in a position to draw the conclusion, she must not be in\na position to endorse both of the premises. Hence (arguably) she isn't\njustified in believing that _p_. But we have to be careful here. If we\nassume also that _p_ is true (as Fantl and McGrath do, because they are\nmostly concerned with knowledge rather than justified belief), then the\nsecond premise is clearly false, since it is a conditional with a true\nantecedent and a false consequent. So the fact that she can't draw the\nconclusion of this argument only shows that she can't endorse *both* of\nthe premises, and that's not surprising since one of the premises is\nmost likely false. (I'm not assuming here that the conditional is true\niff it has a true antecendent or a false consequent, just that it is\nonly true if it has a false antecedent or a true consequent.)\n\nIn order to get around this problem, Fantl and McGrath suggest a few\nother ways that our agent might reason to the bet. They suggest each of\nthe following principles.\n\n> S knows that p only if, for any act A, if _S_ knows that if p, then A\n> is the best thing she can do, then _S_ is rational to do A. (72)\n>\n> _S_ knows that p only if, for any states of affairs A and B, if $S$\n> knows that if p, then A is better for her than B, then _S_ is rational\n> to prefer A to B. (74)\n>\n> **(PC)** _S_ is justified in believing that p only if _S_ is rational\n> to prefer as if p. (77)\n\nHawthorne [-@Hawthorne2004 174-181] appears to endorse the second of\nthese principles. He considers an agent who endorses the following\nimplication concerning a proposed sell of a lottery ticket for a cent,\nwhich is well below its actuarially fair value.\n\n> I will lose the lottery.\\\n> If I keep the ticket, I will get nothing.\\\n> If I sell the ticket, I will get a cent.\\\n> So I ought to sell the ticket. (174)\n\n(To make this fully explicit, it helps to add the tacit premise that a\ncent is better than nothing.) Hawthorne says that this is intuitively a\n*bad* argument, and concludes that the agent who attempts to use it is\nnot in a position to know its first premise. But that conclusion only\nfollows if we assume that the argument form is acceptable. So it is\nplausible to conclude that he endorses Fantl and McGrath's second\nprinciple.\n\nThe interesting question here is whether the theory endorsed in this\npaper can validate the true principles that Fantl and McGrath\narticulate. (Or, more precisely, we can validate the equivalent true\nprinciples concerning justified belief, since knowledge is outside the\nscope of the paper.) I'll argue that it can in the following way. First,\nI'll just note that given the fact that the theory here implies the\nclosure principles we outlined in section 5, we can easily enough\nendorse Fantl and McGrath's first two principles. This is good, since\nthey seem true. The longer part of the argument involves arguing that\ntheir principle (PC), which doesn't hold on the theory endorsed here, is\nin fact incorrect.\n\nOne might worry that the qualification on the closure principles in\nsection 5 mean that we can't fully endorse the principles Fantl and\nMcGrath endorse. In particular, it might be worried that there could be\nan agent who believes that _p_, believes that if _p_, then A is better\nthan B, but doesn't put these two beliefs together to infer that A is\nbetter than B. This is certainly a possibility given the qualifications\nlisted above. But note that in this position, if those two beliefs were\njustified, the agent would certainly be *rational* to conclude that A is\nbetter than B, and hence rational to prefer A to B. So the constraints\non the closure principles don't affect our ability to endorse these two\nprinciples.\n\nThe real issue is (PC). Fantl and McGrath offer a lot of cases where\n(PC) holds, as well as arguing that it is plausibly true given the role\nof implications in practical reasoning. What's at issue is that (PC) is\nstronger than a deductive closure principle. It is, in effect,\nequivalent to endorsing the following schema as a valid principle of\nimplication.\n\n> _p_\\\n> Given _p_, A is preferable to B\\\n> So, A is preferable to B\n\nI call this Practical Modus Ponens, or PMP. The middle premise in PMP is\n*not* a conditional. It is not to be read as *If p, then A is preferable\nto B*. Conditional valuations are not conditionals. To see this, again\nconsider the proposed bet on (true) _p_ at exorbitant odds, where A is\nthe act of taking the bet, and B the act of declining the bet. It's true\nthat given _p_, A is preferable to B. But it's not true that if _p_,\nthen A is preferable to B. Even if we restrict our attention to cases\nwhere the preferences in question are perfectly valid, this is a case\nwhere PMP is invalid. Both premises are true, and the conclusion is\nfalse. It might nevertheless be true that whenever an agent is justified\nin believing both of the premises, she is justified in believing the\nconclusion. To argue against this, we need a *very* complicated case,\ninvolving embedded bets and three separate agents, Quentin, Robby and\nThom. All of them have received the same evidence, and all of them are\nfaced with the same complex bet, with the following properties.\n\n-   _p_ is an historical proposition that is well (but not conclusively)\n    supported by their evidence, and happens to be true. All the agents\n    have a high credence in _p_, which is exactly what the evidence\n    supports.\n\n-   The bet A, which they are offered, wins if _p_ is true, and loses if\n    _p_ is false.\n\n-   If they win the bet, the prize is the bet B.\n\n-   _S_ is also an historical proposition, but the evidence tells\n    equally for and against it. All the agents regard _S_ as being about\n    as likely as not. Moreover, _S_ turns out to be false.\n\n-   The bet B is worth \\$2 if _S_ is true, and worth -\\$1 if _S_ is\n    false. Although it is actually a losing bet, the agents all\n    rationally value it at around 50 cents.\n\n-   How much A costs is determined by which proposition from the\n    partition {$q, r, s$} is true.\n\n-   If $q$ is true, A costs \\$2\n\n-   If $r$ is true, A costs \\$500\n\n-   If $t$ is true, A costs \\$1\n\n-   The evidence the agents has strongly supports $r$, though $t$ is in\n    fact true\n\n-   Quentin believes $q$\n\n-   Robby believes $r$\n\n-   Thom believes $t$\n\nAll of the agents make the utility calculations that their beliefs\nsupport, so Quentin and Thom take the bet and lose a dollar, while Robby\ndeclines it. Although Robby has a lot of evidence in favour of _p_, he\ncorrectly decides that it would be unwise to bet on _p_ at effective\nodds of 1000 to 1 against. I'll now argue that both Quentin and Thom are\npotential counterexamples to (PC). There are three possibilities for\nwhat we can say about those two.\n\nFirst, we could say that they are justified in believing _p_, and\nrational to take the bet. The problem with this position is that if they\nhad rational beliefs about the partition {$q, r, t$} they would realise\nthat taking the bet does not maximise expected utility. If we take\nrational decisions to be those that maximise expected utility given a\nrational response to the evidence, then the decisions are clearly not\nrational.\n\nSecond, we could say that although Quentin and Thom are not rational in\naccepting the bet, nor are they justified in believing that _p_. This\ndoesn't seem particularly plausible for several reasons. The\nirrationality in their belief systems concerns whether $q, r$ or $t$ is\ntrue, not whether _p_ is true. If Thom suddenly got a lot of evidence\nthat $t$ is true, then all of his (salient) beliefs would be well\nsupported by the evidence. But it is bizarre to think that whether his\nbelief in _p_ is rational turns on how much evidence he has for $t$.\nFinally, even if we accept that agents in higher stakes situations need\nmore evidence to have justified beliefs, the fact is that the agents are\nin a low-risk situation, since $t$ is actually true, so the most they\ncould lose is \\$1.\n\nSo it seems like the natural thing to say is that Quentin and Thom *are*\njustified in believing that _p_, and are justified in believing that\ngiven _p_, it maximises expected utility to take the bet, but they are\nnot rational to take the bet. (At least, in the version of the story\nwhere they are thinking about which of $q, r$ and $t$ are correct given\ntheir evidence when thinking about whether to take the bet they are\ncounterexamples to (PC).) Against this, one might respond that if belief\nin _p_ is justified, there are arguments one might make to the\nconclusion that the bet should be taken. So it is inconsistent to say\nthat the belief is justified, but the decision to take the bet is not\nrational. The problem is finding a premise that goes along with _p_ to\nget the conclusion that taking the bet is rational. Let's look at some\nof the premises the agent might use.\n\n-   If _p_, then the best thing to do is to take the bet.\n\nThis isn't true (_p_ is true, but the best thing to do isn't to take the\nbet). More importantly, the agents think this is only true if _S_ is\ntrue, and they think _S_ is a 50/50 proposition. So they don't believe\nthis premise, and it would not be rational to believe it.\n\n-   If _p_, then probably the best thing to do is to take the bet.\n\nAgain this isn't true, and it isn't well supported, and it doesn't even\nsupport the conclusion, for it doesn't follow from the fact that $x$ is\nprobably the best thing to do that $x$ should be done.\n\n-   If _p_, then taking the bet maximises rational expected utility.\n\nThis isn't true -- it is a conditional with a true antecedent and a\nfalse consequent. Moreover, if Quentin and Thom were rational, like\nRobby, they would recognise this.\n\n-   If _p_, then taking the bet maximises expected utility relative to\n    their beliefs.\n\nThis is true, and even reasonable to believe, but it doesn't imply that\nthey should take the bet. It doesn't follow from the fact that doing\nsomething maximises expected utility relative to my crazy beliefs that I\nshould do that thing.\n\n-   Given _p_, taking the bet maximises rational expected utility.\n\nThis is true, and even reasonable to believe, but it isn't clear that it\nsupports the conclusion that the agents should take the bet. The\nimplication appealed to here is PMP, and in this context that's close\nenough to equivalent to (PC). If we think that this case is a prima\nfacie problem for (PC), as I think is intuitively plausible, then we\ncan't use (PC) to show that it *doesn't* post a problem. We could\nobviously continue for a while, but it should be clear it will be very\nhard to find a way to justify taking the bet even spotting the agents\n_p_ as a premise they can use in rational deliberation. So it seems to\nme that (PC) is not in general true, which is good because as we'll see\nin cases like this one the theory outlined here does not support it.\n\nThe theory we have been working with says that belief that _p_ is\njustified iff the agent's degree of belief in _p_ is sufficient to\namount to belief in their context, and they are justified in believing\n_p_ to that degree. Since by hypothesis Quentin and Thom are justified\nin believing _p_ to the degree that they do, the only question left is\nwhether this amounts to belief. This turns out not to be settled by the\ndetails of the case as yet specified. At first glance, assuming there\nare no other relevant decisions, we might think they believe that _p_\nbecause (a) they prefer (in the relevant sense) believing _p_ to not\nbelieving _p_, and (b) conditionalising on _p_ doesn't change their\nattitude towards the bet. (They prefer taking the bet to declining it,\nboth unconditionally and conditional on _p_.)\n\nBut that isn't all there is to the definition of belief *tout court*. We\nmust also ask whether conditionalising on _p_ changes any preferences\nconditional on any active proposition. And that may well be true.\nConditional on $r$, Quentin and Thom prefer not taking the bet to taking\nit. But conditional on $r$ and _p_, they prefer taking the bet to not\ntaking it. So if $r$ is an active proposition, they don't believe that\n_p_. If $r$ is not active, they do believe it. In more colloquial terms,\nif they are concerned about the possible truth of $r$ (if it is salient,\nor at least not taken for granted to be false) then _p_ becomes a\npotentially high-stakes proposition, so they don't believe it without\nextraordinary evidence (which they don't have). Hence they are only a\ncounterexample to (PC) if $r$ is not active. But if $r$ is not active,\nour theory predicts that they are a counterexample to (PC), which is\nwhat we argued above is intuitively correct.\n\nStill, the importance of $r$ suggests a way of saving (PC). Above I\nrelied on the position that if Quentin and Thom are not maximising\nrational expected utility, then they are being irrational. This is\nperhaps too harsh. There is a position we could take, derived from some\nsuggestions made by Gilbert Harman in *Change in View*, that an agent\ncan rationally rely on their beliefs, even if those beliefs were not\nrationally formed, if they cannot be expected to have kept track of the\nevidence they used to form that belief. If we adopt this view, then we\nmight be able to say that (PC) is compatible with the correct normative\njudgments about this case.\n\nTo make this compatibility explicit, let's adjust the case so Quentin\ntakes $q$ for granted, and cannot be reasonably expected to have\nremembered the evidence for $q$. Thom, on the other hand, forms the\nbelief that $t$ rather than $r$ is true in the course of thinking\nthrough his evidence that bears on the rationality of taking or\ndeclining the bet. (In more familiar terms, $t$ is part of the inference\nThom uses in coming to conclude that he should take the bet, though it\nis not part of the final implication he endorses whose conclusion is\nthat he should take the bet.) Neither Quentin nor Thom is a\ncounterexample to (PC) thus understood. (That is, with the notion of\nrationality in (PC) understood as Harman suggests that it should be.)\nQuentin is not a counterexample, because he is *rational* in taking the\nbet. And Thom is not a counterexample, because in his context, where $r$\nis active, his credence in _p_ does not amount to belief in _p_, so he\nis not justified in believing _p_.\n\nWe have now two readings of (PC). On the strict reading, where a\nrational choice is one that maximises rational expected utility, the\nprinciple is subject to counterexample, and seems generally to be\nimplausible. On the loose reading, where we allow agents to rely on\nbeliefs formed irrationally in the past in rational decision making,\n(PC) *is* plausible. Happily, the theory sketched here agrees with (PC)\non the plausible loose reading, but not on the implausible strict\nreading. In the previous section I argued that the theory also accounts\nfor intuitions about particular cases like *Local and Express*. And now\nwe've seen that the theory accounts for our considered opinions about\nwhich principles connecting justified belief to rational decision making\nwe should endorse. So it seems at this stage that we can account for the\nintuitions behind the pragmatic encroachment view while keeping a\nconcept of probabilistic epistemic justification that is free of\npragmatic considerations.\n\n### Conclusions\n\nGiven a pragmatic account of belief, we don't need to have a pragmatic\naccount of justification in order to explain the intuitions that whether\n$S$ justifiably believes that _p_ might depend on pragmatic factors. My\nfocus here has been on sketching a theory of belief on which it is the\nbelief part of the concept of a justified belief which is pragmatically\nsensitive. I haven't said much about why we should prefer to take that\noption than say that the notion of epistemic justification is a\npragmatic notion. I've mainly been aiming to show that a particular\nposition is an open possibility, namely that we can accept that whether\na particular agent is justified in believing _p_ can be sensitive to\ntheir practical environment without thinking that the primary epistemic\nconcepts are themselves pragmatically sensitive.\n\n[^1]: To say the agent prefers A to B given $q$ is not to say that if\n    the agent were to learn $q$, she would prefer A to B. It's rather to\n    say that she prefers the state of the world where she does A and $q$\n    is true to the state of the world where she does B and $q$ is true.\n    These two will come apart in cases where learning $q$ changes the\n    agent's preferences. We'll return to this issue below.\n\n[^2]: This might seem *much* too simple, especially when compared to all\n    the bells and whistles that functionalists usually put in their\n    theories to (further) distinguish themselves from crude versions of\n    behaviourism. The reason we don't need to include those\n    complications here is that they will all be included in the analysis\n    of *preference*. Indeed, the theory here is compatible with a\n    thoroughly anti-functionalist treatment of preference. The claim is\n    not that we can offer a functional analysis of belief in terms of\n    non-mental concepts, just that we can offer a functionalist\n    reduction of belief to other mental concepts. The threshold view is\n    *also* such a reduction, but it is such a crude reduction that it\n    doesn't obviously fall into any category.\n\n[^3]: Conditionalising on the proposition *There are space aliens about\n    to come down and kill all the people writing epistemology papers*\n    will make me prefer to stop writing this paper, and perhaps grab\n    some old metaphysics papers I could be working on. So that\n    proposition satisfies the second clause of the definition of\n    relevance. But it clearly doesn't satisfy the first clause. This\n    part of the definition of relevance won't do much work until the\n    discussion of agents with mistaken environmental beliefs in section\n    7.\n\n[^4]: I think putting things numerically is misleading because it\n    suggests that the kind of bets we usually use to measure degrees of\n    belief are open, salient options for Louise and Harry. But if those\n    bets were open and salient, they wouldn't *believe* the train is a\n    local. Using qualitative rather than quantitative language to\n    describe them is just as accurate, and doesn't have misleading\n    implications about their practical environment.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
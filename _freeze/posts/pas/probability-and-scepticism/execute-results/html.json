{
  "hash": "4c1953b69a729c0907443c2f894108e2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Probability and Scepticism\"\ndescription: |\n  One way to motivate scepticism is by looking at the ways we might possibly know we aren’t brains in vats. Could we know we aren’t brains in vats a priori? Many will say no, since it is possible to be a brain in a vat. Could we know it on the basis of evidence? The chapter argues that given some commonly held assumptions, the answer is no. In particular, there is a kind of sceptical hypothesis whose probability is decreased by conditionalising on the evidence we have. Using this fact, I argue that if we want to say our knowledge that we aren’t brains in vats is a posteriori, we have to give up the view that all updating on evidence is by conditionalisation.\ndate: June 10 2014\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ndoi: \"10.1093/acprof:oso/9780199658343.003.0004\"\ncategories:\n  - epistemology\n  - scepticism\ncitation_url: https://doi.org/10.1093/acprof:oso/9780199658343.003.0004\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: demon.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 4\n    number_sections: true\n---\n\n\n### The Humean Sceptical Argument\n\nThe following, broadly Humean, sceptical argument is fascinating for\nmany reasons.[^1] In the argument $E$ is the agent's evidence, $H$ is\nsome hypothesis derived by ampliative reasoning from her evidence, and\n$\\supset$ is the (classical) material conditional, i.e.,\n$\\neg E \\vee H$.[^2]\n\n1.  It is not possible for the agent to know $E \\supset H$ *a priori*.\n\n2.  It is not possible for the agent to know $E \\supset H$ *a\n    posteriori*.\n\n3.  So, it is not possible for the agent to know $E \\supset H$.\n\nIf we add as an extra premise that if the agent does know $H$, then it\nis possible for her to know $E \\supset H$ by using $\\vee$-introduction,\nwe get the conclusion that the agent does not really know $H$. But even\nwithout that closure premise, or something like it, the conclusion seems\nquite dramatic.\n\n<aside>\nPublished in Scepticism and Perceptual Justification, edited by Dylan Dodd and Elia Zardini, OUP 71-86.\n\nPicture by [aa.thompson](https://www.flickr.com/photos/23258714@N08) via [Creative Commons](https://search.creativecommons.org/photos/85a588eb-03f2-42c6-9a8d-0e4120fa6fad).\n</aside>\n\nOne possible response to the argument, floated by both Descartes and\nHume, is to accept the conclusion and embrace scepticism. We cannot know\nanything that goes beyond our evidence, so we do not know very much at\nall. This is a remarkably sceptical conclusion, so we should resist it\nif at all possible.\n\nA more modern response, associated perhaps most strongly with Timothy\nWilliamson's view that our evidence just is as our knowledge, is to\naccept the conclusion but deny it is as sceptical as it first appears\n[@Williamson1998-WILCOK; @Williamson2000-WILSAE-2]. The Humean argument,\neven if it works, only shows that our evidence and our knowledge are\nmore closely linked than we might have thought. Perhaps that's true\nbecause we have a lot of evidence, not because we have very little\nknowledge.\n\nThere's something right about this response I think. We have more\nevidence than Descartes or even Hume thought we had. But I think we\nstill need the idea of ampliative knowledge. It stretches the concept of\nevidence to breaking point to suggest that all of our knowledge,\nincluding knowledge about the future, is part of our evidence. So the\nconclusion really is unacceptable. Or, at least, I think we should try\nto see what an epistemology that rejects the conclusion looks like.\n\nI'm going to argue here that such an epistemology has to deviate in one\nway or another from orthodox views. In particular, I'll argue that it\nhas to accept deeply contingent *a priori* knowledge, or reject the idea\nthat probabilistic updating should always go by conditionalisation.\n\n### A Probabilistic Argument for the *a posteriori* Premise\n\nRejecting the conclusion would be easy if it was easy to reject the\npremises. But in fact there are quite strong defences of each of the\npremises. Let's look at some of them.\n\nThe simplest argument in favour of premise 1 uses a little bit of\nempiricism. It could turn out to be true that $E \\supset H$. What could\nturn out to be false can only be known *a posteriori*.[^3] So we can't\nknow *a priori* that $E \\supset H$. The crucial premise there, about the\nlimits of the *a priori*, is the distinctively empiricist assumption,\nbut it is shared by a lot of contemporary philosophers.[^4]\n\nThe simplest argument in favour of premise 2 uses a little bit of\nrationalism, though I think it takes a little more to see that it is a\nrationalist assumption. Here's the argument in premise-conclusion form;\nwe'll go through each of the premises at some length below. So as to\navoid confusion with the Humean argument, I've named the premises rather\nthan numbered them.\n\nCredences are Classical Probabilities (CCP)\n\n:   *Cr* is a classical probability function.\n\nUpdating Theorem (UT)\n\n:   Let $E = E_1 \\wedge \\dots \\wedge E_n$,\n    $\\Pr(E) > 0, \\Pr(H) < 1, \\Pr(E \\supset H) < 1$ and for each $i$,\n    $\\Pr(E_i) < 1$. And assume $\\Pr$ is a classical probability\n    function. Then $\\Pr(E \\supset H| E_i) < \\Pr(E \\supset H)$.\n\nUpdating is Conditionalisation (UIC)\n\n:   If we use *Cr* to measure our rational agent's credences, and\n    *Cr*$_Y$ to be her credences after updating with evidence $Y$, then\n    *Cr*$_Y(X) = \\textit{Cr}(X | Y)$ for all $X, Y$.\n\nLearning Doesn't Lower Credence (LDLC)\n\n:   It is impossible for a rational agent to learn $X$ on the basis of\n    evidence $Y$ if *Cr*$_Y(X) < \\textit{Cr}(X)$.\n\nKnowledge Requires Learning (KRL)\n\n:   If the agent knows $E \\supset H$ *a posteriori*, i.e., on the basis\n    of her empirical evidence, then there is some part of her evidence\n    $E_i$ on the basis of which she learned $E \\supset H$, and before\n    she learned it, her credence in $E_i$ is less than 1.[^5]\n\nHumean Conclusion (HC)\n\n:   So, it is impossible to know $E \\supset H$ *a posteriori*.[^6]\n\nNow if someone wants to reject the Humean argument at premise 2, they\nbetter reject one of these five principles. But the principles are each\nreasonably strong.\n\n#### Classical and Non-Classical Credences\n\nThere is a huge literature on whether credence functions should be\nprobability functions. For a good recent overview, see @Hajek2008. Most\nof that literature has assumed that the underlying logic we use in\nreasoning under uncertainty should be classical. But this assumption can\nbe questioned too, as I did in @Weatherson2004. It turns out this\nmatters for the argument here. Without some extensive use of classical\nassumptions, it doesn't always hold that\n$\\Pr(E \\supset H| E) < \\Pr(E \\supset H)$. (For more on this, see\n@JehleWeatherson.) In principle, that's one possible way out of the\nargument. But I imagine it will be too costly a way out for most\nphilosophers.\n\n#### The Updating Theorem\n\nThis is a theorem, so it is harder to reject! It's not a new theorem by\nany stretch; in fact it is a fairly simple result. Here's a proof of it.\nThe proof uses the following very familiar result from the classical\nprobability calculus.\n\n$$\\Pr(X) = \\Pr(X | Y)\\Pr(Y) + \\Pr(X | \\neg Y)\\Pr(\\neg Y)$$\n\nWe'll substitute $E \\supset H$ for $X$ and $E_i$ for $Y$, to get.\n\n$$\\Pr(E \\supset H) = \\Pr(E \\supset H| E_i)\\Pr(E_i) + \\Pr(E \\supset H| \\neg E_i)\\Pr(\\neg E_i)$$\n\nSince $\\neg E_i$ entails $E \\supset H$ it follows that\n$\\Pr(E \\supset H| \\neg E_i) = 1$. And since\n$\\Pr(E \\supset H| E_i) \\leq 1$, it follows that\n$\\Pr(E \\supset H| E_i) \\leq  \\Pr(E \\supset H| \\neg E_i)$. So\nsubstituting $\\Pr(E \\supset H|  E_i)$ for $\\Pr(E \\supset H| \\neg E_i)$\non the right-hand side of that equation, and noting that we can't make\nthe right-hand side larger by that substitution, we get,\n\n$$\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)\\Pr(E_i) + \\Pr(E \\supset H| E_i)\\Pr(\\neg E_i)$$\n\nwith equality only if\n$\\Pr(E \\supset H| E_i) =  \\Pr(E \\supset H| \\neg E_i) = 1$ or\n$\\Pr(\\neg E_i) = 0$. But we assumed that $\\Pr(E_i) < 1$, so\n$\\Pr(\\neg E_i) \\neq 0$. We'll come back to the argument that\n$\\Pr(E \\supset H| E_i) < 1$. Note for now that we can rewrite that\ninequality by factoring out $\\Pr(E \\supset H| E_i)$, to get\n\n$$\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)(\\Pr(E_i) + \\Pr(\\neg E_i))$$\n\nBut $\\Pr(E_i) + \\Pr(\\neg E_i) = 1$, is a trivial theorem of the\nclassical probability calculus, so this just reduces to:\n\n$$\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)$$\n\nSince $\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)$, and we assumed\n$\\Pr(E \\supset H) < 1$, it follows that $\\Pr(E \\supset H| E_i) < 1$. But\nthat means neither condition for the inequality introduced above to not\nbe strict is satisfied. So in fact we can conclude:\n\n$$\\Pr(E \\supset H) > \\Pr(E \\supset H| E_i)$$\n\nas required.\n\n#### Updating and Conditionalising\n\nIn @Weatherson2007, I argue that philosophers who are sympathetic to\nempiricism (broadly construed) should reject (UIC). That's because (UIC)\nembodies a very implausible picture of the relationship between evidence\nand hypotheses. We can see this more clearly if we think about the\nnon-probabilistic case first. Consider the following hypothesis.\n\n-   After learning $E$, an agent should believe $H$ iff they believed\n    $E \\supset H$ before learning $E$.\n\nThis picture suggests that all a rational agent has to do is line up all\ntheir thoughts at the beginning of time, or I guess of inquiry, and then\ngo around collecting evidence and applying modus ponens. Indeed, it says\nthere is nothing else that would be rational to do. This strikes me as\nimplausible in the extreme. There are many more rules we can use to get\nfrom evidence to conclusion than modus ponens applied to pre-known\nconditionals. Sometimes, it is only by getting some evidence that we are\nin a position to see what that evidence supports.[^7]\n\nNow the rule that we should always update by conditionalisation is like\nthe rule that we should always update by modus ponens in the way just\nsuggested. Instead of saying that learning $E$ doesn't change which\nconditionals with antecedent $E$ we can know to be true, it says that\nlearning $E$ doesn't change the conditional probability of anything\ngiven $E$. And it seems equally implausible for just the same reason.\n\nSo I don't think (UIC) is right, and I suspect at the end of the day\nrejecting it is the best way to avoid the Humean sceptical argument. But\nI do think that there are many people who are not so sceptical (as a\ncasual perusal of the literature on conditionalisation will show). And\nthere may be several others who are implicitly committed to (UIC),\nwhether or not they explicitly acknowledge that fact. So I think it is\ninteresting to see how (UIC) can promote a certain kind of\nscepticism.[^8]\n\n#### Learning and Credence\n\nWe will look at a lot of cases that seem to raise problems for (LDLC)\nbelow. But first I just wanted to note that the restriction to rational\nagents avoids one quick problem for the principle. An irrational agent\nmight simply ignore very good evidence for $p$, and then come to believe\n$p$ on the basis of evidence that undermines that initial evidence for\n$p$, but provides an independent (but weaker) basis for believing $p$.\nShe really could learn $p$ on the second basis, even though its\nprobability was lowered.\n\nThe restriction to rational agents is intended to rule out such a case.\nWe assume that the agent has antecedently taken correct account of the\navailable evidence. If that isn't the case, then something which lowers\nthe probability of $p$ can ground knowledge that $p$, perhaps because it\nreinforces evidence that $S$ had, but was not properly using. What's\ninteresting is whether we can have violations of (LDLC) without\nirrationality.\n\n#### Learning and Knowing\n\nYou might think that the last premise, (KRL), would be the easiest one\nto defend. Arguably something even stronger is an analytic truth, namely\nthat $S$ learns $p$ at $t$ iff $S$ knows $p$ at $t$ but not before $p$.\nIndeed, I used to think this. But it isn't actually true. What is\nplausibly true, as we'll see by some reflections on learning, is that\nknowing requires either innate knowledge or learning. But the\nrelationship between the learning and the knowing may be very\ncomplicated indeed. Let's turn to that relationship now.\n\n### Learning and Defeaters\n\nIn an earlier version of this paper, I worked with a much simpler\npremise, namely that coming to know required probability non-decrease.\nBut that isn't right.[^9] The problem is that the view in question\ndoesn't account for defeaters.\n\nHere's a schematic version of the kind of case that causes problems.\nAssume $S$ has a justified true belief that $p$. Assume also that there\nis some defeater $D$ that blocks $S$'s belief from being knowledge. Now\nimagine an event happens that (a) slightly lowers the evidential\nprobability of $p$ for $S$, and (b) defeats the defeater $D$. Then after\nthe event, it may be that $S$ knows that $p$, although she does so in\npart in virtue of an event that lowered the probability of $p$.\n\nThe schematic version of this argument is much more plausible than any\nparticular case, since defeaters are often very hard to get clear\njudgments about. But here are three cases that may illustrate the kind\nof thing I have in mind.\n\n> **Dead Dictator**\n>\n> Carol is trapped in Gilbert Harman's dead dictator story [@Harman1973\n> 75]. At $t_1$ she reads the one newspaper that correctly (and\n> sensitively) reported that the dictator has died. She hasn't seen the\n> copious other reports that the dictator is alive, but the existence of\n> those reports defeats her putative knowledge that the dictator is\n> alive. At $t_2$, all the other news sources change their tune, and\n> acknowledge the dictator has died. Carol doesn't see any of those\n> newspapers; she's too busy playing Farmville. But Carol's memory very\n> slowly degrades over time (as most memories do), so at $t_2$ her\n> evidence that the dictator died is slightly weaker than at $t_1$.\n> Still, over the time between $t_1$ and $t_2$ while she played\n> Farmville, she came to know the dictator had died, even while the\n> (evidential) probability of that decreased.\n\n> **Fake Barns**\n>\n> Bob starts our story in Fake Barn Country [@Goldman1976]. At $t_1$, he\n> starts looking straight at a genuine barn on a distant hill, and forms\n> the belief that there is a barn on that hill. Since he's in fake barn\n> country, he doesn't know there is a barn on the hill. At $t_2$, while\n> Bob is still looking at the one genuine barn, all the fake barns are\n> instantly destroyed by a visiting spaceship, from a race which doesn't\n> put up with nonsense like fake barns. The mist from the vaporised\n> barns slightly clouds Bob's vision, so he doesn't have quite as clear\n> a view of the barn on the hill. But he still has an excellent view, so\n> after the barns are destroyed, Bob's belief that there is a barn on\n> that hill is knowledge. So at $t_2$ he comes to know, for the first\n> time, that there is a barn on that hill. But the vaporisation of the\n> fake barns, which is what lets him come to know that there is a barn\n> on that hill, doesn't raise the (evidential) probability that there is\n> a barn there.[^10] Indeed, by making Bob's vision a little cloudier,\n> it lowers that probability.\n\n> **Gettier Cases**\n>\n> Ted starts our story believing (truly, at least in the world of the\n> story) that Bertrand Russell was the last analytic philosopher to win\n> the Nobel Prize in literature. The next day, the 2011 Nobel Prize in\n> literature is announced. At $t_1$, a trustworthy and very reliable\n> friend of Ted's tells him that Fred has won the Nobel Prize in\n> literature. Ted believes this, and since Fred is an analytic\n> philosopher, Ted reasonably infers that, as of 2011 at least, Bertrand\n> Russell was not the last analytic philosopher to win the Nobel Prize\n> in literature. This conclusion is true, but not because Fred won. In\n> fact, Ed, who is also an analytic philosopher, won the 2011 Nobel\n> Prize in literature. At $t_2$, Ted is told by a friend who is just\n> slightly less reliable than the first friend that it is Ed, not Fred,\n> who won the prize.[^11] Since Ted knows that Ed is also an analytic\n> philosopher, this doesn't change his belief that Bertrand Russell was\n> not the last analytic philosopher to win the Nobel Prize in\n> literature. But it does change that belief from a mere justified true\n> belief into knowledge.\n>\n> At $t_1$, Ted didn't know that Bertrand Russell was not the last\n> analytic philosopher to win the Nobel Prize in literature, since his\n> true belief was based on a falsehood.[^12] At $t_2$, he did know this,\n> on the basis of the second friend's testimony. But since the second\n> friend was less reliable, and since the second piece of testimony\n> raised doubts about the first in ways that render each of them\n> suspect, the probability of Ted's conclusion was lower at $t_2$ than\n> $t_1$. So the second piece of testimony both lowered the probability\n> of Ted's conclusion, and turned it into knowledge.\n\nIn every one of those cases, something happens that ever so slightly\nlowers the probability of $p$, and also defeats a defeater of the\nagent's knowledge that $p$. So the agent gets knowledge that $p$ in\nvirtue of an event that lowers the probability of $p$.\n\nBut there is, in general, something odd about events that bring about a\nconclusion by double prevention. There's a big difference between being\nresponsible for a pot of soup in virtue of preparing and cooking it, and\nbeing responsible for it in virtue of removing the banana peel that the\nchef would have slipped on when bringing the pot to the table. The same\ngoes for knowledge; things that remove defeaters of knowledge are\nimportantly different in kind to the underlying bases for knowledge.\n\nThe difference in question is one that we mark in language. We say that\nthe chef cooked, or prepared, the soup. We don't say that the banana\npeel remover did either of those things, although she may have caused\nthe soup to be ready to eat. In the three cases described above, I think\nit's odd to say that the agent *learns* that $p$ in virtue of the\ndefeater being defeated.[^13]\n\nCarol can't learn that the dictator has died while she is busy playing\nFarmville, and not being in any contact (of the right kind) with the\noutside world. So the passage of time from $t_1$ to $t_2$ doesn't cause\nher to *learn* the dictator has died. If she ever learned this, she\nlearned it at $t_1$. And surely she did learn it. It wasn't innate\nknowledge, and it wasn't knowledge that was somehow implanted in her, in\nthe way characters in the movie *The Matrix* can have knowledge\nimplanted directly into their brain.[^14] So she learned the dictator\ndied, and the only learning she did took place at $t_1$, so she learned\nthat the dictator died at $t_1$.\n\nI think the same thing is true in the other cases. Bob learns that there\nis a barn on that hill at $t_1$, but doesn't know this until $t_2$. And\nTed learns that Russell is not the last analytic philosopher to win at\n$t_1$, but doesn't know this until $t_2$. So actually cases where\ndefeaters are defeated by probability lowerers are not counterexamples\nto (LDLC).\n\nOfficially, that completes my defence of (LDLC) from this kind of\nobjection. But I know that not everyone agrees with my judgments about\nthese three cases, especially the last. So I wanted to say a bit about\nwhy the overall argument is not overly affected even if I'm wrong about\n(LDLC).\n\nNote that in all three of the cases, there are two distinctive things\nthat happen at $t_1$. The agent gets a lot of evidence in favour of $p$.\nAnd the agent gets some kind of defeater that prevents beliefs based on\nthat evidence turning into knowledge. Now let's say that the\nprobabilistic argument that $E \\supset H$ can't be known *a\nposteriori* fails because of an analogy with these cases. That is, let's\nsuppose that $E \\supset H$ can be known *a posteriori* even though all\nthe empirical evidence *lowers* its probability, and the explanation for\nhow this is possible is by analogy with cases like **Dead Dictator**.\nThen we should be able to find analogies for these two properties:\nsomething sometime raises the probability of $p$, and there is a\ndefeater that prevents $p$ being known despite having a high\nprobability.\n\nThe first putative point of analogy obviously fails. After all,\n$E \\supset H$ was designed so that the agent *never* gets evidence that\nraises its probability. So we should already be suspicious of such an\nanalogy going through. But the second putative point of analogy is\nactually pretty interesting. Could there be a defeater that prevents\nsomeone *knowing a priori* that $E \\supset H$ even though the *a\npriori* probability of $E \\supset H$ is very high?[^15]\n\nI don't have a conclusive argument that there is no such defeater, but\nit's worth noting that most of the usual suspects don't seem to work.\n\nSensitivity\n\n:   It's true that the *a priori* belief that $E \\supset H$ is\n    insensitive. That is, even if it were false, it would still be held.\n    But the *a posteriori* belief that $E \\supset H$ is also\n    insensitive. So if insensitivity is a barrier to knowledge, this is\n    a quick argument for the conclusion of the Humean sceptical\n    argument, not a way to block a premise in an argument for premise\n    2.[^16]\n\nSafety\n\n:   The belief that $E \\supset H$ is true seems to be safe. After all,\n    any world in which it is false must be rather distant. If not, then\n    we don't know very much about the external world, which means we\n    have a direct argument for the conclusion of the Humean sceptical\n    conclusion, not a way to block a premise in an argument for\n    premise 2.\n\nReliability\n\n:   There are a few reliable ways in which $E \\supset H$ could be\n    believed. One is the rule, in any circumstance, believe\n    $E \\supset H$. More practically, the rule that says whenever $X$ is\n    good evidence for $Y$, good enough to ground knowledge that $Y$, and\n    one doesn't have any evidence for $X \\wedge \\neg Y$, then believe\n    $X \\supset Y$ seems fairly reliable too. So there isn't an obvious\n    reliability argument that $E \\supset H$ is not knowable *a priori*.\n\nFalse Belief\n\n:   It's possible to infer $E \\supset H$ *a priori* from a false\n    premise. But it isn't necessary. The inference from the premise that\n    $E$ is good evidence for $H$ to the conclusion $E \\supset H$ seems\n    reasonable, and based on true (indeed knowable) premises.\n\nIn short, the following position looks untenable to me: It's possible to\nhave *a priori* a justified true belief in $E \\supset H$, but defeaters\nalways conspire to ensure that this cannot rise to the level of\nknowledge. There just aren't the defeaters around to ensure this works.\n\nA corollary to this is that it is impossible to learn $E \\supset H$ on\nthe basis of a probability lowerer that simultaneously defeats an *a\npriori* defeater to $E \\supset H$. There just aren't enough defeaters\naround for that strategy to work.\n\n### Learning, Probability and Interests\n\nA slightly different kind of objection to (LDLC) comes from\nconsiderations about lottery cases. My reply, in short, is going to be\nthat standard treatments of lottery cases are not very promising, that\nwe should adopt a kind of interest-relative approach to lottery cases\ninstead, and when we do that the problem goes away. But first I'll set\nout the problem.[^17]\n\n#### Lotteries and Learning\n\nThe case we will focus on concerns testimony from a source not certain\nto be reliable or knowledgeable, and we need a way to model that. I'll\nassume that if $Ra$ is the proposition that $R$ is a knowledgeable\ntestifier, and $Sap$ the proposition that $a$ said that $p$, then our\nagent's credences satisfy the following constraints for any testifier\n$a$.\n\n-   *Cr*$(p | Ra \\wedge Sap) = 1$\n\n-   *Cr*$(p | \\neg Ra \\wedge Sap) = \\textit{Cr}(p | \\neg Sap)$\n\nThat is, testimony from a knowledgeable source is maximally valuable\ntestimony, while testimony from other sources has no evidential value.\nThe second assumption is a little extreme[^18], but more moderate models\nwill also generate the kind of example we're interested in here.[^19]\n\nThe case concerns a lottery that is based around a series of coin flips.\nEach lottery ticket consists of a 20-character string of H's and T's. A\nfair coin is flipped 20 times in a row. The agent wins iff the sequence\nof H's and T's on one's ticket matches the sequence of Heads and Tails\nthat come up as the coin is flipped. The rational agent has one ticket\nin this lottery, so their initial credence that they will lose the\nlottery is $1 - 2^{-20}$. Let $X$ be the proposition that they will lose\nthe lottery.\n\nThe agent will get some testimony from two sources, first $b$, then $c$.\nThe agent's prior credence in $Rb$ is 1. That is, she is certain that\nwhat $b$ says is true. And her credence in $Rc$ is 0.99, which is\nreasonably high. (But we'll come back to the question of just how high\nit is by everyday standards.) Still, she does allow there is a non-zero\nprobability that $c$'s vision was inaccurate, or that their memory was\ninaccurate, or that they are being deliberately misleading, or that any\none of the myriad ways in which individual testifiers fail to be\naccurate infected $c$'s testimony. The agent then gets the following two\npieces of evidence.\n\n-   The agent is told by $b$ that the first 19 characters on their\n    ticket match the first 19 flips of the coin.\n\n-   The agent is told by $c$ that the last character on their ticket\n    does not match the last flip of the coin.\n\nIn both cases we'll assume that the testifiers know the truth of their\nassertions, though we won't make any assumptions yet about whether the\nagent shares in this knowledge. After she gets the first piece of\nevidence, her credence in $X$ drops to 0.5. After she gets the second\npiece of evidence, her credence in $X$ rises back up to 0.995. That's\nhigh, but notably it is less than her prior credence in $X$.\n\nStill, we might think that the agent is now in a position to know $X$,\nand she wasn't before getting this evidence. She has learned that her\nticket lost from a knowledgeable source. (Strictly, she has learned\nsomething that entailed this, but this doesn't affect the overall\nargument.) To be sure, she has some minor reservations about the\nreliability of this source, but those reservations are no greater than\nmost of us have about the testimony we get from friends and\nacquaintances everyday. And we typically take that testimony to produce\nknowledge. So it looks like, if $Y$ is the combination of these two\npieces of testimony, then $Y$ lowers her credence in $X$, as we'll put\nit, it makes $X$ less credible, but it also grounds knowledge of $X$.\nThat's a counterexample to (LDLC), or so it looks.\n\nSomeone might object here that for many everyday pieces of knowledge,\nthe prior credibility of our testifier is greater than 0.99. That\ndoesn't mean the testifier is right 99% of the time, just that on this\noccasion the credibility of their knowledgeability is greater than 0.99.\nI'm sympathetic to this line of criticism---I think we often\noverestimate the likelihood of error in everyday settings. But I don't\nthink it matters much here. For one thing, we often learn things by\ntestimony when our credence in the reliability of the testifier is much\nlower than 0.99. For another, we could make the prior credence in $c$'s\nknowledgeability as high as $1 - 2^{-19}$ without affecting the\nargument. (And by increasing the number of coin flips, we can make the\ncredence even higher; arbitrarily close to 1 if need be.) And that's a\nvery high degree of credibility indeed. It seems to me that $c$ is a lot\nlike an ordinary testifier, and rejecting $c$'s testimony as a grounds\nfor knowledge puts one at grave risk of embracing an overarching\nscepticism about testimonial knowledge. That is a sufficient reason to\nstay away from this kind of objection.\n\nThe first thing to note about this example is that what we have here is\na case where there is no single piece of evidence that both lowers the\ncredibility of $X$ and grounds knowledge of $X$. True, if we take $Y$ to\nbe the combination of the two pieces of evidence the agent gets, then\n$Y$ both lowers the credibility of $X$ and grounds knowledge of $X$. But\nthat's because $Y$ has two parts, and one part lowers the credibility of\n$X$ while not grounding knowledge of it, and the other raises the\ncredibility of $X$ and grounds knowledge of it. If we restrict our\nattention to single pieces of evidence, says the objector, then (LDLC)\nis clearly true, and is untouched by this objection.\n\nIt isn't at all clear that anything similar is happening in the case of\n$E$ grounding knowledge of $E \\supset H$. After all, the point of the\ntheorem we earlier proved was that *every single part* of $E$ lowers the\nprobability of $E \\supset H$. Now I don't want to rest too much on a\ntheory of how evidence divides into parts, and maybe there won't be any\nway to make sense of the notion of parts of evidence in a way that is\nneeded for the point I'm making here to work. If we are to have a theory\nof parts of evidence, I like a causal theory of evidence that naturally\nlends itself to individuating parts as being evidence that arrives via\ndifferent causal chains. But I don't think we know nearly enough about\nthe ontology of evidence to make this kind of response compelling.\n\nSo if we are to defend (LDLC), and hence defend the Humean argument from\nattack at this point, we need to say what goes wrong with the example. I\nwill offer a somewhat disjunctive response, with both disjuncts turning\non the interest-relative account of justified belief that I defend in\n@Weatherson2005-WEACWD and @Weatherson2011-WEAKBI. I'll argue on the one\nhand that philosophers have been too quick to accept that we do not know\nwe'll lose lotteries. As David @Lewis1996b pointed out, in many contexts\nit seems perfectly reasonable to say that people do have such knowledge.\nI'll argue that it often sounds right to say that because it's often\ntrue. On the other hand, I'll argue that in those settings where we do\nnot know that the ticket will lose, $c$'s testimony does not help us\ngain knowledge.\n\n#### Interest-Relativity, Knowledge and Justification\n\nIn @Weatherson2005-WEACWD I defended an interest-relative theory of\nbelief. This implied an interest-relative theory of justified belief,\neven though the theory of justification was not, fundamentally,\ninterest-relative. Rather, that theory held that what it was to\njustifiably believe that $p$ was to have a high enough credence to\nbelieve $p$, and for that credence to be justified. What is 'high\nenough'? That, I claimed, was interest-relative. The agent's credence in\n$p$ is high enough for her to believe $p$ if her attitudes conditional\non $p$ matched her unconditional attitudes on every issue that was\nrelevant to her. In particular, I said that for her to believe $p$, then\nfor any $A$ and $B$ where the choice between doing $A$ and $B$ is a\n*live* question (in a sense I describe in much more detail in the\nearlier paper), and $U$ is her utility function, then\n$[U(A) > U(B)] \\leftrightarrow [U(A | p) > U(B | p)]$.\n\nIn that paper I also noted that sometimes the theoretical interests of\nthe agent could be relevant to what she knows, but I don't think I went\nfar enough down that road. Here's what I should have said.[^20] The idea\nbehind my theory was that if you believe $p$, taking $p$ as given in any\ninquiry doesn't change the results of that inquiry. If you believe $p$,\nyou've already factored it in. Now one of the things that we can inquire\ninto is the evidential probability of certain propositions. If we\nalready believe $p$, the results of those inquiries shouldn't change\nwhen we conditionalise on $p$. In particular, we should have the\nfollowing two constraints on belief that $p$.\n\n-   If whether $q$ is more probable than $x$ is a live question, then\n    *Cr*$(q) > x \\leftrightarrow \\textit{Cr}(q | p) > x$.\n\n-   If the comparative probability of $r$ and $s$ is a live question,\n    then\n    *Cr*$(r) > \\textit{Cr}(s) \\leftrightarrow \\textit{Cr}(r | p) > \\textit{Cr}(s | p)$.\n\nThe restriction to live questions here is important. If our credence in\n$p$ is less than 1, even marginally less than 1, then there will be some\ninquiries whose results are altered by conditionalising on $p$. For\ninstance, the question of whether $p$'s probability is or isn't exactly\n1 will be affected by whether we conditionalise on $p$. But that doesn't\nmean that belief requires probability 1. It means that not all inquiries\nare relevant to all agents, and in particular, the question of whether\n$p$'s credence is exactly 1 isn't always relevant.\n\nBut consider one special case. Assume the agent *is* interested in\nexactly what the probability of $p$ is. That is, for all $x$, the\nquestion of whether $\\Pr(p) > x$ is live for her. And assume that she\njudges that probability, on her evidence, to be less than 1. Assume also\nthat she's rational enough to know that $\\Pr(p | p) = 1$. Then she can't\nbelieve that $p$, because there will be some $x$ such that $\\Pr(p) < x$,\nbut $\\Pr(p | p) > x$, and whether $\\Pr(p) > x$ is live.\n\nI think that's a quite nice result. When we're trying to say what the\nrelation is between credence and outright belief, it is tempting for\nmany reasons to say that belief requires credence 1. One reason for that\nis that if we know the objective chance of $p$, and it's less than 1, it\ncan feel very odd to say, without qualification, that we believe that\n$p$. It's much better to say that we believe $p$ is probable. But it's\nvery implausible to say that in general belief requires credence 1,\nbecause that would mean we believe very little.\n\nThe interest-relative view makes sense of this conundrum. On the one\nhand, belief does not in general require credence 1. On the other hand,\nwhen the agent is themselves focussed on the probability of $p$, they\nmust judge that probability to be 1 to outright believe that $p$. I\nthink that's a nice way to steer between the conflicting intuitions\nhere.\n\nGiven all this, it's probably easy to imagine what I'll say about the\nchallenge to (LDLC). The idea behind the challenge was two-fold. First,\npurely probabilistic evidence is not enough for knowledge. Second, other\nsources of evidence, such as testimony, can be the basis for knowledge\neven if we would, if pressed, say that they do not provide more support\nthan purely probabilistic evidence. I'm going to accept the second claim\n(with some qualifications) but reject the first.\n\nI think there are circumstances where we can, with Lewis, say the\nfollowing.\n\n> Pity poor Bill! He squanders all his spare cash on the pokies, the\n> races, and the lottery. He will be a wage slave all his days. We know\n> he will never be rich. (Lewis 1996: 443)\n\nHow, you might ask, can we know Bill will never be rich? The answer is\nthat we know the odds are massively against him winning the lottery.\nThat justifies a very high credence in his losing. For anything we care\nabout, the odds are close enough to 1 that the difference doesn't\nmatter. So our high credence is belief, and since it is justified, true,\nand undefeated, it is knowledge.[^21]\n\nBut wait, you say, isn't there some chance of Bill winning the lottery,\nand hence being rich? Why yes, there is. And doesn't that mean that we\ndon't know he'll never be rich? Indeed it does. And doesn't that mean\nthe previous paragraph is all mistaken? No, it doesn't. It means that\nasking all these questions changes the subject. In particular, it raises\nthe question of whether the chance of Bill winning is equal to zero or\ngreater than zero to salience. And once that question is salient, our\ndegree of belief that Bill will lose is not close enough to 1 that the\ndifference doesn't matter. The difference matters a lot, to the question\nyou just raised. So I insist that given what I cared about a paragraph\nago, I was speaking truly.[^22]\n\nThis explains why we think we can't get knowledge on probabilistic\ngrounds. Here's what we can't do. We can't simultaneously try to figure\nout what the probability of $p$ is, conclude it is less than 1, and\nbelieve $p$. But that's simply because once the question of $p$'s\nprobability is live, we lose the belief that $p$. We can, I think,\ninvestigate whether the probability of $p$ is, say, over 0.9, conclude\nthat it is, and conclude on that basis that $p$. As long as there are no\nfurther questions whose answer turns on whether $p$'s probability is 1\nor a little less, that could be enough for knowledge.\n\nThe converse is true about testimony. It's true that we can gain\nknowledge from testimony. And it's true that, if pressed, we may admit\nthat that testimony is less than perfectly reliable. But what I deny we\ncan do is admit the unreliability, work on figuring out just how\nunreliable it is, and hold onto the knowledge gained from testimony. But\nit's fairly intuitive that this would be impossible. Simultaneously\nthinking that my only reason for believing $p$ is that $S$ told me that\n$p$, and holding that $S$ is somewhat unreliable, and may have been\nmistaken on this occasion, but nevertheless simply believing $p$, is an\nunstable state.\n\nThe difference between probabilistic grounds for belief, as when we\nbelieve we'll lose the lottery, and testimonial grounds then is not that\none of them requires higher standards. It is rather that when we use\nexplicitly probabilistic grounds, we tend to make probabilistic\nquestions salient, and hence live.[^23] And the salience of those\nquestions destroys belief, and hence destroys knowledge. If we make the\nsame questions salient in both the probabilistic and testimonial cases,\nwe get the same criteria for knowledge. Hence the kind of case we've\nbeen considering is not a threat to (LDLC). Indeed, it is hard to see\nwhat could be a threat to (LDLC), without changing the salience of\nprobabilistic questions. So I think (LDLC) survives, and anyone who\nwants to resist the Humean conclusion will have to look elsewhere to\nfind the weak link in the argument.\n\nHere's a crude summary of these reflections. If questions about the\nprecise probability of $H$ or $E \\supset H$ are salient, then\n$E \\supset H$ can't be known before or after learning $E$. If they\naren't, $E \\supset H$ can be known both *a priori* and *a posteriori*.\nThe only way we get that $E \\supset H$ is only knowable after learning\n$E$ is if we equivocate between the two positions on what is salient.\n\n### Conclusions\n\nSo I think (LDLC) is invulnerable to these kinds of objections. Since it\nis intuitively a very plausible principle, and these attempts to\ncounterexample it have failed, I think we should adopt as a working\nhypothesis that it is true. That means, I think, that we really have two\noptions for responding to the Humean argument.\n\n1.  Accept that $E \\supset H$ is *a priori* knowable.\n\n2.  Reject (UIC), and say some updating is not by conditionalisation.\n\nI don't think either of these are bad options. You can read\n@WeathersonSRE as an attempt to defend the first, and @Weatherson2007 as\nan attempt to defend the second. But I do think these options aren't\navailable to everyone.\n\nIf $E \\supset H$ is *a priori* knowable, then any kind of 'modal'\naccount of the *a priori* has to fail. That is, we can't understand *a\npriority* as any kind of metaphysical necessity, since $E \\wedge \\neg H$\nis clearly possible.[^24] It's just that we have defeasible, fallible *a\npriori* knowledge that it isn't true. And I noted above that (UIC) will\nfollow from some other independently attractive views about what we can\nknown *a priori* about epistemology, and when it is that\nconditionalising seems wrong. Many years ago, I held both (UIC) and that\ndeeply contingent truths like $E \\supset H$ could not be known *a\npriori*. I now think that's an unstable combination of views; it leaves\nyou without resources to turn back the Humean argument.\n\n[^1]: On how closely this argument resembles Hume's argument for\n    inductive scepticism, see @Okasha2001 [@Okasha2005]. I've previously\n    discussed the argument in @WeathersonSRE and @Weatherson2007.\n\n[^2]: I'm going to assume throughout that we\n    aren't dealing with the special case where the prior credence of $E$\n    is 0, or of $H$ is 1. That will do some work in section 2.\n\n[^3]: I'm using 'could turn out to be false' in the sense described in\n    @Yablo2002.\n\n[^4]: When I say it is an 'empiricist' assumption, I mean that the two\n    ways of rejecting it correspond to two things classical empiricists\n    rejected. One is that we can reason our way, perhaps abductively, to\n    substantive knowledge about the external world, a la @Vogel1990 or\n    @BonJour1997 . The other is that we have substantial innate\n    knowledge about the external world, and this is not justified by\n    empirical evidence, but perhaps by its reliability. It's interesting\n    that these two forms of rejection are associated with very different\n    views in contemporary philosophy, but they seem both anti-empiricist\n    to me.\n\n[^5]: When I say 'part' here, I just mean that $E_i$ is one of the\n    conjuncts of $E$. This may require relabelling, if for instance the\n    basis for $E \\supset H$ consists of many conjuncts of $E$ under some\n    representation; just collect all those into a single conjunct.\n\n[^6]: *Proof*: Assume for *reductio* that we can know $E \\supset H$ *a\n    posteriori*. So by (KRL) there is some $E_i$ that is the basis of\n    this knowledge, such that before she learned it, her credence in it\n    was less than 1. When she does learn $E_i$, she conditionalises her\n    credences, as required by (UIC). So updating (i.e.,\n    conditionalising) on $E_i$ raised its credence to 1, but by (LDLC)\n    did not lower the agent's credence in $E \\supset H$. As we noted in\n    footnote 2, we're assuming *Cr*$(E) > 0$, and by\n    (CCP) we're assuming (Cr) is a classical probability function, so\n    *Cr*$(E_i) > 0$. We also assumed *Cr*$(H) < 1$. So the conditions\n    for applying (UIT) are all satisfied, and hence her credence in\n    $E \\supset H$ goes down when she updates in $E_i$. That contradicts\n    our earlier conclusion that it does not go down, completing the\n    *reductio*.\n\n[^7]: In [@Weatherson2007] I argue for this by considering agents with\n    radically different kinds of evidence to ours, and noting how much\n    we could know about what kinds of conclusions their evidence\n    supports, and what they could know about what kinds of conclusions\n    our evidence supports.\n\n[^8]: Of course, I'm hardly the only person to promote doubts about\n    (UIC). See @Arntzenius2003-ARNSPF for some very different kind of\n    criticisms.\n\n[^9]: The essential reason it isn't right was pointed out by Martin\n    Smith in comments on this paper at the 2009 Arché scepticism\n    conference. This section is basically a response to the good\n    objections he raised to the earlier version of the paper.\n\n[^10]: It does raise the probability that a randomly selected barn-like\n    structure in Bob's vicinity is a barn, but that's not the evidential\n    probability for Bob of there being a barn in that hill.\n\n[^11]: Presumably for [@Gettier1963].\n\n[^12]: I'm not presupposing here that we can never get knowledge from\n    false beliefs, just that the falsity of Ted's initial belief\n    explains why his subsequent belief is not knowledge. For more on\n    this point, see @Warfield2005.\n\n[^13]: A quick sample of informants suggests that this is much less odd\n    in the Gettier case than in the other two cases. We'll come back to\n    this point below.\n\n[^14]: It's a delicate question whether this kind of procedure is\n    properly called learning. I'm inclined to say that it is, but I\n    suspect a lot of people aren't, so didn't want to presuppose my own\n    idiosyncratic usage here. Thanks here to Jonathan Livengood and\n    Daniele Sgaravatti.\n\n[^15]: Why are we interested in whether we can prevent *a\n    priori* knowledge of $E \\supset H$? Because we're interested in ways\n    in which $E \\supset H$ can be known *a posteriori*, and by\n    definition that means that it isn't known *a priori*. The idea I'm\n    floating here, which I don't think will work, is that the first\n    *knowledge* of $E \\supset H$ is after the agent gets some evidence,\n    and because she gets that evidence, although $E \\supset H$ has\n    maximal probability *a priori*, i.e., before she gets any evidence.\n\n[^16]: @Vogel1987 makes a similar point that sensitivity and induction\n    don't mix.\n\n[^17]: I'm grateful to David Chalmers, Crispin Wright and Elia Zardini\n    for pressing me on the need to address these cases. The cases are\n    discussed in more detail in @MartinSmith2010 and @Zardini2012.\n\n[^18]: I'm interpreting $R$ in such a way that $Ra$ entails what $a$\n    says is true, so $Ra \\wedge Sap$ entails $p$, so the first\n    assumption is natural. Making the second assumption more realistic\n    would just increase the complexity of the model without revealing\n    anything insightful. Since this model is meant to raise problems for\n    my view, I think it is fine to use an extreme case, and not complain\n    about its extremity.\n\n[^19]: I think this kind of model is more realistic than a model that is\n    based around Jeffrey-conditionalising, where we have to specify in\n    advance what the posterior probability of some salient proposition\n    is. That's not required here; the posterior probability of $p$ is an\n    output of the prior probabilities of $p$ and $Ra$, not an input to a\n    Jeffrey-conditionalising formula.\n\n[^20]: I go into much more detail on this in @Weatherson2011-WEAKBI.\n\n[^21]: So I'm disagreeing with those such as @Nelkin2000 who think high\n    probability can't suffice for knowledge. But I think the comments\n    below help explain away the motivations for such views.\n\n[^22]: Two technical points about how what I said relates to the broader\n    debates about interest-relativity.\n\n    I think that what's going on in cases like these involves the\n    interest-relativity of belief, not in the first-instance the\n    interest-relativity of knowledge. Does that mean that if an agent\n    held on to their beliefs across changes of interest, then their\n    knowledge would not be affected by changes of interest? No; because\n    the only way to hold on to beliefs when interests change may involve\n    raising one's credence so high that it would be irrational, and when\n    credences are irrational the resulting beliefs are irrational, and\n    irrational beliefs can't constitute knowledge.\n\n    My positive view is a form a interest-relative *invariantism*; that\n    is, I don't think contextualism is true about 'knows'. But I haven't\n    relied on that here, just on the interest-relativity. If one wanted\n    to hold a form of interest-relative *contextualism*, a la\n    @FantlMcGrath2009, this explanation would still go through. There\n    are puzzles that might push one towards interest-relative\n    *contextualism*, but I think there are larger puzzles that should\n    push one back towards invariantism [@Weatherson2006-WEAQC].\n\n[^23]: Salient to the person doing the reasoning that is. As an\n    invariantist, I think that matters. But a contextualist who thought\n    what's relevant to subjects is thereby relevant could say the same\n    thing.\n\n[^24]: I mean both that it's true in some possible worlds, and in some\n    worlds considered as actual, so a 'two-dimensional' equation of a\n    priority with a kind of metaphysical possible is ruled out.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
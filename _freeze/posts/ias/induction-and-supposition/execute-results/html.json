{
  "hash": "e12adb69a7861ff2c58924be72b431a7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Induction and Supposition\"\ndescription: |\n  An argument that we should not treat rules of inductive inference in ordinary life as being anything like the inference rules in natural deduction systems.\ndate: May 1 2012\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ncategories:\n  - epistemology\n  - induction\n  - scepticism\ncitation_url: https://cdn-researchkent.pressidium.com/reasoning/wp-content/uploads/sites/1804/2019/06/TheReasoner-65.pdf\njournal:\n    title: \"The Reasoner\"\n    publisher: \"University of Kent\"\nvolume: 6\nnumber: 5\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: kent.jpg\noutput:\n  distill::distill_article:\n    toc: false\n    toc_depth: 4\n    number_sections: true\n---\n\n\nHere's a fairly quick argument that there is contingent a priori\nknowledge. Assume there are some ampliative inference rules. Since the\nalternative appears to be inductive scepticism, this seems like a safe\nenough assumption. Such a rule will, since it is ampliative, licence\nsome particular inference *From $A$ infer $B$* where $A$ does not entail\n$B$. That's just what it is for the rule to be ampliative. Now run that\nrule inside suppositional reasoning. In particular, first assume $A$,\nthen via this rule infer $B$. Now do a step of\n$\\rightarrow$-introduction, inferring $A \\rightarrow B$ and discharging\nthe assumption $A$. Since $A$ does not entail $B$, this will be\ncontingent, and since it rests on a sound inference with no\n(undischarged) assumptions, it is a priori knowledge.\n\n<aside>\nPublished in [The Reasoner](https://cdn-researchkent.pressidium.com/reasoning/wp-content/uploads/sites/1804/2019/06/TheReasoner-65.pdf) 6: 78-80.\n</aside>\n\nThis argument is hardly new; John @Hawthorne2002 suggested a similar\nargument ten years ago. But it is a quick argument for a striking\nconclusion, and deserves close scrutiny. I'm going to argue that it\nfails because it falsely assumes that we can treat rules of ampliative\ninference like rules in a natural deduction system, and hence as rules\nthat we can apply inside the scope of a supposition. That assumption has\nrecently been defended by Stewart @Cohen2010 and Sinan @Dogramaci2010,\nbut I'm going to argue, using a construction similar to one found in\nDogramaci, that it leads to absurdity given other plausible premises.\n\nHere's the main argument. If any ampliative inference is justified, I\nthink the following rule, called 'IR', is justified, since this is a\nvery weak form of an inductive inference.\n\nIR\n\n:   From *There are infinitely many Fs, and at most one is not G* and *x\n    is F* infer *x is G* unless there is some $H$ such that it is\n    provable from the undischarged assumptions that *x is F and H* and\n    *There are finitely many things that are both F and H, and one of\n    them is not G*.\n\nNote that the rule doesn't say that merely one $F \\wedge \\neg G$ has\nbeen observed; it requires that just one such thing exists. So this\nseems like a very plausible inference; it really is just making an\ninference within a known distribution, not outside it. And it is\nexplicitly qualified to deal with defeaters. And yet even this rule,\nwhen applied inside the scope of suppositions, can lead to absurdity.\n\nIn the following proof, we'll let $N$ be the predicate 'is a natural\nnumber', and $P$ be the predicate 'is the predecessor of', and I'll\nappeal to the fact that there are infinitely many natural numbers, and\neach number has at most one predecessor. I'll use a version of the proof\nsystem in E. J. Lemmon's *Beginning Logic*, but it should be easy to\ntransform the proof into any other proof system.\n\n$$\\begin{aligned}\n1 && (1) && &Na && \\text{assumption} \\\\\n2 && (2) && &Nb && \\text{assumption} \\\\\n1, 2 && (3) && &\\neg Pab && \\text{(1), (2), IR} \\\\\n1 && (4)  && &Nb \\rightarrow \\neg Pab && \\text{(2), (3), CP} \\\\\n1 && (5)  && &\\forall y (Ny \\rightarrow \\neg Pay) && \\text{(4), UI} \\\\\n && (6)  && &Na \\rightarrow \\forall y (Ny \\rightarrow \\neg Pay) && \\text{(1), (5), CP} \\\\\n && (7) && &\\forall x (Nx \\rightarrow \\forall y (Ny \\rightarrow \\neg Pxy)) && \\text{(6), UI} \\\\\n && (8) && &N2 \\rightarrow \\forall y (Ny \\rightarrow \\neg P2y) && \\text{(7), UE}\\end{aligned}$$\nSo we get the absurd result that if 2 is a number (which it is!), then\nit is the predecessor of no number. But that's absurd, since obviously 3\nis a number and 2 is the predecessor of it. Note that at step 3 we use\nrule IR with $F$ being the predicate *is a natural number*, $G$ being\nthe predicate *does not have a as a predecessor*, and $b$ being $x$.\n\nWhat could have gone wrong? I think the problem is using IR in the\ncontext of a suppositional proof, as we've done here. But let's check if\nthere is another guilty suspect.\n\nIf the problem is Conditional Proof (CP in Lemmon's system), then that's\nabout as bad for the proof in the first paragraph that there are\ncontingent a priori truths as if the problem is IR. Since we're\ninterested in whether that proof works, we won't investigate this option\nfurther. In any case, if $\\rightarrow$ is material implication, that\nrule seems unobjectionable. A referee suggested that if we've used an\nampliative rule earlier, then $\\rightarrow$ should be weaker than\nmaterial implication, and under that interpretation (5) through (8) may\nbe plausible. I think that claim is basically right, but note that if we\ndo this the argument for contingent a priori knowledge with which I\nstarted will fail, since the contingency of $A \\supset B$ will not imply\nthe contingency of $A \\rightarrow B$ if $\\rightarrow$ is weaker than\n$\\supset$.\n\nIt is hard to imagine that Universal Elimination (UE) is the problem. In\nany case, line (7) is obviously bad anyway, so something must have gone\nwrong in the proof before that.\n\nPerhaps the problem is with Universal Introduction (UI); this is what\nDogramaci suggests. One objection he offers is that although we can\nprove every instance of the universal quantifier, inferring the\nuniversal version creates an undue aggregation of risks. Even if line\n(4) is very probable, and it would still be probable if $a$ were\nreplaced with $c$, $d$ or any other name, it doesn't follow that the\nuniversal at line (5) is very probable. But I think this is to confuse\ndefeasible reasoning with probabilistic reasoning. The only way to\nimplement this restriction on making inferences that aggregate risk\nwould be to prevent us making any inference where the conclusion was\nless probable than the premises. That will rule out uses of\n$\\forall$-introduction as at (5). But it will also rule\n$\\wedge$-introduction, and indeed any other inference with more than one\ninput step. To impose such a restriction would be to cripple natural\ndeduction.\n\nAnother objection he offers (UI) is simply that it is the least\nplausible, or least intuitive, of the rules used here. But in fact (UI)\nis extremely intuitive. If we can prove every instance of a schema, we\nshould be able to prove its universal closure. On the other hand,\nallowing ampliative rules to be used inside the scope of a supposition\nallows a quick proof of contingent a priori knowledge, as shown in the\nfirst paragraph. Now maybe there is such knowledge, but its existence is\nhardly intuitive.\n\nSo I conclude the weakest link in the argument is step (3). Although IR\nis a good rule, it can't be used inside the scope of a supposition. And\nsince IR is about as weak an inductive rule as we can imagine, I\nconclude that ampliative inference rules can't in general be used inside\nthe scope of suppositions.\n\nThe general lesson here is that, as was made clear many years ago by\nGilbert @Harman1986 is that there is a difference between rules of\ninference and rules of implication. The quick proof that there's\ncontingent a priori knowledge uses a rule of inference as if it is a\nrule of implication. Not respecting this distinction between inference\nand implication leads to disaster, as we've shown here, and should be\nshunned.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "75281846cb6f13e1c51f3cd24d52c458",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Knowledge, Bets and Interests\"\ndescription: |\n  This paper argues that the interest-relativity of knowledge cannot be explained by the interest-relativity of belief. The discussion starts with an argument that knowledge plays a key pair of roles in decision theory. It is then argued that knowledge cannot play that role unless knowledge is interest-relative. The theory of the interest-relativity of belief is reviewed and revised. That theory can explain some of the cases that are used to suggest knowledge is interest-relative. But it canâ€™t explain some cases involving ignorance, or mistake, about the odds at which a bet is offered. The paper ends with an argument that these cases require positing interest-relative defeaters, which affect whether an agent knows something without affecting whether she believes it, or is justified in believing it.\ndate: July 26 2012\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ndoi: \"10.1093/acprof:oso/9780199693702.003.0004\"\ncategories:\n  - epistemology\n  - interest-relativity\n  - games and decisions\ncitation: false\ncitation_url: https://doi.org/10.1093/acprof:oso/9780199693702.003.0004\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: sully.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 4\n    number_sections: true\n---\n\n\n\nWhen you pick up a volume like this one, which describes itself as being\nabout 'knowledge ascriptions', you probably expect to find it full of\npapers on epistemology, broadly construed. And you'd probably expect\nmany of those papers to concern themselves with cases where the\ninterests of various parties (ascribers, subjects of the ascriptions,\netc.) change radically, and this affects the truth values of various\nascriptions. And, at least in this paper, your expectations will be\nclearly met.\n\n<aside>\nPublished in _Knowledge Ascriptions_, edited by Jessica Brown and Mikkel Gerken, OUP, 75-103.\n\nPicture by [Phillip Ritz](https://www.flickr.com/photos/82193492@N00) via [Creative Commons](https://search.creativecommons.org/photos/63654f36-455f-481d-ba91-ba1befb23a40).\n</aside>\n\nBut here's an interesting contrast. If you'd picked up a volume of\npapers on 'belief ascriptions', you'd expect to find a radically\ndifferent menu of writers and subjects. You'd expect to find a lot of\nconcern about names and demonstratives, and about how they can be used\nby people not entirely certain about their denotation. More generally,\nyou'd expect to find less epistemology, and much more mind and language.\nI haven't read all the companion papers to mine in this volume, but I\nbet you won't find much of that here.\n\nThis is perhaps unfortunate, since belief ascriptions and knowledge\nascriptions raise at least some similar issues. Consider a kind of\ncontextualism about belief ascriptions, which holds that (L) can be\ntruly uttered in some contexts, but not in others, depending on just\nwhat aspects of Lois Lane's psychology are relevant in the\nconversation.[^1]\n\n(L) Lois Lane believes that Clark Kent is vulnerable to kryptonite.\n\nWe could imagine a theorist who says that whether (L) can be uttered\ntruly depends on whether it matters to the conversation that Lois Lane\nmight not recognise Clark Kent when he's wearing his Superman uniform.\nAnd, this theorist might continue, this isn't because 'Clark Kent' is a\ncontext-sensitive expression; it is rather because 'believes' is\ncontext-sensitive. Such a theorist will also, presumably, say that\nwhether (K) can be uttered truly is context-sensitive.\n\n(K) Lois Lane knows that Clark Kent is vulnerable to kryptonite.\n\nAnd so, our theorist is a kind of contextualist about knowledge\nascriptions. But they might agree with approximately none of the\nmotivations for contextualism about knowledge ascriptions put forward by\n@Cohen1988, @DeRose1995 or @Lewis1996b. Rather, they are a contextualist\nabout knowledge ascriptions solely because they are contextualist about\nbelief ascriptions like (L).\n\nCall the position I've just described **doxastic contextualism** about\nknowledge ascriptions. It's a kind of contextualism all right; it says\nthat (K) is context sensitive, and not merely because of the\ncontext-sensitivity of any term in the 'that'-clause. But it explains\nthe contextualism solely in terms of the contextualism of belief\nascriptions. The more familiar kind of contextualism about knowledge\nascriptions we'll call **non-doxastic contextualism**. Note that the way\nwe're classifying theories, a view that holds that (K) is\ncontext-sensitive both because (L) is context-sensitive *and* because\nCohen *et al* are correct is a version of non-doxastic contextualism.\nThe label 'non-doxastic' is being used to mean that the contextualism\nisn't solely doxastic, rather than as denying contextualism about belief\nascriptions.\n\nWe can make the same kind of division among interest-relative\ninvariantist, or IRI, theories of knowledge ascriptions. Any kind of IRI\nwill say that there are sentences of the form *S knows that p* whose\ntruth depends on the interests, in some sense, of $S$. But we can divide\nIRI theories up the same way that we divide up contextualist theories.\n\nDoxastic IRI\n\n:   Knowledge ascriptions are interest-relative, but their\n    interest-relativity traces solely to the interest-relativity of the\n    corresponding belief ascriptions.\n\nNon-Doxastic IRI\n\n:   Knowledge ascriptions are interest-relative, and their\n    interest-relativity goes beyond the interest-relativity of the\n    corresponding belief ascriptions.\n\nAgain, a theory that holds both that belief ascriptions are\ninterest-relative, and that some of the interest-relativity of knowledge\nascriptions is not explained by the interest-relativity of belief\nascriptions, will count as a version of non-doxastic IRI. I'm going to\ndefend a view from this class here.\n\nIn my @Weatherson2005-WEACWD I tried to motivate Doxastic IRI. It isn't\ncompletely trivial to map my view onto the existing views in the\nliterature, but the idea was to renounce contextualism and all its empty\npromises, and endorse a position that's usually known as 'strict\ninvariantism' about these classes of statements:\n\n-   $S$ is justified in having credence $x$ in $p$;\n\n-   If $S$ believes that $p$, she knows that $p$;\n\nwhile holding that the interests of S are relevant to the truth of\nstatements from these classes:\n\n-   $S$ believes that $p$;\n\n-   $S$ justifiably believes that $p$;\n\n-   $S$ knows that $p$.\n\nBut I didn't argue for all of that. What I argued for was Doxastic IRI\nabout ascriptions of justified belief, and I hinted that the same\narguments would generalise to knowledge ascriptions. I now think those\nhints were mistaken, and want to defend Non-Doxastic IRI about knowledge\nascriptions.[^2] My change of heart has been prompted by cases like\nthose Jason @Stanley2005-STAKAP calls 'Ignorant High Stakes' cases.[^3]\nBut to see why these cases matter, it will help to start with why I\nthink some kind of IRI must be true.\n\nHere's the plan of attack. In section 1, I'm going to argue that\nknowledge plays an important role in decision theory. In particular,\nI'll argue (a) that it is legitimate to write something onto a decision\ntable iff the decision maker knows it to be true, and (b) it is\nlegitimate to leave a possible state of the world off a decision table\niff the decision maker knows it not to obtain. I'll go on to argue that\nthis, plus some very plausible extra assumptions about the rationality\nof certain possible choices, implies that knowledge is\ninterest-relative. In section 2 I'll summarise and extend the argument\nfrom @Weatherson2005-WEACWD that belief is interest-relative. People who\nare especially interested in the epistemology rather than the theory of\nbelief may skip this. But I think this material is important; most of\nthe examples of interest-relative knowledge in the literature can be\nexplained by the interest-relativity of belief. I used to think all such\ncases could be explained. Section 3 describes why I no longer think\nthat. Reflections on cases like the Coraline example suggests that there\nare coherence constraints on knowledge that go beyond the coherence\nconstraints on justified true belief. The scope of these constraints is,\nI'll argue, interest-relative. So knowledge, unlike belief or justified\nbelief, has interest-relative defeaters. That's inconsistent with\nDoxastic IRI, so Doxastic IRI is false.\n\n## The Interest-Relativity of Knowledge\n\n### The Struction of Decision Problems\n\nProfessor Dec is teaching introductory decision theory to her\nundergraduate class. She is trying to introduce the notion of a dominant\nchoice. So she introduces the following problem, with two states, $S_1$\nand $S_2$, and two choices, $C_1$ and $C_2$, as is normal for\nintroductory problems.\n\n::: {.center}\n  ------- -------- --------\n           $S_1$    $S_2$\n    $C_1$  -\\$200   \\$1000\n    $C_2$  -\\$100   \\$1500\n  ------- -------- --------\n:::\n\nShe's hoping that the students will see that $C_1$ and $C_2$ are bets,\nbut $C_2$ is clearly the better bet. If $S_1$ is actual, then both bets\nlose, but $C_2$ loses less money. If $S_2$ is actual, then both bets\nwin, but $C_2$ wins more. So $C_2$ is better. That analysis is clearly\nwrong if the state is causally dependent on the choice, and\ncontroversial if the states are evidentially dependent on the choices.\nBut Professor Dec has not given any reason for the students to think\nthat the states are dependent on the choices in either way, and in fact\nthe students don't worry about that kind of dependence.\n\nThat doesn't mean, however, that the students all adopt the analysis\nthat Professor Dec wants them to. One student, Stu, is particularly\nunwilling to accept that $C_2$ is better than $C_1$. He thinks, on the\nbasis of his experience, that when more than \\$1000 is on the line,\npeople aren't as reliable about paying out on bets. So while $C_1$ is\nguaranteed to deliver \\$1000 if $S_2$, if the agent bets on $C_2$, she\nmight face some difficulty in collecting on her money.\n\nGiven the context, i.e., that they are in an undergraduate decision\ntheory class, it seems that Stu has misunderstood the question that\nProfessor Dec intended to ask. But it is a little harder than it first\nseems to specify just exactly what Stu's mistake is. It isn't that he\nthinks Professor Dec has *misdescribed* the situation. It isn't that he\nthinks the agent won't collect \\$1500 if she chooses $C_2$ and is in\n$S_2$. He just thinks that she *might* not be able to collect it, so the\nexpected payout might really be a little less than \\$1500.\n\nBut Stu is not the only problem that Professor Dec has. She also has\ntrouble convincing Dom of the argument. He thinks there should be a\nthird state added, $S_3$. In $S_3$, there is a vengeful God who is about\nto end the world, and take everyone who chose $C_1$ to heaven, while\nsending everyone who chose $C_2$ to hell. Since heaven is better than\nhell, $C_2$ does not dominate $C_1$; it is worse in $S_3$. If decision\ntheory is to be useful, we must say something about why we can leave\nstates like $S_3$ off the decision table.\n\nSo in order to teach decision theory, Professor Dec has to answer two\nquestions.[^4]\n\n1.  What makes it legitimate to write something on the decision table,\n    such as the '\\$1500' we write in the bottom right cell of Dec's\n    table?\n\n2.  What makes it legitimate to leave something off a decision table,\n    such as leaving Dom's state $S_3$ off the table?\n\nLet's start with a simpler problem that helps with both questions. Alice\nis out of town on a holiday, and she faces the following decision choice\nconcerning what to do with a token in her hand.\n\n::: {.center}\n  --------------------- -------------\n             **Choice**  **Outcome**\n     Put token on table  Win \\$1000\n    Put token in pocket  Win nothing\n  --------------------- -------------\n:::\n\nThis looks easy, especially if we've taken Professor Dec's class.\nPutting the token on the table dominates putting the token in her\npocket. It returns \\$1000, versus no gain. So she should put the token\non the table.\n\nI've left Alice's story fairly schematic; let's fill in some of the\ndetails. Alice is on holiday at a casino. It's a fair casino; the\nprobabilities of the outcomes of each of the games is just what you'd\nexpect. And Alice knows this. The table she's standing at is a roulette\ntable. The token is a chip from the casino worth \\$1000. Putting the\ntoken on the table means placing a bet. As it turns out, it means\nplacing a bet on the roulette wheel landing on 28. If that bet wins she\ngets her token back and another token of the same value. There are many\nother bets she could make, but Alice has decided not to make all but one\nof them. Since her birthday is the 28$^{\\text{th}}$, she is tempted to\nput a bet on 28; that's the only bet she is considering. If she makes\nthis bet, the objective chance of her winning is $\\frac{1}{38}$, and\nshe knows this. As a matter of fact she will win, but she doesn't know\nthis. (This is why the description in the table I presented above is\ntruthful, though frightfully misleading.) As you can see, the odds on\nthis bet are terrible. She should have a chance of winning around\n$\\frac{1}{2}$ to justify placing this bet.[^5] So the above table,\nwhich makes it look like placing the bet is the dominant, and hence\nrational, option, is misleading.\n\nJust how is the table misleading though? It isn't because what is says\nis false. If Alice puts the token on the table she wins \\$1000; and if\nshe doesn't, she stays where she is. It isn't, or isn't just, that Alice\ndoesn't believe the table reflects what will happen if she places the\nbet. As it turns out, Alice is smart, so she doesn't form beliefs about\nchance events like roulette wheels. But even if she did, that wouldn't\nchange how misleading the table is. The table suggests that it is\nrational for Alice to put the token on the table. In fact, that is\nirrational. And it would still be irrational if Alice believes,\n*irrationally*, that the wheel will land on 28.\n\nA better suggestion is that the table is misleading because Alice\ndoesn't *know* that it accurately depicts the choice she faced. If she\ndid know that these were the outcomes to putting the token on the table\nversus in her pocket, it seems it would be rational for her to put it on\nthe table. If we take it as tacit in a presentation of a decision\nproblem that the agent knows that the table accurately depicts the\noutcomes of various choices in different states, then we can tell a\nplausible story about what the miscommunication between Professor Dec\nand her students was. Stu was assuming that if the agent wins \\$1500,\nshe might not be able to easily collect. That is, he was assuming that\nthe agent does not know that she'll get \\$1500 if she chooses $C_2$ and\nis in state $S_2$. Professor Dec, if she's anything like other decision\ntheory professors, will have assumed that the agent did know exactly\nthat. And the miscommunication between Professor Dec and Dom also\nconcerns knowledge. When Dec wrote that table up, she was saying that\nthe agent knew that $S_1$ or $S_2$ obtained. And when she says it is\nbest to take dominating options, she means that it is best to take\noptions that one knows to have better outcomes. So here are the answers\nto Stu and Dom's challenges.\n\n1.  It is legitimate to write something on the decision table, such as\n    the '\\$1500' we write in the bottom right cell of Dec's table, iff\n    the decision maker knows it to be true.\n\n2.  It is legitimate to leave something off a decision table, such as\n    leaving Dom's state $S_3$ off the table, iff the decision maker\n    knows it not to obtain.\n\nPerhaps those answers are not correct, but what we can clearly see by\nreflecting on these cases is that the standard presentation of a\ndecision problem presupposes not just that the table states what will\nhappen, but the agent stands in some special doxastic relationship to\nthe information explicitly on the table (such as that Alice will get\n\\$1500 if $C_2$ and $S_2$) and implied by where the table ends (such as\nthat $S_3$ will not happen). Could that relationship be weaker than\nknowledge? It's true that it is hard to come up with clear\ncounterexamples to the suggestion that the relationship is merely\njustified true belief. But I think it is somewhat implausible to hold\nthat the standard presentation of an example merely presupposes that the\nagent has a justified true belief that the table is correct, and does\nnot in addition know that the table is correct.\n\nMy reasons for thinking this are similar to one of the reasons Timothy\nWilliamson [@Williamson2000-WILKAI Ch. 9] gives for doubting that one's\nevidence is all that one justifiably truly believes. To put the point in\nLewisian terms, it seems that knowledge is a much more *natural*\nrelation than justified true belief. And when ascribing contents,\nespecially contents of tacitly held beliefs, we should strongly prefer\nto ascribe more rather than less natural contents.[^6]\n\nSo the 'special doxastic relationship' is not weaker than knowledge.\nCould it be stronger? Could it be, for example, that the relationship is\ncertainty, or some kind of iterated knowledge? Plausibly in some\ngame-theoretic settings it is stronger -- it involves not just knowing\nthat the table is accurate, but knowing that the other player knows the\ntable is accurate. In some cases, the standard treatment of games will\nrequire positing even more iterations of knowledge. For convenience, it\nis sometimes explicitly stated that iterations continue indefinitely, so\neach party knows the table is correct, and knows each party knows this,\nand knows each party knows that, and knows each party knows *that*, and\nso on. An early example of this in philosophy is in the work by David\n@Lewis1969a on convention. But it is usually acknowledged (again in a\ntradition extending back at least to Lewis) that only the first few\niterations are actually needed in any problem, and it seems a mistake to\nattribute more iterations than are actually used in deriving solutions\nto any particular game.\n\nThe reason that would be a mistake is that we want game theory, and\ndecision theory, to be applicable to real-life situations. There is very\nlittle that we know, and know that we know, and know we know we know,\nand so on indefinitely [@Williamson2000-WILKAI Ch. 4]. There is,\nperhaps, even less that we are certain of. If we only could say that a\nperson is making a particular decision when they stand in these very\nstrong relationships to the parameters of the decision table, then\npeople will almost never be making the kinds of decision we study in\ndecision theory. Since decision theory and game theory are not meant to\nbe that impractical, I conclude that the 'special doxastic relationship'\ncannot be that strong. It could be that in some games, the special\nrelationship will involve a few iterations of knowledge, but in decision\nproblems, where the epistemic states of others are irrelevant, even that\nis unnecessary, and simple knowledge seems sufficient.\n\nIt might be argued here that we shouldn't expect to apply decision\ntheory directly to real-life problems, but only to idealised versions of\nthem, so it would be acceptable to, for instance, require that the\nthings we put in the table are, say, things that have probability\nexactly 1. In real life, virtually nothing has probability 1. In an\nidealisation, many things do. But to argue this way seems to involve\nusing 'idealisation' in an unnatural sense. There is a sense in which,\nwhenever we treat something with non-maximal probability as simply given\nin a decision problem that we're ignoring, or abstracting away from,\nsome complication. But we aren't *idealising*. On the contrary, we're\nmodelling the agent as if they were irrationally certain in some things\nwhich are merely very very probable.\n\nSo it's better to say that any application of decision theory to a\nreal-life problem will involve ignoring certain (counterfactual) logical\nor metaphysical possibilities in which the decision table is not\nactually true. But not any old abstraction will do. We can't ignore just\nanything, at least not if we want a good model. Which abstractions are\nacceptable? The response I've offered to Dom's challenge suggests an\nanswer to this: we can abstract away from any possibility in which\nsomething the agent actually knows is false. I don't have a knock-down\nargument that this is the best of all possible abstractions, but nor do\nI know of any alternative answer to the question which abstractions are\nacceptable which is nearly as plausible.\n\nWe might be tempted to say that we can abstract away from anything such\nthat the difference between its probability and 1 doesn't make a\ndifference to the ultimate answer to the decision problem. More\ncarefully, the idea would be that we can have the decision table\nrepresent that $p$ iff $p$ is true and treating $\\Pr(p)$ as 1 rather\nthan its actual value doesn't change what the agent should do. I think\nthis is the most plausible story one could tell about decision tables if\none didn't like the knowledge first story that I tell. But I also don't\nthink it works, because of cases like the following.\n\nLuc is lucky; he's in a casino where they are offering better than fair\nodds on roulette. Although the chance of winning any bet is , if Luc\nbets \\$10, and his bet wins, he will win \\$400. (That's the only bet on\noffer.) Luc, like Alice, is considering betting on 28. As it turns out,\n28 won't come up, although since this is a fair roulette wheel, Luc\ndoesn't know this. Luc, like most agents, has a declining marginal\nutility for money. He currently has \\$1,000, and for any amount of money\n$x$, Luc gets utility $u(x) = x^{\\frac{1}{2}}$ out of having $x$. So\nLuc's current utility (from money) is, roughly, 31.622. If he bets and\nloses, his utility will be, roughly, 31.464. And if he bets and wins,\nhis utility will be, roughly, 37.417. So he stands to gain about 5.794,\nand to lose about 0.159. So he stands to gain about 36.5 as much as he\nstands to lose. Since the odds of winning are less than\n$\\frac{1}{36.5}$, his expected utility goes down if he takes the\nbet, so he shouldn't take it. Of course, if the probability of losing\nwas 1, and not merely $\\frac{37}{38}$, he shouldn't take the bet\ntoo. Does that mean it is acceptable, in presenting Luc's decision\nproblem, to leave off the table any possibility of him winning, since he\nwon't win, and setting the probability of losing to 1 rather than\n$\\frac{37}{38}$ doesn't change the decision he should make? Of\ncourse not; that would horribly misstate the situation Luc finds himself\nin. It would misrepresent how sensitive Luc's choice is to his utility\nfunction, and to the size of the stakes. If Luc's utility function was\n$u(x) = x^{\\frac{3}{4}}$, then he should take the bet. If his\nutility function is unchanged, but the bet was \\$1 against \\$40, rather\nthan \\$10 against \\$400, he should take the bet. Leaving off the\npossibility of winning hides these facts, and badly misrepresents Luc's\nsituation.\n\nI've argued that the states we can 'leave off' a decision table are the\nstates that the agent knows not to obtain. The argument is largely by\nelimination. If we can only leave off things that have probability 1,\nthen decision theory would be useless; but it isn't. If we say we can\nleave off things if setting their probability at 1 is an accepable\nidealisation, we need a theory of acceptable idealisations. If this is\nto be a rival to my theory, the idealisation had better not be it's\nacceptable to treat anything known as having probability 1. But the most\nnatural alternative idealisation badly misrepresents Luc's case. If we\nsay that what can be left off is not what's known not to obtain, but\nwhat is, say, justifiably truly believed not to obtain, we need an\nargument for why people would naturally use such an unnatural standard.\nThis doesn't even purport to be a conclusive argument, but these\nconsiderations point me towards thinking that knowledge determines what\nwe can leave off.\n\nI also cheated a little in making this argument. When I described Alice\nin the casino, I made a few explicit comments about her information\nstates. And every time, I said that she *knew* various propositions. It\nseemed plausible at the time that this is enough to think those\npropositions should be incorporated into the table we use to represent\nher decision. That's some evidence against the idea that more than\nknowledge, perhaps iterated knowledge or certainty, is needed before we\nadd propositions to the decision table.\n\n### From Decision Theory to Interest-Relativity\n\nThis way of thinking about decision problems offers a new perspective on\nthe issue of whether we should always be prepared to bet on what we\nknow.[^7] To focus intuitions, let's take a concrete case. Barry is\nsitting in his apartment one evening when he hears a musician performing\nin the park outside. The musician, call her Beth, is one of Barry's\nfavourite musicians, so the music is familiar to Barry. Barry is excited\nthat Beth is performing in his neighbourhood, and he decides to hurry\nout to see the show. As he prepares to leave, a genie appears an offers\nhim a bet.[^8] If he takes the bet, and the musician is Beth, then the\ngenie will give Barry ten dollars. On the other hand, if the musician is\nnot Beth, he will be tortured in the fires of hell for a millenium.\nLet's put Barry's options in table form.\n\n::: {.center}\n  ----------------- ---------------------- --------------------------\n                     **Musician is Beth**   **Musician is not Beth**\n       **Take Bet**        Win \\$10          1000 years of torture\n    **Decline Bet**       Status quo               Status quo\n  ----------------- ---------------------- --------------------------\n:::\n\nIntuitively, it is extremely irrational for Barry to take the bet.\nPeople do make mistakes about identifying musicians, even very familiar\nmusicians, by the strains of music that drift up from a park. It's not\nworth risking a millenium of torture for \\$10.\n\nBut it also seems that we've misstated the table. Before the genie\nshowed up, it seemed clear that Barry knew that the musician was Beth.\nThat was why he went out to see her perform. (If you don't think this is\ntrue, make the sounds from the park clearer, or make it that Barry had\nsome prior evidence that Beth was performing which the sounds from the\npark remind him of. It shouldn't be too hard to come up with an\nevidential base such that (a) in normal circumstances we'd say Barry\nknew who was performing, but (b) he shouldn't take this genie's bet.)\nNow our decision tables should reflect the knowledge of the agent making\nthe decision. If Barry knows that the musician is Beth, then the second\ncolumn is one he knows will not obtain. So let's write the table in the\nstandard form.\n\n::: {.center}\n  ----------------- ---------------------- --\n                     **Musician is Beth**  \n       **Take Bet**        Win \\$10        \n    **Decline Bet**       Status quo       \n  ----------------- ---------------------- --\n:::\n\nAnd it is clear what Barry's decision should be in this situation.\nTaking the bet dominates declining it, and Barry should take dominating\noptions.\n\nWhat has happened? It is incredibly clear that Barry should decline the\nbet, yet here we have an argument that he should take the bet. If you\naccept that the bet should be declined, then it seems to me that there\nare three options available.\n\n1.  Barry never knew that the musician was Beth.\n\n2.  Barry did know that the musician was Beth, but this knowledge was\n    destroyed by the genie's offer of the bet.\n\n3.  States of the world that are known not to obtain should still be\n    represented in decision problems, so taking the bet is not a\n    dominating option.\n\nThe first option is basically a form of scepticism. If the take-away\nmessage from the above discussion is that Barry doesn't know the\nmusician is Beth, we can mount a similar argument to show that he knows\nnext to nothing.[^9] And the third option would send us back into the\nproblems about interpreting and applying decision theory that we spent\nthe first few pages trying to get out of.\n\nSo it seems that the best solution here, or perhaps the least bad\nsolution, is to accept that knowledge is interest-relative. Barry did\nknow that the musician was Beth, but the genie's offer destroyed that\nknowledge. When Barry was unconcerned with bets at extremely long odds\non whether the musician is Beth, he knows Beth is the musician. Now that\nhe is interested in those bets, he doesn't know that.[^10]\n\nThe argument here bears more than a passing resemblance to the arguments\nin favour of interest-relativity that are made by Hawthorne, Stanley,\nand Fantl and McGrath. But I think the focus on decision theory shows\nhow we can get to interest-relativity with very weak premises.[^11] In\nparticular, the only premises I've used to derive an interest-relative\nconclusion are:\n\n1.  Before the genie showed up, Barry knew the musician was Beth.\n\n2.  It's rationally permissible, *in cases like Barry's*, to take\n    dominating options.\n\n3.  It's always right to model decision problems by including what the\n    agent knows in the 'framework'. That is, our decision tables should\n    include what the agent knows about the payoffs in different states,\n    and leave off any state the agent knows not to obtain.\n\n4.  It is rationally impermissible for Barry to take the genie's offered\n    bet.\n\nThe second premise there is *much* weaker than the principles linking\nknowledge and action defended in previous arguments for\ninterest-relativity. It isn't the claim that one can always act on what\none knows, or that one can only act on what one knows, or that knowledge\nalways (or only) provides reason to act. It's just the claim that in one\nvery specific type of situation, in particular when one has to make a\nrelatively simple bet, which affects nobody but the person making the\nbet, it's rationally permissible to take a dominating option. In\nconjunction with the third premise, it entails that *in those kind of\ncases*, the fact that one knows taking the bet will lead to a better\noutcome suffices for making acceptance of the bet rationally\npermissible. It doesn't say anything about what else might or might not\nmake acceptance rationally permissible. It doesn't say anything about\nwhat suffices for rationally permissibility in other kinds of cases,\nsuch as cases where someone else's interests are at stake, or where\ntaking the bet might violate a deontological constraint, or any other\nway in which real-life choices differ from the simplest decision\nproblems.[^12] It doesn't say anything about any other kind of\npermissibility, e.g., moral permissibility. But it doesn't need to,\nbecause we're only in the business of proving that there is *some*\ninterest-relativity to knowledge, and an assumption about practical\nrationality in some range of cases suffices to prove that.[^13]\n\nThe case of Barry and Beth also bears some relationship to one of the\nkinds of case that have motivated contextualism about knowledge. Indeed,\nit has been widely noted in the literature on interest-relativity that\ninterest-relativity can explain away many of the puzzles that motivate\ncontextualism. And there are difficulties that face any contextualist\ntheory [@Weatherson2006-WEAQC]. So I prefer an *invariantist* form of\ninterest-relativity about knowledge. That is, my view is a form of\ninterest-relative-invariantism, or IRI.[^14]\n\nNow everything I've said here leaves it open whether the\ninterest-relativity of knowledge is a natural and intuitive theory, or\nwhether it is a somewhat unhappy concession to difficulties that the\ncase of Barry and Beth raise. I think the former is correct, and\ninterest-relativity is fairly plausible on its own merits, but it would\nbe consistent with my broader conclusions to say that in fact the\ninterest-relative theory of knowledge is very implausible and\ncounterintuitive. If we said that, we could still justify the\ninterest-relative theory by noting that we have on our hands here a\nparadoxical situation, and any option will be somewhat implausible. This\nconsideration has a bearing on how we should think about the role of\nintuitions about cases, or principles, in arguments that knowledge is\ninterest-relative. Several critics of the view have argued that the view\nis counter-intuitive, or that it doesn't accord with the reactions of\nnon-expert judges.[^15] In a companion paper, \"Defending\nInterest-Relative Invariantism\", I note that those arguments usually\nmisconstrue what the consequences of interest-relative theories of\nknowledge are. But even if they don't, I don't think there's any quick\nargument that if interest-relativity is counter-intuitive, it is false.\nAfter all, the only alternatives that seem to be open here are very\ncounter-intuitive.\n\nFinally, it's worth noting that if Barry is rational, he'll stop (fully)\nbelieving that the musician is Beth once the genie makes the offer.\nAssuming the genie allows this, it would be very natural for Barry to\ntry to acquire more information about the singer. He might walk over to\nthe window to see if he can see who is performing in the park. So this\ncase leaves it open whether the interest-relativity of knowledge can be\nexplained fully by the interest-relativity of belief. I used to think it\ncould be; I no longer think that. To see why this is so, it's worth\nrehearsing how the interest-relative theory of belief runs.\n\n## The Interest-Relativity of Belief\n\n### Interests and Functional Roles\n\nThe previous section was largely devoted to proving an existential\nclaim: there is *some* interest-relativity to knowledge. Or, if you\nprefer, it proved a negative claim: the best theory of knowledge is\n*not* interest-neutral. But this negative conclusion invites a\nphilosophical challenge: what is the best explanation of the\ninterest-relativity of knowledge? My answer is in two parts. Part of the\ninterest-relativity of knowledge comes from the interest-relativity of\nbelief, and part of it comes from the fact that interests generate\ncertain kinds of doxastic defeaters. It's the second part, the part that\nis new to this paper, that makes the theory a version of non-doxastic\nIRI.\n\nHere's my theory of belief. $S$ believes that $p$ iff conditionalising\non $p$ doesn't change $S$'s answer to any relevant question. I'm using\n'relevance' here in a non-technical sense; I say a lot more about how to\ncash out the notion in my [-@Weatherson2005-WEACWD]. The key thing to\nnote is that relevance is interest-relative, so the theory of belief is\ninterest-relative. There is a bit more to say about what kind of\n*questions* are important for this definition of belief. In part because\nI've changed my mind a little bit on this since the earlier paper, I'll\nspend a bit more time on it. The following four kinds of questions are\nthe most important.\n\n-   How probable is $q$?\n\n-   Is $q$ or $r$ more probable?\n\n-   How good an idea is it to do $\\phi$?\n\n-   Is it better to do $\\phi$ or $\\psi$?\n\nThe theory of belief says that someone who believes that $p$doesn't\nchange their answer to any of these questions upon conditionalising on\n$p$. Putting this formally, and making the restriction to relevant\nquestions explicit, we get the following theorems of our theory of\nbelief.[^16]\n\nBAP\n\n:   For all relevant $q, x$, if $p$ is believed then $\\Pr(q) = x$ iff\n    $\\Pr(q | p) = x$.\n\nBCP\n\n:   For all relevant $q, r$, if $p$ is believed then\n    $\\Pr(q) \\geq \\Pr(r)$ iff $\\Pr(q | p) \\geq \\Pr(r | p)$.\n\nBAU\n\n:   For all relevant $\\phi, x$, if $p$ is believed then $U(\\phi) = x$\n    iff $U(\\phi | p) = x$.\n\nBCU\n\n:   For all relevant $\\phi, \\psi$, if $p$ is believed then\n    $U(\\phi) \\geq U(\\psi)$ iff $U(\\phi | p) \\geq U(\\psi | p)$.\n\nIn the earlier paper I focussed on **BAU** and **BCU**. But **BAP** and\n**BCP** are important as well. Indeed, focussing on them lets us derive\na nice result.\n\nCharlie is trying to figure out exactly what the probability of $p$ is.\nThat is, for any $x \\in [0, 1]$, whether $\\Pr(p) = x$ is a relevant\nquestion. Now Charlie is well aware that $\\Pr(p | p) = 1$. So unless\n$\\Pr(p) = 1$, Charlie will give a different answer to the questions *How\nprobable is p?* and *Given p, how probable is p?*. So unless Charlie\nholds that $\\Pr(p)$ is 1, she won't count as believing that $p$. One\nconsequence of this is that Charlie can't reason, \"The probability of\n$p$ is exactly 0.978, so $p$.\" That's all to the good, since that looks\nlike bad reasoning. And it looks like bad reasoning even though in some\ncircumstances Charlie can rationally believe propositions that she\n(rationally) gives credence 0.978 to. Indeed, in some circumstances she\ncan rationally believe something *in virtue* of it being 0.978 probable.\n\nThat's because the reasoning in the previous paragraph assumes that\nevery question of the form *Is the probability of p equal to x?* is\nrelevant. In practice, fewer questions than that will be relevant. Let's\nsay that the only questions relevant to Charlie are of the form *What is\nthe probability of $p$ to one decimal place?*. And assume that no other\nquestions become relevant in the course of her inquiry into this\nquestion.[^17] Charlie decides that to the first decimal place,\n$\\Pr(p) = 1.0$, i.e., $\\Pr(p) > 0.95$. That is compatible with simply\nbelieving that $p$. And that seems right; if for practical purposes, the\nprobability of $p$ is indistinguishable from 1, then the agent is\nconfident enough in $p$ to believe it.\n\nSo there are some nice features of this theory of belief. Indeed, there\nare several reasons to believe it. It is, I have argued, the best\nfunctionalist account of belief. I'm not going to argue for\nfunctionalism about the mind, since the argument would take at least a\nbook. (The book in question might look a lot like @DBMJackson2007.) But\nI do think functionalism is true, and so the best functionalist theory\nof belief is the best theory of belief.\n\nThe argument for this theory of belief in my [-@Weatherson2005-WEACWD]\nrested heavily on the flaws of rival theories. We can see those flaws by\nlooking at a tension that any theory of the relationship between belief\nand credence must overcome. Each of the following three principles seems\nto be plausible.\n\n1.  If $S$ has a greater credence in $p$ than in $q$, and she believes\n    $q$, then she believes $p$ as well; and if her credences in both $p$\n    and $q$ are rational, and her belief in $q$ is rational, then so is\n    her belief in $p$.\n\n2.  If $S$ rationally believes $p$ and rationally believes $q$, then it\n    is open to her to rationally believe $p \\wedge q$ without changing\n    her credences.\n\n3.  $S$ can rationally believe $p$ while having credence of less than 1\n    in $p$.\n\nBut these three principles, together with some principles that are\ngenuinely uncontroversial, entail an absurd result. By 3, there is some\n$p$ such that *Cr*$(p) = x < 1$, and $p$ is believed. (*Cr* is the\nfunction from any proposition to our agent's credence in that\npropositions.) Let $S$ know that a particular fair lottery has $l$\ntickets, where $l > \\frac{1}{1-x}$. The uncontroversial principle\nwe'll use is that in such a case $S$'s credence that any given ticket\nwill lose should be $\\frac{l-1}{l}$. Since $\\frac{l-1}{l} > x$,\nit follows by 1 that $S$ believes of each ticket that it will lose.\nSince her credences are rational, these beliefs are rational. By\nrepeated applications of 2 then, the agent can rationally believe that\neach ticket will lose. But she rationally gives credence 0 to the\nproposition that each ticket will lose. So by 1 she can rationally\nbelieve any proposition in which her credence is greater than 0. This is\nabsurd.[^18]\n\nI won't repeat all the gory details here, but one of the consequences of\nthe discussion in @Weatherson2005-WEACWD was that we could hold on to 3,\nand onto restricted versions of 1 and 2. In particular, if we restricted\n1 and 2 to relevant propositions (in some sense) they became true,\nalthough the unrestricted version is false. A key part of the argument\nof the earlier paper was that this was a better option than the more\ncommonly taken option of holding on to unrestricted versions of 1 and 3,\nat the cost of abandoning 2 even in clear cases. But one might wonder\nwhy I'm holding so tightly on to 3. After all, there is a functionalist\nargument that 3 is false.\n\nA key functional role of credences is that if an agent has credence $x$\nin $p$ she should be prepared to buy a bet that returns 1 util if $p$,\nand 0 utils otherwise, iff the price is no greater than $p$ utils. A key\nfunctional role of belief is that if an agent believes $p$, and\nrecognises that $\\phi$ is the best thing to do given $p$, then she'll do\n$\\phi$. Given $p$, it's worth paying any price up to 1 util for a bet\nthat pays 1 util if $p$. So believing $p$ seems to mean being in a\nfunctional state that is like having credence 1 in $p$.\n\nBut this argument isn't quite right. If we spell out more carefully what\nthe functional roles of credence and belief are, a loophole emerges in\nthe argument that belief implies credence 1. The interest-relative\ntheory of belief turns out to exploit that loophole. What's the\ndifference, in functional terms, between having credence $x$ in $p$, and\nhaving credence $x + \\varepsilon$ in $p$? Well, think again about the\nbet that pays 1 util if $p$, and 0 utils otherwise. And imagine that bet\nis offered for $x + \\frac{\\varepsilon}{2}$ utils. The person whose\ncredence is $x$ will decline the offer; the person whose credence is\n$x + \\varepsilon$ will accept it. Now it will usually be that no such\nbet is on offer.[^19] No matter; as long as one agent is *disposed* to\naccept the offer, and the other agent is not, that suffices for a\ndifference in credence.\n\nThe upshot of that is that differences in credences might be, indeed\nusually will be, constituted by differences in dispositions concerning\nhow to act in choice situations far removed from actuality. I'm not\nusually in a position of having to accept or decline a chance to buy a\nbet for 0.9932 utils that the local coffee shop is currently open. Yet\nwhether I would accept or decline such a bet matters to whether my\ncredence that the coffee shop is open is 0.9931 or 0.9933. This isn't a\nproblem with the standard picture of how credences work. It's just an\nobservation that the high level of detail embedded in the picture relies\non taking the constituents of mental states to involve many\ndispositions.\n\nOne of the crucial features of the theory of belief I'm defending is\nthat what an agent believes is in general *insensitive* to such abtruse\ndispositions, although it is very sensitive to dispositions about\npractical matters. It's true that if I believe that $p$, and I'm\nrational enough, I'll act as if $p$ is true. Is it also true that if I\nbelieve $p$, I'm disposed to act as if $p$ is true no matter what\nchoices are placed in front of me? The theory being defended here says\nno, and that seems plausible. As we say in the case of Barry and Beth,\nBarry can believe that $p$, but be disposed to *lose that belief* rather\nthan act on it if odd choices, like that presented by the genie, emerge.\n\nThis suggests the key difference between belief and credence 1. For a\nrational agent, a credence of 1 in $p$ means that the agent is disposed\nto answer a wide range of questions the same way she would answer that\nquestion conditional on $p$. That follows from the fact that these four\nprinciples are trivial theorems of the orthodox theory of expected\nutility.[^20]\n\nC1AP\n\n:   For all $q, x$, if $\\Pr(p) = 1$ then $\\Pr(q) = x$ iff\n    $\\Pr(q | p) = x$.\n\nC1CP\n\n:   For all $q, r$, if $\\Pr(p) = 1$ then $\\Pr(q) \\geq \\Pr(r)$ iff\n    $\\Pr(q | p) \\geq \\Pr(r | p)$.\n\nC1AU\n\n:   For all $\\phi, x$, if $\\Pr(p) = 1$ then $U(\\phi) = x$ iff\n    $U(\\phi | p) = x$.\n\nC1CP\n\n:   For all $\\phi, \\psi$, if $\\Pr(p) = 1$ then $U(\\phi) \\geq U(\\psi)$\n    iff $U(\\phi | p) \\geq U(\\psi | p)$.\n\nThose look a lot like the theorems of the theory of belief that we\ndiscussed above. But note that these claims are *unrestricted*, whereas\nin the theory of belief, we restricted attention to relevant actions,\npropositions, utilities and probabilities. That turns out to be the\ndifference between belief and credence 1. Since that difference is\ninterest-relative, belief is interest-relative.\n\nI used to think that that was all the interest-relativity we needed in\nepistemology. Now I don't, for reasons that I'll go through in section\nthree. (Readers who care more about the theory of knowledge than the\ntheory of belief may want to skip ahead to that section.) But first I\nwant to clean up some loose ends in the acount of belief.\n\n### Two Caveats\n\nThe theory sketched so far seems to me right in the vast majority of\ncases. It fits in well with a broadly functionalist view of the mind,\nand it handles difficult cases, like that of Charlie, nicely. But it\nneeds to be supplemented and clarified a little to handle some other\ndifficult cases. In this section I'm going to supplement the theory a\nlittle to handle what I call 'impractical propositions', and say a\nlittle about morally loaded action.\n\nJones has a false geographic belief: he believes that Los Angeles is\nwest of Reno.[^21] This isn't because he's ever thought about the\nquestion. Rather, he's just disposed to say \"Of course\" if someone asks,\n\"Is Los Angeles west of Reno?\" That disposition has never been\ntriggered, because no one's ever bothered to ask him this. Call the\nproposition that Los Angeles is west of Reno $p$.\n\nThe theory given so far will get the right result here: Jones does\nbelieve that $p$. But it gets the right answer for an odd reason. Jones,\nit turns out, has very little interest in American geography right now.\nHe's a schoolboy in St Andrews, Scotland, getting ready for school and\nworried about missing his schoolbus. There's no inquiry he's currently\nengaged in for which $p$ is even close to relevant. So conditionalising\non $p$ doesn't change the answer to any inquiry he's engaged in, but\nthat would be true no matter what his credence in $p$ is.\n\nThere's an immediate problem here. Jones believes $p$, since\nconditionalising on $p$ doesn't change the answer to any relevant\ninquiry. But for the very same reason, conditionalising on $\\neg p$\ndoesn't change the answer to any relevant inquiry. It seems our theory\nhas the bizarre result that Jones believes $\\neg p$ as well. That is\nboth wrong and unfair. We end up attributing inconsistent beliefs to\nJones simply because he's a harried schoolboy who isn't currently\nconcerned with the finer points of geography of the American southwest.\n\nHere's a way out of this problem in four relatively easy steps.[^22]\nFirst, we say that which questions are relevant questions is not just\nrelative to the agent's interests, but also relevant to the proposition\nbeing considered. A question may be relevant relative to $p$, but not\nrelative to $q$. Second, we say that relative to $p$, the question of\nwhether $p$ is more probable than $\\neg p$ is a relevant question.\nThird, we infer from that that an agent only believes $p$ if their\ncredence in $p$ is greater than their credence in $\\neg p$, i.e., if\ntheir credence in $p$ is greater than $\\frac{1}{2}$. Finally, we say\nthat when the issue is whether the subject believes that $p$, the\nquestion of whether $p$ is more probable than $\\neg p$ is not only\nrelevant on its own, but it stays being a relevant question conditional\non any $q$ that is relevant to the subject. In the earlier paper\n[@Weatherson2005-WEACWD] I argue that this solves the problem raised by\nimpractical propositions in a smooth and principled way.\n\nThat's the first caveat. The second is one that isn't discussed in the\nearlier paper. If the agent is merely trying to get the best outcome for\nthemselves, then it makes sense to represent them as a utility\nmaximiser. And within orthodox decision theory, it is easy enough to\ntalk about, and reason about, conditional utilities. That's important,\nbecause conditional utilities play an important role in the theory of\nbelief offered at the start of this section. But if the agent faces\nmoral constraints on her decision, it isn't always so easy to think\nabout conditional utilities.\n\nWhen agents have to make decisions that might involve them causing harm\nto others if certain propositions turn out to be true, then I think it\nis best to supplement orthodox decision theory with an extra assumption.\nThe assumption is, roughly, that for choices that may harm others,\nexpected value is absolute value. It's easiest to see what this means\nusing a simple case of three-way choice. The kind of example I'm\nconsidering here has been used for (slightly) different purposes by\nFrank @Jackson1991.\n\nThe agent has to do $\\varphi$ or $\\psi$. Failure to do either of these\nwill lead to disaster, and is clearly unacceptable. Either $\\varphi$ or\n$\\psi$ will avert the disaster, but one of them will be moderately\nharmful and the other one will not. The agent has time before the\ndisaster to find out which of $\\varphi$ and $\\psi$ is harmful and which\nis not for a nominal cost. Right now, her credence that $\\varphi$ is the\nharmful one is, quite reasonably, $\\frac{1}{2}$. So the agent has\nthree choices:\n\n-   Do $\\varphi$;\n\n-   Do $\\psi$; or\n\n-   Wait and find out which one is not harmful, and do it.\n\nWe'll assume that other choices, like letting the disaster happen, or\nfinding out which one is harmful and doing it, are simply out of\nconsideration. In any case, they are clearly dominated options, so the\nagent shouldn't do them. Let $p$ be the propostion that $\\varphi$ is the\nharmful one. Then if we assume the harm in question has a disutility of\n10, and the disutility of waiting to act until we know which is the\nharmful one is 1, the values of the possible outcomes are as follows:\n\n::: {.center}\n  --------------------------- ----- ----------\n                               $p$   $\\neg p$\n             **Do $\\varphi$**  -10      0\n                **Do $\\psi$**   0      -10\n    Find out which is harmful  -1       -1\n  --------------------------- ----- ----------\n:::\n\nGiven that $Pr(p) = \\frac{1}{2}$, it's easy to compute that the\nexpected value of doing either $\\varphi$ or $\\psi$ is -5, while the\nexpected value of finding out which is harmful is -1, so the agent\nshould find out which thing is to be done before acting. So far most\nconsequentialists would agree, and so probably would most\nnon-consequentialists for most ways of fleshing out the abstract example\nI've described.[^23]\n\nBut most consequentialists would also say something else about the\nexample that I think is not exactly true. Just focus on the column in\nthe table above where $p$ is true. In that column, the highest value, 0,\nis alongside the action *Do* $\\psi$. So you might think that conditional\non $p$, the agent should do $\\psi$. That is, you might think the\nconditional expected value of doing $\\psi$, conditional on $p$ being\ntrue, is 0, and that's higher than the conditional expected value of any\nother act, conditional on $p$. If you thought that, you'd certainly be\nin agreement with the orthodox decision-theoretic treatment of this\nproblem.\n\nIn the abstract statement of the situation above, I said that one of the\noptions would be *harmful*, but I didn't say who it would be harmful to.\nI think this matters. I think what I called the orthodox treatment of\nthe situation is correct when the harm accrues to the person making the\ndecision. But when the harm accrues to another person, particularly when\nit accrues to a person that the agent has a duty of care towards, then I\nthink the orthodox treatment isn't quite right.\n\nMy reasons for this go back to Jackson's original discussion of the\npuzzle. Let the agent be a doctor, the actions $\\varphi$ and $\\psi$ be\nher prescribing different medication to a patient, and the harm a severe\nallergic reaction that the patient will have to one of the medications.\nAssume that she can run a test that will tell her which medication the\npatient is allergic to, but the test will take a day. Assume that the\npatient will die in a month without either medication; that's the\ndisaster that must be averted. And assume that the patient is is some\ndiscomfort that either medication would relieve; that's the small cost\nof finding out which medication is risk. Assume finally that there is no\nchance the patient will die in the day it takes to run the test, so the\ncost of running the test is really nominal.\n\nA good doctor in that situation will find out which medication the\npatient is allergic to before ascribing either medicine. It would be\n*reckless* to ascribe a medicine that is unnecessary and that the\npatient might be allergic to. It is worse than reckless if the patient\nis actually allergic to the medicine prescribed, and the doctor harms\nthe patient. But even if she's lucky and prescribes the 'right'\nmedication, the recklessness remains. It was still, it seems, the wrong\nthing for her to do.\n\nAll of that is in Jackson's discussion of the case, though I'm not sure\nhe'd agree with the way I'm about the incorporate these ideas into the\nformal decision theory. Even under the assumption that $p$, prescribing\n$\\psi$ is still wrong, because it is reckless. That should be\nincorporated into the values we ascribe to different actions in\ndifferent circumstances. The way I do it is to associate the value of\neach action, in each circumstance, with its actual expected value. So\nthe decision table for the doctor's decision looks something like this.\n\n::: {.center}\n  --------------------------- ----- ----------\n                               $p$   $\\neg p$\n             **Do $\\varphi$**  -5       -5\n                **Do $\\psi$**  -5       -5\n    Find out which is harmful  -1       -1\n  --------------------------- ----- ----------\n:::\n\nIn fact, the doctor is making a decision under certainty. She knows that\nthe value of prescribing either medicine is -5, and the value of running\nthe tests is -1, so she should run the tests.\n\nIn general, when an agent has a duty to maximise the expected value of\nsome quantity $Q$, then the value that goes into the agent's decision\ntable in a cell is *not* the value of $Q$ in the world-action pair the\nagent represents. Rather, it's the expected value of $Q$ given that\nworld-action pair. In situations like this one where the relevant facts\n(e.g., which medicine the patient is allergic to) don't affect the\nevidence the agent has, the decision is a decision under *certainty*.\nThis is all as things should be. When you have obligations that are\ndrawn in terms of the expected value of a variable, the actual values of\nthat variable cease to be directly relevant to the decision problem.\n\nSimilar morals carry across to theories that offer a smaller role to\nexpected utility in determining moral value. In particular, it's often\ntrue that decisions where it is uncertain what course of action will\nproduce the best outcome might still, in the morally salient sense, be\ndecisions under certainty. That's because the uncertainty doesn't impact\nhow we should weight the different possible outcomes, as in orthodox\nutility theory, but how we should value them. That's roughly what I\nthink is going on in cases like this one, which Jessica Brown has argued\nare problematic for the epistemological theories John Hawthorne and\nJason Stanley have recently been defending.[^24]\n\n> A student is spending the day shadowing a surgeon. In the morning he\n> observes her in clinic examining patient A who has a diseased left\n> kidney. The decision is taken to remove it that afternoon. Later, the\n> student observes the surgeon in theatre where patient A is lying\n> anaesthetised on the operating table. The operation hasn't started as\n> the surgeon is consulting the patient's notes. The student is puzzled\n> and asks one of the nurses what's going on:\n>\n> **Student**: I don't understand. Why is she looking at the patient's\n> records? She was in clinic with the patient this morning. Doesn't she\n> even know which kidney it is?\n>\n> **Nurse**: Of course, she knows which kidney it is. But, imagine what\n> it would be like if she removed the wrong kidney. She shouldn't\n> operate before checking the patient's records. [@Brown2008-BROKAP\n> 1144-1145]\n\nIt is tempting, but for reasons I've been going through here mistaken,\nto represent the surgeon's choice as follows. Let **Left** mean the left\nkidney is diseased, and **Right** mean the right kidney is diseased.\n\n::: {.center}\n  ------------------------- ----------------- -----------------\n                                **Left**          **Right**\n     **Remove left kidney**        $1$              $-1$\n    **Remove right kidney**       $-1$               $1$\n            **Check notes**  $1-\\varepsilon$   $1-\\varepsilon$\n  ------------------------- ----------------- -----------------\n:::\n\nHere $\\varepsilon$ is the trivial but non-zero cost of checking the\nchart. Given this table, we might reason that since the surgeon knows\nthat she's in the left column, and removing the left kidney is the best\noption in that column, she should remove the left kidney rather than\nchecking the notes.\n\nBut that reasoning assumes that the surgeon does not have any epistemic\nobligations over and above her duty to maximise expected utility. And\nthat's very implausible. It's totally implausible on a\nnon-consequentialist moral theory. A non-consequentialist may think that\nsome people have just the same obligations that the consequentialist\nsays they have -- legislators are frequently mentioned as an example --\nbut surely they wouldn't think *surgeons* are in this category. And even\na consequentialist who thinks that surgeons have special obligations in\nterms of their institutional role should think that the surgeon's\nobligations go above and beyond the obligation every agent has to\nmaximise expected utility.\n\nIt's not clear exactly what the obligation the surgeon has. Perhaps it\nis an obligation to not just know which kidney to remove, but to know\nthis on the basis of evidence she has obtained while in the operating\ntheatre. Or perhaps it is an obligation to make her belief about which\nkidney to remove as sensitive as possible to various possible scenarios.\nBefore she checked the chart, this counterfactual was false: *Had she\nmisremembered which kidney was to be removed, she would have a true\nbelief about which kidney was to be removed.* Checking the chart makes\nthat counterfactual true, and so makes her belief that the left kidney\nis to be removed a little more sensitive to counterfactual\npossibilities.\n\nHowever we spell out the obligation, it is plausible given what the\nnurse says that the surgeon has some such obligation. And it is\nplausible that the 'cost' of violating this obligation, call it $\\delta$\nis greater than the cost of checking the notes. So here is the decision\ntable the surgeon faces.\n\n::: {.center}\n  ------------------------- ----------------- -----------------\n                                **Left**          **Right**\n     **Remove left kidney**    $1-\\delta$        $-1-\\delta$\n    **Remove right kidney**    $-1-\\delta$       $1-\\delta$\n            **Check notes**  $1-\\varepsilon$   $1-\\varepsilon$\n  ------------------------- ----------------- -----------------\n:::\n\nAnd it isn't surprising, or a problem for an interest-relative theory of\nknowledge or belief, that the surgeon should check the notes, even if\nshe believes *and knows* that the left kidney is the diseased one.\n\n## Interest-Relative Defeaters\n\nAs I said at the top, I've changed my view from Doxastic IRI to\nNon-Doxastic IRI. The change of heart is occasioned by cases like the\nfollowing, where the agent is mistaken, and hence ignorant, about the\nodds at which she is offered a bet on $p$. In fact the odds are much\nlonger than she thinks. Relative to what she stands to win, the stakes\nare too high.\n\n### The Coraline Example\n\nThe problem for Doxastic IRI arises because of cases like that of\nCoraline. Here's what we're going to stipulate about Coraline.\n\n-   She knows that $p$ and $q$ are independent, so her credence in any\n    conjunction where one conjunct is a member of $\\{p,  \\neg p\\}$ and\n    the other is a member of $\\{q, \\neg q\\}$ will be the product of her\n    credences in the conjuncts.\n\n-   Her credence in $p$ is 0.99, just as the evidence supports.\n\n-   Her credence in $q$ is also 0.99. This is unfortunate, since the\n    rational credence in $q$ given her evidence is 0.01.\n\n-   The only relevant question for her which is sensitive to $p$ is\n    whether to take or decline a bet with the following payoff\n    structure.[^25] (Assume that the marginal utility of money is close\n    enough to constant that expected dollar returns correlate more or\n    less precisely with expected utility returns.)\n\n::: {.center}\n  ----------------- ------------------ ----------------------- --------------\n                     **$p \\wedge q$**   **$p \\wedge \\neg q$**   **$\\neg p$**\n       **Take bet**        100                    1                 1000\n    **Decline bet**         0                     0                  0\n  ----------------- ------------------ ----------------------- --------------\n:::\n\nAs can be easily computed, the expected utility of taking the bet given\nher credences is positive, it is just over \\$89. And Coraline takes the\nbet. She doesn't compute the expected utility, but she is sensitive to\nit.[^26] That is, had the expected utility given her credences been\nclose to 0, she would have not acted until she made a computation. But\nfrom her perspective this looks like basically a free \\$100, so she\ntakes it. Happily, this all turns out well enough, since $p$ is true.\nBut it was a dumb thing to do. The expected utility of taking the bet\ngiven her evidence is negative, it is a little under -\\$8. So she isn't\nwarranted, given her evidence, in taking the bet.\n\n### What Coraline Knows and What She Believes\n\nAssume, for *reductio*, that Coraline knows that $p$. Then the choice\nshe faces looks like this.\n\n::: {.center}\n  ----------------- --------- --------------\n                     **$q$**   **$\\neg q$**\n       **Take bet**    100          1\n    **Decline bet**     0           0\n  ----------------- --------- --------------\n:::\n\nSince taking the bet dominates declining the bet, she should take the\nbet if this is the correct representation of her situation. She\nshouldn't take the bet, so by *modus tollens*, that can't be the correct\nrepresentation of her situation. If she knew $p$, that would be the\ncorrect representation of her situation. So, again by *modus tollens*,\nshe doesn't know $p$.\n\nNow let's consider three possible explanations of why she doesn't know\nthat $p$.\n\n1.  She doesn't have enough evidence to know that $p$, independent of\n    the practical stakes.\n\n2.  In virtue of the practical stakes, she doesn't believe that $p$;\n\n3.  In virtue of the practical stakes, she doesn't justifiably believe\n    that $p$, although she does actually believe it.\n\n4.  In virtue of the practical stakes, she doesn't know that $p$,\n    although she does justifiably believe it.\n\nI think option 1 is implausibly sceptical, at least if applied to all\ncases like Coraline's. I've said that the probability of $p$ is 0.99,\nbut it should be clear that all that matters to generating a case like\nthis is that $p$ is not completely certain. Unless knowledge requires\ncertainty, we'll be able to generate Coraline-like cases where there is\nsufficient evidence for knowledge. So that's ruled out.\n\nOption 2 is basically what the Doxastic IRI theorist has to say. If\nCoraline has enough evidence to know $p$, but doesn't know $p$ due to\npractical stakes, then the Doxastic IRI theorist is committed to saying\nthat the practical stakes block *belief* in $p$. That's the Doxastic IRI\nposition; stakes matter to knowledge because they matter to belief.\n\nBut that's also an implausible description of Coraline's situation. She\nis very confident that $p$. Her confidence is grounded in the evidence\nin the right way. She is insensitive in her actual deliberations to the\ndifference between her evidence for $p$ and evidence that guarantees\n$p$. She would become sensitive to that difference if someone offered\nher a bet that she knew was a 1000-to-1 bet on $p$, but she doesn't know\nthat's what is on offer. In short, there is no difference between her\nunconditional attitudes, and her attitudes conditional on $p$, when it\ncomes to any live question. That's enough, I think, for belief. So she\nbelieves that $p$. And that's bad news for the Doxastic IRI theorist;\nsince it means here that stakes matter to knowledge without mattering to\nbelief. I conclude, reluctantly, that Doxastic IRI is false.\n\n### Stakes as Defeaters\n\nThat still leaves two options remaining, what I've called options 3 and\n4 above. Option 3, if suitably generalised, says that knowledge is\npractically sensitive because the justification condition on belief is\npractically sensitive. Option 4 says that practical considerations\nimpact knowledge directly. As I read them, Jeremy Fantl and Matthew\nMcGrath defend a version of Option 3. In the next and last subsection,\nI'll argue against that position. But first I want to sketch what a\nposition like option 4 would look like.\n\nKnowledge, unlike justification, requires a certain amount of internal\ncoherence among mental states. Consider the following story from David\nLewis:\n\n> I speak from experience as the repository of a mildly inconsistent\n> corpus. I used to think that Nassau Street ran roughly east-west; that\n> the railroad nearby ran roughly north-south; and that the two were\n> roughly parallel. [@Lewis1982c 436]\n\nI think in that case that Lewis doesn't know that Nassau Street runs\nroughly east-west. (From here on, call the proposition that Nassau\nStreet runs roughly east-west $N$.) If his belief that it does was\nacquired and sustained in a suitably reliable way, then he may well have\na justified belief that $N$. But the lack of coherence with the rest of\nhis cognitive system, I think, defeats any claim to knowledge he has.\n\nCoherence isn't just a requirement on belief; other states can cohere or\nbe incoherent. Assume Lewis corrects the incoherence in his beliefs, and\ndrops the belief that Nassau Street the railway are roughly parallel.\nStill, if Lewis believed that $N$, preferred doing $\\varphi$ to doing\n$\\psi$ conditional on $N$, but actually preferred doing $\\psi$ to doing\n$\\varphi$, his cognitive system would also be in tension. That tension\ncould, I think, be sufficient to defeat a claim to know that $N$.\n\nAnd it isn't just a requirement on actual states; it can be a\nrequirement on rational states. Assume Lewis believed that $N$,\npreferred doing $\\varphi$ to doing $\\psi$ conditional on $N$, and\npreferred doing $\\varphi$ to doing $\\psi$, but should have preferred\ndoing $\\psi$ to doing $\\varphi$ given his interests. Then I think the\nfact that the last preference is irrational, plus the fact that were it\ncorrected there would be incoherence in his cognitive states defeats the\nclaim to know that $N$.\n\nA concrete example of this helps make clear why such a view is\nattractive, and why it faces difficulties. Assume there is a bet that\nwins \\$2 if $N$, and loses \\$10 if not. Let $\\varphi$ be taking that\nbet, and $\\psi$ be declining it. Assume Lewis shouldn't take that bet;\nhe doesnt have enough evidence to do so. Then he clearly doesn't know\nthat $N$. If he knew that $N$, $\\varphi$ would dominate $\\psi$, and\nhence be rational. But it isn't, so $N$ isn't known. And that's true\nwhether Lewis's preferences between $\\varphi$ and $\\psi$ are rational or\nirrational.\n\nAttentive readers will see where this is going. Change the bet so it\nwins a penny if $N$, and loses \\$1,000 if not. Unless Lewis's evidence\nthat $N$ is incredibly strong, he shouldn't take the bet. So, by the\nsame reasoning, he doesn't know that $N$. And we're back saying that\nknowledge requires incredibly strong evidence. The solution, I say, is\nto put a pragmatic restriction on the kinds of incoherence that matter\nto knowledge. Incoherence with respect to irrelevant questions, such as\nwhether to bet on $N$ at extremely long odds, doesn't matter for\nknowledge. Incoherence (or coherence obtained only through\nirrationality) does. The reason, I think, that Non-Doxastic IRI is true\nis that this coherence-based defeater is sensitive to practical\ninterests.\n\nThe string of cases about Lewis and $N$ has ended up close to the\nCoraline example. We already concluded that Coraline didn't know $p$.\nNow we have a story about why - belief that $p$ doesn't cohere\nsufficiently well with what she should believe, namely that it would be\nwrong to take the bet. If all that is correct, just one question\nremains: does this coherence-based defeater also defeat Coraline's claim\nto have a justified belief that $p$? I say it does not, for three\nreasons.\n\nFirst, her attitude towards $p$ tracks the evidence perfectly. She is\nmaking no mistakes with respect to $p$. She is making a mistake with\nrespect to $q$, but not with respect to $p$. So her attitude towards\n$p$, i.e. belief, is justified.\n\nSecond, talking about beliefs and talking about credences are simply two\nways of modelling the very same things, namely minds. If the agent both\nhas a credence 0.99 in $p$, and believes that $p$, these are not two\ndifferent states. Rather, there is one state of the agent, and two\ndifferent ways of modelling it. So it is implausible to apply different\nvaluations to the state depending on which modelling tools we choose to\nuse. That is, it's implausible to say that while we're modelling the\nagent with credences, the state is justified, but when we change tools,\nand start using beliefs, the state is unjustified. Given this outlook on\nbeliefs and credences, it is natural to say that her belief is\njustified. Natural, but not compulsory, for reasons Jeremy Fantl pointed\nout to me.[^27] We don't want a metaphysics on which persons and\nphilosophers are separate entities. Yet we can say that someone is a\ngood person but a bad philosopher. Normative statuses can differ\ndepending on which property of a thing we are considering. That suggests\nit is at least coherent to say that one and the same state is a good\ncredence but a bad belief. But while this may be coherent, I don't think\nit is well motivated, and it is natural to have the evaluations go\ntogether.\n\nThird, we don't *need* to say that Coraline's belief in $p$ is\nunjustified in order to preserve other nice theories, in the way that we\ndo need to say that she doesn't know $p$ in order to preserve a nice\naccount of how we understand decision tables. It's this last point that\nI think Fantl and McGrath, who say that the belief is unjustified, would\nreject. So let's conclude with a look at their arguments.\n\n### Fantl and McGrath on Interest-Relativity\n\nFantl and McGrath's argue for the principle (JJ), which entails that\nCoraline is not justified in believing $p$.\n\n(JJ)\n\n:   If you are justified in believing that $p$, then $p$ is warranted\n    enough to justify you in $\\varphi$-ing, for any $\\varphi$.\n    [@FantlMcGrath2009 99]\n\nIn practice, what this means is that there can't be a salient\n$p, \\varphi$ such that:\n\n-   The agent is justified in believing $p$;\n\n-   The agent is not warranted in doing $\\varphi$; but\n\n-   If the agent had more evidence for $p$, and nothing else, the agent\n    would be be warranted in doing $\\varphi$.\n\nThat is, once you've got enough evidence, or warrant, for justified\nbelief in $p$, then you've got enough evidence for $p$ as matters for\nany decision you face. This seems intuitive, and Fantl and McGrath back\nup its intuitiveness with some nicely drawn examples. But I think it is\nfalse, and the Coraline example shows it is false. Coraline isn't\njustified in taking the bet, and is justified in believing $p$, but more\nevidence for $p$ would suffice for taking the bet. So Coraline's case\nshows that (JJ) is false. But there are a number of possible objections\nto that position. I'll spend the rest of this section, and this paper,\ngoing over them.[^28]\n\n*Objection*: The following argument shows that Coraline is not in fact\njustified in believing that $p$.\n\n1.  $p$ entails that Coraline should take the bet, and Coraline knows\n    this.\n\n2.  If $p$ entails something, and Coraline knows this, and she\n    justifiably believes $p$, she is in a position to justifiably\n    believe the thing entailed.\n\n3.  Coraline is not in a position to justifiably believe that she should\n    take the bet.\n\n4.  So, Coraline does not justifiably believe that $p$\n\n*Reply*: The problem here is that premise 1 is false. What's true is\nthat $p$ entails that Coraline will be better off taking the bet than\ndeclining it. But it doesn't follow that she should take the bet.\nIndeed, it isn't actually true that she should take the bet, even though\n$p$ is actually true. Not just is the entailment claim false, the world\nof the example is a counterinstance to it.\n\nIt might be controversial to use this very case to reject premise 1. But\nthe falsity of premise 1 should be clear on independent grounds. What\n$p$ entails is that Coraline will be best off by taking the bet. But\nthere are lots of things that will make me better off that I shouldn't\ndo. Imagine I'm standing by a roulette wheel, and the thing that will\nmake me best off is betting heavily on the number than will actually\ncome up. It doesn't follow that I should do that. Indeed, I should not\ndo it. I shouldn't place any bets at all, since all the bets have a\nhighly negative expected return.\n\nIn short, all $p$ entails is that taking the bet will have the best\nconsequences. Only a very crude kind of consequentialism would identify\nwhat I should do with what will have the best returns, and that crude\nconsequentialism isn't true. So $p$ doesn't entail that Coraline should\ntake the bet. So premise 1 is false.\n\n*Objection*: Even though $p$ doesn't *entail* that Coraline should take\nthe bet, it does provide inductive support for her taking the bet. So if\nshe could justifiably believe $p$, she could justifiably (but\nnon-deductively) infer that she should take the bet. Since she can't\njustifiably infer that, she isn't justified in taking the bet.\n\n*Reply*: The inductive inference here looks weak. One way to make the\ninductive inference work would be to deduce from $p$ that taking the bet\nwill have the best outcomes, and infer from that that the bet should be\ntaken. But the last step doesn't even look like a reliable ampliative\ninference. The usual situation is that the best outcome comes from\ntaking an *ex ante* unjustifiable risk.\n\nIt may seem better to use $p$ combined with the fact that conditional on\n$p$, taking the bet has the highest *expected* utility. But actually\nthat's still not much of a reason to take the bet. Think again about\ncases, completely normal cases, where the action with the best outcome\nis an *ex ante* unjustifiable risk. Call that action $\\varphi$, and let\n$B \\varphi$ be the proposition that $\\varphi$ has the best outcome. Then\n$B \\varphi$ is true, and conditional on $B \\varphi$, $\\varphi$ has an\nexcellent expected return. But doing $\\varphi$ is still running a dumb\nrisk. Since these kinds of cases are normal, it seems it will very often\nbe the case that this form of inference leads from truth to falsity. So\nit's not a reliable inductive inference.\n\n*Objection*: In the example, Coraline isn't just in a position to\njustifiably believe $p$, she is in a position to *know* that she\njustifiably believes it. And from the fact that she justifiably believes\n$p$, and the fact that if $p$, then taking the bet has the best option,\nshe can infer that she should take the bet.\n\n*Reply*: It's possible at this point that we get to a dialectical\nimpasse. I think this inference is non-deductive, because I think the\nexample we're discussing here is one where the premises are true and the\nconclusion false. Presumably someone who doesn't like the example will\nthink that it is a good deductive inference.\n\nHaving said that, the more complicated example at the end of\n@Weatherson2005-WEACWD was designed to raise the same problem without\nthe consequence that if $p$ is true, the bet is sure to return a\npositive amount. In that example, conditionalising on $p$ means the bet\nhas a positive expected return, but still possibly a negative return.\nBut in that case (JJ) still failed. If accepting there are cases where\nan agent justifiably believes $p$, and hence justifiably believes taking\nthe bet will return the best outcome, and knows all this, but still\ncan't rationally bet on $p$ is too much to accept, that more complicated\nexample might be more persuasive. Otherwise, I concede that someone who\nbelieves (JJ) and thinks rational agents can use it in their reasoning\nwill not think that a particular case is a counterexample to (JJ).\n\n*Objection*:If Coraline were ideal, then she wouldn't believe $p$.\nThat's because if she were ideal, she would have a lower credence in\n$q$, and if that were the case, her credence in $p$ would have to be\nmuch higher (close to 0.999) in order to count as a belief. So her\nbelief is not justified.\n\n*Reply*: The premise here, that if Coraline were ideal she would not\nbelieve that $p$, is true. The conclusion, that she is not justified in\nbelieving $p$, does not follow. It's always a mistake to *identify* what\nshould be done with what is done in ideal circumstances. This is\nsomething that has long been known in economics. The *locus classicus*\nof the view that this is a mistake is @LipseyLancaster. A similar point\nhas been made in ethics in papers such as @Watson1977 and\n@KennettSmith1996b [@KennettSmith1996a]. And it has been extended to\nepistemology by @Williamson1998-WILCOK.\n\nAll of these discussions have a common structure. It is first observed\nthat the ideal is both $F$ and $G$. It is then stipulated that whatever\nhappens, the thing being created (either a social system, an action, or\na cognitive state) will not be $F$. It is then argued that given the\nstipulation, the thing being created should not be $G$. That is not just\nthe claim that we shouldn't *aim* to make the thing be $G$. It is,\nrather, that in many cases being $G$ is not the best way to be, given\nthat $F$-ness will not be achieved. Lipsey and Lancaster argue that (in\nan admittedly idealised model) that it is actually quite unusual for $G$\nto be best given that the system being created will not be $F$.\n\nIt's not too hard to come up with examples that fit this structure.\nFollowing [@Williamson2000-WILKAI 209], we might note that I'm justified\nin believing that there are no ideal cognitive agents, although were I\nideal I would not believe this. Or imagine a student taking a ten\nquestion mathematics exam who has no idea how to answer the last\nquestion. She knows an ideal student would correctly answer an even\nnumber of questions, but that's no reason for her to throw out her good\nanswer to question nine. In general, once we have stipulated one\ndeparture from the ideal, there's no reason to assign any positive\nstatus to other similarities to the idea. In particular, given that\nCoraline has an irrational view towards $q$, she won't perfectly match\nup with the ideal, so there's no reason it's good to agree with the\nideal in other respects, such as not believing $p$.\n\nStepping back a bit, there's a reason the interest-relative theory says\nthat the ideal and justification come apart right here. On the\ninterest-relative theory, like on any pragmatic theory of mental states,\nthe *identification* of mental states is a somewhat holistic matter.\nSomething is a belief in virtue of its position in a much broader\nnetwork. But the *evaluation* of belief is (relatively) atomistic.\nThat's why Coraline is justified in believing $p$, although if she were\nwiser she would not believe it. If she were wiser, i.e., if she had the\nright attitude towards $q$, the very same credence in $p$ would not\ncount as a belief. Whether her state counts as a belief, that is,\ndepends on wide-ranging features of her cognitive system. But whether\nthe state is justified depends on more local factors, and in local\nrespects she is doing everything right.\n\n*Objection*: If Coraline is justified in believing $p$, then Coraline\ncan use $p$ as a premise in practical reasoning. If Coraline can use $p$\nas a premise in practical reasoning, and $p$ is true, and her belief in\n$p$ is not Gettiered, then she knows $p$. By hypothesis, her belief is\ntrue, and her belief is not Gettiered. So she should know $p$. But she\ndoesn't know $p$. So by several steps of modus tollens, she isn't\njustified in believing $p$.[^29]\n\n*Reply*: This objection this one turns on an equivocation over the\nneologism 'Gettiered'. Some epistemologists use this to simply mean that\na belief is justified and true without constituting knowledge. By that\nstandard, the third sentence is false. Or, at least, we haven't been\ngiven any reason to think that it is true. Given everything else that's\nsaid, the third sentence is a raw assertion that Coraline knows that\n$p$, and I don't think we should accept that.\n\nThe other way epistemologists sometimes use the term is to pick out\njustified true beliefs that fail to be knowledge for the reasons that\nthe beliefs in the original examples from @Gettier1963 fail to be\nknowledge. That is, it picks out a property that beliefs have when they\nare derived from a false lemma, or whatever similar property is held to\nbe doing the work in the original Gettier examples. Now on this reading,\nCoraline's belief that $p$ is not Gettiered. But it doesn't follow that\nit is known. There's no reason, once we've given up on the JTB theory of\nknowledge, to think that whatever goes wrong in Gettier's examples is\nthe *only* way for a justified true belief to fall short of knowledge.\nIt could be that there's a practical defeater, as in this case. So the\nsecond sentence of the objection is false, and the objection again\nfails.\n\nOnce we have an expansive theory of defeaters, as I've adopted here, it\nbecomes problematic to describe the case in the language Fantl and\nMcGrath use. They focus a lot on whether agents like Coraline have\n'knowledge-level justification' for $p$, which is defined as\n\"justification strong enough so that shortcomings in your strength of\njustification stand in the way of your knowing\". [@FantlMcGrath2009 97].\nAn important part of their argument is that an agent is justified in\nbelieving $p$ iff they have knowledge-level justification for $p$. I\nhaven't addressed this argument, so I'm not really addressing the case\non their terms.\n\nWell, does Coraline have knowledge-level justification for $p$? I'm not\nsure, because I'm not sure I grasp this concept. Compare the agent in\nHarman's dead dictator case [@Harman1973 75]. Does she have\nknowledge-level justification that the dictator is dead? In one sense\nyes; it is the existence of misleading news sources that stops her\nknowing. In another sense no; she doesn't know, but if she had better\nevidence (e.g., seeing the death happen) she would know. I want to say\nthe same thing about Coraline, and that makes it hard to translate the\nCoraline case into Fantl and McGrath's terminology.\n\n[^1]: The reflections in the next few paragraphs are inspired by some\n    comments by Stalnaker in his [-@Stalnaker2008], though I don't want\n    to suggest the theory I'll discuss is actually Stalnaker's.\n\n[^2]: Whether Doxastic or Non-Doxastic IRI is true about justified\n    belief ascriptions turns on some tricky questions about what to say\n    when a subject's credences are nearly, but not exactly appropriate\n    given her evidence. Space considerations prevent a full discussion\n    of those cases here. Whether I can hold onto the strict invariantism\n    about claims about justified credences depends, I now think, on\n    whether an interest-neutral account of evidence can be given.\n    Discussions with Tom Donaldson and Jason Stanley have left me less\n    convinced than I was in 2005 that this is possible, but this is far\n    too big a question to resolve here.\n\n[^3]: I mean here the case of Coraline, to be discussed in section 3\n    below. Several people have remarked in conversation that Coraline\n    doesn't look to them like a case of Ignorant High Stakes. This isn't\n    surprising; Coraline is better described as being *mistaken* than\n    *ignorant*, and she's mistaken about odds not stakes. If they're\n    right, that probably means my argument for Non-Doxastic IRI is less\n    like Stanley's, and hence more original, than I think it is. So I\n    don't feel like pressing the point! But I do want to note that *I*\n    thought the Coraline example was a variation on a theme Stanley\n    originated.\n\n[^4]: If we are convinced that the right decision is the one that\n    maximises expected utility, there is a sense in which these\n    questions collapse. For the expected utility theorist, we can solve\n    Dom's question by making sure the states are logically exhaustive,\n    and making the 'payouts' in each state be expected payouts. But the\n    theory that the correct decision is the one that maximises expected\n    utility, while plausibly true, is controversial. It shouldn't be\n    assumed when we are investigating the semantics of decision tables.\n\n[^5]: Assuming Alice's utility curve for money curves downwards, she\n    should be looking for a slightly higher chance of winning than\n    $\\frac{1}{2}$ to place the bet, but that level of detail isn't\n    relevant to the story we're telling here.\n\n[^6]: I'm here retracting some things I said a few years ago in a paper\n    on philosophical methodology [@Weatherson2003-WEAWGA]. There I\n    argued that identifying knowledge with justified true belief would\n    give us a theory on which knowledge was more natural than a theory\n    on which we didn't identify knowledge with any other epistemic\n    property. I now think that is wrong for a couple of reasons. First,\n    although it's true (as I say in the earlier paper) that knowledge\n    can't be primitive or perfectly natural, this doesn't make it less\n    natural than justification, which is also far from a fundamental\n    feature of reality. Indeed, given how usual it is for languages to\n    have a simple representation of knowledge, we have some evidence\n    that it is very natural for a term from a special science. Second, I\n    think in the earlier paper I didn't fully appreciate the point\n    (there attributed to Peter Klein) that the Gettier cases show that\n    the property of being a justified true belief is not particularly\n    natural. In general, when $F$ and $G$ are somewhat natural\n    properties, then so is the property of being $F \\wedge G$. But there\n    are exceptions, especially in cases where these are properties that\n    a whole can have in virtue of a part having the property. In those\n    cases, a whole that has an $F$ part and a $G$ part will be\n    $F \\wedge G$, but this won't reflect any distinctive property of the\n    whole. And one of the things the Gettier cases show is that the\n    properties of *being justified* and *being true*, as applied to\n    belief, fit this pattern.\n\n    Note that even if you think that philosophers are generally too\n    quick to move from instinctive reactions to the Gettier case to\n    abandoning the justified true belief theory of knowledge, this point\n    holds up. What is important here is that on sufficient reflection,\n    the Gettier cases show that some justified true beliefs are not\n    knowledge, and that the cases in question also show that being a\n    justified true belief is not a particularly natural or unified\n    property. So the point I've been making in the last this footnote is\n    independent of the point I wanted to stress in \"What Good are\n    Counterexamples?\", namely, that philosophers in some areas\n    (especially epistemology) are insufficiently reformist in their\n    attitude towards our intuitive reactions to cases.\n\n[^7]: This issue is of course central to the plotline in @Hawthorne2004.\n\n[^8]: Assume, perhaps implausibly, that the sudden appearance of the\n    genie is evidentially irrelevant to the proposition that the\n    musician is Beth. The reasons this may be implausible are related to\n    the arguments in [@RunyonGuysDolls 14-15]. Thanks here to Jeremy\n    Fantl.\n\n[^9]: The idea that interest-relativity is a way of fending off\n    scepticism is a very prominent theme in @FantlMcGrath2009.\n\n[^10]: On the version of IRI I'm defending, Barry is free to be\n    interested in whatever he likes. If he started wondering about\n    whether it would be rational to take such a bet, he loses the\n    knowledge that Beth is the musician, even if there is no genie and\n    the bet isn't offered. The existence of the genie's offer makes the\n    bet a practical interest; merely wondering about the genie's offer\n    makes the bet a cognitive interest. But both kinds of interests are\n    relevant to knowledge.\n\n[^11]: As they make clear in their [-@Hawthorne2008-HAWKAA], Hawthorne\n    and Stanley are interested in defending relatively strong premises\n    linking knowledge and action independently of the argument for the\n    interest-relativity of knowledge. What I'm doing here is showing how\n    that conclusion does not rest on anything nearly as strong as the\n    principles they believe, and so there is plenty of space to disagree\n    with their general principles, but accept interest-relativity. The\n    strategy here isn't a million miles from the point noted in Fantl\n    and McGrath [-@FantlMcGrath2009 72n14] when they note that much\n    weaker premises than the ones they endorse imply a failure of\n    'purism'.\n\n[^12]: I have more to say about those cases in section 2.2.\n\n[^13]: Also note that I'm not taking as a premise any claim about what\n    Barry knows after the bet is offered. A lot of work on\n    interest-relativity has used such premises, or premises about\n    related intuitions. This seems like a misuse of the method of cases\n    to me. That's not because we should never use intuitions about\n    cases, just that these cases are too hard to think that snap\n    judgments about them are particularly reliable. In general, we can\n    know a lot about cases by quickly reflecting on them. Similarly, we\n    know a lot about which shelves are level and which are uneven by\n    visual inspection, i.e., 'eyeballing'. But when different eyeballs\n    disagree, it's time to bring in other tools. That's the approach of\n    this paper. I don't have a story about why the various eyeballs\n    disagree about cases like Barry's; that seems like a task best\n    undertaken by a psychologist not a philosopher [@Ichikawa2009].\n\n[^14]: This is obviously not a full argument against contextualism; that\n    would require a much longer paper than this.\n\n[^15]: See, for instance, @MBT2009, or @FeltzZarpentine2010.\n\n[^16]: In the last two lines, I use $U(\\phi)$ to denote the expected\n    utility of $\\phi$, and $U(\\phi | p)$ to denote the expected utility\n    of $\\phi$ conditional on $p$. It's often easier to write this as\n    simply $U(\\phi \\wedge p)$, since the utility of $\\phi$ conditional\n    on $p$ just is the utility of doing $\\phi$ in a world where $p$ is\n    true. That is, it is the utility of $\\phi \\wedge p$ being realised.\n    But we get a nicer symmetry between the probabilistic principles and\n    the utility principles if we use the explictly conditional notation\n    for each.\n\n[^17]: This is probably somewhat unrealistic. It's hard to think about\n    whether $\\Pr(p)$ is closer to 0.7 or 0.8 without raising to salience\n    questions about, for example, what the second decimal place in\n    $\\Pr(p)$ is. This is worth bearing in mind when coming up with\n    intuitions about the cases in this paragraph.\n\n[^18]: See @Sturgeon2008-STURAT for discussion of a similar puzzle for\n    anyone trying to tell a unified story of belief and credence.\n\n[^19]: There are exceptions, especially in cases where $p$ concerns\n    something significant to financial markets, and the agent trades\n    financial products. If you work through the theory that I'm about to\n    lay out, one consequence is that such agents should have very few\n    unconditional beliefs about financially-sensitive information, just\n    higher and lower credences. I think that's actually quite a nice\n    outcome, but I'm not going to rely on that in the argument for the\n    view.\n\n[^20]: The presentation in this section, as in the earlier paper,\n    assumes at least a weak form of consequentialism in the sense of\n    @Hammond1988. This was arguably a weakness of the earlier paper.\n    We'll return to the issue of what happens in cases where the agent\n    doesn't, and perhaps shouldn't, maximise expected utility, at the\n    end of the section.\n\n[^21]: I'm borrowing this example from Fred Dretske, who uses it to make\n    some interesting points about dispositional belief.\n\n[^22]: The recipe here is similar to that given in\n    @Weatherson2005-WEACWD, but the motivation is streamlined. Thanks to\n    Jacob Ross for helpful suggestions here.\n\n[^23]: Some consequentialists say that what the agent should do depends\n    on whether $p$ is true. If $p$ is true, she should do $\\psi$, and if\n    $p$ is false she should do $\\varphi$. As we'll see, I have reasons\n    for thinking this is rather radically wrong.\n\n[^24]: The target here is not directly the interest-relativity of their\n    theories, but more general principles about the role of knowledge in\n    action and assertion. Since my theories are close enough, at least\n    in consequences, to Hawthorne and Stanley's, it is important to note\n    how my theory handles the case.\n\n[^25]: I'm more interested in the abstract structure of the case than in\n    whether any real-life situation is modelled by just this structure.\n    But it might be worth noting the rough kind of situation where this\n    kind of situation can arise. So let's say Coraline has a particular\n    bank account that is uninsured, but which currently paying 10%\n    interest, and she is deciding whether to deposit another \\$1000 in\n    it. Then $p$ is the proposition that the bank will not collapse, and\n    she'll get her money back, and $q$ is the proposition that the\n    interest will stay at 10%. To make the model exact, we have to also\n    assume that if the interest rate on her account doesn't stay at 10%,\n    it falls to 0.1%. And we have to assume that the interest rate and\n    the bank's collapse are probabilistically independent. Neither of\n    these are at all realistic, but a realistic case would simply be\n    more complicated, and the complications would obscure the\n    philosophically interesting point.\n\n[^26]: If she did compute the expected utility, then one of the things\n    that would be salient for her is the expected utility of the bet.\n    And the expected utility of the bet is different to its expected\n    utility given $p$. So if that expected utility is salient, she\n    doesn't believe $p$. And it's going to be important to what follows\n    that she *does* believe $p$.\n\n[^27]: The following isn't Fantl's example, but I think it makes much\n    the same point as the examples he suggested.\n\n[^28]: Thanks here to a long blog comments thread with Jeremy Fantl and\n    Matthew McGrath for making me formulate these points much more\n    carefully. The original thread is at\n    <http://tar.weatherson.org/2010/03/31/do-justified-beliefs-justify-action/>.\n\n[^29]: Compare the 'subtraction argument' on page 99 of\n    @FantlMcGrath2009.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
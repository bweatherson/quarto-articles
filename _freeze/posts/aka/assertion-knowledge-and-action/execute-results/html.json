{
  "hash": "033ca76f9925472a450971328948c2b1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assertion, Knowledge and Action\"\ndescription: | \n    We argue against the knowledge rule of assertion, and in favour of integrating the account of assertion more tightly with our best theories of evidence and action. We think that the knowledge rule has an incredible consequence when it comes to practical deliberation, that it can be right for a person to do something that she can’t properly assert she can do. We develop some vignettes that show how this is possible, and how odd this consequence is. We then argue that these vignettes point towards alternate rules that tie assertion to sufficient evidence-responsiveness or to proper action. These rules have many of the virtues that are commonly claimed for the knowledge rule, but lack the knowledge rule’s problematic consequences when it comes to assertions about what to do.\ndate: March 23 2010\nauthor:\n  - name: Ishani Maitra\n    url: https://lsa.umich.edu/philosophy/people/faculty/imaitra.html\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\njournal: \n  title: \"Philosophical Studies\"\n  publisher: Springer\nvolume: 149\nissue: 1\ndoi: \"10.1007/s11098-010-9542-z\"\nslug: maitraweatherson2010aka\npreview: commons.jpg\ncitation_url: https://doi.org/10.1007/s11098-010-9542-z\ncitation: false\nbibliography: ../../../articles/Rbib.bib\ncategories:\n  - epistemology\n  - language\n  - interest-relativity\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    number_sections: TRUE\n---\n\n\n\nIt is widely believed that the mere truth of *p* is insufficient for *p*\nto be properly assertable, even if *p* is relevant to current\nconversation. If a speaker simply guessed that *p* is true, then she\nshouldn't say *p*, for example. There is some dissent from this view\n(e.g., @Weiner2005), but it is something close to orthodoxy in the\ncurrent literature on assertion that something further is needed. The\nmost common 'something else' is *knowledge*: a speaker shouldn't say *p*\nunless they know *p*. This view is nowadays commonly associated with\nTimothy Williamson [-@Williamson1996-WILKAA; -@Williamson2000-WILKAI],\nbut it has historical antecedents tracing back at least to Max Black's\n[-@Black1952] paper \"Saying and Disbelieving\".[^2] Call Williamson's\nposition *The Knowledge Rule*.\n\n<aside>\nPublished in _Philosophical Studies_ 149: 99-118.\n\nWe'd like to thank Matthew Benton, Jessica Brown, Andy Egan, and\n    Susanna Schellenberg, as well as an audience at the Bellingham\n    Summer Philosophy Conference, for helpful discussion of earlier\n    drafts of this paper.\n</aside>\n\nThe Knowledge Rule\n\n:   Assert that *p* only if you know that *p*.\n\nThis paper aims to raise trouble for The Knowledge Rule, and several\nrelated positions, by focussing on a particular kind of assertion. We'll\nbe looking at assertions about what is to be done. The boldest statement\nof our position is that if an agent should do *X*, then that agent is in\na position to say that they should do *X*. (We'll qualify this a little\nbelow, but it's helpful to start with the bold position.) We argue,\nfollowing Williamson's 'anti-luminosity' arguments, that its being true\nthat *X* is the thing to do for an agent doesn't entail that that agent\nknows it's the thing to do.[^3] If both these claims are true, then\nthere will be cases where it is fine to assert that *X* is what to do,\neven though the agent doesn't know this. So, The Knowledge Rule is\nmistaken. Slightly more formally, we'll be interested in arguments of\nthis structure.\n\n<aside>\nPicture by [UK Parliament](https://www.flickr.com/photos/25334641@N08) via [Creative Commons](https://search.creativecommons.org/photos/93581ab1-acd2-4f57-a934-10bc0ae9f7ce).\n</aside>\n\n> *Master Argument (First Attempt)*\n>\n> 1.  If act *X* is what to do for agent *S*, then *S* can properly\n>     assert that *X* is what to do (assuming that this assertion is\n>     relevant to the current conversation).\n>\n> 2.  It is possible that *X* is what to do for *S,* even though *S* is\n>     not in a position to know this.\n>\n> 3.  So, it is possible that *S* can properly assert that *X* is what\n>     to do even though she does not know, and is not even in a position\n>     to know, that *X* is what to do.\n\nIn section 1, we'll motivate premise 1 with a couple of vignettes. In\nsection 2, we'll qualify that premise and make it more plausible. In\nsection 3, we'll motivate premise 2. In section 4, we'll look at one of\nthe positive arguments for The Knowledge Rule, the argument from Moore's\nparadox, and conclude that it is of no help. In section 5, we'll look at\nwhat could be put in place of The Knowledge Rule, and suggest two\nalternatives.\n\nThe Evidence Responsiveness Rule\n\n:   Assert that *p* only if your attitude towards *p* is properly\n    responsive to the evidence you have that bears on *p*.\n\nThe Action Rule\n\n:   Assert that *p* only if acting as if *p* is true is the thing for\n    you to do.\n\nWe're not going to argue for these rules in detail; that would take a\nmuch longer paper. Nor are we going to decide between them. What we are\ngoing to suggest is that these rules have the virtues that are commonly\nclaimed for The Knowledge Rule, but lack The Knowledge Rule's\nproblematic consequences when it comes to assertions about what to do.\n\n## Speaking about What to Do\n\nWe start by motivating premise 1 of the Master Argument with a couple of\nexamples. Both cases are direct counterexamples to The Knowledge Rule,\nbut we're interested in the first instance in what the cases have in\ncommon. After presenting the vignettes, we offer three distinct\narguments to show that, in such cases, it *is* proper for the speakers\nto assert what they do assert, even though they don't know it to be\ntrue.\n\n### Going to War {#going-to-war .unnumbered}\n\nImagine that a country, Indalia, finds itself in a situation in which\nthe thing for it to do, given the evidence available to its leaders, is\nto go to war against an enemy. (Those pacifists who think it is never\nright to go to war won't like this example, but we think war can at\nleast sometimes be justified.) But it is a close call. Had the evidence\nbeen a bit weaker, had the enemy been a little less murderous, or the\nrisk of excessive civilian casualties a little higher, it would have\nbeen preferable to wait for more evidence, or use non-military measures\nto persuade the enemy to change its ways. So, while going to war is the\nthing to do, the leaders of Indalia can't know this. We'll come back to\nthis in section 2, but the crucial point here is that knowledge has a\nsafety constraint, and any putative knowledge here would violate this\nconstraint.\n\nOur leaders are thus in a delicate position here. The Prime Minister of\nIndalia decides to launch the war, and gives a speech in the House of\nCommons setting out her reasons. All the things she says in the speech\nare true, and up to her conclusion they are all things that she knows.\nShe concludes with (1).\n\nNow (1) is also true, and the Prime Minister believes it, but it is not\nsomething she knows. So, the Prime Minister violates The Knowledge Rule\nwhen she asserts (1). But it seems to us that she doesn't violate any\nnorms in making this assertion. We'll have a lot more to say about why\nthis is so in a few paragraphs. But first, here's a less dramatic case\nthat is also a counterexample to The Knowledge Rule, one that involves\nprudential judgments rather than moral judgments.\n\n### Buying Flood Insurance {#buying-flood-insurance .unnumbered}\n\nRaj and Nik are starting a small business. The business is near a river\nthat hasn't flooded in recent memory, but around which there isn't much\nflood protection. They could buy flood insurance which would be useful\nin a flood, naturally, but would be costly in the much more likely event\nthat there is not a flood. Raj has done the calculations of the\nlikelihood of a flood, the amount this would damage the business, the\nutility loss of not having this damage insured, and the utility loss of\npaying flood insurance premiums. He has concluded that buying flood\ninsurance is the thing to do. As it happens, this was a good conclusion\nto draw: it does, in fact, maximise his (and Nik's) expected utility\nover time. (It doesn't maximise their actual utility, as there actually\nwon't be a flood over the next twelve months. So, the insurance premium\nis an expense they could have avoided. But that doesn't seem\nparticularly relevant for prudential evaluation. Prudential buyers of\ninsurance should maximise expected utility, not actual utility. Or so we\nmust say unless we want to be committed to the view that everyone who\nbuys an insurance policy and doesn't make a claim on it is imprudent.)\n\nBut again, it's a close call. If there had been a little less evidence\nthat a flood was a realistic possibility, or the opportunity cost of\nusing those dollars on insurance premiums had been a little higher, or\nthe utility function over different outcomes a little different, it\nwould have been better to forego flood insurance. That suggests that\nsafety considerations make it the case that Raj doesn't know that buying\nflood insurance is the thing to do, though in fact it is.\n\nLet's now assume Raj has done everything he should do to investigate the\ncosts and benefits of flood insurance. We can imagine a conversation\nbetween him and Nik going as follows.\n\n> Nik: Should we get flood insurance?\\\n> Raj: I don't know. Hold on; I'm on the phone.\\\n> Nik: Who are you calling?\\\n> Raj: The insurance agent. I'm buying flood insurance.\n\nThere is clearly a pragmatic tension in Raj's actions here. But given\nThe Knowledge Rule, there's little else he can do. It would be a serious\nnorm violation to say nothing in response to Nik's question. And given\nthat he can't say \"Yes\" without violating The Knowledge Rule, he has to\nsay \"I don't know\". Moreover, since by hypothesis buying flood insurance\nis the thing to do in his situation, he can't not buy the insurance\nwithout doing the wrong thing. So, given The Knowledge Rule, he's doing\nthe best he can. But it's crazy to think that this is the best he can\ndo.\n\nWe think that these cases are problems for The Knowledge Rule. In\nparticular, we think that in each case, there is a non-defective\nassertion of something that is not known. It seems to us intuitively\nclear that those assertions are non-defective, but for those who don't\nshare this intuition, we have three independent arguments. The arguments\nfocus on **Going to War**, but they generalize easily enough to **Buying\nFlood Insurance**.\n\n### Argument One: \"That was your first mistake\" {#argument-one-that-was-your-first-mistake .unnumbered}\n\nImagine that the Prime Minister has a philosophical advisor. And the\nadvisor's job is to inform the Prime Minister whenever she violates a\nnorm, and stay silent otherwise. If The Knowledge Rule is correct, then\nthe advisor should stay silent as the Prime Minister orders the\ninvasion, silent as the Prime Minister sets out the reasons for the\ninvasion, then speak up at the very last line of the speech. That\nstrikes us as absurd. It's particularly absurd when you consider that\nthe last line of the speech is supported by what came earlier in the\nspeech, and the Prime Minister believes it, and asserts it, because it\nis well supported by what came earlier in the speech. Since we think\nthis couldn't be the right behaviour for the advisor, we conclude that\nthere's no norm violation in the Prime Minister asserting (1).\n\nWe've heard two replies to this kind of argument. According to one sort\nof reply, The Knowledge Rule is not meant to be an\n'all-things-considered' norm. The defender of The Knowledge Rule can say\nthat the Prime Minister's assertion is defective because it violates\nthat rule, but allow that it is nevertheless all-things-considered\nproper, because some other norm outweighs The Knowledge Rule on this\noccasion. We agree that The Knowledge Rule is not intended to be an\nall-things-considered norm. But even keeping clearly in mind the\ndistinction between being defective in some respect and being defective\nall-things-considered, it is still deeply unintuitive to say that the\nPrime Minister's assertion is defective in a respect. That is, we don't\nthink the philosophical advisor should speak up just at the very end of\nthe Prime Minister's speech even if she's meant to observe *all* the\nnorm violations (rather than just the all-things-considered norm\nviolations).\n\nPerhaps the defender of The Knowledge Rule needn't just appeal to an\nintuition here. Another reply we've heard starts from the premise that\nthe Prime Minister's assertion would be better, in a certain respect, if\nshe *knew* that it was true. Therefore, there is a respect in which that\nassertion is defective, just as The Knowledge Rule requires. To this\nsecond reply, our response is that the premise is true, but the\nreasoning is invalid. Saying why requires reflecting a bit on the nature\nof norms.\n\nThere are lots of ways for assertions to be better. It is better,\n*ceteris paribus*, for assertions to be funny rather than unfunny. It is\nbetter for assertions to be sensitive rather than insensitive. (We mean\nthis both in the Nozickian sense, i.e., an assertion is sensitive iff it\nwouldn't have been made if it weren't true, and in the Hallmark greeting\ncard sense.) It is better for speakers to be certain of the truth of\ntheir assertions than for them to be uncertain. But these facts don't\nimply that humour, sensitivity, or certainty are norms of assertion, for\nit doesn't follow that assertions that lack humour (or sensitivity or\ncertainty) are *always* defective. Similarly, the fact that it is better\nto know what you say than not doesn't imply that asserting what you\ndon't know is always defective. In slogan form: *Not every absence of\nvirtue is a vice*. We think knowledge is a virtue of assertions. (In\nfact, we think that pretty much every norm of assertion that has been\nproposed in the literature picks out a virtue of assertion.) What we\ndeny is that the absence of knowledge is (always) a vice. Since not\nevery absence of virtue is a vice, one can't argue that the Prime\nMinister's assertion is *defective* by arguing it could have been\nbetter. And that's why the argument being considered is invalid.\n\n### Argument Two: \"Actions speak louder than words\" {#argument-two-actions-speak-louder-than-words .unnumbered}\n\nIt's a bit of folk wisdom that actions speak louder than words. It isn't\ncrystal clear just what this wisdom amounts to, but we think one aspect\nof it is that an agent incurs more normative commitments by *doing* *X*\nthan by *talking* about *X*. But if The Knowledge Rule is right, then\nthis piece of wisdom is in this aspect back-to-front. According to that\nrule, an agent incurs a greater normative commitment by *saying* that\n*X* is what to do than they do by just doing *X*. If they do *X*, and\n*X* is indeed what to do, then they've satisfied all of their normative\ncommitments. If, by contrast, they say that *X* is what to do, then not\nonly must *X* be what to do, but they must know this fact as well. This\nstrikes us as completely back-to-front. We conclude that there is\nnothing improper about asserting that *X* is what to do (as the Prime\nMinister does), when *X* is in fact what to do.\n\n### Argument Three: \"What else could I do?\" {#argument-three-what-else-could-i-do .unnumbered}\n\nHere's a quite different argument that *Going to War* *is* a\ncounterexample to The Knowledge Rule.\n\n1.  If ending the speech the way she did was a norm violation, there is\n    a better way for the Prime Minister to end her speech.\n\n2.  There is no better way for the Prime Minister to end the speech\n    without saying something that she does not know to be true.\n\n3.  So, ending the speech the way she did was not a norm violation.\n\n4.  So, The Knowledge Rule is subject to counterexample.\n\nPremise 1 is a kind of 'ought-implies-can' principle, and as such, it\nisn't completely obvious that it is true. But when we've presented this\nargument to various groups, the focus has always been on premise two.\nThe common complaint has been that the Prime Minister could have ended\nthe speech in one of the following ways, thereby complying with The\nKnowledge Rule.\n\n-   I've decided that going to war is the thing to do in the\n    circumstances.\n\n-   I believe that going to war is the thing to do in the circumstances.\n\n-   It seems to me that going to war is the thing to do in the\n    circumstances.\n\nOur first reply to this suggestion is that we'd fire a speechwriter who\nrecommended that a Prime Minister end such a speech in such a weaselly\nway, so this hardly counts as a criticism of premise 2. Our more serious\nreply is that even if the Prime Minister ended the speech this way,\nshe'd *still* violate The Knowledge Rule. To see why this is so, we need\nto pay a little closer attention to what The Knowledge Rule says.\n\nNote that The Knowledge Rule is *not* a rule about what kind of\ndeclarative utterance you can properly make. An actor playing Hamlet\ndoes not violate The Knowledge Rule if he fails to check, before\nentering the stage, whether something is indeed rotten in the state of\nDenmark. The rule is a rule about what one *asserts*. And just as you\ncan assert *less* than you declaratively utter (e.g., on stage), you can\nalso assert *more* than you declaratively utter.[^4] For instance,\nsomeone who utters *The F is G* in a context in which it is common\nground that *a* is the *F* typically asserts both that the *F* is *G*,\nand that *a* is *G*. Similarly, someone who utters *I think that S*\ntypically asserts both asserts that they have a certain thought, and\nasserts the content of that thought. We can see this is so by noting\nthat we can properly challenge an utterance of *I think that S* by\nproviding reasons that *S* is false, even if these are not reasons that\nshow that the speaker does not (or at least did not) have such a\nthought. In the context of her speech of the House of Commons, even if\nthe Prime Minister were to end with one of the options above, she would\nstill assert the same thing she would assert by uttering (1) in the\ncircumstances, and she'd still be right to make such an assertion.\n\n## Bases for Action and Assertion\n\nOne might worry that premise 1 in our master argument is mistaken, in\nthe following way. We said that if *X* is the thing to do for *S*, then\n*S* can say that *X* is what to do. But one might worry about cases\nwhere *S* makes a lucky guess about what is to be done. Above we\nimagined that Raj had taken all of the factors relevant to buying flood\ninsurance into account. But imagine a different case, one involving\nRaj\\*, Raj's twin in a similar possible world. Raj\\* decides to buy\nflood insurance because he consults his Magic 8-Ball. Then, even if\nbuying flood insurance would still maximize his expected utility, it\ndoesn't seem right for Raj\\* to say that buying flood insurance is what\nto do.\n\nHere is a defence of premise 1 that seems initially attractive, though\nnot, we think, ultimately successful. The Magic 8-ball case isn't a\nclear counterexample to premise 1, it might be argued, because it isn't\nclear that buying flood insurance for these reasons is the thing for\nRaj\\* to do. On one hand, we do have the concept of doing the right\nthing for the wrong reasons, and maybe that is the right way to describe\nwhat Raj\\* does if he follows the ball's advice. But it isn't clearly a\ncorrect way to describe Raj\\*. It's not true, after all, that he's\nmaximising actual utility. (Remember that there will be no claims on the\npolicy he buys.) And it isn't clear how to think about expected utility\nmaximisation when the entrepreneur in question relies on the old Magic\n8-Ball for decision making. And we certainly want to say that there's\nsomething wrong about this very decision when made using the Magic\n8-Ball. So, perhaps we could say that buying flood insurance isn't what\nto do for Raj\\* in this variant example, because he has bad reasons.\n\nBut this seems like a tendentious defence of the first premise. Worse\nstill, it is an unnecessary defence. What we really want to focus on are\ncases where people do the right thing for the right reasons. Borrowing a\nleaf from modern epistemology, we'll talk about actions having a basis.\nAs well as there being a thing to do in the circumstances (or, more\nplausibly, a range of things to do), there is also a correct *basis* for\ndoing that thing (or, more plausibly, a range of correct bases). What we\ncare about is when *S* does *X* on basis *B*, and doing *X* on basis *B*\nis the thing to do in *S*'s situation. Using this notion of a basis for\naction, we can restate the main argument.\n\n> *Master Argument (Corrected)*\n>\n> 1.  If doing *X* on basis *B* is what to do for agent *S*, then *S*\n>     can properly, on basis *B*, assert that *X* is what to do\n>     (assuming this is relevant to the conversation).\n>\n> 2.  It is possible that doing *X* on basis *B* is what to do for *S*,\n>     even though *S* is not in a position to know, and certainly not in\n>     a position to know on basis *B*, that *X* is what to do.\n>\n> 3.  So, it is possible that *S* properly can assert that *X* is what\n>     to do, even though she does not know, and is not even in a\n>     position to know, that *X* is what to do.\n\nWe endorse this version of the master argument. Since its conclusion is\nthe denial of The Knowledge Rule, we conclude that The Knowledge Rule is\nmistaken. But we perhaps haven't said enough about premise 2 to seal the\nargument. The next section addresses that issue.\n\n## Marginal Wars\n\nThe argument for premise 2 is just a simple application of Williamson's\nanti-luminosity reasoning. (The canonical statement of this reasoning is\nin [@Williamson2000-WILKAI Ch. 4]).) Williamson essentially argues as\nfollows, for many different values of *p*. There are many ways for *p*\nto be true, and many ways for it to be false. Some of the ways in which\n*p* can be true are extremely similar to ways in which it can be false.\nIf one of those ways is the actual way in which *p* is true, then to\nknow that *p* we have to know that situations very similar to the actual\nsituation do not obtain. But in general we can't know that. So, some of\nthe ways in which *p* can be true are not compatible with our knowing\nthat *p* is true. In Williamson's nice phrase, *p* isn't *luminous*,\nwhere a luminous proposition is one that can be known (by a salient\nagent) whenever it is true. The argument of this paragraph is called 'an\nanti-luminosity argument', and we think that many instances of it are\nsound.\n\nThere is a crucial epistemic premise in the middle of that argument:\nthat we can't know something if it is false in similar situations. There\nare two ways that we could try to motivate this premise. First, we could\ntry to motivate it with the help of conceptual considerations about the\nnature of knowledge. That's the approach that Williamson takes. But his\napproach is controversial. It is criticised by @Sainsbury1996 and\n@Weatherson2004-WEALMT on the grounds that his safety principle goes\nawry in some special cases. Sainsbury focuses on mathematical knowledge,\nWeatherson on introspective knowledge. But the cases in which we're most\ninterested in this paper -- Indalia going to war, Raj and Nik buying\nflood insurance -- don't seem to fall into either of these problem\ncategories. Nevertheless, rather than pursue this line, we'll consider a\ndifferent approach to motivating this premise.\n\nThe second motivation for the epistemic premise comes from details of\nthe particular cases. In the two cases on which we're focusing, the\nagents simply lack fine discriminatory capacities. They can't tell some\npossibilities apart from nearby possibilities. That is, they can't know\nwhether they're in one world or in some nearby world. That's not because\nit's conceptually impossible to know something that fine, but simply an\nunfortunate fact about their setup. If they can't know that they're not\nin a particular nearby world in which $\\neg$*p*, they can't know *p*.\nUsing variants of *Going to War*, we'll describe a few ways this could\ncome about.\n\nThe simplest way for this to come about is if war-making is the thing to\ndo given what we know, but some of the crucial evidence consists of\nfacts that we know, but don't know that we know. Imagine that a crucial\npiece of Indalia's case for war comes from information from an Indalian\nspy working behind enemy lines. As it turns out, the spy is reliable, so\nthe leaders of Indalia can acquire knowledge from her testimony. But she\ncould easily enough have been unreliable. She could, for instance, have\nbeen bought off by the enemy's agents. As it happens, the amount of\nmoney that would have taken was outside the budget the enemy has\navailable for counterintelligence. But had the spy been a little less\nloyal, or the enemy a little less frugal with the counterintelligence\nbudget, she could easily have been supplying misinformation to Indalia.\nSo, while the spy is a safe knowledge source, the Indalian leaders don't\n*know* that she is safe. They don't, for instance, know the size of the\nenemy's counterintelligence budget, or how much it would take to buy off\ntheir spy, so for all they know, she is very much at risk of being\nbought off.\n\nIn this case, if the spy tells the Indalian leaders that *p*, they come\nto know that *p*, and they can discriminate *p* worlds from $\\neg$*p*\nworlds. But they don't know that they know that *p*, so for all they\nknow, they don't know *p*. And for some *p* that they learn from the\nspy, if they don't know *p*, then going to war isn't the thing for them\nto do in the circumstances. So, given that they don't know the spy is\nreliable, they don't know that going to war is the thing for them to do.\nBut the spy really is reliable, so they do know *p*, so going to war is\nindeed the thing for them to do.\n\nOr consider a slightly less fanciful case, involving statistical\nsampling. Part of the Prime Minister's case for starting the war was\nthat the enemy was killing his own citizens. Presumably she meant that\nhe was killing them in large numbers. (Every country with capital\npunishment kills its own citizens, but arguably that isn't a sufficient\nreason to invade.) In practice, our knowledge of the scope of this kind\nof governmental killing comes from statistical sampling. And this\nsampling has a margin of error. Now imagine that the Indalian leaders\nknow that a sample has been taken, and that it shows that the enemy has\nkilled *n* of his citizens, with a margin of error of *m*. So, assuming\nthere really are *n* killings, they know that the enemy has killed\nbetween *n* - *m* and *n* + *m* of his citizens. Since knowing that he's\nkilled *n* - *m* people is sufficient to make going to war the thing to\ndo, the war can be properly started.\n\nBut now let's think about what the Indalian leaders know that they know\nin this case. The world where the enemy has killed *n* - *m* people is\nconsistent with their knowledge. And their margin of error on estimates\nof how many the enemy has killed is *m*. So, if that world is actual,\nthey don't know the enemy has killed more than *n* - 2*m* of his\ncitizens. And that knowledge might not be enough to make going to war\nthe thing to do, especially if *m* is large. (Think about the case where\n*m* = *n*/2, for instance.) So, there's a world consistent with their\nknowledge (the *n* - *m* killings world), in which they don't know\nenough about what the enemy is doing to make going to war the thing to\ndo. In general, if there's a world consistent with your knowledge where\n*p* is false, you don't know *p*. Letting *p* be *Going to war is what\nto do*, it follows then that they don't know that going to war is what\nto do, even though it actually is the thing to do.\n\nAnother way we could have a borderline war is a little more\ncontroversial. Imagine a case where the leaders of Indalia know all the\nsalient descriptive facts about the war. They know, at least well enough\nfor present purposes, what the costs and benefits of the war might be.\nBut it is a close call whether the war is the thing to do given those\ncosts and benefits. Perhaps different plausible moral theories lead to\ndifferent conclusions. Or perhaps the leaders know what the true moral\ntheory is, but that theory offers ambiguous advice. We can imagine a\ncontinuum of cases where the true theory says war is clearly what to do\nat one end, clearly not what to do at another, and a lot of murky space\nbetween. Unless we are willing to give up on classical logic, we must\nthink that somewhere there is a boundary between the cases where it is\nand isn't what to do, and it seems in cases near the boundary even a\ntrue belief about what to do will be unsafe. That is, even a true belief\nwill be based on capacities that can't reliably discriminate situations\nwhere going to war is what to do from cases where it isn't.\n\nWe've found, when discussing this case with others, that some people\nfind this outcome quite intolerable. They think that there must be some\nepistemic constraints on war-making. And we agree. They go on to think\nthat these constraints will be incompatible with the kind of cases we\nhave in mind that make premise 2 true. And here we disagree. It's worth\ngoing through the details here, because they tell us quite a bit about\nthe nature of epistemic constraints on action.\n\nConsider all principles of the form\n\n(KW)\n\n:   Going to war is *N1* only if the war-maker knows that going to war\n    is *N2*.\n\nwhere *N1* and *N2* are normative statuses, such as being the thing to\ndo, being right, being good, being just, being utility increasing, and\nso on. All such principles look like epistemic constraints on\nwar-making, broadly construed. One principle of this form would be that\ngoing to war is right only if the war-maker knows that going to war is\njust. That would be an epistemic constraint on war-making, and a\nplausible one. Another principle of this form would be that going to war\nis the thing to do only if the war-maker knows that going to war\nincreases actual utility. That would be a very strong epistemic\nconstraint on war-making, one that would rule out pretty much every\nactual war, and one that is consistent with the anti-luminosity argument\nwith which we started this section. So, the anti-luminosity argument is\nconsistent with there being quite strong epistemic constraints on\nwar-making.\n\nWhat the anti-luminosity argument is *not* consistent with is there\nbeing any true principle of the form (KW) where *N1* equals *N2*. In\nparticular, it isn't consistent with the principle that going to war is\nthe thing to do only if the war maker knows that it is the thing to do.\nBut that principle seems quite implausible, because of cases where going\nto war is, but only barely, the thing to do. More generally, the\nfollowing luminosity of action principle seems wrong for just about\nevery value of *X*.\n\n(LA)\n\n:   *X* is the thing for *S* to do only if *S* knows that *X* is the\n    thing for her to do.\n\nNot only is (LA) implausible, things look bad for The Knowledge Rule if\nit has to rely on (LA) being true. None of the defenders of The\nKnowledge Rule has given us an argument that (LA) is true. One of them\nhas given us all we need to show that (LA) is false! It doesn't look\nlike the kind of principle that The Knowledge Rule should have to depend\nupon. So, defending The Knowledge Rule here looks hopeless.\n\nNote that given premise 1 of the Master Argument, as corrected, *every*\ninstance of (LA) has to be true for The Knowledge Rule to be universally\ntrue. Let's say that you thought (LA) was true when *X* is *starting a\nwar*, but not when *X* is *buying flood insurance*. Then we can use the\ncase of Raj and Nik to show that The Knowledge Rule fails, since Raj can\nsay that buying flood insurance is what to do in a case where it is what\nto do, but he doesn't know this.\n\nOne final observation about the anti-luminosity argument. Given the way\nWilliamson presents the anti-luminosity argument, it can appear that in\nall but a few cases, if *p*, the salient agent can know that *p*. After\nall, the only examples Williamson gives are cases that are only picked\nout by something like the Least Number Theorem. So, one might think that\nwhile luminosity principles are false, they are approximately true. More\nprecisely, one might think that in all but a few weird cases near the\nborderline, if *p*, then a salient agent is in a position to know *p*.\nIf so, then the failures of luminosity aren't of much practical\ninterest, and hence the failures of The Knowledge Rule we've pointed out\naren't of much practical interest.\n\nWe think this is all mistaken. Luminosity failures arise because agents\nhave less than infinite discriminatory capacities. The worse the\ndiscriminatory capacities, the greater the scope for luminosity\nfailures. When agents have very poor discriminatory capacities, there\nwill be very many luminosity failures. This is especially marked in\ndecision-making concerning war. The fog of war is thick. There is very\nmuch that we don't know, and what we do know is based on evidence that\nis murky and ephemeral. There is very little empirical information that\nwe know that we know. If there are certain actions (such as starting a\nwar) that are proper only if we know a lot of empirical information, the\ngeneral case will be that we cannot know that these actions are correct,\neven when they are. This suggests that luminosity failures, where an\naction is correct but not known to be correct, or a fact is known but\nnot known to be known, are not philosophical curiosities. In\nepistemically challenging environments, like a war zone, they are\neveryday facts of life.\n\n## Moore's Paradox\n\nThere is a standard argument for The Knowledge Rule that goes as\nfollows. First, if the Knowledge Rule did not hold, then certain Moore\nparadoxical assertions would be acceptable. In particular, it would be\nacceptable to assert *q, but I don't know that q*.[^5] But second, Moore\nparadoxical assertions are never acceptable. Hence, The Knowledge Rule\nholds. We reject both premises of this argument.\n\nTo reject the first premise, it suffices to show that some rule other\nthan The Knowledge Rule can explain the unacceptability of Moore\nparadoxical assertions. Consider, for example, The Undefeated Reason\nrule.\n\nThe Undefeated Reason Rule\n\n:   Assert that *p* only if you have an undefeated reason to believe\n    that *p*.\n\nThe Undefeated Reason Rule says that *q but I don't know that q* can be\nasserted only if the speaker has an undefeated reason to believe it.\nThat means the speaker has an undefeated reason to believe each\nconjunct. That means that the speaker has an undefeated reason to\nbelieve that they don't know *q*. But in every case where it is\nunacceptable to both assert *q* and assert that you don't know *q*, the\nspeaker's undefeated reason to believe they don't know *q* will be a\ndefeater for her belief that *q*. If you have that much evidence that\nyou don't know *q*, that will in general defeat whatever reason you have\nto believe *q*.\n\nWe don't claim that The Undefeated Reason Rule is correct. (In fact, we\nprefer the rules we'll discuss in section 5.) We do claim that it\nprovides an alternative explanation of the unacceptability of instances\nof *q but I don't know that q*. So, we claim that it undermines the\nfirst premise of Williamson's argument from that unacceptability to The\nKnowledge Rule.\n\nWe also think that Williamson's explanation of Moore paradoxicality\nover-generates. There is generally something odd about saying *q but I\ndon't know that q*. We suspect that the best explanation for why this is\nodd will be part of a broader explanation that also explains, for\ninstance, why saying *I promise to do X, but I'm not actually doing to\ndo X* is also defective. Williamson's explanation isn't of this general\nform. He argues that saying *q but I don't know that q* is defective\nbecause it is defective in *every* context to both assert *q* and assert\nthat you don't know that *q*. But we don't think that it is always\ndefective to make both of these assertions.[^6] In particular, if a\nspeaker is asked whether *q* is true, and whether they know that *q*, it\ncan be acceptable to reply affirmatively to the first question, but\nnegatively to the second one. If so, then the second premise of\nWilliamson's argument from Moore paradoxicality is also false.\n\nImagine that the Indalian Prime Minister is a philosopher in her spare\ntime. After the big speech to Parliament she goes to her Peninsula\nReading Group. It turns out Michael Walzer and Tim Williamson are there,\nand have questions about the speech.\n\n> TW: Do you agree that knowledge requires safety?\\\n> PM: Yes, yes I do.\\\n> TW: And do you agree that your belief that going to war is the thing\n> to do is not safe?\\\n> PM: Right again.\\\n> TW: So, you don't know that going to war is the thing to do?\\\n> PM: You're right, I don't.\\\n> MW: But is it the thing to do?\\\n> PM: Yes.\n\nThe Prime Minister's answers in this dialogue seem non-defective to us.\nBut if Williamson's explanation of why Moore paradoxical utterances are\ndefective is correct, her answers should seem defective. So,\nWilliamson's explanation over-generates. Whether or not it is true that\nall assertions of sentences of the form *q but I don't know that q* are\ndefective, it isn't true that there is a defect in any performance that\nincludes both an assertion of *q* and an assertion of the speaker's\nignorance as to whether *q*. The Prime Minister's performance in her\nreading group is one such performance. So, the explanation of Moore\nparadoxicality cannot be that any such performance would violate a norm\ngoverning assertion.\n\nTo sum up, then, we've argued that The Knowledge Rule (a) fails to be\nthe only explanation of Moore paradoxicality, and (b) misclassifies\ncertain performances that are a little more complex than simple\nconjunctive assertions as defective. So, there's no good argument from\nMoore paradoxicality to The Knowledge Rule.\n\n## Action and Assertion\n\nIf we're right, there's a striking asymmetry between certain kinds of\nassertions. In the war example, early in her speech, the Prime Minister\nsays (2).\n\nThat's not the kind of thing she could properly say if it could easily\nhave been false given her evidence. And like many assertions, this is\nnot an assertion whose appropriateness is guaranteed by its truth.\nAsserting (2) accuses someone of murder, and you can't properly make\nsuch accusations without compelling reasons, even if they happen to be\ntrue. On the other hand, we say, the truth of (1) does (at least when it\nis accepted on the right basis) suffice to make it properly assertable.\n\nThere's a similar asymmetry in the flood insurance example. In that\nexample, (3) is true, but neither Raj nor Nik knows it.\n\nAgain, in these circumstances, this isn't the kind of thing Raj can\nproperly say. Even though (3) is true, it would be foolhardy for Raj to\nmake such a claim without very good reasons. By contrast, again, we say\nthat Raj can properly assert that the thing to do, in their\ncircumstances, is to buy flood insurance, even though he does not know\nthis.\n\nThere are two directions one could go at this point. If we're right, any\nproposed theory of the norms governing assertion must explain the\nasymmetry. Theories that cannot explain it, like The Knowledge Rule, or\nthe Certainty Rule proposed by Jason @Stanley2008-STAKAC, or the\nRational Credibility Rule proposed by Igor @Douven2006, are thereby\nrefuted.\n\nThe Certainty Rule\n\n:   Assert only what is certain.\n\nThe Rational Credibility Rule\n\n:   Assert only what is rationally credible.\n\nThe Certainty Rule fails since the Prime Minister is not certain of (1).\nAnd the Prime Minister can't be certain of (1), since certainty requires\nsafety just as much as knowledge does.\n\nIt's a little harder to show our example refutes The Rational\nCredibility Rule. Unlike knowledge, a safety constraint is not built\ninto the concept of rational credibility. (Since rational credibility\ndoes not entail truth, in Douven's theory, it can hardly entail truth in\nnearby worlds.) But we think that safety constraints may still apply to\nrational credibility in some particular cases. If you aren't very good\nat judging building heights of tall buildings to a finer grain than 10\nmeters, then merely looking at a building that is 84 meters tall does\nnot make it rationally credible for you that the building is more than\n80 meters tall. In general, if your evidence does not give you much\nreason to think you are not in some particular world where *p* is false,\nand you didn't have prior reason to rule that world out, then *p* isn't\nrationally credible. So, when evidence doesn't discriminate between\nnearby possibilities, and *p* is false in nearby possibilities, *p*\nisn't rationally credible.\n\nAnd that, we think, is what happens in our two examples. Just as someone\nlooking at an 84 meter building can't rationally credit that it is more\nthan 80 meters tall, unless they are abnormally good at judging heights,\nagents for whom *X* is just barely the thing to do can't rationally\ncredit that *X* is the thing to do. By The Rational Credibility Rule,\nthey can't say *X* is the thing to do. But they can say that; that's\nwhat our examples show. So, The Rational Credibility Rule must be wrong.\n\nBut we can imagine someone pushing in the other direction, perhaps with\nthe help of this abductive argument.\n\n1.  A speaker can only assert things like (2) or (3) if they know them\n    to be true.\n\n2.  The best explanation of premise 1 of this argument is The Knowledge\n    Rule.\n\n3.  So, The Knowledge Rule is correct.\n\nThis isn't a crazy argument. Indeed, it seems to us that it is implicit\nin some of the better arguments for The Knowledge Rule. But we think it\nfails. And it fails because there are alternative explanations of the\nfirst premise, explanations that don't make mistaken predictions about\nthe Prime Minister's speech. For instance, we might have some kind of\nEvidence Responsiveness Rule.\n\nThe Evidence Responsiveness Rule\n\n:   Assert that *p* only if your attitude towards *p* is properly\n    responsive to the evidence you have that bears on *p*.\n\nGiven how much can be covered by 'properly', this is more of a schema\nthan a rule. Indeed, it is a schema that has The Knowledge Rule as one\nof its precisifications. In *Knowledge and Its Limits*, Williamson first\nargues that assertion is \"governed by a non-derivative evidential rule\"\n(249), and then goes on to argue that the proper form of that rule is\nThe Knowledge Rule. We agree with the first argument, and disagree with\nthe second one.[^7]\n\nNote that even a fairly weak version of The Evidence Responsiveness Rule\nwould explain what is going on with cases like (1) and (2). Starting a\nwar is a serious business. You can't properly do it *unless* your views\nabout the war are evidence responsive in the right way. You can't, that\nis, correctly *guess* that starting the war is the thing to do. You\n*can* correctly guess that starting the war will be utility maximizing.\nAnd you can correctly guess that starting the war would be what to\nchoose if you reflected properly on the evidence you have, and the moral\nsignificance of the choices in front of you. But you simply can't guess\nthat starting the war is what to do, and be right. If you're merely\nguessing that starting a war is thing to do, then you're wrong to start\nthat war. So, if (1) is true, and the Prime Minister believes it, her\nbelief simply *must* be evidence responsive. Then, by The Evidence\nResponsiveness Rule, she can assert it.\n\nFor most assertions, however, this isn't the case. Even if it's true\nthat it will rain tomorrow, the Prime Minister's could believe that\nwithout her belief being evidence responsive. In general, *p* does not\nentail that *S* even believes that *p*, let alone that this belief of\n*S*'s is evidence responsive. But in cases like (1), this entailment\ndoes hold, and that's what explains the apparent asymmetry that we\nstarted this section with.\n\nThe Evidence Responsiveness Rule also handles so called 'lottery\npropositions' nicely. If you know that the objective chance of *p* being\ntrue is *c*, where *c* is less than 1, it will seem odd in a lot of\ncontexts to simply assert *p*. In his arguments for The Knowledge Rule,\nWilliamson makes a lot of this fact. In particular, he claims that the\nbest explanation for this is that we can't know that *p* on purely\nprobabilistic grounds. This has proven to be one of the most influential\narguments for The Knowledge Rule in the literature. But some kind of\nEvidence Responsiveness Rule seems to handle lottery cases even more\nsmoothly. In particular, an Evidence Responsiveness Rule that allows for\nwhat constitutes 'proper' responsiveness to be sensitive to the\ninterests of the conversational participants will explain some odd\nfeatures concerning lottery propositions and assertability.\n\nIn the kind of cases that motivate Williamson, we can't say *p* where it\nis objectively chancy whether *p*, and the chance of *p* is less than 1.\nBut there's one good sense in which such an assertion would not be\nproperly responsive to the evidence. After all, in such a case there's a\nnearby world, with all the same laws, and with all the same past fatcs,\nand in which the agent has all the same evidence, in which *p* is false.\nAnd the agent knows all this. That doesn't look like the agent is being\nproperly responsive to her evidence.\n\nOn the other hand, we might suspect that Williamson's arguments\nconcerning lottery propositions overstate the data. Consider this old\nstory from David @Lewis1996b.[^8]\n\n> Pity poor Bill! He squanders all his spare cash on the pokies, the\n> races, and the lottery. He will be a wage slave all his days ... he\n> will never be rich. [@Lewis1996b 443 in reprint]\n\nThese seem like fine assertions. One explanation of the appropriateness\nof those assertions combines The Knowledge Rule with contextualism about\nassertion.[^9] But contextualism has many weaknesses, as shown in\nHawthorne (2004) and Stanley (2005). A less philosophically loaded\nexplanation of Lewis's example is that proper responsiveness comes in\ndegrees, and for purposes of talking about Bill, knowing that it's\noverwhelmingly likely that he's doomed to wage slavery is evidence\nenough to assert that he'll never be rich. The details of this\nexplanation obviously need to be filled in, but putting some of the\nsensitivity to conversational standards, or practical interests, into\nthe norms of assertion seems to be a simpler explanation of the data\nthan a contextualist explanation. (It would be *a priori* quite\nsurprising if the norms of proper assertion were not context-sensitive,\nor interests-sensitive. The norms of appropriateness for most actions\nare sensitive to context and interests.) So The Evidence Responsiveness\nRule seems more promising here than The Knowledge Rule.\n\nA harder kind of case for The Knowledge Rule concerns what we might call\n'academic assertions'. This kind of case is discussed in @Douven2006 and\nin @MaitraANG. In academic papers, we typically make assertions that we\ndo not know. We don't know that most of the things we've said here are\ntrue. (Before the last sentence we're not sure we knew that any of the\nthings we said were true.) But that's because knowledge is a bad\nstandard for academic discourse. Debate and discussion would atrophy if\nwe had to wait until we had knowledge before we could present a view.\nSo, it seems that assertion can properly outrun knowledge in academic\ndebate.\n\nAgain, a context-sensitive version of The Evidence Responsiveness Rule\nexplains the data well. Although you don't need to *know* things to\nassert them in philosophy papers, you have to have evidence for them. We\ncouldn't have just spent this paper insisting louder and louder that The\nKnowledge Rule is false. We needed to provide evidence, and hopefully\nwe've provided a lot of it. In some contexts, such as testifying in\ncourt, you probably need more evidence than what we've offered to ground\nassertions. But in dynamic contexts of inquiry, where atrophy is to be\nfeared more than temporary mistakes, the standards are lower. Good\nevidence, even if not evidence beyond any reasonable doubt, or even if\nnot enough for knowledge, suffices for assertion. That's the standard we\ntypically hold academic papers to. Like with lotteries, we think the\nprospects of explaining these apparently variable standards in terms of\na norm of assertion that is context-sensitive are greater than the\nprospects for explaining them in terms of contextually sensitive\nknowledge ascriptions.\n\nHere's a different and somewhat more speculative proposal idea for a\nrule that also explains the asymmetry we started this section with. We\ncall it the Action Rule.\n\nThe Action Rule\n\n:   Assert that *p* only if acting as if *p* is true is the thing for\n    you to do.\n\nWe take the notion of acting as if something is true from\n@Stalnaker1973-STAP-5. Intuitively, to act as if *p* is true is to build\n*p* into one's plans, or to take *p* for granted when acting. This,\nnote, is not the same as using *p* as a basis for action. When Raj buys\nflood insurance, he acts as if buying flood insurance is the thing to\ndo. But the fact that buying flood insurance is the thing to do isn't\nthe basis for his action. (Since he does not know this, one might\nsuspect it wouldn't be a good basis.) Instead his basis is what he knows\nabout the river, and his business, and its vulnerability to flooding.\nWhen an agent is trying to maximise the expected value of some variable\n(e.g., utility, profit, etc.), then to act as if *p* is true is simply\nto maximise the conditional expected value of that variable, in\nparticular, to maximise the expected value of that variable conditional\non *p*. Even when one is not maximising any expected value, we can still\nuse the same idea. To act as if *p* is to take certain conditional\nobligations or permissions you have -- in particular, those obligations\nor permissions that are conditional on *p* -- to be actual obligations\nor permissions.\n\nTo see how The Action Rule generates the intended asymmetry, we'll need\na bit of formalism. Here are the terms that we will use.\n\n-   *X* denotes an action, agent, circumstance triple\n    $\\langle$*X~Action~, X~Agent~, X~Circumstance~*$\\rangle$. We take\n    such triples to have a truth value*.* *X* is true iff *X~Agent~*\n    performs *X~Action~* in *X~Circumstance~*.\n\n-   ThingToDo(*X*) means that *X* is the thing to do for *X~Agent~* in\n    *X~Circumstance~*.\n\n-   Act(*S,p*) means that agent *S* acts as if *p* is true.\n\n-   Assert(*S*,*p*) means that agent *S* can properly assert that *p*.\n\nSo, The Action Rule is this.\n\n> Assert(*S,p*) ${\\rightarrow}$ ThingToDo(Act(*S,p*))\n\nIn our derivations, the following equivalence will be crucial.\n\n> Act(*X~Agent~,*ThingToDo(*X*)) ${\\leftrightarrow}$ *X*\n\nThat is, acting as if *X* is what to do (in your circumstances) is\nsimply to do *X* (in those circumstances). And in doing *X*, you're\nacting as if *X* is what to do (in your circumstances). We take this\nequivalence to be quite resilient; in particular, it holds under\noperators like 'ThingToDo'. So, adding that operator to the previous\nequivalence, we get another equivalence.\n\n> ThingToDo(Act(*X~Agent~,*ThingToDo(*X*))) ${\\leftrightarrow}$\n> ThingToDo(*X*)\n\nIf we substitute ThingToDo(*X*) for *p* in The Action Rule, we get this.\n\n> Assert(*X~Agent~,*ThingToDo(*X*)) ${\\rightarrow}$\n> ThingToDo(Act(*X~Agent~,*ThingToDo(*X*)))\n\nBut by the equivalence we derived earlier, that's equivalent to the\nfollowing.\n\n> Assert(*X~Agent~,*ThingToDo(*X*)) ${\\rightarrow}$ ThingToDo(*X*)\n\nSo, we get the nice result that The Action Rule is trivially satisfied\nfor any true claim about what is to be done. That is, for the special\ncase where *p* is *X is the thing for you to do*, The Action Rule just\nreduces to something like the Truth Rule. And so we get a nice\nexplanation of why the Prime Minister and Raj can properly make their\nassertions about what to do in their respective circumstances.[^10]\n\nTo explain the other side of the asymmetry with which we began this\nsection, note that these biconditionals do not hold where *p* is an\narbitrary proposition, and *S* an arbitrary agent.\n\n> ThingToDo(Act(*S,p*)) ${\\leftrightarrow}$ *p*\n>\n> Act(*S*,ThingToDo(Act(*S*,*p*))) ${\\leftrightarrow}$ *p*\n\nTo see this, let *p* be the proposition expressed by (4). To act as if\nthis is true is to, *inter alia*, not buy flood insurance. If there\nwon't be a flood, buying flood insurance is throwing away money, and\nwhen you're running a business, throwing away money isn't the thing to\ndo. In symbols, *Act(*Raj and Nik,*p)* is equivalent to *Raj and Nik\ndon't buy flood insurance*. But not buying flood insurance is not the\nthing to do. The prudent plan is to buy flood insurance. So,\n*ThingToDo(Act(*Raj and Nik,*p))* is false, even though *p* is true. So,\nthe first biconditional fails. Since Raj and Nik do go on to buy flood\ninsurance, i.e., since they don't act as if *ThingToDo(Act(*Raj and\nNik,*p))*, the left-hand-side of the second biconditional is also false.\nBut again, the right-hand-side is true. So, that biconditional is false\nas well. And without those biconditionals, The Action Rule doesn't\ncollapse into *Assert(S*,*p)* *${\\rightarrow}$* *p*.\n\nWe have thus far argued that The Action Rule can provide an explanation\nfor the asymmetry we noted at the beginning of this section.[^11] This\nis not, however, meant to be anything like a complete defence of that\nrule. That would require a lot more than we've provided here. But we do\nthink that the Action Rule can explain a lot of the phenomena that are\nmeant to motivate The Knowledge Rule, as well as some phenomena The\nKnowledge Rule struggles with.But we do think The Action Rule has some\nvirtues. We'll close with a discussion of how it explains the two kinds\nof cases that we argued that The Evidence Responsiveness Rule handles\nwell.\n\nTo see this, consider first 'lottery propositions'. If you know that the\nobjective chance of *p* being true is *c*, where *c* is less than 1, it\nwill seem odd in a lot of contexts to simply assert *p*. In his\narguments for The Knowledge Rule, Williamson makes a lot of this fact.\nIn particular, he claims that the best explanation for this is that we\ncan't know that *p* on purely probabilistic grounds. This has proven to\nbe one of the most influential arguments for The Knowledge Rule in the\nliterature.\n\nWe suggest that The Action Rule can offers an alternative a nice\nexplanation for why it's often defective to assert lottery propositions.\nNote first that inIn a lot of cases, it isn't rational for us to act on\n*p* when we have only purely probabilistic evidence for it, especially\nwhen acting on *p* amounts to betting on *p* at sufficiently\nunfavourable odds. This point is something of a staple of the\n'interest-relative-invariantism' literature on knowledge.[^12] To take a\nmundane case, imagine that you're cleaning up your desk, and you come\nacross some lottery tickets. Most are for lotteries that have passed,\nthat you know you lost. One ticket, however, is for a future lottery,\nwhich you know you have very little chance of winning. In such a case,\nto act as if the ticket for the future lottery would lose would be to\nthrow it out along with the other tickets. But that would be irrational,\nand not at all how we'd act in such a case. That is to say, in such a\ncase, we don't (and shouldn't, rationally speaking) act as if the ticket\nfor the future lottery will lose, even though we take that outcome to be\nhighly probable.\n\nIf acting as if a lottery proposition is true isn't the thing to do,\nthen The Action Rule will say that asserting such a proposition\ndefective. Therefore, we think that The Action Rule can capture why in\nmany cases you can't in general assert lottery propositions.\n\nA harder kind of case for The Knowledge Rule concerns what we might call\n'academic assertions'. This kind of case is discussed in @Douven2006 and\nin @MaitraANG. In academic papers, we typically make assertions that we\ndo not know. We don't know that most of the things we've said here are\ntrue. (Before the last sentence we're not sure we knew that any of the\nthings we said were true.) But that's because knowledge is a bad\nstandard for academic discourse. Debate and discussion would atrophy if\nwe had to wait until we had knowledge before we could present a view.\nSo, it seems that assertion can properly outrun knowledge in academic\ndebate.\n\nAcademic assertions raised a problem for The Knowledge Rule because\nproper assertion in the context of inquiry can outrun knowledge. But\nnote that action in such a context can also properly outrun knowledge.\nIt would slow down learning dramatically if people didn't engage in\nvarious projects that really only make sense if some hypothesis is true.\nSo, academics will study in archives, conduct experiments, write papers,\netc. etc., and do so on the basis of reasons they no more know than we\nknow the truth of the speculative claims of this paper. And this is all\nto the good; the alternative is a vastly inferior alternative to\nacademia as we know it. So, in some fields, action requires much less\nthan knowledge. Happily, in those fields, assertion also requires much\nless than knowledge. Indeed, the shortfalls in the two cases seem to\nparallel nicely. And this parallel is neatly captured by The Action\nRule.\n\nAs we said, none of this is a knockdown case for The Action Rule. Our\nprimary purpose is to argue against The Knowledge Rule. As long as the\nAction Rule is plausible, we have defeated the abductive argument for\nThe Knowledge Rule that was discussed at the start of this section, and\nwe think we've done enough to show it is plausible. We also hope we've\nmade a successful case for moving the study of assertability away from\nrules like The Knowledge Rule, and instead have it be more tightly\nintegrated with our best theories about evidence and action.\n\n[^2]: Timothy Williamson [@Williamson2000-WILKAI Ch. 11] has the\n    clearest statement of the view we're considering here. It is also\n    defended by Keith @DeRose2002. Both DeRose and John @Hawthorne2004\n    deploy it extensively as a constraint on theories of knowledge.\n    Jason @Stanley2008-STAKAC argues for an even stronger constraint:\n    that we should only assert *p* if we are certain that *p*. Igor\n    @Douven2006 argues that truth is neither sufficient nor necessary,\n    so the norm should be assert only what is rationally credible. Kent\n    @Bach2010 and Frank @Hindriks2007 both suggest that the only real\n    norm governing *assertion* is belief, but that since knowledge is a\n    norm of belief, we shouldn't generally assert what we do not know.\n    In this paper we're not concerned with the question of whether the\n    rule *Assert only what you know* holds solely in virtue of the\n    normative nature of assertion itself, as Williamson thinks, or\n    partly in virtue of norms applying to related states like belief, as\n    Bach and Hindriks suggest, but rather whether the rule is even a\n    good rule.\n\n[^3]: We'll use the expressions 'thing to do' and 'what to do'\n    interchangeably throughout the paper. By *X is what to do*, we mean\n    *X* ought to be done, all things considered. We take no position on\n    whether *X*'s being what to do entails its being the morally right\n    thing to do. That may be the case, but nothing we say in this paper\n    depends on its being so.\n\n[^4]: The points we're about to make are fairly familiar by now, but for\n    more detail, see @Cappelen2005, which played an important role in\n    reminding the philosophy of language community of their\n    significance.\n\n[^5]: [@Williamson2000-WILKAI], for instance, shows the strength of this\n    argument.\n\n[^6]: This is why we hedged a little two paragraphs ago about what\n    precisely The Undefeated Reason Rule explains. We suspect that many\n    in the literature have misidentified the explicandum.\n\n[^7]: Actually, our agreement with Williamson here is a bit more\n    extensive than the text suggests. Williamson holds that part of what\n    makes a speech act an *assertion* as opposed to some other kind of\n    act is that it is governed by The Knowledge Rule. Although many\n    philosophers agree with Williamson that The Knowledge Rule is true,\n    this fascinating claim about the metaphysics of speech acts has been\n    largely ignored. Translating Williamson's work into the terminology\n    of this paper, we're inclined to agree that a speech act is an\n    assertion partly in virtue of being responsive to evidence in the\n    right way. But filling in the details on this part of the story\n    would take us too far from the main storyline of this paper.\n\n[^8]: We've slightly modified the case. Lewis says we can say that we\n    *know* Bill will never be rich. That seems to us to be a much more\n    controversial than what we've included here.\n\n[^9]: The combination is slightly trickier to state than would be ideal.\n    The explanation we have in mind is that *S* can properly assert *p*\n    only if *S* can truly say *I know that p*, where 'know' in this\n    utterance is context sensitive.\n\n[^10]: The derivation here is deliberately simplified in one way. We\n    haven't included anything about the *bases* for action or assertion.\n    We don't think being sensitive to bases in the formalism would make\n    a material change, but it would obscure the structure of the\n    argument.\n\n[^11]: This explanation makes some interestingly different predictions\n    from the explanation in terms of The Evidence Responsiveness Rule.\n    Suppose that for relatively trivial decisions, like where to go for\n    a walk on a nice summer day, one can correctly guess that *X* is the\n    thing to do. Then the Evidence Responsiveness Rule would suggest\n    that the truth of claims about where to go for a walk is not\n    sufficient grounds for their assertability, while the Action Rule\n    would still imply that truth is sufficient grounds for\n    assertability.\n\n      We're not sure that this supposition -- that for relatively\n    trivial decisions, one can correctly guess that *X* is the thing to\n    do -- is coherent, nor what to say about assertability judgments in\n    (imagined) cases where the supposition holds. So, we're not sure we\n    can really use this to discriminate between the two proposed\n    explanations. Nevertheless, it is interesting to note how the\n    explanations come apart. Thanks here to Susanna Schellenberg.\n\n[^12]: See, for instance, @Fantl2002, @Hawthorne2004,\n    @Stanley2005-STAKAP, and @Weatherson2005-WEACWD.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
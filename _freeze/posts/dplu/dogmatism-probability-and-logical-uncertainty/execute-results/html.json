{
  "hash": "6225c65c52844607a5b69784b820271a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Dogmatism, Probability and Logical Uncertainty\"\ndescription: |\n  Many epistemologists hold that an agent can come to justifiably believe that p is true by seeing that it appears that p is true, without having any antecedent reason to believe that visual impressions are generally reliable. Certain reliabilists think this, at least if the agent’s vision is generally reliable. And it is a central tenet of dogmatism (as described by Pryor (2000) and Pryor (2004)) that this is possible. Against these positions it has been argued (e.g. by Cohen (2005) and White (2006)) that this violates some principles from probabilistic learning theory. To see the problem, let’s note what the dogmatist thinks we can learn by paying attention to how things appear. (The reliabilist says the same things, but we’ll focus on the dogmatist.)\ndate: January 1 2012\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\n  - name: David Jehle\ndoi: \"10.1057/9781137003720\"\ncategories:\n  - epistemology\ncitation_url: https://doi.org/10.1057/9781137003720\ncitation: false\nbibliography: ../../../articles/Rbib.bib\nself-contained: false\npreview: standrews.jpg\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    number_sections: true\n---\n\n\nMany epistemologists hold that an agent can come to justifiably believe\nthat $p$ is true by seeing that it appears that $p$ is true, without\nhaving any antecedent reason to believe that visual impressions are\ngenerally reliable. Certain reliabilists think this, at least if the\nagent's vision is generally reliable. And it is a central tenet of\ndogmatism (as described by @Pryor2000 and @Pryor2004) that this is\npossible. Against these positions it has been argued (e.g. by @Cohen2005\nand @White2006) that this violates some principles from probabilistic\nlearning theory. To see the problem, let's note what the dogmatist\nthinks we can learn by paying attention to how things appear. (The\nreliabilist says the same things, but we'll focus on the dogmatist.)\n\n<aside>\nPublished in _New Waves in Philosophical Logic_, eedited by Greg Restall and Gillian Russell, Palgrave, 95-111.\n\nPicture by [Bravehardt](https://www.flickr.com/photos/51331471@N03) via [Creative Commons](https://search.creativecommons.org/photos/f95fec7a-60f6-4671-86ec-0828e9330b51).\n</aside>\n\nSuppose an agent receives an appearance that $p$, and comes to believe\nthat $p$. Letting *Ap* be the proposition that it appears to the agent\nthat $p$, and $\\rightarrow$ be the material conditional, we can say that\nthe agent learns that $p$, and hence is in a position to infer\n$Ap \\rightarrow p$, once they receive the evidence *Ap*.[^1] This is\nsurprising, because we can prove the following.[^2]\n\n> **Theorem 1**\\\n> If $Pr$ is a classical probability function, then\\\n> $Pr(Ap \\rightarrow p | Ap) \\leq Pr(Ap \\rightarrow p)$.\n\n(All the theorems are proved in the appendix.) We can restate Theorem 1\nin the following way, using classically equvalent formulations of the\nmaterial conditional.\n\n> **Theorem 2**\\\n> If $Pr$ is a classical probability function, then\n>\n> -   $Pr(\\neg(Ap \\wedge \\neg p) | Ap) \\leq Pr(\\neg(Ap \\wedge \\neg p))$;\n>     and\n>\n> -   $Pr(\\neg Ap \\vee p | Ap) \\leq Pr(\\neg Ap \\vee p)$.\n\nAnd that's a problem for the dogmatist if we make the standard Bayesian\nassumption that some evidence $E$ is only evidence for hypothesis $H$ if\n$Pr(H | E) >  Pr(H)$. For here we have cases where the evidence the\nagent receives does not raise the probability of $Ap \\rightarrow p$,\n$\\neg(Ap \\wedge \\neg p)$ or $\\neg Ap \\vee p$, so the agent has not\nreceived any evidence for them, but getting this evidence takes them\nfrom not having a reason to believe these propositions to having a\nreason to get them.\n\nIn this paper, we offer a novel response for the dogmatist. The proof of\nTheorem 1 makes crucial use of the logical equivalence between\n$Ap \\rightarrow p$ and\n$((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap)$.\nThese propositions are equivalent in classical logic, but they are not\nequivalent in intuitionistic logic. Exploiting this non-equivalence, we\nderive two claims. In Section 1 we show that Theorems 1 and 2 fail in\nintuitionistic probability theory. In Section 2 we consider how an agent\nwho is unsure whether classical or intuitionistic logic is correct\nshould apportion their credences. We conclude that for such an agent,\ntheorems analogous to Theorems 1 and 2 fail even if the agent thinks it\nextremely unlikely that intuitionistic logic is the correct logic. The\nupshot is that if it is rationally permissible to be even a little\nunsure whether classical or intuitionistic logic is correct, it is\npossible that getting evidence that $Ap$ raises the rational credibility\nof $Ap \\rightarrow p$, $\\neg(Ap \\wedge \\neg p)$ and $\\neg Ap \\vee p$.\n\n### Intuitionistic Probability\n\nIn @Weatherson2003, the notion of a $\\vdash$-probability function, where\n$\\vdash$ is an entailment relation, is introduced. For any $\\vdash$, a\n$\\vdash$-probability function is a function $Pr$ from sentences in the\nlanguage of $\\vdash$ to $[0, 1]$ satisfying the following four\nconstraints.[^3]\n\n(P0)\n\n:   $Pr(p) = 0$ if $p$ is a $\\vdash$-antithesis, i.e. iff for any\n    $X, p \\vdash X$.\n\n(P1)\n\n:   $Pr(p) = 1$ if $p$ is a $\\vdash$-thesis, i.e. iff for any\n    $X, X \\vdash p$.\n\n(P2)\n\n:   If $p \\vdash q$ then $Pr(p) \\leq Pr(q)$.\n\n(P3)\n\n:   $Pr(p) + Pr(q) = Pr(p \\vee q) + Pr(p \\wedge q)$.\n\nWe'll use $\\vdash_{CL}$ to denote the classical entailment relation, and\n$\\vdash_{IL}$ to denote the intuitionist entailment relation. Then what\nwe usually take to be probability functions are\n$\\vdash_{CL}$-probability functions. And intuitionist probability\nfunctions are $\\vdash_{IL}$-probability functions.\n\nIn what follows we'll make frequent appeal to three obvious consequences\nof these axioms, consequences which are useful enough to deserve their\nown names. Hopefully these are obvious enough to pass without proof.[^4]\n\n(P1$^*$)\n\n:   $0 \\leq Pr(p) \\leq 1$.\n\n(P2$^*$)\n\n:   If $p \\dashv \\vdash q$ then $Pr(p) = Pr(q)$.\n\n(P3$^*$)\n\n:   If $p \\wedge q$ is a $\\vdash$-antithesis, then\n    $Pr(p) + Pr(q) = Pr(p \\vee q)$.\n\n$\\vdash$-probability functions obviously concern unconditional\nprobability, but we can easily extend them into conditional\n$\\vdash$-probability functions by adding the following axioms.[^5]\n\n(P4)\n\n:   If $r$ is not a $\\vdash$-antithesis, then $Pr(\\cdot | r)$ is a\n    $\\vdash$-probability function; i.e., it satisfies P0-P3.\n\n(P5)\n\n:   If $r \\vdash p$ then $Pr(p | r) = 1$.\n\n(P6)\n\n:   If $r$ is not a $\\vdash$-antithesis, then\n    $Pr(p \\wedge q | r) = Pr(p | q \\wedge r)Pr(q | r)$.\n\nThere is a simple way to generate $\\vdash_{CL}$ probability functions.\nLet $\\langle W, V\\rangle$ be a model where $W$ is a finite set of\nworlds, and $V$ a valuation function defined on them with respect to a\n(finite) set $K$ of atomic sentences, i.e., a function from $K$ to\nsubsets of $W$. Let $L$ be the smallest set including all members of $K$\nsuch that whenever $A$ and $B$ are in $L$, so are $A \\wedge B$,\n$A \\vee B$, $A \\rightarrow B$ and $\\neg A$. Extend $V$ to $V^*$, a\nfunction from $L$ to subsets of $W$ using the usual recursive\ndefinitions of the sentential connectives. (So $w \\in V^*(A \\wedge B)$\niff $w \\in V^*(A)$ and $w \\in V^*(B)$, and so on for the other\nconnectives.) Let $m$ be a measure function defined over subsets of W.\nThen for any sentence $S$ in $L$, $Pr(S)$ is $m(\\{w: w \\in V^*(S)\\})$.\nIt isn't too hard to show that Pr is a $\\vdash_{CL}$ probability\nfunction.\n\nThere is a similar way to generate $\\vdash_{IL}$ probability functions.\nThis method uses a simplified version of the semantics for\nintuitionistic logic in @Kripke1965. Let $\\langle W, R, V\\rangle$ be a\nmodel where $W$ is a finite set of worlds, $R$ is a reflexive,\ntransitive relation defined on $W$, and $V$ is a valuation function\ndefined on them with respect to a (finite) set $K$ of atomic sentences.\nWe require that $V$ be closed with respect to $R$, i.e. that if\n$x \\in V(p)$ and $xRy$, then $y \\in V(p)$. We define $L$ the same way as\nabove, and extend $V$ to $V^*$ (a function from $L$ to subsets of $W$)\nusing the following definitions.\n\n> $w \\in V^*(A \\wedge B)$ iff $w \\in V^*(A)$ and $w \\in V^*(B)$.\\\n> $w \\in V^*(A \\vee B)$ iff $w \\in V^*(A)$ or $w \\in V^*(B)$.\\\n> $w \\in V^*(A \\rightarrow B)$ iff for all $w^{\\prime}$ such that\n> $wRw^{\\prime}$ and $w^{\\prime}\\in V^*(A), w^{\\prime} \\in V^*(B)$.\\\n> $w \\in V^*(\\neg A)$ iff for all $w^{\\prime}$ such that $wRw^{\\prime}$,\n> it is not the case that $w^{\\prime} \\in V^*(A)$.\n\nFinally, we let $m$ be a measure function defined over subsets of $W$.\nAnd for any sentence $S$ in $L$, $Pr(S)$ is $m(\\{w: w \\in V^*(S)\\})$.\n@Weatherson2003 shows that any such $Pr$ is a $\\vdash_{IL}$ probability\nfunction.\n\nTo show that Theorem 1 may fail when $Pr$ is $\\vdash_{IL}$ a probability\nfunction, we need a model we'll call $M$. The valuation function in $M$\nis defined with respect to a language where the only atomic propositions\nare $p$ and $Ap$. $$\\begin{aligned}\nW &= \\{1, 2, 3\\} \\\\\nR &=  \\{\\langle 1, 1\\rangle , \\langle 2, 2\\rangle , \\langle 3, 3\\rangle , \\langle 1, 2\\rangle , \\langle 1, 3\\rangle \\} \\\\\nV(p) &= \\{2\\} \\\\\nV(Ap) &= \\{2, 3\\}\\end{aligned}$$\n\nGraphically, $M$ looks like this.\n\n::: {.center}\n(70, 50) (35, 5)(-1, 1)30 (35, 5)(1, 1)30 (35,5) (4.8,35.5) (65.2,35.5)\n(28, 5)$1$ (0,35.5)$2$ (60,35.5)$3$ (7,35.5)$Ap, p$ (67,35.5)$Ap$\n:::\n\nWe'll now consider a family of measures over $m$. For any\n$x \\in (0, 1)$, let $m_x$ be the measure function such that\n$m_x(\\{1\\}) = 1 - x, m_x(\\{2\\}) = x$, and $m_x(\\{3\\}) = 0$.\nCorresponding to each function $m_x$ is a $\\vdash_{IL}$ probability\nfunction we'll call $Pr_x$. Inspection of the model shows that Theorem 3\nis true.\n\n> **Theorem 3**.\\\n> In $M$, for any $x \\in (0, 1)$,\n>\n> 1.  $Pr_x(Ap \\rightarrow p)$ =\n>     $Pr_x((Ap \\rightarrow p) \\wedge Ap) = x$\n>\n> 2.  $Pr_x(\\neg Ap \\vee p)$ = $Pr_x((\\neg Ap \\vee p) \\wedge Ap) = x$\n>\n> 3.  $Pr_x(\\neg(Ap \\wedge \\neg p))$ =\n>     $Pr_x(\\neg(Ap \\wedge \\neg p) \\wedge Ap) = x$\n\nAn obvious corollary of Theorem 3 is\n\n> **Theorem 4**.\\\n> For any $x \\in (0, 1)$,\n>\n> 1.  $1 = Pr_x(Ap \\rightarrow p | Ap) >  Pr_x(Ap \\rightarrow p) = x$\n>\n> 2.  $1 = Pr_x(\\neg Ap \\vee p | Ap) >  Pr_x(\\neg Ap \\vee p) = x$\n>\n> 3.  $1 = Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) >  Pr_x(\\neg(Ap \\wedge \\neg p)) = x$\n\nSo for any $x$, conditionalising on $Ap$ actually raises the probability\nof $Ap \\rightarrow p, \\neg(Ap \\wedge \\neg p)$ and $\\neg Ap \\vee p$ with\nrespect to $Pr_x$. Indeed, since $x$ could be arbitrarily low, it can\nraise the probability of each of these three propositions from any\narbitrarily low value to 1. So it seems that if we think learning goes\nby conditionalisation, then receiving evidence $Ap$ could be sufficient\ngrounds to justify belief in these three propositions. Of course, this\nrelies on our being prepared to use the intuitionist probability\ncalculus. For many, this will be considered too steep a price to pay to\npreserve dogmatism. But in section 2 we'll show that the dogmatist does\nnot need to insist that intuitionistic logic is the correct logic for\nmodelling uncertainty. All they need to show is that it *might* be\ncorrect, and then they'll have a response to this argument.\n\n### Logical Uncertainty\n\nWe're going to build up to a picture of how to model agents who are\nrationally uncertain about whether the correct logic is classical or\nintuitionistic. But let's start by thinking how an agent who is unsure\nwhich of two empirical theories $T_1$ or $T_2$ is correct. We'll assume\nthat the agent is using the classical probability calculus, and the\nagent knows which propositions are entailed by each of the two theories.\nAnd we'll also assume that the agent is sure that it's not the case that\neach of these theories is false, and the theories are inconsistent, so\nthey can't both be true.\n\nThe natural thing then is for the agent to have some credence $x$ in\n$T_1$, and credence $1-x$ in $T_2$. She will naturally have a picture of\nwhat the world is like assuming $T_1$ is correct, and on that picture\nevery proposition entailed by $T_1$ will get probability 1. And she'll\nhave a picture of what the world is like assuming $T_2$ is correct. Her\noverall credal state will be a mixture of those two pictures, weighted\naccording to the credibility of $T_1$ and $T_2$.\n\nIf we're working with unconditional credences as primitive, then it is\neasy to mix two probability functions to produce a credal function which\nis also a probability function. Let $Pr_1$ be the probability function\nthat reflects the agent's views about how things probably are\nconditional on $T_1$ being true, and $Pr_2$ the probability function\nthat reflects her views about how things probably are conditional on\n$T_2$ being true. Then for any $p$, let\n$Cr(p) = xPr_1(p) + (1-x)Pr_2(p)$, where $Cr$ is the agent's credence\nfunction.\n\nIt is easy to see that $Cr$ will be a probability function. Indeed,\ninspecting the axioms P0-P3 makes it obvious that for any $\\vdash$,\nmixing two $\\vdash$-probability functions as we've just done will always\nproduce a $\\vdash$-probability function. The axioms just require that\nprobabilities stand in certain equalities and inequalities that are\nobviously preserved under mixing.\n\nIt is a little trickier to mix conditional probability functions in an\nintuitive way, for the reasons set out in @Jehle2009. But in a special\ncase, these difficulties are not overly pressing. Say that a\n$\\vdash$-probability function is **regular** iff for any *p, q* in its\ndomain, $Pr(p | q) = 0$ iff $p \\wedge q$ is a $\\vdash$-antitheorem.\nThen, for any two regular conditional probability functions $Pr_1$ and\n$Pr_2$ we can create a weighted mixture of the two of them by taking the\nnew unconditional probabilities, i.e. the probabilities of $p$ given\n$T$, where $T$ is a theorem, to be weighted sums of the unconditional\nprobabilities in $Pr_1$ and $Pr_2$. That is, our new function $Pr_3$ is\ngiven by:\n\n$$Pr_3(p | T) = xPr_1(p | T) + (1-x)Pr_2(p | T)$$\n\nIn the general case, this does not determine exactly which function\n$Pr_3$ is, since it doesn't determine the value of $Pr_3(p | q)$ when\n$Pr_1(q | T) = Pr_2(q | T) = 0$. But since we're paying attention just\nto regular functions this doesn't matter. If the function is regular,\nthen we can just let the familiar ratio account of conditional\nprobability be a genuine definition. So in general we have,\n\n$$Pr_3(p | q) = \\frac{Pr_3(p \\wedge q | T)}{Pr_3(q | T)}$$\n\nAnd since the numerator is 0 iff $q$ is an anti-theorem, whenever\n$Pr(p | q)$ is supposed to be defined, i.e. when $q$ is not an\nanti-theorem, the right hand side will be well defined. As we noted,\nthings get a lot messier when the functions are not regular, but those\ncomplications are unnecessary for the story we want to tell.\n\nNow in the cases we've been considering so far, we've been assuming that\n$T_1$ and $T_2$ are empirical theories, and that we could assume\nclassical logic in the background. Given all that, most of what we've\nsaid in this section has been a fairly orthodox treatment of how to\naccount for a kind of uncertainty. But there's no reason, we say, why we\nshould restrict $T_1$ and $T_2$ in this way. We could apply just the\nsame techniques when $T_1$ and $T_2$ are theories of entailment.\n\nWhen $T_1$ is the theory that classical logic is the right logic of\nentailment, and $T_2$ the theory that intuitionistic logic is the right\nlogic of entailment, then $Pr_1$ and $Pr_2$ should be different kinds of\nprobability functions. In particular, $Pr_1$ should be a\n$\\vdash_{CL}$-probability function, and $Pr_2$ should be a\n$\\vdash_{IL}$-probability function. That's because $Pr_1$ represents how\nthings probably are given $T_1$, and given $T_1$, how things probably\nare is constrained by classical logic. And $Pr_2$ represents how things\nprobably are given $T_2$, and given $T_2$, how things probably are is\nconstrained by intuitionistic logic.\n\nIf we do all that, we're pushed towards the thought that the if someone\nis uncertain whether the right logic is intuitionistic or classical\nlogic, then the right theory of probability for them is intuitionistic\nprobability theory. That's because of Theorem 5.\n\n> **Theorem 5** Let $Pr_1$ be a regular conditional\n> $\\vdash_{CL}$-probability function, and $Pr_2$ be a regular\n> conditional $\\vdash_{IL}$-probability function that is not a\n> $\\vdash_{CL}$-probability function. And let $Pr_3$ be defined as in\n> the text. (That is, $Pr_3(A) = xPr_1(A) + (1-x)Pr_2(A)$, and\n> $Pr_3(A | B) = \\frac{Pr_3(A \\wedge B)}{Pr_3(B)}$.) Then $Pr_3$ is a\n> regular conditional $\\vdash_{IL}$-probability function.\n\nThat's to say, if the agent is at all unsure whether classical logic or\nintuitionistic logic is the correct logic, then their credence function\nshould be an intuitionistic probability function.\n\nOf course, if the agent is very confident that classical logic is the\ncorrect logic, then they couldn't rationally have their credences\ndistributed by any old intuitionistic probability function. After all,\nthere are intuitionistic probability functions such that\n$Pr(p \\vee \\neg p) = 0$, but an agent whose credence that classical\nlogic is correct is, say, 0.95, could not reasonably have credence 0 in\n$p \\vee \\neg p$. For our purposes, this matters because we want to show\nthat an agent who is confident, but not certain, that classical logic is\ncorrect can nevertheless be a dogmatist. To fill in the argument we\nneed,\n\n> **Theorem 6** Let $x$ be any real in $(0, 1)$. Then there is a\n> probability function $Cr$ that (a) is a coherent credence function for\n> someone whose credence that classical logic is correct is $x$, and (b)\n> satisfies each of the following inequalities: $$\\begin{aligned}\n> Pr(Ap \\rightarrow p | Ap) &> Pr(Ap \\rightarrow p) \\\\\n> Pr(\\neg Ap \\vee p | Ap) &> Pr(\\neg Ap \\vee p) \\\\\n> Pr(\\neg(Ap \\wedge \\neg p) | Ap) &> Pr(\\neg(Ap \\wedge \\neg p)) \\end{aligned}$$\n\nThe main idea driving the proof of Theorem 6 which is set out in the\nappendix, is that if intuitionistic logic is correct, it's possible that\nconditionalising on *Ap* raises the probability of each of these three\npropositions from arbitrarily low values to 1. So as long as the prior\nprobability of each of the three propositions, conditional on\nintuitionistic logic being correct, is low enough, it can still be\nraised by conditionalising on *Ap*.\n\nMore centrally, we think Theorem 6 shows that the probabilistic argument\nagainst dogmatism is not compelling. The original argument noted that\nthe dogmatist says that we can learn the three propositions in Theorem\n6, most importantly $Ap \\rightarrow p$, by getting evidence *Ap*. And it\nsays this is implausible because conditionalising on *Ap* lowers the\nprobability of $Ap \\rightarrow p$. But it turns out this is something of\nan artifact of the very strong classical assumptions that are being\nmade. The argument not only requires the correctness of classical logic,\nit requires that the appropriate credence the agent should have in\nclassical logic's being correct is one. And that assumption is, we\nthink, wildly implausible. Even if the agent should be *very confident*\nthat classical logic is the correct logic, it shouldn't be a requirement\nof rationality that she be absolutely certain that it is correct.\n\nSo we conclude that this argument fails. A dogmatist about perception\nwho is at least minimally open-minded about logic can marry perceptual\ndogmatism to a probabilistically coherent theory of confirmation.\n\nThis paper is one more attempt on our behalf to defend dogmatism from a\nprobabilistic challenge. @Weatherson2007 defends dogmatism from the\nso-called \"Bayesian objection\". And @JehlePhD not only shows that\ndogmatism can be situated nicely into a probabilistically coherent\ntheory of confirmation, but also that within such a theory, many of the\ntraditional objections to dogmatism are easily rebutted. We look forward\nto future research on the connections between dogmatism and probability,\nbut we remain skeptical that dogmatism will be undermined solely by\nprobabilistic considerations.\n\n### Appendix: Proofs {#appendix-proofs .unnumbered}\n\n> **Theorem 1**\\\n> If $Pr$ is a classical probability function, then\\\n> $Pr(Ap \\rightarrow p | Ap) \\leq Pr(Ap \\rightarrow p)$.\n\n**Proof**: Assume $Pr$ is a classical probability function, and $\\vdash$\nthe classical consequence relation.\n\n$$\\begin{aligned}\n1. &Ap \\rightarrow p \\dashv \\vdash ((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap) & \\text{} \\\\\n2. &Pr(Ap \\rightarrow p) = Pr(((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap)) & \\text{1, P2$^*$} \\\\\n3. & Pr ((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap)) = \\\\&Pr ((Ap \\rightarrow p) \\wedge Ap) + Pr ((Ap \\rightarrow p) \\wedge \\neg Ap)  \n & \\text{P3$^*$}  \\\\\n4. &Pr((Ap \\rightarrow p) \\wedge Ap) = Pr (Ap)Pr(Ap \\rightarrow p|Ap) & \\text{P6} \\\\\n5. &Pr((Ap \\rightarrow p) \\wedge \\neg Ap) = Pr(\\neg Ap)Pr(Ap \\rightarrow p |\\neg Ap) & \\text{P6} \\\\\n6. &Pr(Ap \\rightarrow p) = \\\\&Pr(Ap)Pr(Ap \\rightarrow p|Ap) + Pr (\\neg Ap)Pr(Ap \\rightarrow p |\\neg Ap) & \\text{2, 4, 5} \\\\\n7. &(Ap \\rightarrow p) \\wedge Ap \\dashv \\vdash \\neg Ap & \\text{} \\\\\n8. &Pr((Ap \\rightarrow p) \\wedge Ap) = Pr(\\neg Ap) & \\text{7, P2$^*$} \\\\\n9. &Pr(Ap \\rightarrow p |\\neg Ap) = 1 \\text{ or } Pr(\\neg Ap) = 0 & \\text{8, P6}  \\\\\n10. &Pr(Ap \\rightarrow p | Ap) \\leq 1 & \\text{P4, P5} \\\\\n11. &Pr(Ap \\rightarrow p) \\geq \\\\ &Pr(Ap)Pr(Ap \\rightarrow p|Ap) + Pr (\\neg Ap)Pr(Ap \\rightarrow p |Ap)  & \\text{6, 9, 10} \\\\\n12. &\\vdash Ap \\vee \\neg Ap & \\text{} \\\\\n13. &Pr(Ap \\vee \\neg Ap) = 1 & \\text{12, P1} \\\\\n14. &Pr(Ap) + Pr (\\neg Ap) = 1 & \\text{13, P3$^*$} \\\\\n15. &Pr(Ap \\rightarrow p ) \\geq Pr (Ap \\rightarrow p|Ap) & \\text{11, 14} \\end{aligned}$$\nNote (11) is an equality iff (8) is. The only step there that may not be\nobvious is step 10. The reason it holds is that either $Ap$ is a\n$\\vdash$-antitheorem or it isn't. If it is, then it entails\n$Ap \\rightarrow p$, so by P5, $Pr(Ap \\rightarrow p | Ap) \\leq 1$. If it\nis not, then by P1$^*$, $Pr(x | Ap) \\leq 1$ for any $x$, so\n$Pr(Ap \\rightarrow p | Ap) \\leq 1$.\n\n> **Theorem 2**\\\n> If $Pr$ is a classical probability function, then\n>\n> -   $Pr(\\neg(Ap \\wedge \\neg p) | Ap) \\leq Pr(\\neg(Ap \\wedge \\neg p))$;\n>     and\n>\n> -   $Pr(\\neg Ap \\vee p | Ap) \\leq Pr(\\neg Ap \\vee p)$.\n\n**Proof**: Assume $Pr$ is a classical probability function, and $\\vdash$\nthe classical consequence relation. $$\\begin{aligned}\n1. &Ap \\rightarrow p \\dashv  \\vdash \\neg(Ap \\wedge \\neg p) &  \\\\\n2. &Pr(Ap \\rightarrow p) = Pr(\\neg(Ap \\wedge \\neg p)) & 1, P2^* \\\\\n3. &Pr(Ap \\rightarrow p | Ap) = Pr(\\neg(Ap \\wedge \\neg p) | Ap) & 1, P4, P5 \\\\\n4. &Pr(Ap \\rightarrow p ) \\geq Pr (Ap \\rightarrow p|Ap) & \\text{Theorem 1} \\\\\n5. &Pr(\\neg(Ap \\wedge \\neg p) | Ap) \\geq Pr(\\neg(Ap \\wedge \\neg p)) & 2, 3, 4 \\\\\n6. &Ap \\rightarrow p \\dashv  \\vdash \\neg Ap \\vee p &  \\\\\n7. &Pr(Ap \\rightarrow p) = Pr(\\neg Ap \\vee p) & 6, P2^* \\\\\n8. &Pr(Ap \\rightarrow p | Ap) = Pr(\\neg Ap \\vee p | Ap) & 6, P4, P5 \\\\\n9. &Pr(\\neg Ap \\vee p | Ap) \\geq Pr(\\neg Ap \\vee p) & 4, 7, 8\\end{aligned}$$\n\nThe only minor complication is with step 3. There are two cases to\nconsider, either $Ap$ is a $\\vdash$-antitheorem or it isn't. If it is a\n$\\vdash$-antitheorem, then both the LHS and RHS of (3) equal 1, so they\nare equal. If it is not a $\\vdash$-antitheorem, then by P4,\n$Pr(\\cdot | Ap)$ is a probability function. So by P2$^*$, and the fact\nthat $Ap \\rightarrow p \\dashv  \\vdash \\neg(Ap \\wedge \\neg p)$, we have\nthat the LHS and RHS are equal.\n\n> **Theorem 3**.\\\n> In $M$, for any $x \\in (0, 1)$,\n>\n> 1.  $Pr_x(Ap \\rightarrow p)$ =\n>     $Pr_x((Ap \\rightarrow p) \\wedge Ap) = x$\n>\n> 2.  $Pr_x(\\neg Ap \\vee p)$ = $Pr_x((\\neg Ap \\vee p) \\wedge Ap) = x$\n>\n> 3.  $Pr_x(\\neg(Ap \\wedge \\neg p))$ =\n>     $Pr_x(\\neg(Ap \\wedge \\neg p) \\wedge Ap) = x$\n\nRecall what $M$ looks like.\n\n::: {.center}\n(70, 50) (35, 5)(-1, 1)30 (35, 5)(1, 1)30 (35,5) (4.8,35.5) (65.2,35.5)\n(28, 5)$1$ (0,35.5)$2$ (60,35.5)$3$ (7,35.5)$Ap, p$ (67,35.5)$Ap$\n:::\n\nThe only point where $Ap \\rightarrow p$ is true is at 2. Indeed,\n$\\neg(Ap \\rightarrow p)$ is true at 3, and neither $Ap \\rightarrow p$\nnor $\\neg(Ap \\rightarrow p)$ are true at 1. So\n$Pr_x(Ap \\rightarrow p) = m_x(\\{2\\}) = x$. Since *Ap* is also true at 2,\nthat's the only point where $(Ap \\rightarrow p) \\wedge Ap$ is true. So\nit follows that $Pr_x((Ap \\rightarrow p) \\wedge Ap) = m_x(\\{2\\}) = x$.\n\nSimilar inspection of the model shows that 2 is the only point where\n$\\neg(Ap \\wedge \\neg p)$ is true, and the only point where\n$\\neg Ap \\vee p$ is true. And so (b) and (c) follow in just the same\nway.\n\nIn slight contrast, $Ap$ is true at two points in the model, 2 and 3.\nBut since $m_x(\\{3\\}) = 0$, it follows that\n$m_x(\\{2, 3\\}) = m_x(\\{2\\}) = x$. So $Pr_x(Ap) = x$.\n\n> **Theorem 4**.\\\n> For any $x \\in (0, 1)$,\n>\n> 1.  $1 = Pr_x(Ap \\rightarrow p | Ap) >  Pr_x(Ap \\rightarrow p) = x$\n>\n> 2.  $1 = Pr_x(\\neg Ap \\vee p | Ap) >  Pr_x(\\neg Ap \\vee p) = x$\n>\n> 3.  $1 = Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) >  Pr_x(\\neg(Ap \\wedge \\neg p)) = x$\n\nWe'll just go through the argument for (a); the other cases are similar.\nBy P6, we know that\n$Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) Pr_x(Ap) = Pr_x((Ap \\rightarrow p) \\wedge Ap)$.\nBy Theorem 3, we know that\n$Pr_x(Ap) = Pr_x((Ap \\rightarrow p) \\wedge Ap)$, and that both sides are\ngreater than 0. (Note that the theorem is only said to hold for\n$x > 0$.) The only way both these equations can hold is if\n$Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) = 1$. Note also that by hypothesis,\n$x < 1$, and from this claim (a) follows. The other two cases are\ncompletely similar.\n\n> **Theorem 5** Let $Pr_1$ be a regular conditional\n> $\\vdash_{CL}$-probability function, and $Pr_2$ be a regular\n> conditional $\\vdash_{IL}$-probability function that is not a\n> $\\vdash_{CL}$-probability function. And let $Pr_3$ be defined as in\n> the text. (That is, $Pr_3(A) = xPr_1(A) + (1-x)Pr_2(A)$, and\n> $Pr_3(A | B) = \\frac{Pr_3(A \\wedge B)}{Pr_3(B)}$.) Then $Pr_3$ is a\n> regular conditional $\\vdash_{IL}$-probability function.\n\nWe first prove that $Pr_3$ satisfies the requirements of an\nunconditional $\\vdash_{IL}$-probability function, and then show that it\nsatisfies the requirements of a conditional $\\vdash_{IL}$-probability\nfunction.\n\nIf $p$ is an $\\vdash_{IL}$-antithesis, then it is also a\n$\\vdash_{CL}$-antithesis. So $Pr_1(p) = Pr_2(p) = 0$. So\n$Pr_3(A) = 0x + 0(1-x) = 0$, as required for **(P0)**.\n\nIf $p$ is an $\\vdash_{IL}$-thesis, then it is also a\n$\\vdash_{CL}$-thesis. So $Pr_1(p) = Pr_2(p) = 1$. So\n$Pr_3(p) = x + (1-x) = 1$, as required for **(P1)**.\n\nIf $p \\vdash_{IL} q$ then $p \\vdash_{CL} q$. So we have both\n$Pr_1(p) \\leq Pr(q)$ and $Pr_2(p) \\leq Pr_2(q)$. Since $x \\geq 0$ and\n$(1-x) \\geq 0$, these inequalities imply that $xPr_1(p) \\leq xPr(q)$ and\n$(1-x)Pr_2(p) \\leq (1-x)Pr_2(q)$. Summing these, we get\n$xPr_1(p) + (1-x)Pr_2(p) \\leq xPr_1(q) + (1-x)Pr_2(q)$. And by the\ndefinition of $Pr_3$, that means that $Pr_3(p) \\leq Pr_3(q)$, as\nrequired for **(P2)**.\n\nFinally, we just need to show that\n$Pr_3(p) + Pr_3(q) = Pr_3(p \\vee q) + Pr_3(p \\wedge q)$, as follows:\n\n$$\\begin{aligned}\nPr_3(p) + Pr_3(q) &= xPr_1(p) + (1-x)Pr_2(p) + xPr_1(q) + (1-x)Pr_2(q) \\\\\n &= x(Pr_1(p) + Pr_1(q)) + (1-x)(Pr_2(p) + Pr_2(q)) \\\\\n &= x(Pr_1(p \\vee q) + Pr_1(p \\wedge q)) + (1-x)(Pr_2(p \\vee q) + Pr_2(p \\wedge q)) \\\\\n &= xPr_1(p \\vee q) + (1-x)Pr_2(p \\vee q) + xPr_1(p \\wedge q)) + (1-x)Pr_2(p \\wedge q) \\\\\n &= Pr_3(p \\vee q) + Pr_3(p \\wedge q) \\text{ as required}\\end{aligned}$$\n\nNow that we have shown $Pr_3$ is an unconditional\n$\\vdash_{IL}$-probability function, we need to show it is a conditional\n$\\vdash_{IL}$-probability function, where\n$Pr_3(p | r) =_{df} \\frac{Pr_3(p \\wedge r)}{Pr_3(r)}$. Remember we are\nassuming that both $Pr_1$ and $Pr_2$ are regular, from which it clearly\nfollows that $Pr_3$ is regular, so this definition is always in order.\n(That is, we're never dividing by zero.) The longest part of showing\n$Pr_3$ is a conditional $\\vdash_{IL}$-probability function is showing\nthat it satisfies **(P4**), which has four parts. We need to show that\n$Pr(\\cdot | r)$ satisfies **(P0)-(P3)**. Fortunately these are fairly\nstraightforward.\n\nIf $p$ is an $\\vdash_{IL}$-antithesis, then so is $p \\wedge r$. So\n$Pr_3(p \\wedge r) =  0$, so $Pr_3(p | r) = 0$, as required for **(P0)**.\n\nIf $p$ is an $\\vdash_{IL}$-thesis, then $p \\wedge r \\dashv \\vdash r$, so\n$Pr_3(p \\wedge r) = Pr_3(r)$, so $Pr_3(p | r) = 1$, as required for\n**(P1)**.\n\nIf $p \\vdash_{IL} q$ then $p \\wedge r \\vdash_{IL} q \\wedge r$. So\n$Pr_3(p \\wedge r) \\leq Pr_3(q \\wedge r)$. So\n$\\frac{Pr_3(p \\wedge r)}{Pr_3(r)} \\leq \\frac{Pr_3(q \\wedge r)}{Pr_3(r)}$.\nThat is, $Pr_3(p | r) \\leq Pr_3(q | r)$, as required for **(P2)**.\n\nFinally, we need to show that\n$Pr_3(p | r) + Pr_3(q | r) = Pr_3(p \\vee q | r) + Pr_3(p \\wedge q | r)$,\nas follows, making repeated use of the fact that $Pr_3$ is an\nunconditional $\\vdash_{IL}$-probability function, so we can assume it\nsatisfies **(P3)**, and that we can substitute intuitionistic\nequivalences inside $Pr_3$.\n\n$$\\begin{aligned}\nPr_3(p | r) + Pr_3(q | r) &= \\frac{Pr_3(p \\wedge r)}{Pr_3(r)} + \\frac{Pr_3(q \\wedge r)}{Pr_3(r)} \\\\\n&= \\frac{Pr_3(p \\wedge r) + Pr(q \\wedge r)}{Pr_3(r)} \\\\\n&= \\frac{Pr_3((p \\wedge r) \\vee (q \\wedge r)) + Pr_3((p \\wedge r) \\wedge (q \\wedge r))}{Pr_3(r)} \\\\\n&=\\frac{Pr_3(p \\vee q) \\wedge r) + Pr_3((p \\wedge q) \\wedge r)}{Pr_3(r)} \\\\\n&=\\frac{Pr_3(p \\vee q) \\wedge r)}{Pr_3(r)} + \\frac{Pr_3((p \\wedge q) \\wedge r)}{Pr_3(r)} \\\\\n&=Pr_3(p \\vee q | r) + Pr_3(p \\wedge q | r) \\text{ as required}\\end{aligned}$$\n\nNow if $r \\vdash_{IL} p$, then $r \\wedge p ~_{IL}\\dashv \\vdash_{IL} p$,\nso $Pr_3(r \\wedge p) = Pr_3(p)$, so $Pr_3(p | r) = 1$, as required for\n**(P5)**.\n\nFinally, we show that $Pr_3$ satisfies **(P6)**.\n\n$$\\begin{aligned}\nPr_3(p \\wedge q | r) &= \\frac{Pr_3(p \\wedge q \\wedge r)}{Pr_3(r)} \\\\\n &= \\frac{Pr_3(p \\wedge q \\wedge r)}{Pr_3(q \\wedge r)} \\frac{Pr_3(q \\wedge r)}{Pr_3(r)} \\\\\n &=Pr_3(p | q \\wedge r) Pr_3(q | r) \\text{ as required}\\end{aligned}$$\n\n> **Theorem 6** Let $x$ be any real in $(0, 1)$. Then there is a\n> probability function $Cr$ that (a) is a coherent credence function for\n> someone whose credence that classical logic is correct is $x$, and (b)\n> satisfies each of the following inequalities: $$\\begin{aligned}\n> Pr(Ap \\rightarrow p | Ap) &> Pr(Ap \\rightarrow p) \\\\\n> Pr(\\neg Ap \\vee p | Ap) &> Pr(\\neg Ap \\vee p) \\\\\n> Pr(\\neg(Ap \\wedge \\neg p) | Ap) &> Pr(\\neg(Ap \\wedge \\neg p)) \\end{aligned}$$\n\nWe'll prove this by constructing the function $Pr$. For the sake of this\nproof, we'll assume a very restricted formal language with just two\natomic sentences: $Ap$ and $p$. This restriction makes it easier to\nensure that the functions are all regular, which as we noted in the main\ntext lets us avoid various complications. The proofs will rely on three\nprobability functions defined using this Kripke tree $M$.\n\n::: {.center}\n(100, 40) (50, 5)(-3, 2)45 (50, 5)(-1, 2)15 (50, 5)(1, 2)15 (50, 5)(3,\n2)45 (50,5) (4.5,35.5) (65.2,35.5) (34.8,35.5) (95.5,35.5) (42, 5)$0$\n(0,35.5)$1$ (30,35.5)$2$ (60,35.5)$3$ (90,35.5)$4$ (7,35.5)$Ap, p$\n(37,35.5)$Ap$ (67,35.5)$p$\n:::\n\nWe've shown on the graph where the atomic sentences true: $Ap$ is true\nat 1 and 2, and $p$ is true at 1 and 3. So the four terminal nodes\nrepresent the four classical possibilities that are definable using just\nthese two atomic sentences. We define two measure functions $m_1$ and\n$m_2$ over the points in this model as follows:\n\n::: {.center}\n  ------- --------------- ----------------- ----------------- --------------- ---------------\n            $m(\\{0\\})$       $m(\\{1\\})$        $m(\\{2\\})$       $m(\\{3\\})$      $m(\\{4\\})$\n    $m_1$        0          $\\frac{x}{2}$    $\\frac{1-x}{2}$   $\\frac{1}{4}$   $\\frac{1}{4}$\n    $m_2$  $\\frac{x}{2}$   $\\frac{1-x}{4}$   $\\frac{1-x}{4}$   $\\frac{1}{4}$   $\\frac{1}{4}$\n  ------- --------------- ----------------- ----------------- --------------- ---------------\n:::\n\nWe've just specified the measure of each singleton, but since we're just\ndealing with a finite model, that uniquely specifies the measure of any\nset. We then turn each of these into probability functions in the way\ndescribed in section 1. That is, for any proposition $X$, and\n$i \\in \\{1, 2\\}$, $Pr_i(X) = m_i(M_X)$, where $M_X$ is the set of points\nin $M$ where $X$ is true.\n\nNote that the terminal nodes in $M$, like the terminal nodes in any\nKripke tree, are just classical possibilities. That is, for any\nsentence, either it or its negation is true at a terminal node.\nMoreover, any measure over classical possibilities generates a classical\nprobability function. (And vice versa, any classical probability\nfunction is generated by a measure over classical possibilities.) That\nis, for any measure over classical possibilities, the function from\npropositions to the measure of the set of possibilities at which they\nare true is a classical probability function. Now $m_1$ isn't quite a\nmeasure over classical possibilities, since strictly speaking\n$m_1(\\{0\\})$ is defined. But since $m_1(\\{0\\}) = 0$ it is equivalent to\na measure only defined over the terminal nodes. So the probability\nfunction it generates, i.e., $Pr_1$, is a classical probability\nfunction.Of course, with only two atomic sentences, we can also verify\nby brute force that $Pr_1$ is classical, but it's a little more helpful\nto see why this is so. In contrast, $Pr_2$ is not a classical\nprobability function, since $Pr_2(p \\vee \\neg p) = 1 - \\frac{x}{2}$, but\nit is an intuitionistic probability function.\n\nSo there could be an agent who satisfies the following four conditions:\n\n-   Her credence that classical logic is correct is $x$;\n\n-   Her credence that intuitionistic logic is correct is $1-x$;\n\n-   Conditional on classical logic being correct, she thinks that $Pr_1$\n    is the right representation of how things probably are; and\n\n-   Conditional on intuitionistic logic being correct, she thinks that\n    $Pr_2$ is the right representation of how things are.\n\nSuch an agent's credences will be given by a $\\vdash_{IL}$-probability\nfunction $Pr$ generated by 'mixing' $Pr_1$ and $Pr_2$. For any sentence\n$Y$ in the domain, her credence in $Y$ will be\n$xPr_1(Y) + (1-x)Pr_2(Y)$. Rather than working through each proposition,\nit's easiest to represent this function by mixing the measures $m_1$ and\n$m_2$ to get a new measure $m$ on the above Kripke tree. Here's the\nmeasure that $m$ assigns to each node.\n\n::: {.center}\n  ----- -------------------- --------------------------- ------------------- --------------- ---------------\n             $m(\\{0\\})$              $m(\\{1\\})$              $m(\\{2\\})$        $m(\\{3\\})$      $m(\\{4\\})$\n    $m$  $\\frac{x(1-x)}{2}$   $\\frac{3x^2 - 2x + 1}{4}$   $\\frac{1-x^2}{4}$   $\\frac{1}{4}$   $\\frac{1}{4}$\n  ----- -------------------- --------------------------- ------------------- --------------- ---------------\n:::\n\nAs usual, this measure $m$ generates a probability function $Pr$. We've\nalready argued that $Pr$ is a reasonable function for someone whose\ncredence that classical logic is $x$. We'll now argue that\n$Pr(Ap \\rightarrow p | Ap) > Pr(Ap \\rightarrow p)$.\n\nIt's easy to see what $Pr(Ap \\rightarrow p)$ is. $Ap \\rightarrow p$ is\ntrue at 1, 3 and 4, so\n\n$$\\begin{aligned}\nPr(Ap \\rightarrow p) &= m({1}) + m({3}) + m(4) \\\\\n &= \\frac{3x^2 - 2x + 1}{4} + \\frac{1}{4} + \\frac{1}{4} \\\\\n &= \\frac{3x^2 - 2x + 3}{4} \\end{aligned}$$\n\nSince $Pr$ is regular, we can use the ratio definition of conditional\nprobability to work out $Pr(Ap \\rightarrow p | Ap)$.\n\n$$\\begin{aligned}\nPr(Ap \\rightarrow p | Ap) &= \\frac{Pr((Ap \\rightarrow p) \\wedge Ap)}{Pr(Ap)} \\\\\n &= \\frac{m({1})}{m({1}) + m({2})} \\\\\n &= \\frac{\\frac{3x^2 - 2x + 1}{4}}{\\frac{3x^2 - 2x + 1}{4} + \\frac{1-x^2}{4}} \\\\\n &= \\frac{3x^2 - 2x + 1}{(3x^2 - 2x + 1) + (1-x^2)} \\\\\n &= \\frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \\end{aligned}$$\n\nPutting all that together, we have\n\n$$\\begin{aligned}\n&& Pr(Ap \\rightarrow p | Ap) &> Pr(Ap \\rightarrow p) \\\\\n\\Leftrightarrow &&  \\frac{3x^2 - 2x + 3}{4}  &> \\frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \\\\\n\\Leftrightarrow && 3x^2 - 2x + 3  &> \\frac{6x^2 - 4x + 2}{x^2 - x + 1} \\\\\n\\Leftrightarrow && (3x^2 - 2x + 3)(x^2 + x + 1)  &> 6x^2 - 4x + 2 \\\\\n\\Leftrightarrow && 3x^4 - 5x^3 + 8x^2 - 5x + 3  &> 6x^2 - 4x + 2 \\\\\n\\Leftrightarrow && 3x^4 - 5x^3 + 2x^2 - x + 1 &> 0 \\\\\n\\Leftrightarrow && (3x^2 + x + 1)(x^2 - 2x + 1) &> 0 \\\\\n\\Leftrightarrow && (3x^2 + x + 1)(x - 1)^2 &> 0\\end{aligned}$$\n\nBut it is clear that for any $x \\in (0,1)$, both of the terms of the LHS\nof the final line are positive, so their product is positive. And that\nmeans $Pr(Ap \\rightarrow p | Ap) > Pr(Ap \\rightarrow p)$. So no matter\nhow close $x$ gets to 1, that is, no matter how certain the agent gets\nthat classical logic is correct, as long as $x$ does not reach 1,\nconditionalising on $Ap$ will raise the probability of\n$Ap \\rightarrow p$. As we've been arguing, as long as there is any doubt\nabout classical logic, even a vanishingly small doubt, there is no\nprobabilistic objection to dogmatism.\n\nTo finish up, we show that\n$Pr(\\neg Ap \\vee p | Ap) > Pr(\\neg Ap \\vee p)$ and\n$Pr(\\neg(Ap \\wedge \\neg p) | Ap) > Pr(\\neg(Ap \\wedge \\neg p))$. To do\nthis, we just need to note that $Ap \\rightarrow p$, $\\neg Ap \\vee p$ and\n$\\neg(Ap \\wedge \\neg p)$ are true at the same points in the model, so\ntheir probabilities, both unconditionally and conditional on $Ap$, will\nbe identical. So from $Pr(Ap \\rightarrow p | Ap) > Pr(Ap \\rightarrow p)$\nthe other two inequalities follow immediately.\n\n[^1]: We're assuming here that the agent's evidence really is *Ap*, not\n    $p$. That's a controversial assumption, but it isn't at issue in\n    this debate.\n\n[^2]: @PopperMiller1987 prove a stronger result than Theorem One, and\n    note its significance for probabilistic models of learning.\n\n[^3]: We'll usually assume that the language of $\\vdash$ is a familiar\n    kind of propositional calculus, with a countable infinity of\n    sentence letters, and satisfying the usual recursive constraints.\n    That is, if $A$ and $B$ are sentences of the language, then so are\n    $\\neg A$, $A \\rightarrow B$, $A \\wedge B$ and $A \\vee B$. It isn't\n    entirely trivial to extend some of our results to a language that\n    contains quantifiers. This is because once we add quantifiers,\n    intuitionistic and classical logic no longer have the same\n    anti-theorems. But that complication is outside the scope of this\n    paper. Note that for Theorem 6, we assume a restricted language with\n    just two sentence letters. This merely simplifies the proof. A\n    version of the construction we use there with those two letters\n    being simply the first two sentence letters would be similar, but\n    somewhat more complicated.\n\n[^4]: @Weatherson2003 discusses what happens if we make P2$^*$ or P3$^*$\n    an axiom in place of either P2 and P3. It is argued there that this\n    gives us too many functions to be useful in epistemology. The\n    arguments in @Williamsms provide much stronger reasons for believing\n    this conclusion is correct.\n\n[^5]: For the reasons given in @Hajek2003, it is probably better in\n    general to take conditional probability as primitive. But for our\n    purposes taking unconditional probability to be basic won't lead to\n    any problems, so we'll stay neutral on whether conditional or\n    unconditional probability is really primitive.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
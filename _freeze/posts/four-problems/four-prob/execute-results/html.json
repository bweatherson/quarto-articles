{
  "hash": "2967fa7ab43cc150d76821dd562fa473",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Four Problems in Decision Theory\"\ndescription: |\n  In recent years the literature on decision theory has become disjointed. There isn't as much discussion as there should be on how different problems impact one another. This paper aims to bring together work on problems involving demons, problems about attitudes to risk, problems about incomplete preferences, and problems about dynamic choice. In the first three of these cases, I end up defending a pre-existing view, but in each case the argument for that view is strengthened by seeing how the premises that support it are essential to solving one of the other problems. The most novel part of the view is the theory of dynamic choice that I offer: a sequence of choices is rational only if both the so-called 'resolute' and 'sophisticated' theories of dynamic choice would permit it. This theory would be implausible if paired with many rival solutions to the first three problems, but fits nicely with the view I'll develop through the paper that decision theory is much less constraining than most theorists hold.\ndate: March 5 2024\nauthor:\n  - name: Brian Weatherson \n    url: http://brian.weatherson.org\n    affiliation: University of Michigan\n    affiliation_url: https://umich.edu\n    orcid_id: 0000-0002-0830-141X\ncitation: false\ncategories:\n  - games and decisions\n  - unpublished\nformat:\n  html: default\n  pdf:\n    output-file: \"Four Problems in Decision Theory\"\n    include-after-body: \n      text: |\n         Unpublished. Posted online in 2024.\n---\n\n\nContemporary decision theory has become disjointed. There is less overlap than there should be in work on adjacent problems. This paper aims to undo some of that, by showing that four problems that have largely been worked on in isolation cast useful light on each other. Some of the conclusions that draw will be familiar: on one of the problems I’m going to defend a similar answer to what Melissa @Fuscond has defended; on another I'm going to defend a similar answer to one defended by Harvey @Ledermannd. What's primarily distinctive about the arguments here is that they show these questions are connected, and the arguments for my preferred answers are going to be intertwined.\n\nThis paper is part of a broader project of identifying the decision theory that is implicit in standard, textbook approaches to game theory, and arguing that this decision theory is better than the ones currently on the philosophical market. I used to think the first part of this project would be boring - game theorists are just typical Causal Decision Theorists. This can't be true for five reasons. First, these textbooks don't mention counterfactuals at all, but counterfactuals are central to typical presentations of Causal Decision Theory. Second, solution concepts in game theory are typically not _single-valued_, in the technical sense defined by @Pearce1984, while typical versions of Causal Decision Theory are single-valued. Third, sometimes the unique solution to a game involves mixed strategies, while Causal Decision Theory, in its typical formulations, never says that a mixed strategy is uniquely optimal. Fourth, the solution concepts used for things like the beer-quiche game [@ChoKreps1987] put constraints that go beyond coherence constraints on the players, and typical formulations of Causal Decision Theory allow any coherent credence function. Finally, the textbook solution concepts for dynamic games don't correspond to any view in the philosophical literature on dynamic games.\n\nGame theory textbooks tend to be several hundred pages, and identifying all the unique characteristics of the implicit decision theory, like the five from the previous paragraph, would take just as much space. So I'm going to simplify a lot here. In particular, I'm not going to talk about mixed strategies, except occasionally in footnotes. That is, I'm not going to assume anything about the availability or unavailability of mixed strategies in the arguments I put forward. That said, some of the positions I put forward are similar enough to existing positions that there are well known objections in the literature, and in many cases my preferred response to those objections does rely on the availability of mixed strategies. Getting all the details of those right would massively extend the paper, so I'll stay away from those discussions here. Relatedly, while I will spend a lot of time on problems where there are multiple pure strategy equilibria, I won't discuss any problems where there are no pure strategy equilibria. Those are for another day. With those qualifications in place, it's time to get to the four problems I will discuss.\n\n# Four Problems\n\n## Demons\n\nWhen a student starts decision theory, they are introduced to a view that is simple, elegant, and wrong. The view starts by assuming that a chooser, hereafter called Chooser, has a set of possible actions *A* available. We'll use *a* to represent an arbitrary member of that set. And there is a set of possible states *S*, with *s* being used to pick out an arbitrary member. It is assumed that a probability distribution Pr over *S* is given, and that each action-state pair has a numerical value. I'll write *V* for the value function, so *V*(*as*) is the value of performing act *a* in state *s*.\n\nThe simple, elegant, and wrong theory is that Chooser should value each act *a* by its expected value. That is, the value of act *a* is Σ~*s* ∈ *S*~ Pr(*s*)*V*(*as*). And Chooser should then choose the act with the highest value.\n\nThe problem with this view is that if Chooser has any influence over which state is actual, then this view will recommend obviously bad actions. Assume that the only possibly actions are *a* and *b*, the only two states are *s* and *t*, and while *a* will almost certainly cause *s* to be actual, *b* will almost certainly cause *b* to be actual. Now let the payoffs for all four action-state combinations be as in @tbl-joycewindow.\n\n|     | *s* |  *t*  |\n|:---:|:---:|:-----:|\n| *a* |  1  | 1001  |\n| *b* |  0  | 1000  |\n\n: A counterexample to the simple theory. {#tbl-joycewindow}\n\nThe problem is that in @tbl-joycewindow it obviously makes sense to do *b*, since that brings about the best option, but the simple theory says that the value of *a* is 1 more than the value of *b*. So @tbl-joycewindow is a counterexample to the simple theory. So far every decision theorist would agree. But here agreement ends. There is no agreement on either why the simple theory fails in this case, or what should go in its place.\n\nEvidential decision theorists such as Arif @Ahmed2014 say the problem is that there is an evidential connection between the acts and the states. They say that instead of the simple theory Chooser should value options using this formula.\n\nEDT\n:    *V*(*a*) = ΣΣ~*s* ∈ *S*~ (*s* | *a*)*V*(*as*)\n\nAs with the simple theory, the only rule is that Chooser should maximise value. The difference between EDT and the simple theory is that EDT replaces an unconditional probability with a conditional probability in the formula that gives the value of options. This will get the right result in @tbl-joycewindow, but gives some strange results in other cases.\n\nReinterpret @tbl-joycewindow so that the states are causally independent of the actions, but which action Chooser chooses provides excellent evidence about which state they are in. To use the standard example, going back to @Nozick1969, imagine that a demon (hereafter called Demon) has predicted Chooser's choice. There is no backwards causation, so Chooser's choice is causally independent of Demon's prediction. But Chooser believes Demon is incredibly reliable, so Pr(*s* | *a*) ≈ 1, and Pr(*t* | *b*) ≈ 1. For ease of reference, I'll use @tbl-newcomb as the game table for this problem, where the states are the predictions of an accurate Demon. In @tbl-newcomb, Chooser selects **U**p or **D**own, and Demon **P**redicts this choice. In general in what follows, if a state is labelled **PX**, it means that Demon has predicted that Chooser will select X. Using that notation @tbl-newcomb is just Newcomb's Problem.\n\n|       | **PU** |  **PD**  |\n|:-----:|:------:|:--------:|\n| **U** |  1     |  1001    |\n| **D** |  0     |  1000    |\n\n: Newcomb's Problem {#tbl-newcomb}\n\nIn @tbl-newcomb, EDT says that Chooser should do *a*. There is a simple argument that Chooser should do *b*: whatever the world is like, it will have a higher return. This argument convinced many people that we need a different theory, and over the 1970s and 1980s a lot of people settled on something like CfDT as the right alternative.^[The canonical statement of this view is @GibbardHarper1978.]\n\nCfDT\n:    *V*(*a*) = ΣΣ~*s* ∈ *S*~ Pr(*a* □→ *s*)*V*(*as*)\n\nThat's a way to value options; the theory is just that one should choose the option with maximal value. Recently Brian @Hedden2023 has argued that this theory is preferable to *Causal* Decision Theory, properly so called. I'm sympathetic to the reply offered by Dmitri @Gallowndppq that CfDT just is what Causal Decision Theorists in the 1970s and 1980s were typically defending. But I also think, for reasons that will soon become clear, that some other theories which are quite different to this are also *causal* in the relevant sense. So from now on I'll use \"Causal Decision Theory\" to name a family of theories, and CfDT will be a distinctive member of that family.\n\nAnother theory in that family says that the simple theory was essentially correct, it was just applied at the wrong time. This theory, which I'll call Gamified Decision Theory, or GDT, starts with the following two claims. First, the relevant state probabilities are those at the end of deliberation, once a choice has been made, not at the start of deliberation. Second, when we use those _ex post_ probabilities, the simple theory is fine. In symbols, the core formula that GDT uses is this.\n\nGDT\n:    V(a) = Σ~*s* ∈ *S*~ Pr′(*s*)*V*(*as*)\n\nIn this formula, Pr′ is the probability distribution over states after Chooser has made their decision. GDT says that only options that have maximal value using this formula are choice-worthy.^[My preferred version of GDT adds several more constraints to this - it has a separate constraint for ruling out weakly dominated options, and a constraint for solving beer-quiche games, and maybe a constraint for ruling out mixed strategies in coordination games. But having maximal value ex post is a necessary condition for choice-worthiness.] This allows that different options, with different values, could be choice-worthy. All that matters is that given the probability distribution over states that Chooser has when they have decided to perform an act, that act is utility maximising. In @tbl-first-coord, GDT says that both Up and Down are choice-worthy.\n\n|       | **PU** |  **PD**  |\n|:-----:|:------:|:--------:|\n| **U** |  3     |     0    |\n| **D** |  0     |     2    |\n\n: An asymmetric coordination problem {#tbl-first-coord}\n\nOne of our four problems is to work out which of these theories is right. I'll be arguing for GDT.\n\nIt's sometimes said that problems involving Demon should not be treated as central to decision theory because Demon is so unrealistic. I think this view is mistaken twice over. For one thing, Demon isn't that much more unrealistic than the precise probabilistic models of the future of humanity that routinely do get used. More importantly, the problems that come up in this section arise in some very ordinary models. As @Lewis1979en pointed out, Prisoners' Dilemma with a twin raises much the same problems. Standard approaches to game theory presuppose that other players are like perfectly accurate demons.^[Matthias @Risse2000 criticises these standard approaches on this point, and while I'm sympathetic to his criticism, it's worth taking seriously how wide-spread the assumption of perfect prediction is across the academy.] Most importantly, all the views about how to make decisions in Newcomb-like problems come apart as soon as we assume Demon is better than chance at predicting Chooser. And better than chance predictions can be reasonably believed. I suspect if I was allowed to interview and observe people before they chose, I could predict their choices at well over 60% accuracy, and probably over 70%. To simplify the math, I'll work with a Demon who is perfectly accurate, or at least arbitrarily accurate.^[That is, I'll assume Demon's accuracy is 1‑ε, for arbitrarily small ε.] But with some extra attention to detail, we could rewrite every example in the paper with a realistic Demon. I think having very accurate Demons is a worthwhile tradeoff of clarity for realism, but if you disagree it's not that hard to imagine the paper rewritten with demons only somewhat better than chance.\n\n## Risk\n\nThink about what value of *x* would make Chooser indifferent between these two options, and why that would be the right value\n\n1. \\$1,000,000\n2. A gamble that returns \\$2,000,000 with probability *x*, and \\$0 with probability 1-*x*.\n \nWhat factors are relevant to solving for *x*? One factor is the declining marginal utility of money. Money primarily has exchange value, and if Chooser won $2,000,000, the things Chooser would buy with the second million dollars are largely things they declined to buy with the first million. Hence the second million will be worth much less to them than the first, barring a pronounced taste for expensive goods that lack valuable parts. That's one factor that goes into solving for *x*. Every decision theorist agrees it is important, and that it is part of why whatever value *x* takes, it is surely well above ½.\n\nBut is it the only factor? If Chooser is rational, is knowing the function from the money they have to the utility they get from money enough to solve for *x*? The orthodox answer is that it is. Lara @BuchakRisk has argued that it is not. We also need to know how much Chooser values, or more likely disvalues, risk. That is, we need to know how risk-seeking, or risk-averse, Chooser is.\n\nThe orthodox view is that all we need to know are three numbers:\n\n- The value Chooser assigns to their current wealth, which we can set as 0 for ease of calculation.\n- The value Chooser assigns to having $1,000,000 more than their current wealth, which we can set as 1 again for ease of calculation.\n- The value Chooser assigns to having $2,000,000 more than their current wealth, which we will label *c*.\n\nThen on the standard view, the value of the gamble is *cx*, so the gamble is equal to the sure million iff *x* = 1/*c*. On Buchak's view, rational Chooser has a risk function *f*, that measures their sensitivity to risk. The function must be monotonic increasing, with *f*(0) = 0, and *f*(1) = 1. If Chooser is risk-averse, then typically *f*(*x*) < *x*.\n\nBuchak's view reduces to the orthodox view if *f*(*x*) = *x*. I'm going to argue that given one very natural constraint, we can show that *f*(*x*) must indeed equal *x*. I'm far from the first to make an argument on these lines; I think the arguments that @Briggs2015 and @Thoma2019 make for the same conclusion are also successful. What's novel about what I'm going here is two-fold. First, the premise I'll use is, I think, weaker and more plausible than the premises used in other arguments. Second, and more importantly, I'll be using the same premise to resolve problems involving demons as to argue against Buchak's view. A big aim of this paper is to bring different parts of contemporary decision theory together. As a quick glance at the literature will tell you, there isn't much overlap between work on views like Buchak's and work on problems involving demons, though both of them are large literatures. This is a mistake, and one I'm hoping to help rectify here.\n\n## Non-Linearity\n\nStandard approaches to decision theory assign to Chooser a probability function and a utility function, both defined over (some) propositions. The domain of each function is some subset of the reals; the interval \\[0,1\\] for the probability, and some bounded interval for the utilities. The real numbers have a distinctive topology. Among other things, they are totally ordered: for any two numbers, either one is greater, or they are equal. So assuming that probabilities and utilities are numerical involves assuming, among other things, that they are also totally ordered. That is, for any two propositions, the probability(/utility) of the first is either greater than, less than, or equal to, that of the other. Call this assumption Ordering.\n\nOrdering is controversial, both for probabilities and utilities. For probabilities, it has been criticised since Keynes's _Treatise on Probability_ [-@Keynes1921], and in recent times has been criticised by, among others, Peter @Walley1991 and James @Joyce2010. For utilities, the most prominent critic has been Ruth Chang [-@Chang2002; -@Chang2015].\n\nIt takes a little work to create a counterexample to Ordering. It's no good to just put forward two things and say it isn't clear which is larger. For one thing, it might simply be unknown which is larger. For another, they might be equal. We'll come back to the first concern in a bit. Ruth @Chang2002 points out a natural way to avoid the second complaint. Consider three options *A*, *B*, and *A*+, with the following features.\n\n- *A* and *B* concern different subject matters.\n- It is unclear whether the value of *A* or of *B* is larger. (The 'value' here could be either probability or utility.)\n- *A*+ is by design fractionally larger than *A*. If the value is utility, *A*+ could be *A* plus a cookie. If it is probability, *A*+ could be the disjunction *A or this lottery ticket wins*.\n- If *A* and *B* were equal in value, then since *A*+ is greater than *A*, *A*+ would be greater than *B*.\n- But it is also unclear whether *A*+ is greater than *B*.\n\nCall this the sweetening argument, since *A*+ is generated from *A* by making it a bit better, sweetening it.\n\nJust like there are many critics of Ordering, there are many defenders. @DorrEtAl2023 defend it on semantic grounds. Adam @Elga2010 argues that violations of Ordering for probabilities leads to susceptibility to a money pump. Johan @Gustafsson2022 makes a similar in favour of Ordering for utilities.\n\nEven critics of Ordering have noted its unintuitive characteristics. @BradleySteele2016 argue that violations of Ordering for probabilities leads to thinking it is acceptable to pay to avoid information.^[It's uncontroversial that in some cases we pay to avoid information, e.g., we take efforts to avoid spoilers for movies. Even if the information doesn't change the value of the final product, we might pay to avoid it if the information is not partitional [@Das2023], or we don't know we'll conditionalise [@Nethnd]. But if none of these three conditions are met, and probabilities and utilities satisfy Ordering, we should never pay to avoid information [@Blackwell1951].] Harvey @Ledermannd argues that violations of Ordering for utilities leads to violations of a principle he calls Negative Dominance.\n\n> **Negative Dominance**    \n> It’s rationally required that: if [one] strictly prefers\none game of chance to another, one prefers one of the prizes that the\nfirst might yield, to one of the prizes that the second might yield.\n\nBoth Bradley and Steele, and Lederman, think that ultimately Ordering should be rejected, and we should live with these unintuitive results. They are both pointing out troubling features of their own view. (Something philosophers should do more often.)\n\nIn each case it isn't hard to convert the argument they give to a problem for the other kind of Ordering violation. If Ordering fails for utilities, a Bradley and Steele-style argument shows that it is worth paying to avoid information, and if it fails for probabilities, a Lederman style argument shows that Negative Dominance fails.\n\nI'm going to offer a new defence of Ordering violations. The defence has two parts. First, I'll argue that even if Ordering holds for probabilities and for values of states, it does not hold for values of actions. A bit loosely, even if Ordering is true for preferences over ends, it isn't true for preferences over means. This shows we have independent reason to reject any principle that entails Ordering is true in general. That includes Negative Dominance^[Negative Dominance doesn't on its own entail Ordering, but it does in conjunction with some other principles that I accept, and indeed will be indirectly defending in this paper.], and the semantic principles Dorr et al endorse. Second, I'm going to argue that many of the criticisms of views that permit Ordering violations presuppose a false view about how rational dynamic choice works. This is how I'll respond to Elga, Gustafsson, and Bradley and Steele.\n\n## Dynamic Choice {#sec-dynamic-choice}\n\nOn that note, it's time to introduce the last of our four problems - what the general theory of rational dynamic choice should look like. First, I'll set up how I'm conceiving of dynamic choice situations.\n\nFor the purposes of this paper, a **decision tree** is a sextuple ⟨*W*, *R*, *V*, *a*, *I*, Pr⟩ such that:\n\n- *W* is a finite set of nodes. One of these nodes, call it *o* for origin, is designated as the initial node.\n- *R* is a relation on *W* such that for any *x* ∈ *W*, ¬*xRo*, and if *y* ≠ *o*, there is a unique *x* such that *xRy*. Intuitively, the decision problem starts at *o*, and continues by moving from a node *x* to another node *y* such that *xRy* until there is nowhere further to go. Say that *x* is a predecessor of *y* if *xR+y*, where *R+* is the ancestral of *R*.\n- *V* is a value function. It maps each terminal node of *W* to a real number. A node *x* is a terminal node iff there is no *y* such that *xRy*.\n- *a* is a function from non-terminal nodes in *W* to the set \\{C, D, N\\} that says who the agent is for each node. Intuitively, C is for Chooser, D is for Demon, and N is for Nature. That agent 'chooses' where the game goes next.\n- *I* is a partition of the nodes the non-terminal nodes *x: a(x) =*C. The elements of this partition are called information sets. Intuitively, when Chooser reaches a node where they must choose, they know that they are in one member of this partition, i.e., one information set, and nothing more. Any two nodes in the same information set have the same number of outbound links.\n- Pr is a conditional probability function. It says that given a _strategy_ for Chooser, and that a particular non-terminal node *x* which is assigned to Demon or Nature has reached, what the probability is that we'll move to some further node *y* such that *xRy*. If *x* is assigned to Nature, this probability is independent of Chooser's strategy.\n\nA **strategy** for one of the three players, Demon, Chooser or Nature, is a function from all the nodes in the tree which are assigned to them, to the move they will make if that node is reached.^[It doesn't matter much for our purposes, but note that in general a strategy includes what to do if one reaches a node that is ruled out by one's own prior choices.] Given any decision tree, one can generate a **strategic decision problem** where the possible actions are strategies for Chooser, and the states are pairs of strategies for Demon and strategies for Nature. One question that will be central \n\nThere are two standard positions in philosophy for how to navigate decision trees. The **resolute** view says that Chooser should use the correct static theory of choice to pick a strategy at the start, and then resolutely stick with it. The **sophisticated** theory says that Chooser should take each node as a new choice, treat their past choices as fixed, and treat their future choices as another more-or-less knowable part of the world, and do whatever is best given those constraints. My view is that both of these are wrong.\n\nThe **dual mandate** approach, which I favour, says that Chooser should adopt a strategy that makes sense and stick to it, just like the resolute theory says, *and* Chooser should make choices that make sense at each point, just like the sophisticated theory says. It disagrees with the two existing theories on two counts. First, it denies that either provides a sufficient theory for a sequence of choices being rational. Second, it says that if Chooser adopts a plan that makes sense now, and will continue to make sense at each node conditional on reaching that node, Chooser does not have to regard the future as unknowable. Rather, Chooser can know that they will keep following the sensible plan they have adopted. The point is not just that Chooser knows they will continue to be rational. If Chooser has many rational choices, once they adopt one, Chooser can know they'll stick to it.\n\nThis leads to the first reason for adopting the Dual Mandate view: it respects the distinctive relationship that holds between time-slices of the same person. On the resolute view, later stages of Chooser regard earlier stages as their Lord and Master, dictating what to do even if it no longer makes sense. On the sophisticated view, later stages of Chooser regard earlier stages as just someone that they used to know. As @Stalnaker1999 points out, neither of these feels right; we want something between those two pictures. Now this doesn't entail that the Dual Mandate view is right, since there are a lot of theories that are intuitively between the two pictures. But it should suggest that we look for something like the Dual Mandate view.\n\nThe second argument for it is that widely adopted in other disciplines. In most textbook presentations of game theory, the first solution concept for dynamic games that gets introduced is subgame perfect equilibrium. This idea traces back to @Selten1965. It says that in an equilibrium, all players will adopt strategies that are in equilibrium over the whole game, and which are in equilibrium when restricted to 'subgames'.^[A subgame of the original game is the set of all nodes reachable from a particular node that is in a single information set, with all the other properties and relations of those nodes held fixed.] The Dual Mandate View is my attempt to translate this idea into decision theoretic language. But what I want to stress here is that the idea that choices should be rational both at a time, and over time, is completely uncontroversial in game theory; it's just presented in the textbooks as the way to solve dynamic games.\n\nThe third argument is that decision theorists appeal to something like the Dual Mandate View already. Jack @Spencer2023 argues that (some versions of) Causal Decision Theory are \"dynamically inconsistent\". By that he means that there are some decision trees where the target version of CDT, plus the sophisticated approach to rational choice, ends up selecting a choice that is strictly worse than an available choice. Spencer's example relies on the unavailability of mixed strategies, and I don't think his targets should accept that assumption. So I don't think his overall argument works. But I do think the reasoning he uses is correct. If a sequence of choices leaves one necessarily worse off than some other available sequence of choices, that shows the first sequence was bad. But why does this show the first sequence is bad, rather than just, say, unlucky? The Dual Mandate View has an answer to this question; sequences of choices must be part of rationally playable strategies, and dominated strategies are not rationally playable.\n\n# The Single Choice Principle\n\n## Equivalence Principles\n\nIn @sec-dynamic-choice, I set out two ways of describing certain problems: as decision trees and as strategy tables. You might wonder whether this is just notational variation. Do the tree and its associated table represent the same problem? Or, at least, do they represent problems that have the same answers for deep reasons.\n\nAt first glance, the answer to this question is obviously no. The problems have different kinds of answers. Consider the following game, which I'll call Non-Credible Threat.^[The example is based on the example Wikipedia uses to illustrate the game-theoretic concept of a non-credible threat @wiki-non-credible.]\n\n1. First, Demon chooses Left or Right.\n2. Then, after Demon's choice is revealed, Chooser selects Up or Down.\n3. If Chooser selects Down, they get 1. If Chooser selects Up, they get 3 if Demon chose Left, and 0 if Demon chose Right.\n4. Demon's choice was driven by their (very accurate) prediction of Chooser's strategy. If they predicted Chooser would adopt the strategy (Up-if-Left, Up-if-Right), they chose Left; otherwise they chose Right.\n\n@fig-first-dynamic is the tree for Non-Credible Threat, and @tbl-first-dynamic is the strategy table for it. (In the table, and from now on when discussing this game, I'll use XY to mean the strategy (X-if-Left, Y-if-Right). So UU is the strategy of going Up whatever Demon does.) The way to read figures like @fig-first-dynamic is to start at the open circle. In this case, it's in the middle of the figure. That's the origin of the game. Lines between nodes show what can be reached from one node. These are directional, but I won't include arrows because the position of the origin determines the direction. The circle nodes are points where a choice is to be made, and each such node is labelled with who makes the choice. The square nodes are terminal nodes, and they are labelled with values showing Chooser's payout if that node is reached. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Tree Diagram of the Non-Credible Threat game.](four-prob_files/figure-html/fig-first-dynamic-1.png){#fig-first-dynamic width=384}\n:::\n:::\n\n\n|          |   **PUU**    |    ¬**PUU**  |\n|:--------:|:------------:|:------------:|\n|  **UU**  |     3        |     0        |\n|  **UD**  |     3        |     1        |\n|  **DU**  |     1        |     0        |\n|  **DD**  |     1        |     1        |\n\n: Strategy table for @fig-first-dynamic, the Non-Credible Threat game. {#tbl-first-dynamic}\n\nCall a *run* through the game a sequence of moves from the origin to a terminal node. A run through the game is *rational* iff every move Chooser makes is rational. Now we can phrase the questions from the opening paragraph of this section a bit more precisely. What is the relationship between rational runs through trees like @fig-first-dynamic, and rational strategies in tables like @tbl-first-dynamic? And we can see an immediate complication. In any run through the tree, Chooser makes one binary selection; but to select a strategy is to make two binary selections.\n\nThere is a simple way around this problem. (What I'm about to describe is too simplistic for many purposes, but it will do for ours.) Say that a tree and its associated table are *dynamically-strategically equivalent*, for short *ds-equivalent*, iff the following two conditions are met.\n\n1. For any rational run through the tree, there is a rational strategy in the table that agrees with the run on what to do at nodes where Chooser actually made choices during the run.\n2. For any rational strategy in the table, any run that follows this strategy is rational.\n\nThen say a class of tree-table pairs is ds-equivalent iff every member of the class is. With those definitions on board, we can ask a bunch of questions.\n\n- Is the class of all tree-table pairs ds-equivalent?\n- Is the class of all tree-table pairs that don't involve demons ds-equivalent?\n- Is the class of all tree-table pairs where Chooser moves at most once in each run ds-equivalent?\n\nAnd this list is obviously not exhaustive.\n\nA positive answer to the first question would render all the other questions redundant. And a positive answer there is not completely implausible. But most decision theorists would answer it negatively. Many would say that @fig-first-dynamic/@tbl-first-dynamic is already a counterexample. In @fig-first-dynamic the only rational strategy is UD. By the time Chooser moves, there is no uncertainty; Chooser just selects the larger or the smaller value, and larger is better. On the other hand, many theories say that UU is a rational strategy in @tbl-first-dynamic. Evidential Decision Theory says this, but so do some Causal Decision Theories. Whether this is plausible or not turns on tricky questions about the normative significance of Weak Dominance reasoning, and on whether we should thin of Demon as perfectly accurate or just arbitrarily accurate. For what it's worth, I think the only rational move in @tbl-first-dynamic is UD, so I think this pair is ds-equivalent. But I do not think that's obvious, and I certainly don't think it could be a premise in an argument for or against any decision theory.\n\nThere is a much more restricted ds-equivalence claim that can properly serve as a premise in reasoning about decision theory, and it's time to introduce it.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
---
title: "Against Inconclusive Updating Rules"
abstract: |
  Many formal epistemologists think that there should be a rule for how rational agents update their credences when some part of their credal state is adjusted in a way that does not involve learning, with certainty, some proposition. The most popular such rule is Jeffrey conditionalization. This note argues that no such rule is correct. The only rational rule of update is that one should conditionalize on one's evidence.
date: June 11 2024
draft: true
author:
  - name: Brian Weatherson 
    url: http://brian.weatherson.org
    affiliation: University of Michigan
    affiliation_url: https://umich.edu
    orcid_id: 0000-0002-0830-141X
citation: false
categories:
  - games and decisions
  - epistemology
  - unpublished
bibliography: ../../../brian-quarto.bib
csl: ../../../chicago-with-note.csl
format:
  html: default
  docx:
    reference-doc: ../../../quarto-articles-template.docx
  pdf:
    output-file: "Against Inconclusive Updating Rules"
    include-after-body: 
      text: |
         \noindent Unpublished. Posted online in 2024.
---

# Introduction {#sec-intro}

The two central commitments of Bayesian epistemology are that credences should obey the probability calculus, and that they should be updated by conditionalization on new evidence. But many Bayesian epistemologists hold that there are other rules too. After all, they say, conditionalization has its limits. To update on *E* means to move one's credence in *E* to 1, and hence to be willing to bet on *E* at any odds. Since there are few if any propositions that we would bet on at any odds, there isn't much evidence in this strong sense. So something else is needed. In particular, they say, we need extra rules that say what to do when some other change is externally imposed on a credal state. The simplest such imposition is that some proposition *p* has its credence set to be *x*. In that case the most popular rule is Jeffrey conditionalisation^[For more details on Jeffrey conditionalization, see Lin [-@Lin2022 sec 5.3]]:

$$
\Pr_{\text{New}}(q) = x\Pr(q | p) + (1-x)\Pr(q | \neg p)
$$

There are a couple of initial inconclusive reasons to be sceptical of the possibility of any such rule. 

One reason, loosely following @Williamson2000, says that all updating involves learning, learning involves acquiring evidence, evidence is propositional, and an externally imposed move of one's credences is not the acquisition of a proposition.^[Though note that someone else has a particular credence is a proposition. This will become important in @sec-defer.] Now as an argument against the importance of Jeffrey conditionalisation, this is question-begging at every step. Still, question-begging arguments can be valuable.^[As @Lewis1982c points out, the most powerful argument against dialethism is blatantly question-begging.] They can help those of us who are antecedently disposed to reject a view see where our rejection is coming from. But it would be nice to turn that into an argument.

A second reason, loosely following @vanFraassen1981, says that this rule is too much of a special case. Unconditional credences are special cases of two things. They are expectations of a very special kind of random variable: one whose value is 1 when a particular proposition is true, and 0 otherwise. And they are conditional credences conditional on a trivial proposition. We might hope that any rule around here would fall out as a special case of a plausible rule that is general enough to cover these more general cases. The absence of such a rule from the literature might give one pause here. But again, this is hardly a conclusive argument. Maybe someone will develop such a rule tomorrow.

There are a couple of natural ways that one might try to build such a rule: the way of deference, and the way of accuracy. Unfortunately, they both don't work. And they don't work in ways that undermine the normative force of any rule that is restricted just to changes in the credence of a single, unconditional, proposition. Let's take these in turn.

# Deference {#sec-defer}

Start with the following natural thought. What one should do when one changes one's credence in *p* to *x* is what one would do if one learned that an expert to whom one properly defers has credence *x* in *p*. This approach has two attractive features. First, it is easy to see its justification. Even a sceptic who only believed that updating goes by learning could agree that sometimes one should act as if one had learned something about an expert. Second, it promises to generalise. One could also ask about what credences one would have if one learned the expert's credence in *p* given *q* was *x*, or one learned the expert's expected value for a particular random variable was *x*.

The rule requires that one be able to identify experts to whom one properly defers. But here again there is a simple model. Let Thinker be the person doing the deferring. Let Experimenter be someone who was a cognitive duplicate of Thinker just before conducting an experiment, and now they have conducted the experiment and learned the result.^[The underlying picture I'm using here is taken from @Blackwell1951.] Assume the evidence Experimenter gets is luminous; they know what they learned, and they know what they didn't learn. So we don't have any concerns about learning having negative value, as happens when evidence is not luminous (@Dasnd). Then, given the venerable rule that one should use as much evidence as possible (@Hosiasson1931; @Torsellnd), Thinker should defer to Experimenter.

All this seems fine, but the problem is that we haven't uniquely specified an Experimenter here. Different background assumptions about what the Experimenter is like might lead to different posterior credences if following this rule. So any general rule, like Jeffrey conditionalization, which is silent about the nature of the experimenter, cannot be justified this way. Let's see this with an example.

Let X, Y and Z be normal distributions with mean 0 and variance 1. In symbols, each of them is $\mathcal{N}$(0,1). So the sum of any two of them has distribution $\mathcal{N}$(0,2), and the sum of all three has distribution $\mathcal{N}$(0,3). Let *p* be the proposition that this sum, X + Y + Z, is positive, and *q* be the proposition that the sum is greater than 1. Let C be a probability function that incorporates all these facts, but has no other direct information about X, Y, and Z. So C(*p*) = ½, since in all respects C's opinions are symmetric around 0.

C knows some things about A and B. Both of them know everything C knows about X, Y, Z, and each are logically and mathematically omniscient, and know precisely what evidence they have. Also, A just conducted an experiment that revealed the value of X, and nothing else, and B conducted an experiment that revealed the value of X + Y, and nothing else. Both A and B are experts for C, in the above sense. What we'll do now is work out what C's posterior credence in *q* should be after first learning that A(*p*) = 0.6, and second learning that B(*p*) = 0.6.

We can work out the value of X, which we'll call *x*, from A(*p*) = 0.6. In what follows, $\Phi$(*x*) is the cumulative distribution for the standard normal distribution, i.e., for $\mathcal{N}$(0,1), and $\Phi$^-1^ is its inverse. If X = *x*, then *p* is true iff Y + Z > -*x*. Since Y + Z is a normal distribution with mean 0 and variance 2, i.e., standard deviation $\sqrt{2}$, the probability of this is $\Phi$($\frac{x}{\sqrt{2}}$). So *x* = $\sqrt{2}\Phi$^-1^(0.6), which is about 0.358. So A(*q*) is the probability that Y + Z is greater than 1 - *x*, i.e., about 0.642. Since the distribution of Y + Z is $\mathcal{N}$(0,2), we can work out this probability: it is very close to 0.325. So if C learns that A's credence in *p* is 0.6, C's new credence in *q* should be about 0.325.

We can also do the same calculation for B(*p*) = 0.6. Since the only thing B doesn't know is Z, and Z has distribution $\mathcal{N}$(0,1), it must be that X + Y has value $\Phi$^-1^(0.6), which is about 0.253. So for B, the probability of *q* is the probability that Z is greater than (about) 0.747. That is, B(*q*) is about 0.228. So if C learns that B's credence in *p* is 0.6, C's new credence in *q* should be about 0.228.

Now we have a problem. There is no such thing as *the* credence in *q* that an expert to whom C should defer would have, if their credence in *p* was 0.6. If that expert knows X, the posterior credence in *q* is around 0.325; if the expert knows X + Y, the posterior credence in *q* is around 0.228. For what it's worth, neither of these is the number you'd get by Jeffrey conditionalising C's prior credence with the condition that *p* goes to 0.6; that would take *q* to around 0.338.

So the way of deference doesn't work. The idea seems right; see what an expert who does this would do. But there are too many experts who do *this*, and they differ in other respects. Let's move on to a different strategy.

# Accuracy {#sec-accuracy}

A popular move in recent years, tracing back to @Joyce1998, has been to justify probabilistic principles by appeal to accuracy. As @GreavesWallace200x show, we can justify ordinary conditionalisation this way. And there is a natural way to try to justify inconclusive updates too. Assume Thinker starts with some credence function, and consider the class of probabilistically coherent credence functions that satisfy some constraint. Ask which of them has highest expected accuracy given Thinker's prior credences. That should be the one that Thinker adopts after updating on the new constraint.

Here's a simple example of how this might work. Say that there are three exclusive and exhaustive possibilities: *p*, *q*, and *r*. Initially Thinker has credence 1/6 in *p*, 1/6 in *q*, and 2/3 in *r*, and the new constraint is that *p* moves to 0.3. Assume for now that we're using the Brier score to measure inaccuracy. If the credence in *p* is fixed, there is only one degree of freedom left. Let *c* be Thinker's new credence in *r*, so their credence in *q* is 0.7-*c*. Then the expected inaccuracy 
What are we doing when we do decision theory? What questions are decision theorists trying to answer? And what do we gain by knowing those answers?

Most philosophical decision theorists would answer these questions with something like the view I'll call **prescriptivism**. We're trying to say how people should ideally decide, and knowing how people should decide will help people make better decisions. On this model, the decision theorist is like a high school coach saying "Here's how the good ones make decisions, now go out and act like them."

I'm going to argue against prescriptivism, and in favour of **explanationism**. According to explanationism, decision theory is primarily a descriptive project. It says this is how people actually make decisions, at least approximately, some of the time. What we gain from this is access to a certain kind of explanation of various social phenomena. On this model, the decision theorist is like a high school science teacher saying "Here's how things work in a very simplified model, but even this simplified model shows you something about the world."

The big difference between the two views comes in how they treat the idealisations involved in decision theory. I'll spend some time in this paper on just what those idealisations are, though hopefully the point that there are some is common ground. On the prescriptivist view, when we say that a decision maker ideally does X, we mean that doing X is perfect, and we should aim to be a more perfect deciders. On the explanationist view, the idealisations are models.^[Whether they are what @Weisberg2007 calls "Galilean idealisations" or "minimal idealisations" depends on just what use they are being put to. The ones that I'm going to talk about are primarily the latter.] These models are not standards of perfection. We don't think the fact that molecules in the toy model used to derive the ideal gas law are point-sized mean that having zero volume is a perfection, or that it is better for molecules to be smaller. It's just that their size is unimportant for the phenomena we're explaining, and so we abstract away from it.

I'll start with two reasons for rejecting the prescriptivist view. One is that it does not treat like cases alike. In particular, it makes a distinction between empirical uncertainty and arithmetic uncertainty that it should not, were it a prescriptive theory. The second is that it would be bad to resemble the 'ideal' in some respects. To play its prescriptive role, decision theory would have to be supplemented with a theory about which aspects of the ideal are and are not worthy of emulation. There isn't much work on this question, and it isn't at all clear that it would be easier to solve it than to directly provide workable advice.

Setting out the objections to prescriptivism is relatively easy. Defending explanationism is harder going. There are three obvious objections. The first is that it's a bad description, since people obviously don't act the way decision theory says they do. This, I think, is a fairly weak objection and I won't spend much time on it. People do not act exactly like the theory says, but no model describes exactly how the subject behaves. As the cliche goes, all models are wrong, but some models are useful.

This leads to the second objection, which is much more worrying. This model, says the objector, is not in fact useful. Sure it tells us that people who prefer vanilla ice cream are more likely to order it. But we didn't need a whole field of philosophy to tell us that. What are the interesting explanations the theory offers? Here I follow a suggestion from Kate @Vradenbergh2025. The interestiing explanations around here are not individual, but social. Vradenbergh uses the model of segregation in @Schelling19xx as her main example.^[This is also used as one kind of paradigm model by @Weisberg2013, so it clearly fits in nicely with the picture of decision theory as modeling.]

I'm going to focus on a closely related but distinct class of explanations, what I'll call **reflexive explanations**. A reflexive explanation is one where the fact that agents in the model believe that the model is correct is partially responsible for the model working. Most game-theoretic explanations are reflexive in this sense. The main example I'll use is the model of the used car market that George @Akerlof1970 developed, though I'll also look at some recent work on the software industry. In both cases I'll argue that decision theory is a vital input into interesting explanations.

The third objection is that if decision theory earns its keep in the explanation of empirical regularities, like the surprisingly low price of used cars, it's odd that it is such an a priori discipline. Here the fact that it plays a role in explanations that are reflexive will be crucial. It's only the fact that decision theory is putatively a theory of rational choice that makes it fit to enter into reflexive explanations.
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.479">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">
<meta name="dcterms.date" content="2018-06-29">
<meta name="description" content="Pragmatic encroachment theories have a problem with evidence. On the one hand, the arguments that knowledge is interest-relative look like they will generalise to show that evidence too is interest-relative. On the other hand, our best story of how interests affect knowledge presupposes an interest-invariant notion of evidence. The aim of this paper is to sketch a theory of evidence that is interest-relative, but which allows that ‘best story’ to go through with minimal changes. The core idea is that the evidence someone has is just what evidence a radical interpreter says they have. And a radical interpreter is playing a kind of game with the person they are interpreting. The cases that pose problems for pragmatic encroachment theorists generate fascinating games between the interpreter and the interpretee. They are games with multiple equilibria. To resolve them we need to detour into the theory of equilibrium selection. I’ll argue that the theory we need is the theory of risk-dominant equilibria. That theory will tell us how the interpreter will play the game, which in turn will tell us what evidence the person has. The evidence will be interest-relative, because what the equilibrium of the game is will be interest-relative. But it will not undermine the story we tell about how interests usually affect knowledge.">

<title>Online Articles - Brian Weatherson - Interests, Evidence and Games</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://use.typekit.net/uzz2drx.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Online Articles - Brian Weatherson</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://brian.weatherson.org"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/bweatherson.bsky.social"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Interests, Evidence and Games</h1>
                  <div>
        <div class="description">
          <p>Pragmatic encroachment theories have a problem with evidence. On the one hand, the arguments that knowledge is interest-relative look like they will generalise to show that evidence too is interest-relative. On the other hand, our best story of how interests affect knowledge presupposes an interest-invariant notion of evidence. The aim of this paper is to sketch a theory of evidence that is interest-relative, but which allows that ‘best story’ to go through with minimal changes. The core idea is that the evidence someone has is just what evidence a radical interpreter says they have. And a radical interpreter is playing a kind of game with the person they are interpreting. The cases that pose problems for pragmatic encroachment theorists generate fascinating games between the interpreter and the interpretee. They are games with multiple equilibria. To resolve them we need to detour into the theory of equilibrium selection. I’ll argue that the theory we need is the theory of <strong>risk-dominant equilibria</strong>. That theory will tell us how the interpreter will play the game, which in turn will tell us what evidence the person has. The evidence will be interest-relative, because what the equilibrium of the game is will be interest-relative. But it will not undermine the story we tell about how interests usually affect knowledge.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">epistemology</div>
                <div class="quarto-category">interest-relativity</div>
                <div class="quarto-category">games and decisions</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="http://brian.weatherson.org">Brian Weatherson</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Michigan
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 29, 2018</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.1017/epi.2018.26">10.1017/epi.2018.26</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#encroachmentreductionandexplanation" id="toc-encroachmentreductionandexplanation" class="nav-link active" data-scroll-target="#encroachmentreductionandexplanation"><span class="header-section-number">0.1</span> Encroachment, Reduction and Explanation</a></li>
  <li><a href="#theproblemswithevidence" id="toc-theproblemswithevidence" class="nav-link" data-scroll-target="#theproblemswithevidence"><span class="header-section-number">0.2</span> The Problems with Evidence</a></li>
  <li><a href="#asimplebutunsatisfyingsolution" id="toc-asimplebutunsatisfyingsolution" class="nav-link" data-scroll-target="#asimplebutunsatisfyingsolution"><span class="header-section-number">0.3</span> A Simple, but Unsatisfying, Solution</a></li>
  <li><a href="#gamifyingtheproblem" id="toc-gamifyingtheproblem" class="nav-link" data-scroll-target="#gamifyingtheproblem"><span class="header-section-number">0.4</span> Gamifying the Problem</a></li>
  <li><a href="#equilibriumselectionprinciples" id="toc-equilibriumselectionprinciples" class="nav-link" data-scroll-target="#equilibriumselectionprinciples"><span class="header-section-number">0.5</span> Equilibrium Selection Principles</a></li>
  <li><a href="#objectionsandreplies" id="toc-objectionsandreplies" class="nav-link" data-scroll-target="#objectionsandreplies"><span class="header-section-number">0.6</span> Objections and Replies</a></li>
  <li><a href="#appendixone:carlssonandvandammesgame" id="toc-appendixone:carlssonandvandammesgame" class="nav-link" data-scroll-target="#appendixone\:carlssonandvandammesgame">Appendix One: Carlsson and van Damme’s Game</a></li>
  <li><a href="#appendixtwo:themodifiedgame" id="toc-appendixtwo:themodifiedgame" class="nav-link" data-scroll-target="#appendixtwo\:themodifiedgame">Appendix Two: The Modified Game</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>Pragmatic encroachment theories have a problem with evidence. On the one hand, the arguments that knowledge is interest-relative look like they will generalise to show that evidence too is interest-relative. On the other hand, our best story of how interests affect knowledge presupposes an interest-invariant notion of evidence.</p>
<aside>
<p>Published in <em>Episteme</em> 15: 329-344.</p>
Image by <a href="https://www.flickr.com/photos/13422316@N00/">Paul Wordingham</a> via <a href="https://search.creativecommons.org/photos/457692bc-4ca5-4f3d-a964-d0840e034e3a">Creative Commons</a>.
</aside>
<p>The aim of this paper is to sketch a theory of evidence that is interest-relative, but which allows that ‘best story’ to go through with minimal changes. The core idea is that the evidence someone has is just what evidence a radical interpreter says they have. And a radical interpreter is playing a kind of game with the person they are interpreting. The cases that pose problems for pragmatic encroachment theorists generate fascinating games between the interpreter and the interpretee. They are games with multiple equilibria. To resolve them we need to detour into the theory of equilibrium selection. I’ll argue that the theory we need is the theory of <strong>risk-dominant equilibria</strong>. That theory will tell us how the interpreter will play the game, which in turn will tell us what evidence the person has. The evidence will be interest-relative, because what the equilibrium of the game is will be interest-relative. But it will not undermine the story we tell about how interests usually affect knowledge.</p>
<section id="encroachmentreductionandexplanation" class="level3 page-columns page-full" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="encroachmentreductionandexplanation"><span class="header-section-number">0.1</span> Encroachment, Reduction and Explanation</h3>
<p>I will start with an argument for a familiar disjunctive conclusion: either knowledge is interest-relative, or scepticism is true. The argument will resemble arguments to the same disjunctive conclusion in <span class="citation" data-cites="Hawthorne2004">Hawthorne (<a href="#ref-Hawthorne2004" role="doc-biblioref">2004</a>)</span> and <span class="citation" data-cites="FantlMcGrath2009">Fantl and McGrath (<a href="#ref-FantlMcGrath2009" role="doc-biblioref">2009</a>)</span>. Indeed, it is inspired by those discussions. But it uses less controversial premises than previous versions.</p>
<p>The argument starts by considering a game, one I’ll call the red-blue game. Here are the rules of the game.</p>
<ol type="1">
<li><p>Two sentences will be written on the board, one in red, one in blue.</p></li>
<li><p>The player will make two choices.</p></li>
<li><p>First, they will pick a colour, red or blue.</p></li>
<li><p>Second, they say whether the sentence in that colour is true or false.</p></li>
<li><p>If they are right, they win. If not, they lose.</p></li>
<li><p>If they win, they get $50, and if they lose, they get nothing.</p></li>
</ol>
<p>Our player is Parveen. She is an epistemologist who works on pragmatic encroachment, and (as will become important in a minute), she has frequently cited both <em>Knowledge and Lotteries</em> &nbsp;<span class="citation" data-cites="Hawthorne2004">(<a href="#ref-Hawthorne2004" role="doc-biblioref">Hawthorne 2004</a>)</span>, and <em>Knowledge and Practical Interests</em> &nbsp;<span class="citation" data-cites="Stanley2005">(<a href="#ref-Stanley2005" role="doc-biblioref">Stanley 2005</a>)</span>. She knows the rules of the game, and no other relevant facts about the game. When the game starts, the following two sentences are written on the board, the first in red, the second in blue.</p>
<ol type="1">
<li><p>Two plus two equals four.</p></li>
<li><p><em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em>.</p></li>
</ol>
<p>Intuitively, there is a unique rational play in this game: Red-True. That is, Parveen announces that she will evaluate the truth value of the red sentence, and then announce that it’s true. That’s a sure $50.</p>
<p>On the other hand, in normal circumstances, we would say that Parveen does know that <em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em>. After all, she has looked up their publication dates many times in checking over her papers.</p>
<p>There is a puzzle in reconciling these intuitions. The pragmatic encroachment theorist has a solution to these puzzles. In normal circumstances, Parveen does know that <em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em>. But these are not normal circumstances. Right now, it matters whether her reason to believe that <em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em> is as strong as her reason to believe that two plus two equals four. And (unless something very weird is happening), that isn’t true for Parveen. So she knows that red-true will win, she doesn’t know any other play will win, so she should play Red-True.</p>
<p>If we reject pragmatic encroachment, and we are not sceptics, we should say that Parveen does know that <em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em>. And then it is a mystery why playing Red-True is more rational than playing Blue-True. After all, Parveen knows the rules of the game, and she knows (by hypothesis) the blue sentence is true, so if she can do even basic logical reasoning in a knowledge preserving way, she knows she will get as good a result as possible by playing Blue-True. So it is a bit of a mystery why it would be anything other than maximally rational to play Blue-True.</p>
<p>One way we might try to resolve this mystery is by saying that although Parveen knows that Blue-True will win $50, she super-knows that Red-True will win $50. What do we mean here by <em>super knowledge</em>? Think of this as a placeholder for certainty, or knowledge that one knows, or anything other epistemic state that you think might be relevant to her practical decision making. Perhaps the fact that she super-knows what two plus two is, but doesn’t super-know when the epistemology books were published, could be the explanation for why Red-True is the unique rational play.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;That we need some kind of super-knowledge for action, and not mere knowledge, is a popular, and natural, explanation of the case. For versions of this explanation, obviously with more details than I’ve given here, see for example Jessica <span class="citation" data-cites="Brown2008">Brown (<a href="#ref-Brown2008" role="doc-biblioref">2008</a>)</span> and Jennifer <span class="citation" data-cites="Lackey2010">Lackey (<a href="#ref-Lackey2010" role="doc-biblioref">2010</a>)</span>.</p></li></div><p>But no such explanation can work, because Parveen doesn’t super-know that playing Red-True will win $50. She super-knows that two plus two is four. But we have not assumed that she super-knows the rules of the game. So she doesn’t super-know that Red-True will win, she just knows it. And she also, by hypothesis, knows that Blue-True will win. So looking at any kind of super-knowledge can’t break the intuitive asymmetry between Red-True and Blue-True.</p>
<p>Put another way, if Parveen knows that <em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em>, then she knows that she is playing the following game.</p>
<ol type="1">
<li><p>Two sentences will be written on the board, one in red, one in blue.</p></li>
<li><p>The player chooses to play either Blue-True, Blue-False, Red-True, or Red-False.</p></li>
<li><p>If they play Blue-True, they win $50.</p></li>
<li><p>If they play Blue-False, they win nothing.</p></li>
<li><p>If they play Red-True, they win $50 if the red sentence is true, and nothing otherwise.</p></li>
<li><p>If they play Red-False, they win $50 if the red sentence is false, and nothing otherwise.</p></li>
</ol>
<p>And is is rational to play Blue-True in that game. (It might also be rational to pay Red-True depending on what the red sentence is, but it is always rational to play Blue-True.) Yet it is not rational to play Blue-True in the original game. So Parveen does not know, when she plays the original game, that <em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em>.</p>
<p>So to avoid pragmatic encroachment here we must deny that Parveen ever knew that <em>Knowledge and Lotteries</em> was published before <em>Knowledge and Practical Interests</em>. On its own, that’s not a sceptical conclusion: lots of people don’t know that. But once we go down that path, it looks like not much knowledge will be left. After all, we can repeat the game with any number of different things in the place of the blue sentence. If we adopt the constraint that Parveen only knows <em>p</em>, right now, if it is rationally permissible for her to play Blue-True when <em>p</em> is the blue sentence, no matter what the red sentence is, then either we have to say very unintuitive things about rational plays of the game, or we have to say she knows very little.</p>
<p>So we’ve got the conclusion that either pragmatic encroachment is true, or scepticism is true. Since I’m not a sceptic, I’m happy to conclude that pragmatic encroachment is true. But note that we’ve done this without any reference to high stakes situations. The stakes in Parveen’s game are just $50. That’s not nothing, but it’s not ‘high stakes’ in the way that phrase is normally used.</p>
<p>The version of pragmatic encroachment we get is that what matters for knowledge are not the stakes involved in any bet on <em>p</em>, but the odds.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Parveen loses knowledge because she is being asked, in effect, to make a super long odds bet on a fact about publication schedules. She is in no position to rationally make a bet at those odds. So she doesn’t know the fact about publication schedules.</p>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Jessica <span class="citation" data-cites="Brown2008">Brown (<a href="#ref-Brown2008" role="doc-biblioref">2008, 176</a>)</span> shows that pragmatic encroachment theories that rely just on the stakes involved are subject to serious counterexample. Katherine <span class="citation" data-cites="Rubin2015">Rubin (<a href="#ref-Rubin2015" role="doc-biblioref">2015</a>)</span> argues that if we have a ‘global’ version of pragmatic encroachment, where all our epistemic notions are interest-relative, then it is implausible that it is the stakes the subject faces that matter for knowledge. Since I’m defending such a global version of pragmatic encroachment, Rubin’s arguments show that it is important that I’m relying on odds, not stakes. Baron <span class="citation" data-cites="Reed2014">Reed (<a href="#ref-Reed2014" role="doc-biblioref">2014</a>)</span> argues that if it is stakes alone that matter to pragmatic encroachment, then agents who the pragmatic encroachment theorist takes to be perfectly rational would be subject to a Dutch Book.</p></li></div><p>And that’s the general principle: agents only know a proposition if they are in a position to rationally bet on that proposition at the odds currently being offered to them. In practice, high stakes situations tend to feature bets at long odds, so in practice much knowledge dissipates in high stakes cases. But the explanation of the dissipation is the odds the agent faces, not the stakes.</p>
<p>More precisely, I endorse these principles as constraints on knowledge:</p>
<ul>
<li><p>If the agent knows that <em>p</em>, then for any question they have an interest in, the answer to that question is identical to the answer to that question conditional on <em>p</em>.</p></li>
<li><p>When an agent is considering the choice between two options, the question of which option has a higher expected utility given their evidence is a question they have an interest in.</p></li>
</ul>
<p>Those principles are meant to not merely be extensionally adequate. They are meant to explain why agents lose knowledge when considering some sets of options, like in the Red-Blue game. In some sense, they are meant to be part of reductive explanations. These reductive explanations take as primitive inputs facts about the agent’s evidence, and facts about evidential probability. I’m going to set aside worries about the metaphysics of evidential probability, and just focus on evidence. Because it turns out that there is a real problem in getting a plausible theory of evidence that can function as an input to that reductive explanation.</p>
</section>
<section id="theproblemswithevidence" class="level3 page-columns page-full" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="theproblemswithevidence"><span class="header-section-number">0.2</span> The Problems with Evidence</h3>
<p>Go back to the red-blue game. Consider a version of the game where:</p>
<ul>
<li><p>The red sentence is that two plus two equals four.</p></li>
<li><p>The blue sentence is something that, if known, would be part of the agent’s evidence.</p></li>
</ul>
<p>I’m going to argue that there are cases where the only rational play is Red-True, but the blue sentence is something we want to say that, ordinarily, the subject knows. And I’ll argue that this is a problem for the kind of reductive explanation I just sketched. If pragmatic effects matter to what the evidence is, we can’t take the evidence as a fixed input into an explanation of how and when pragmatic effects matter.</p>
<p>Let’s have Parveen play the game again. She’s going to be playing the game in a restaurant, one in Ann Arbor where she lives. Just before the game starts, she notices an old friend, Rahul, across the room. Rahul is someone she knows well, and can ordinarily recognise, but she had no idea he was in town. She thought Rahul was living in Italy. Still, we would ordinarily say that she now knows Rahul is in the restaurant; indeed that he is in the restaurant. It would be perfectly acceptable for her to say to someone else, “I saw Rahul here”, for example. Now the game starts.</p>
<ul>
<li><p>The red sentence is <em>Two plus two equals four</em>.</p></li>
<li><p>The blue sentence is <em>Rahul is in this restaurant</em>.</p></li>
</ul>
<p>Now we have a problem. On the one hand, there is only one rational play here: Red-True. If you haven’t seen someone for a long time, then you can’t be completely certain it’s them when you spot them across a restaurant. It would be foolish to be as confident that it’s Rahul as that two and two make four. It looks like this is a case where pragmatic effects defeat knowledge.</p>
<p>On the other hand, our story for why Parveen loses knowledge here has run into problems. I wanted to tell a story roughly like the following. She can’t play Blue-True when the probability of the blue sentence, given her evidence, is less than the probability of the red sentence, given her evidence. That explanation can only go through if the blue sentence is itself not part of her evidence, since the probability of anything given itself is one. So we need a story about how it is that it is not part of Parveen’s evidence that Rahul is not in the restaurant.</p>
<p>That story can’t be the one that presupposes facts about what is in Parveen’s evidence. So it can’t use facts about the probability of some proposition given her evidence; at least not in any simple way. If we can independently identify Parveen’s evidence, then we can go back to using evidential probability. But until we’ve done that, we’re stuck.</p>
<p>There are two options here that seem possible for the pragmatic encroachment theorist, but not particularly attractive.</p>
<p>One is to say that propositions like <em>Rahul is in this restaurant</em> are never part of Parveen’s evidence. Perhaps her evidence just consists of things like <em>I am being appeared to Rahul-like</em>. Such an approach is problematic for two reasons. The first is that it is subject to all the usual objections to psychological theories of evidence &nbsp;<span class="citation" data-cites="Williamson2007-WILTPO-17">(<a href="#ref-Williamson2007-WILTPO-17" role="doc-biblioref">Williamson 2007</a>)</span>. The second is that we can re-run the argument with the blue sentence being some claim about Parveen’s psychological state, and still get the result that the only rational play is Red-True. A retreat to a psychological conception of evidence will only help with this problem if agents are infallible judges of their own psychological states, and that is not in general true &nbsp;<span class="citation" data-cites="Schwitzgebel2008">(<a href="#ref-Schwitzgebel2008" role="doc-biblioref">Schwitzgebel 2008</a>)</span>.</p>
<p>Another option is to deny that a reductive explanation is needed here. Perhaps pragmatic effects, like the particular sentences that are chosen for this instance of the Red-Blue game, mean that Parveen’s evidence no longer includes facts about Rahul, but this isn’t something we can give a reductive account of. We shouldn’t assume that everything will have a simple reductive explanation, so this isn’t so bad in theory. The problem in practice is that without a reductive explanation, we don’t have a predictive theory of when pragmatic effects matter. And that seems to be a bad thing. For instance, the following theory is completely consistent with Parveen’s case as described.</p>
<ol type="1">
<li><p>E=K; i.e., one’s evidence is all and only what one knows.</p></li>
<li><p>Someone does not know <em>p</em> if the evidential probability of <em>p</em> is not close enough to one for current purposes.</p></li>
<li><p>Since it is part of Parveen’s evidence that Rahul is in the restaurant, the probability that he is there is one, so it is close enough to one for current purposes.</p></li>
<li><p>So this is not a case where pragmatic effects change what she knows.</p></li>
</ol>
<p>That theory seems to me to be badly mistaken, since it goes on to predict that it is rationally permissible to play Blue-True. But we need a pragmatic account that says that it is mistaken, and says something about which alternative situations would not threaten Parveen’s knowledge. We don’t yet, as far as I can see, have such an account. The aim of the rest of this paper is to provide one.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;You can read this paper as a reply to the challenge posed by <span class="citation" data-cites="IchikawaEtAl2012">Ichikawa, Jarvis, and Rubin (<a href="#ref-IchikawaEtAl2012" role="doc-biblioref">2012</a>)</span>. They note that there are challenges facing the pragmatic encroachment theorist whether they make evidence interest-relative, or interest-invariant. I’m going to show how to have an interest-relative theory of evidence, and keep what was desirable about pragmatic encroachment theories.</p></li></div></section>
<section id="asimplebutunsatisfyingsolution" class="level3" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="asimplebutunsatisfyingsolution"><span class="header-section-number">0.3</span> A Simple, but Unsatisfying, Solution</h3>
<p>Let’s take a step back and look at the puzzle more abstractly. We have an agent <em>S</em>, who has some option <em>O</em>, and it really matters whether or not the value of <em>O</em>, i.e., <span class="math inline">\(V(O)\)</span> is at least <span class="math inline">\(x\)</span>. It is uncontroversial that the agent’s evidence includes some background <span class="math inline">\(K\)</span>, and controversial whether it includes some contested proposition <span class="math inline">\(p\)</span>. It is also uncontroversial that <span class="math inline">\(V(O | p) \geq x\)</span>, and we’re assuming that for any proposition <span class="math inline">\(q\)</span> that is in the agent’s evidence, <span class="math inline">\(V(O | q) = V(O)\)</span>. That is, we’re assuming the relevant values are conditional on evidence. We can capture that last assumption with one big assumption that probably isn’t true, but is a harmless idealisation for these purposes. Say there is a prior value function <span class="math inline">\(V^-\)</span>, with a similar metaphysical status to the mythical, mystical prior probability function. Then for any choice <span class="math inline">\(C\)</span>, <span class="math inline">\(V(C) = V^-(C | E)\)</span>, where <span class="math inline">\(E\)</span> is the evidence the agent has.</p>
<p>Now we’re in a position to state a simple, but unsatisfying, solution. Let <span class="math inline">\(p\)</span> be the proposition that the agent might or might not know, and the question of whether <span class="math inline">\(V(O) \geq x\)</span> be the only salient one that <span class="math inline">\(p\)</span> is relevant to. Then the agent knows <span class="math inline">\(p\)</span> only if the following is true:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\frac{V^-(O | K) + V^-(O | K \wedge p)}{2} \geq x\)</span></p>
</blockquote>
<p>That is, we work out the value of <span class="math inline">\(O\)</span> with and without the evidence <span class="math inline">\(p\)</span>, and if the average is greater than <span class="math inline">\(x\)</span>, good enough!</p>
<p>That solves the problem of Parveen and Rahul. Parveen’s evidence may or may not include that Rahul is in the restaurant. If it does, then Blue-True has a value of $50. If it does not, then Blue-True’s value is somewhat lower. Even if the evidence includes that someone who looks a lot like Rahul is in the restaurant, the value of Blue-True might only be $45. Averaging them out, the value is less than $50. But you’d only play Blue-True if it was worthwhile it play it instead of Red-True, which is worth $50. So you shouldn’t play Blue-True.</p>
<p>Great! Well, great except for two monumental problems. The first problem is that what we’ve said here really only helps with very simple cases, where there is a single decision problem that a single contested proposition is relevant to. We need some way to generalise the case to less constrained situations. The second (and bigger) problem is that the solution is completely ad hoc. Why should we use the arithmetic mean of these two things rather than any other formula that would have implied the intuitively correct result in the Parveen-Rahul case? Pragmatic encroachment starts with a very elegant, very intuitive, principle: you only know the things you can reasonable take to be settled for the purposes of current deliberation. And that deliberation should be driven by the aim of maximising expected utility. It does not look like any such elegant, intuitive, principles will lead to some theorem about averaging out the value of an option with and without new evidence.</p>
<p>Happily, the two problems have a common solution. But the solution requires a detour into some technical work. It’s time for some game theory.</p>
</section>
<section id="gamifyingtheproblem" class="level3 page-columns page-full" data-number="0.4">
<h3 data-number="0.4" class="anchored" data-anchor-id="gamifyingtheproblem"><span class="header-section-number">0.4</span> Gamifying the Problem</h3>
<p>We can usefully think of some philosophical problems as games, and hence subjects for study using game theoretic techniques. This is especially when the problems involve interactions of rational agents. Here, for example, is the game table for Newcomb’s problem, with the human who is usually the focus of the problem as Row, and the demon as Column.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;In these games, Row chooses a row, and Column chooses a column, and that determines the cell that is the outcome of the game. The cells include two numbers. The first is Row’s payout, and the second is Column’s. The games are non-competitive; the players are simply trying to maximise their own returns, not maximise the difference between their return and the other player’s return.</p></li></div><div class="center">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">Predict 1 Box</th>
<th style="text-align: center;">Predict 2 Boxes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Choose 1 Box</td>
<td style="text-align: center;">1000, 1</td>
<td style="text-align: center;">0,0</td>
</tr>
<tr class="even">
<td>Choose 2 Boxes</td>
<td style="text-align: center;">1001, 0</td>
<td style="text-align: center;">1, 1</td>
</tr>
</tbody>
</table>
</div>
<p>This game has a unique Nash equilbrium; the bottom right corner.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> And that’s one way of motivating the view that (a) the game is possible, and (b) the rational move for the human is to choose two boxes.</p>
<div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;A Nash equilibrium is an outcome of the game where every player does as well as they can given the moves of the other players. Equivalently, it is an outcome where no player can improve their payout by unilaterally defecting from the equilibrium.</p></li><li id="fn6"><p><sup>6</sup>&nbsp;The Radical Interpreter feels like they should be a humanesque character in <em>Alice in Wonderland</em> or <em>The Phantom Tollbooth</em>, but for now they are resolutely abstract.</p></li></div><p>Let’s look at a more complicated game. I’ll call it The Interpretation Game. The game has two players. Just like in Newcomb’s problem, one of them is a human, the other is a philosophical invention. But in this case the invention is not a demon, but The Radical Interpreter.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> To know the payouts for the players, we need to know their value function. More colloquially, we need to know their goals.</p>
<ul>
<li><p>The Radical Interpreter assigns mental states to Human in such a way as to predict Human’s actions given Human rationality. We’ll assume here that evidence is a mental state, so saying what evidence Human has is among Radical Interpreter’s tasks. (Indeed, in the game play to come, it will be their primary task.)</p></li>
<li><p>Human acts so as to maximise the expected utility of their action, conditional on the evidence that they have. Human doesn’t always know what evidence they have; it depends on what The Radical Interpreter says.</p></li>
</ul>
<p>The result is that the game is a coordination game. The Radical Interpreter wants to assign evidence in a way that predicts rational Human action, and Human wants to do what’s rational given that assignment of evidence. Coordination games typically have multiple equilibria, and this one is no exception.</p>
<p>Let’s make all that (marginally) more concrete. Human is offered a bet on <em>p</em>. If the bet wins, it wins 1 util; if the bet loses, it loses 100 utils. Human’s only choice is to Take or Decline the bet. The proposition <em>p</em>, the subject of the bet, is like the claim that Rahul is in the restaurant. It is something that is arguably part of Human’s evidence. Unfortunately, it is also arguable that it is not part of Human’s evidence. We will let <span class="math inline">\(K\)</span> be the rest of Human’s evidence (apart from <span class="math inline">\(p\)</span>, and things entailed by <span class="math inline">\(K \cup \{p\}\)</span>), and stipulate that <span class="math inline">\(\Pr(p | K) = 0.9\)</span>. Each party now faces a choice.</p>
<ul>
<li><p>The Radical Interpreter has to choose whether <em>p</em> is part of Human’s evidence or not.</p></li>
<li><p>Human has to decide whether to Take or Decline the bet.</p></li>
</ul>
<p>The Radical Interpreter achieves their goal if human takes the bet iff <em>p</em> is part of their evidence. If <em>p</em> is part of the evidence, then The Radical Interpreter thinks that the bet has positive expected utility, so Human will take it. And if <em>p</em> is not part of the evidence, then The Radical Interpreter thinks that the bet has negative expected utility, so Human will decline it. Either way, The Radical Interpreter wants Human’s action to coordinate with theirs. And Human, of course, wants to maximise expected utility. So we get the following table for the game.</p>
<div class="center">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(p \in E\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p \notin E\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Take the bet</td>
<td style="text-align: center;">1, 1</td>
<td style="text-align: center;">-9.1, 0</td>
</tr>
<tr class="even">
<td>Decline the bet</td>
<td style="text-align: center;">0, 0</td>
<td style="text-align: center;">0, 1</td>
</tr>
</tbody>
</table>
</div>
<p>We have, in effect, already covered The Radical Interpreter’s payouts. They win in the top-left and lower-right quadrants, and lose otherwise. Human’s payouts are only a little trickier. In the bottom row, they are guaranteed 0, since the bet is declined. In the top-left, the bet is a sure winner; their evidence entails it wins. So they get a payout of 1. In the top-right, the bet wins with probability 0.9, so the expected return<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> of taking it is <span class="math inline">\(1 \times 0.9 - 100 \times 0.1 = -9.1\)</span>.</p>
<div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;I am making a large, if orthodox, assumption here: that the payouts that we use for equilibrium analysis should be expected returns, not actual returns. I think that’s the right thing to do, since it is usually impossible to say what the actual return of a game is. Even when we say that the payout is a certain number of dollars, we are really saying that the return is a certain kind of gamble. Maybe the value of the currency will deprecate quickly, and the dollars are not that valuable. Maybe the revolution will come and wealth will be a liability. Almost all games have probabilistic payouts, and this game is no different.</p></li></div><p>There are two Nash equilibria for the game - I’ve bolded them below.</p>
<div class="center">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(p \in E\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p \notin E\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Take the bet</td>
<td style="text-align: center;"><strong>1, 1</strong></td>
<td style="text-align: center;">-9.1, 0</td>
</tr>
<tr class="even">
<td>Decline the bet</td>
<td style="text-align: center;">0, 0</td>
<td style="text-align: center;"><strong>0, 1</strong></td>
</tr>
</tbody>
</table>
</div>
<p>The mathematical result that there are two equilibria to this game should not come as a surprise. In discussing games like this earlier, we said that general principles connecting evidence, knowledge and action are not predictive; they are consistent both with <em>p</em> being part of the evidence, and with it not being part of the evidence. The general principles we had stated rule out, in effect, non-equilibrium solutions to games like this one. But they are not predictive in cases where there are multiple equilibria.</p>
<p>To make more progress, we need to turn to more contested areas of game theory. In particular, we need to look at some work on equilibrium choice. We’ll introduce this material via a game that is inspired by an example of Rousseau’s.</p>
</section>
<section id="equilibriumselectionprinciples" class="level3 page-columns page-full" data-number="0.5">
<h3 data-number="0.5" class="anchored" data-anchor-id="equilibriumselectionprinciples"><span class="header-section-number">0.5</span> Equilibrium Selection Principles</h3>
<p>At an almost maximal level of abstraction, a two player, two option each game looks like this.</p>
<div class="center">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(a\)</span></th>
<th style="text-align: center;"><span class="math inline">\(b\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td style="text-align: center;"><span class="math inline">\(r_{11}\)</span>, <span class="math inline">\(c_{11}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(r_{12}\)</span>, <span class="math inline">\(c_{12}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(B\)</span></td>
<td style="text-align: center;"><span class="math inline">\(r_{21}\)</span>, <span class="math inline">\(c_{21}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(r_{22}\)</span>, <span class="math inline">\(c_{22}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>We’re going to focus on games that have the following eight properties:</p>
<ul>
<li><p><span class="math inline">\(r_{11} &gt; r_{21}\)</span></p></li>
<li><p><span class="math inline">\(r_{22} &gt; r_{12}\)</span></p></li>
<li><p><span class="math inline">\(c_{11} &gt; c_{12}\)</span></p></li>
<li><p><span class="math inline">\(c_{22} &gt; c_{21}\)</span></p></li>
<li><p><span class="math inline">\(r_{11} &gt; r_{22}\)</span></p></li>
<li><p><span class="math inline">\(c_{11} \geq c_{22}\)</span></p></li>
<li><p><span class="math inline">\(\frac{r_{21}+r_{22}}{2} &gt; \frac{r_{11}+r_{12}}{2}\)</span></p></li>
<li><p><span class="math inline">\(\frac{c_{12}+c_{22}}{2} \geq \frac{c_{11}+c_{21}}{2}\)</span></p></li>
</ul>
<p>The first four clauses say that the game has two (strict) Nash equilibria: <span class="math inline">\(Aa\)</span> and <span class="math inline">\(Bb\)</span>. The fifth and sixth clauses say that the <span class="math inline">\(Aa\)</span> equilibria is <strong>Pareto-optimal</strong>: no one prefers the other equilibria to it. In fact it says something a bit stronger: one of the players strictly prefers the <span class="math inline">\(Aa\)</span> equilibria, and the other player does not prefer <span class="math inline">\(Bb\)</span>. The seventh and eighth clauses say that the <span class="math inline">\(Bb\)</span> equilibria is <strong>risk-optimal</strong>. Risk-optimality is a somewhat complicated notion in general; see <span class="citation" data-cites="HarsanyiSelten1988">Harsanyi and Selten (<a href="#ref-HarsanyiSelten1988" role="doc-biblioref">1988</a>)</span> for more details. But for our purposes, we can focus on a simple characterisation of it. Neither player would prefer playing <span class="math inline">\(A\)</span>/<span class="math inline">\(a\)</span> to playing <span class="math inline">\(B\)</span>/<span class="math inline">\(b\)</span> if they thought it was a coin flip which equilibrium the other player was aiming for.</p>
<p>I’m going to offer an argument from Hans Carlsson and Eric van Damme <span class="citation" data-cites="CarlssonVanDamme1993">(<a href="#ref-CarlssonVanDamme1993" role="doc-biblioref">1993</a>)</span> for the idea that in these games, rational players will end up at <span class="math inline">\(Bb\)</span>. The game that Human and The Radical Interpreter are playing fits these eight conditions, and The Radical Interpreter is perfectly rational, so this will imply that in that game, The Radical Interpreter will say that <span class="math inline">\(p \notin E\)</span>, which is what we aimed to show.</p>
<p>Games satisfying these eight inequalities are sometimes called <em>Stag Hunt</em> games. There is some flexibility, and some vagueness, in which of the eight inequalities need to be strict, but that level of detail isn’t important here. The name comes from a thought experiment in Rousseau’s <em>Discourse on Inequality</em>.</p>
<blockquote class="blockquote">
<p>[T]hey were perfect strangers to foresight, and were so far from troubling themselves about the distant future, that they hardly thought of the morrow. If a deer was to be taken, every one saw that, in order to succeed, he must abide faithfully by his post: but if a hare happened to come within the reach of any one of them, it is not to be doubted that he pursued it without scruple, and, having seized his prey, cared very little, if by so doing he caused his companions to miss theirs. &nbsp;<span class="citation" data-cites="Rousseau1913">(<a href="#ref-Rousseau1913" role="doc-biblioref">Rousseau 1913, 209–10</a>)</span></p>
</blockquote>
<p>It is rather interesting to think through which real-life situations are best modeled as Stag Hunts, especially in situations where people have thought that the right model was a version of Prisoners’ Dilemma. This kind of thought is one way in to appreciating the virtues of Rousseau’s political outlook, and especially the idea that social coordination might not require anything like the heavy regulatory presence that, say, Hobbes thought was needed. But that’s a story for another day. What we’re going to be interested in is why Rousseau was right to think that a ‘stranger to foresight’, who is just focussing on this game, should take the rabbit.</p>
<p>To make matters a little easier, we’ll focus on a very particular instance of Stag Hunt, as shown here. (From here I’m following Carlsson and van Damme very closely; this is their example, with just the labelling slightly altered.)</p>
<div class="center">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(a\)</span></th>
<th style="text-align: center;"><span class="math inline">\(b\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td style="text-align: center;">4, 4</td>
<td style="text-align: center;">0, 3</td>
</tr>
<tr class="even">
<td><span class="math inline">\(B\)</span></td>
<td style="text-align: center;">3, 0</td>
<td style="text-align: center;">3, 3</td>
</tr>
</tbody>
</table>
</div>
<p>At first glance it might seem like <span class="math inline">\(Aa\)</span> is the right choice; it produces the best outcome. This isn’t like Prisoners Dilemma, where the best collective outcome is dominated. In fact <span class="math inline">\(Aa\)</span> is the best outcome for each individual. But it is risky, and Carlsson and van Damme show how to turn that risk into an argument for choosing <span class="math inline">\(Bb\)</span>.</p>
<p>Embed this game in what they call a <em>global game</em>. We’ll start the game with each player knowing just that they will play a game with the following payout table, with <span class="math inline">\(x\)</span> to be selected at random from a flat distribution over <span class="math inline">\([-1, 5]\)</span>.</p>
<div class="center">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(a\)</span></th>
<th style="text-align: center;"><span class="math inline">\(b\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td style="text-align: center;">4, 4</td>
<td style="text-align: center;">0, x</td>
</tr>
<tr class="even">
<td><span class="math inline">\(B\)</span></td>
<td style="text-align: center;">x, 0</td>
<td style="text-align: center;">x, x</td>
</tr>
</tbody>
</table>
</div>
<p>Before they play the game, each player will get a noisy signal about the value of <span class="math inline">\(x\)</span>. There will be signals <span class="math inline">\(s_R\)</span> and <span class="math inline">\(s_C\)</span> chosen (independently) from a flat distribution over <span class="math inline">\([x - 0.25, x + 0.25]\)</span>, and shown to Row and Column respectively. So each player will know the value of <span class="math inline">\(x\)</span> to within <span class="math inline">\(\frac{1}{4}\)</span>, and know that the other player knows it to within <span class="math inline">\(\frac{1}{4}\)</span> as well. But this is a margin of error model, and in those models there is very little that is common knowledge. That, they argue, makes a huge difference.</p>
<p>In particular, they prove that iterated deletion of strictly dominated strategies (almost) removes all but one strategy pair.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Each player will play <span class="math inline">\(A\)</span>/<span class="math inline">\(a\)</span> if the signal is greater than 2, and <span class="math inline">\(B\)</span>/<span class="math inline">\(b\)</span> otherwise.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> Surprisingly, this shows that players should play the risk-optimal strategy even when they know the other strategy is Pareto-optimal. When a player gets a signal in <span class="math inline">\((2, 3.75)\)</span>, then they know that <span class="math inline">\(x &lt; 4\)</span>, so <span class="math inline">\(Bb\)</span> is the Pareto-optimal equilibrium. But the logic of the global game suggests the risk-dominant equilibrium is what to play.</p>
<div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;A sketch of the proof is in Appendix One.</p></li><li id="fn9"><p><sup>9</sup>&nbsp;Strictly speaking, we can’t rule out various mixed strategies when the signal is precisely 2, but this makes little difference, since that occurs with probability 0.</p></li></div><p>Carlsson and van Damme go on to show that many of the details of this case don’t matter. As long as (a) there is a margin of error in each side’s estimation of the payoffs, and (b) every choice is a dominant option in some version of the global game, then iterated deletion of strongly dominant strategies will lead to each player making the risk-dominant choice.</p>
<p>I conclude from that that risk-dominant choices are rational in these games. There is a limit assumption involved here; what’s true for games with arbitrarily small margins of error is true for games with no margin of error. (We’ll come back to that assumption below.) And since The Radical Interpreter is rational, they will play the strategy that is not eliminated by deleting dominant strategies. That is, they will play the risk-dominant strategy.</p>
<p>In game with Human, the rational (i.e., risk-dominant) strategy for The Radical Interpreter is to say that <span class="math inline">\(p \notin E\)</span>. And in the case of Parveen and Rahul, rational (i.e., risk-dominant) strategy for The Radical Interpreter is to say that it is not part of Parveen’s evidence that Rahul is in the restaurant. And this is an interest-relative theory of evidence; had Parveen been playing a different game, The Radical Interpreter would have said that it is part of Parveen’s evidence that Rahul was in the restaurant.</p>
<p>And from this point we can say all the things we wanted to say about the case. If it is part of Parveen’s evidence that Rahul is in the restaurant, then she knows this. Conversely, if she knows it, then The Radical Interpreter would have said it is part of her evidence, so it is part of her evidence. Parveen will perform the action that maximises expected utility given her evidence. And she will lose knowledge when that disposition makes her do things that would be known to be sub-optimal if she didn’t lose knowledge.</p>
<p>In short, this model gives us a way to keep what was good about the pragmatic encroachment theory, while also allowing that evidence can be interest-relative. It does require a slightly more complex theory of rationality than we had previously used. Rather than just say that agents maximise evidential expected utility, we have to say that they play risk-dominant strategies in coordination games. But it turns out that this is little more than saying that they maximise evidential expected utility, and they expect others (at least perfectly rational abstract others) to do the same, and they expect those others to expect they will maximise expected utility, and so on.</p>
</section>
<section id="objectionsandreplies" class="level3" data-number="0.6">
<h3 data-number="0.6" class="anchored" data-anchor-id="objectionsandreplies"><span class="header-section-number">0.6</span> Objections and Replies</h3>
<p>We’ll end the body of the paper with some objections that might be raised to this model. And then the appendix will contain proofs of a couple of the formal claims.</p>
<p><em>Objection</em>: The formal result of the previous section only goes through if we assume that the agents do not know precisely what the payoffs are in the game. We shouldn’t assume that what holds for arbitrarily small margins of error will hold in the limit, i.e., when they do know the payoffs.</p>
<p><em>Reply</em>: If pushed, I would defend the use limit assumptions like this to resolve hard cases like Stag Hunt. But I don’t need that assumption here, What we really need is that Parveen doesn’t know precisely the probability of Rahul being in the restaurant given the rest of her evidence. Given that evidence is not luminous, as <span class="citation" data-cites="Williamson2000">Williamson (<a href="#ref-Williamson2000" role="doc-biblioref">2000</a>)</span> shows, this is a reasonable assumption. So the margin of error assumption that Carlsson and van Damme make is not, in our case, an assumption that merely makes the math easier; it is built into the case.</p>
<p><em>Objection</em>: Even if Parveen doesn’t know the payoffs precisely, The Radical Interpreter does. They are an idealisation, so they can be taken to be ideal.</p>
<p><em>Reply</em>: It turns out that Carlsson and van Damme’s result doesn’t require that both parties are ignorant of the precise values of the payoffs. As long as one party doesn’t know the exact value of the payoff, the argument goes through. I prove this in Appendix Two.</p>
<p><em>Objection</em>: The formal argument requires that in the ‘global game’ there are values for <span class="math inline">\(x\)</span> that make <span class="math inline">\(A\)</span> the dominant choice. These cases serve as a base step for an inductive argument that follows. But in Parveen’s case, there is no such setting for <span class="math inline">\(x\)</span>, so the inductive argument can’t get going.</p>
<p><em>Reply</em>: What matters is that there are values of <span class="math inline">\(x\)</span> such that <span class="math inline">\(A\)</span> is the strictly dominant choice, and Human (or Parveen) doesn’t know that they know that they know, etc., that those values are not actual. And that’s true in our case. For all Human (or Parveen) knows that they know that they know that they know…, the proposition in question is not part of their evidence under a maximally expansive verdict on The Radical Interpreter’s part. So the relevant cases are there in the model, even if for some high value of <span class="math inline">\(n\)</span> they are known<span class="math inline">\(^n\)</span> not to obtain.</p>
<p><em>Objection</em>: This model is much more complex than the simple motivation for pragmatic encroachment.</p>
<p><em>Reply</em>: Sadly, this is true. I would like to have a simpler model, but I don’t know how to create one. The argument I gave earlier that our simple principles underdetermine what to say in cases like Parveen and Rahul’s seems fairly compelling. So more complexity will be needed, one way or another. I think paying this price in complexity is worth it overall, but I can see how some people might think otherwise.</p>
<p><em>Objection</em>: Change the case involving Human so that the bet loses 15 utils if <em>p</em> is false, rather than 100. Now the risk-dominant equilibrium is that Human takes the bet, and The Radical Interpreter says that <em>p</em> is part of Human’s evidence. But note that if it was clearly true that <em>p</em> was not part of Human’s evidence, then this would still be too risky a situation for them to know <em>p</em>. So whether it is possible that <em>p</em> is part of Human’s evidence matters.</p>
<p><em>Reply</em>: This is all true, and it shows that the view I’m putting forward is incompatible with some programs in epistemology. In particular, it is incompatible with E=K, since the what it takes to be evidence on this story is slightly different from what it takes to be knowledge. I don’t think E=K is so intuitively obvious that this refutes the theory, but it is potentially a cost that I have to give it up.</p>
<p><em>Objection</em>: Carlsson and van Damme discuss one kind of global game. But there are other global games that have different equilibria. For instance, changing the method by which the noisy signal is selected would change the equilibrium of the global game. So this kind of argument can’t show that the risk-dominant equilibrium is the one true solution.</p>
<p><em>Reply</em>: This is somewhat true. There are other ways of embedding the game involving Human and The Radical Interpreter in global games that lead to different outcomes. They are usually somewhat artificial; e.g., by having the signal be systematically biased in one way. But what really matters is the game where the error in Human’s knowledge of the payoffs is determined by their actual epistemic limitations. I think that will lead to something like the model we have here. But it is possible that the final result will differ a bit from what I have here, or (more likely) have some indeterminacy about just how interests interact with evidence and knowledge. The precise details are ultimately less important to me than whether we can provide a motivated story of how interests affect knowledge and evidence that does not presuppose we know what the agent’s evidence is. And the method I’ve outlined here shows that we can do that, even if we end up tinkering a bit with the details.</p>
</section>
<section id="appendixone:carlssonandvandammesgame" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="appendixone:carlssonandvandammesgame">Appendix One: Carlsson and van Damme’s Game</h3>
<p>Two players, Row (or R) and Column (or C) will a version of the following game.</p>
<div class="center">
<table class="table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(a\)</span></th>
<th style="text-align: center;"><span class="math inline">\(b\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(A\)</span></td>
<td style="text-align: center;">4, 4</td>
<td style="text-align: center;">0, x</td>
</tr>
<tr class="even">
<td><span class="math inline">\(B\)</span></td>
<td style="text-align: center;">x, 0</td>
<td style="text-align: center;">x, x</td>
</tr>
</tbody>
</table>
</div>
<p>They won’t be told what <span class="math inline">\(x\)</span> is, but they will get a noisy signal of <span class="math inline">\(x\)</span>, drawn from an even distribution over <span class="math inline">\([x - 0.25, x + 0.25]\)</span>. Call these signals <span class="math inline">\(s_R\)</span> and <span class="math inline">\(s_C\)</span>. Each player must then choose <span class="math inline">\(A\)</span>, getting either 4 or 0 depending on the other player’s choice, or choose <span class="math inline">\(B\)</span>, getting <span class="math inline">\(x\)</span> for sure.</p>
<p>Before getting the signal, the players must choose a strategy. A strategy is a function from signals to choices. Since the higher the signal is, the better it is to play <span class="math inline">\(B\)</span>, we can equate strategies with ‘tipping points’, where the player plays <span class="math inline">\(B\)</span> if the signal is above the tipping point, and <span class="math inline">\(A\)</span> below the tipping point. Strictly speaking, a tipping point will pick out not a strategy but an equivalence class of strategies, which differ in how they act if the signal is the tipping point. But since that happens with probability 0, the strategies in the equivalence class have the same expected return, and so we won’t aim to distinguish them.</p>
<p>Also, strictly speaking, there are strategies that are not tipping points, because they map signals onto probabilities of playing <span class="math inline">\(A\)</span>, where the probability decreases as <span class="math inline">\(A\)</span> rises. I won’t discuss these directly, but it isn’t too hard to see how these are shown to be suboptimal using the argument that is about to come. It eases exposition to focus on the pure strategies, and to equate these with tipping points. And since my primary aim here is to explain why the result holds, not to simply repeat an already existing proof, I’ll mostly ignore these mixed strategies.</p>
<p>Call the tipping points for Row and Column respectively <span class="math inline">\(T_R\)</span> and <span class="math inline">\(T_C\)</span>. Since the game is symmetric, we’ll just have to show that in conditions of common knowledge of rationality, <span class="math inline">\(T_R = 2\)</span>. It follows by symmetry that <span class="math inline">\(T_C = 2\)</span> as well. And the only rule we’ll use is iterated deletion of strictly dominated strategies. That is, we’ll assume players won’t play strategies where another strategy does better no matter what the opponent chooses, and they won’t play strategies where another strategy does better provided the other player does not play a dominated strategy, and they won’t play strategies where another strategy does better provided the other player does not play a strategy ruled out by these first two conditions, and so on.</p>
<p>The return to a strategy is uncertain, even given the other player’s strategy. But given the strategies of each player, we can work out an expected return for each player. And that’s what we’ll assume is the return to a strategy pair.</p>
<p>Note first that <span class="math inline">\(T_R = 4.25\)</span> strictly dominates any strategy where <span class="math inline">\(T_R = y &gt; 4.25\)</span>. If <span class="math inline">\(s_R \in (4.25, y)\)</span>, then <span class="math inline">\(T_R\)</span> is guaranteed to return above 4, and the alternative strategy is guaranteed to return 4. In all other cases, the strategies have the same return. And there is some chance that <span class="math inline">\(s_R \in (4.25, y)\)</span>. So we can delete all strategies <span class="math inline">\(T_R = y &gt; 4.25\)</span>, and similarly all strategies <span class="math inline">\(T_C = y &gt; 4.25\)</span>. By similar reasoning, we can rule out <span class="math inline">\(T_R &lt; -0.25\)</span> and <span class="math inline">\(T_C &lt; -0.25\)</span>.</p>
<p>If <span class="math inline">\(s_R \in [-0.75, 4.75]\)</span>, then it is equally likely that <span class="math inline">\(x\)</span> is above <span class="math inline">\(s_R\)</span> as it is below it. Indeed, the posterior distribution of <span class="math inline">\(x\)</span> is flat over <span class="math inline">\([s_R - 0.25, s_R + 0.25]\)</span>. From this it follows that the expected return of playing <span class="math inline">\(B\)</span> after seeing signal <span class="math inline">\(s_R\)</span> is just <span class="math inline">\(s_R\)</span>.</p>
<p>Now comes the important step. Assume that we know that <span class="math inline">\(T_C \leq y &gt; 2\)</span>. Now consider the expected return of playing <span class="math inline">\(A\)</span> given various values for <span class="math inline">\(s_R &gt; 2\)</span>. Given that the lower <span class="math inline">\(T_C\)</span> is, the higher the expected return is of playing <span class="math inline">\(A\)</span>, we’ll just work on the simple case where <span class="math inline">\(T_C = y\)</span>, realizing that this is an upper bound on the expected return of <span class="math inline">\(A\)</span> given <span class="math inline">\(T_C \leq y\)</span>. The expected return of <span class="math inline">\(A\)</span> is 4 times the probability that Column will play <span class="math inline">\(a\)</span>, i.e., 4 times the probability that <span class="math inline">\(s_C &lt; T_C\)</span>. Given all the symmetries that have been built into the puzzle, we know that the probability that <span class="math inline">\(s_C &lt; s_R\)</span> is 0.5. So the expected return of playing <span class="math inline">\(A\)</span> is at most 2 if <span class="math inline">\(s_R \geq y\)</span>. But the expected return of playing <span class="math inline">\(B\)</span> is, as we showed in the last paragraph, <span class="math inline">\(s_R\)</span>, which is greater than 2. So it is better to play <span class="math inline">\(B\)</span> than <span class="math inline">\(A\)</span> if <span class="math inline">\(s_R \geq y\)</span>. And the difference is substantial, so even if <span class="math inline">\(s_R\)</span> is epsilon less than that <span class="math inline">\(y\)</span>, it will still be better to play <span class="math inline">\(B\)</span>. (This is hand-wavy of course, but we’ll make it rigorous in just a second.)</p>
<p>So if <span class="math inline">\(T_C \leq y &gt; 2\)</span> we can prove that <span class="math inline">\(T_R\)</span> should be lower still, because given that assumption it is better to play <span class="math inline">\(B\)</span> even if the signal is just less than <span class="math inline">\(y\)</span>. Repeating this reasoning over and over again pushes us to it being better to play <span class="math inline">\(B\)</span> than <span class="math inline">\(A\)</span> as long as <span class="math inline">\(s_R &gt; 2\)</span>. And the same kind of reasoning from the opposite end pushes us to it being better to play <span class="math inline">\(A\)</span> than <span class="math inline">\(B\)</span> as long as <span class="math inline">\(s_R &lt; 2\)</span>. So we get <span class="math inline">\(s_R = 2\)</span> as the uniquely rational solution to the game.</p>
<p>Let’s make that a touch more rigorous. Assume that <span class="math inline">\(T_C = y\)</span>, and <span class="math inline">\(s_r\)</span> is slightly less than <span class="math inline">\(y\)</span>. In particular, we’ll assume that <span class="math inline">\(z = y - s_R\)</span> is in <span class="math inline">\((0, 0.5)\)</span>. Then the probability that <span class="math inline">\(s_C &lt; y\)</span> is <span class="math inline">\(0.5 + 2z - 2z^2\)</span>. So the expected return of playing <span class="math inline">\(A\)</span> is <span class="math inline">\(2 + 8z - 8z^2\)</span>. And the expected return of playing <span class="math inline">\(B\)</span> is, again, <span class="math inline">\(s_R\)</span>. These will be equal when the following is true. (The working out is a tedious but trivial application of the quadratic formula, plus some rearranging.)</p>
<p><span class="math display">\[s_R = y + \frac{\sqrt{145-32y} - 9}{16}\]</span> So if we know that <span class="math inline">\(T_C \geq y\)</span>, we know that <span class="math inline">\(T_R \geq y + \frac{\sqrt{145-32y} - 9}{16}\)</span>, which will be less than <span class="math inline">\(y\)</span> if <span class="math inline">\(y &gt; 2\)</span>. And then by symmetry, we know that <span class="math inline">\(T_C\)</span> must be at most as large as that as well. And then we can use that fact to derive a further upper bound on <span class="math inline">\(T_R\)</span> and hence on <span class="math inline">\(T_C\)</span>, and so on. And this will continue until we push both down to 2. It does require quite a number of steps of iterated deletion. Here is the upper bound on the threshold after <span class="math inline">\(n\)</span> rounds of deletion of dominated strategies. (These numbers are precise for the first two rounds, then just to three significant figures after that.)</p>
<div class="center">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Round</th>
<th style="text-align: center;">Upper Bound on Threshold</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">4.250</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">3.875</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">3.599</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">3.378</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">3.195</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">3.041</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">2.910</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">2.798</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">2.701</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">2.617</td>
</tr>
</tbody>
</table>
</div>
<p>That is, <span class="math inline">\(T_R = 4.25\)</span> dominates any strategy with a tipping point above 4.25. And <span class="math inline">\(T_R = 3.875\)</span> dominates any strategy with a higher tipping point than that, assuming <span class="math inline">\(T_C \leq 4.25\)</span>. And <span class="math inline">\(T_R \approx 3.599\)</span> dominates any strategy with a higher tipping point than that, assuming <span class="math inline">\(T_C \leq 3.875\)</span>. And so on.</p>
<p>And similar reasoning shows that at each stage not only are all strategies with higher tipping points dominated, but so are strategies that assign positive probability (whether it is 1 or less than 1), to playing <span class="math inline">\(A\)</span> when the signal is above the ‘tipping point’. So this kind of reasoning rules out all mixed strategies (except those that respond probabilistically to <span class="math inline">\(s_R = 2\)</span>).</p>
<p>So we’ve shown what was intended, namely that iterated deletion of dominated strategies will rule out all strategies except the risk-optimal equilibrium. We needed the possibility that <span class="math inline">\(x\)</span> is greater than the maximal return for <span class="math inline">\(A\)</span> to get the iterated dominance going. And we needed the signal to have an error bar to it, so that each round of iteration removes more strategies. But that’s all we needed; the particular values we chose are irrelevant to the proof.</p>
</section>
<section id="appendixtwo:themodifiedgame" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="appendixtwo:themodifiedgame">Appendix Two: The Modified Game</h3>
<p>The aim of this section is to prove something that Carllson and van Damme did not prove, namely that the analysis of the previous appendix goes through with very little change if one party gets a perfect signal, while the other gets a noisy signal. That is, we’re going to consider the game that is just like the game of appendix one, but it is common knowledge that the signal Column gets, <span class="math inline">\(s_C\)</span>, equals <span class="math inline">\(x\)</span>.</p>
<p>Since the game is no longer symmetric, we can’t appeal to the symmetry of the game as we frequently did in the previous appendix. But this only slows the proof down, it doesn’t stop it.</p>
<p>We can actually rule out slightly more at the first step in this game than in the previous game. Since Column could not be wrong about <span class="math inline">\(x\)</span>, Column knows that if <span class="math inline">\(s_C &gt; 4\)</span> then playing <span class="math inline">\(b\)</span> dominates playing <span class="math inline">\(a\)</span>. So one round of deleting dominated strategies rules out <span class="math inline">\(T_C &gt; 4\)</span>, as well as ruling out <span class="math inline">\(T_R &gt; 4.25\)</span>.</p>
<p>At any stage, if we know <span class="math inline">\(T_C \leq y &gt; 2\)</span>, then <span class="math inline">\(T_R = y\)</span> dominates <span class="math inline">\(T_R &gt; y\)</span>. That’s because if <span class="math inline">\(s_R \geq y\)</span>, and <span class="math inline">\(T_C \leq y\)</span>, then the probability that Column will play <span class="math inline">\(a\)</span> (given Row’s signal) is less than 0.5. After all, the signal is just as likely to be above <span class="math inline">\(x\)</span> as below it (as long as the signal isn’t too close to the extremes). So if <span class="math inline">\(s_R\)</span> is at or above <span class="math inline">\(T_C\)</span>, then it is at least 0.5 likely that <span class="math inline">\(s_C = x\)</span> is at or above <span class="math inline">\(T_C\)</span>. So the expected return of playing <span class="math inline">\(A\)</span> is at most 2. But the expected return of playing <span class="math inline">\(B\)</span> equals the signal, which is greater than 2. So if Row knows <span class="math inline">\(T_C \leq y &gt; 2\)</span>, Row also knows it is better to play <span class="math inline">\(B\)</span> if <span class="math inline">\(s_R \geq y\)</span>. And that just means that <span class="math inline">\(T_R \leq y\)</span>.</p>
<p>Assume now that it is common knowledge that <span class="math inline">\(T_R \leq y\)</span>, for some <span class="math inline">\(y &gt; 2\)</span>. And assume that <span class="math inline">\(x = s_C\)</span> is just a little less than <span class="math inline">\(y\)</span>. In particular, define <span class="math inline">\(z = y -x\)</span>, and assume <span class="math inline">\(z \in (0, 0.25)\)</span>. We want to work out the upper bound on the expected return to Column of playing <span class="math inline">\(a\)</span>. (The return of playing <span class="math inline">\(b\)</span> is known, it is <span class="math inline">\(x\)</span>.) The will be highest when <span class="math inline">\(T_R\)</span> is lowest, so assume <span class="math inline">\(T_R \leq y\)</span>. Then the probability that Row plays <span class="math inline">\(A\)</span> is <span class="math inline">\((1 + 2z)/2\)</span>. So the expected return of playing <span class="math inline">\(a\)</span> is <span class="math inline">\(2 + 4z\)</span>, i.e., <span class="math inline">\(2 + 4(y - x)\)</span>. That will be greater than <span class="math inline">\(x\)</span> only when</p>
<p><span class="math display">\[x &lt; \frac{2 + 4y}{5}\]</span> And so if it is common knowledge that <span class="math inline">\(T_R \leq y\)</span>, then it is best for Column to play <span class="math inline">\(b\)</span> unless <span class="math inline">\(x &lt; \frac{2 + 4y}{5}\)</span>. That is, if it is common knowledge that <span class="math inline">\(T_R \leq y\)</span>, then <span class="math inline">\(T_C\)</span> must be at most <span class="math inline">\(\frac{2 + 4y}{5}\)</span>.</p>
<p>So now we proceed in a zig-zag fashion. At one stage, we show that <span class="math inline">\(T_R\)</span> must be as low as <span class="math inline">\(T_C\)</span>. At the next, we show that if it has been proven that <span class="math inline">\(T_R\)</span> takes a particular value greater than 2, then <span class="math inline">\(T_C\)</span> must be lower still. And this process will eventually rule out all values for <span class="math inline">\(T_R\)</span> and <span class="math inline">\(T_C\)</span> greater than 2.</p>
<p>This case is crucial to the story of the paper because The Radical Interpreter probably does not have an error bar in their estimation of the game they are playing. But it turns out the argument for risk-dominant equilibria being the unique solution to interpretation games is consistent with that. As long as one player has a margin of error, each player should play the risk-dominant equilibria.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Brown2008" class="csl-entry" role="listitem">
Brown, Jessica. 2008. <span>“Subject-Sensitive Invariantism and the Knowledge Norm for Practical Reasoning.”</span> <em>No<span>û</span>s</em> 42 (2): 167–89. <a href="https://doi.org/10.1111/j.1468-0068.2008.00677.x">https://doi.org/10.1111/j.1468-0068.2008.00677.x</a>.
</div>
<div id="ref-CarlssonVanDamme1993" class="csl-entry" role="listitem">
Carlsson, Hans, and Eric van Damme. 1993. <span>“Global Games and Equilibrium Selection.”</span> <em>Econometrica</em> 61 (5): 989–1018. <a href="https://doi.org/10.2307/2951491">https://doi.org/10.2307/2951491</a>.
</div>
<div id="ref-FantlMcGrath2009" class="csl-entry" role="listitem">
Fantl, Jeremy, and Matthew McGrath. 2009. <em>Knowledge in an Uncertain World</em>. Oxford: Oxford University Press.
</div>
<div id="ref-HarsanyiSelten1988" class="csl-entry" role="listitem">
Harsanyi, John C., and Reinhard Selten. 1988. <em>A General Theory of Equilibrium Selection in Games</em>. Cambridge, MA: <span>MIT</span> Press.
</div>
<div id="ref-Hawthorne2004" class="csl-entry" role="listitem">
Hawthorne, John. 2004. <em>Knowledge and Lotteries</em>. Oxford: Oxford University Press.
</div>
<div id="ref-IchikawaEtAl2012" class="csl-entry" role="listitem">
Ichikawa, Jonathan Jenkins, Benjamin Jarvis, and Katherine Rubin. 2012. <span>“Pragmatic Encroachment and Belief-Desire Psychology.”</span> <em>Analytic Philosophy</em> 53 (4): 327–43. <a href="https://doi.org/10.1111/j.2153-960X.2012.00564.x">https://doi.org/10.1111/j.2153-960X.2012.00564.x</a>.
</div>
<div id="ref-Lackey2010" class="csl-entry" role="listitem">
Lackey, Jennifer. 2010. <span>“Acting on Knowledge.”</span> <em>Philosophical Perspectives</em> 24: 361–82. <a href="https://doi.org/10.1111/j.1520-8583.2010.00196.x">https://doi.org/10.1111/j.1520-8583.2010.00196.x</a>.
</div>
<div id="ref-Reed2014" class="csl-entry" role="listitem">
Reed, Baron. 2014. <span>“Practical Matters Do Not Affect Whether You Know.”</span> In <em>Contemporary Debates in Epistemology</em>, edited by Matthias Steup, John Turri, and Ernest Sosa, 2nd ed., 95–106. Chicester: Wiley-Blackwell.
</div>
<div id="ref-Rousseau1913" class="csl-entry" role="listitem">
Rousseau, Jean-Jacques. 1913. <em>Social Contract &amp; Discourses</em>. Translated by G. D. H. Cole. New York: J. M. Dent &amp; Sons.
</div>
<div id="ref-Rubin2015" class="csl-entry" role="listitem">
Rubin, Katherine. 2015. <span>“Total Pragmatic Encroachment and Epistemic Permissiveness.”</span> <em>Pacific Philosophical Quarterly</em> 96: 12–38. <a href="https://doi.org/10.1111/papq.12060">https://doi.org/10.1111/papq.12060</a>.
</div>
<div id="ref-Schwitzgebel2008" class="csl-entry" role="listitem">
Schwitzgebel, Eric. 2008. <span>“The Unreliability of Naive Introspection.”</span> <em>Philosophical Review</em> 117 (2): 245–73. <a href="https://doi.org/10.1215/00318108-2007-037">https://doi.org/10.1215/00318108-2007-037</a>.
</div>
<div id="ref-Stanley2005" class="csl-entry" role="listitem">
Stanley, Jason. 2005. <em><span class="nocase">Knowledge and Practical Interests</span></em>. Oxford University Press.
</div>
<div id="ref-Williamson2000" class="csl-entry" role="listitem">
Williamson, Timothy. 2000. <em><span class="nocase">Knowledge and its Limits</span></em>. Oxford University Press.
</div>
<div id="ref-Williamson2007-WILTPO-17" class="csl-entry" role="listitem">
———. 2007. <em><span class="nocase">The Philosophy of Philosophy</span></em>. Blackwell.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
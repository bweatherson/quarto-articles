---
title: "Anti-Anti-Desire-As-Belief"
abstract: |
  David Lewis put forward a decision theoretic argument against there being a tight connection between desires and beliefs about the good. I argue that his argument fails twice over. It makes inconsistent background assumptions about his opponents' views, and it over-generates so broadly that if it worked, it would also rule out some standard economic models. I end with a puzzle that arises from the response to Lewis. If one responds to moral uncertainty by saying one should maximise expected moral value, how does one treat cases where one's action is evidence for or against the goodness of different actions?
date: 26 July 2024
author:
  - name: Brian Weatherson 
    url: http://brian.weatherson.org
    affiliation: University of Michigan
    affiliation_url: https://umich.edu
    orcid_id: 0000-0002-0830-141X
image: "aadab.jpg"
categories:
  - games and decisions
  - ethics
  - in progress
  - unpublished
format:
    html:
      css: ../trad_defn.css
    pdf:
        output-file: "Anti-Anti-Desire-As-Belief"
        include-after-body: 
          text: |
            \noindent Unpublished. Posted online in 2024.
---

A particular anti-Humean, call her Auntie, believes there is a tight connection between wanting something and believing that it is good. David Lewis [-@Lewis1988; -@Lewis1996] has a famous argument that Auntie's view is incoherent. The point of this note is to respond on Auntie's behalf.

This is not because I agree with Auntie. On the broader question I think Lewis is right and Auntie is wrong. But Lewis's argument doesn't show that Auntie is wrong, and it's useful to see why it does not.

The point of this paper is also not to stick up for Auntie when no one has stuck up for her before. There are lots of replies to Lewis on Auntie's behalf from all sorts of directions. What's new here is that I show how two criticisms in particular, one by Huw @Price1989 and one by Jessica @Collins2015, fit together. Both of them say that Auntie should reject one of the assumptions that Lewis attributes to her. My primary contribution is to show that the two assumptions they reject are inconsistent. That is, Lewis's argument must fail because it requires attributing inconsistent background assumptions to Auntie, and that's certainly unfair.

# The Ludovician Argument

I'm not going to go over the argument in Lewis's 1988 paper, which relies on some really controversial assumptions. Instead I'll go over the 1996 argument, relying heavily on the presentation of it by Collins and by Ittay @NissanRozen2013.

Assume that we have a finite set of worlds. We will use *w* as a variable over worlds. A world, in this sense, is a specification of the truth value of all the truth-apt things that are relevant to a particular decision. The worlds in this sense are more coarse grained than Ludovician concreta in that they only specify truth values of relevant propositions, not of all propositions. That's why we can assume that there are finitely many of them. But these worlds are more fine grained than Ludovician concreta in a different sense. They will be used to represent moral uncertainty. So there can be pairs of them that are descriptively alike but evaluatively distinct. Given the supervenience of the evaluative on the descriptive, this is impossible for Ludovician worlds.

For any descriptive proposition A, assume there is a distinct proposition Å, meaning that A is good. Let V be an agent's value function, and Pr their credence function, with subscripts representing what those functions are like after updating. So V~A~ and Pr~A~ are the values of the value and credence functions after updating on A. Strictly speaking given how I've set this up, it is sets of worlds not individual worlds that get values. But I'll sometimes write V(*w*) when strictly it should be V({*w*}); I don't think this can lead to any confusion. (Later I'll also write Pr(*w*) for the probability of Pr({*w*}); again it shouldn't result in confusion.)

Lewis's argument against Auntie uses five assumptions. In these assumptions B is an arbitrary proposition, and A is an arbitrary *descriptive* proposition. 

Equation
:    The way to represent Auntie's anti-Humean view is V(A) = Pr(Å).

Invariance
:    V~A~(*w*) = V(*w*)

Additivity
:    V(A) = Σ~*w*~V(*w*)Pr(*w* | A)

Restricted Conditionalisation
:    Pr~A~(B) = Pr(B | A)

Good-Bad
:    All worlds are either GOOD or BAD. If *w* is GOOD, then V(*w*) = 1, and otherwise V(*w*) = 0.

The last assumption is obviously absurd, but it is useful for setting out the argument. In any case, if the first four assumptions are true, then they should be consistent with **Good-Bad**. Given those assumptions, here is Lewis's argument.

|   |           |              |
|--:|:----------|:-------------|
| Pr(Å) | = V(A) |               |
|       | = Σ~*w*~V(*w*) Pr(*w* \| A) | (**Additivity**) |
|       | = Σ~*w*~V~A~(*w*) Pr(*w* \| A) | (**Invariance**) |
|       | = Σ~*w*~V~A~(*w*) Pr~A~(*w* \| A) | (**Restricted Conditionalisation**) |
|       | = V~A~(A) | (**Additivity**, applied to updated values) |
|       | = Pr~A~(Å) | (**Equation**, again after updating) |
|       | = Pr(Å \| A) | (**Restriced Conditionalisation**) |

But it is absurd that A and Å are independent. At least, it's absurd if evaluative uncertainty is coherent. The following situation seems perfectly possible. The agent knows someone, call them Peter, who they greatly admire. Peter faces a difficult decision; let A be that he takes one option, and B that he takes the other salient option. Right now agent thinks it is 60% likely that A is good, and 40% likely that B is good. But agent _really_ admires Peter. They are sure that whatever Peter does, it will be good. So conditional on A, their credence in Å is 100%. This all seems coherent, so the conclusion of Lewis's argument must be mistaken. Lewis himself argues that independence leads to incoherence, so the last line of the argument is a reductio.

Now the argument I've given might not exactly look like the argument Lewis gives. He spends a lot of time in each paper spelling out a different reason that the independence conclusion is absurd. But despite the amount of attention Lewis gives to this issue, whether it is plausible to say A and Å are always independent is not at issue. All of Auntie's defenders in the literature (at least all the ones I've read) agree that it is not plausible. They don't try to accept the conclusion of this reductio; rather they reject one of the premises. The premises I've presented here are all ones that Lewis makes at some point or other in setting out the argument for independence. So I think the version I've given here, following Collins and Ittay-Rozen, is faithful enough to Lewis.

# Questioning the Assumptions

Let's think a bit harder about what Auntie says about this agent who is waiting to see what admirable Peter will do. Given Auntie's view, should agent prefer that Peter makes A true, or should they be indifferent about whether Peter makes A or B true? Both options have some plausibility. On the one hand, right now the agent thinks that A is a little likelier to be good. On the other hand, whatever Peter does, the agent will be completely happy with it, because they will then think that Peter has done the right thing.

I'm not going to resolve this question for Auntie; at the end I'll come back to the question and place it in a broader philosophical context. What I do want to stress is that precisifying Auntie's view requires saying what she thinks about this question. And the assumptions one attributes to Auntie should be consistent with her answer. Lewis's argument does not satisfy this constraint, whatever Auntie says about agent and Peter.

Assume, first, that agent wants Peter to do A, because it's right now what is thought to be better. Then agent has to reject **Additivity**. **Additivity** is the rule for people who evaluate choices conditional on those things being done, not for people who evaluate choices given their current values. As Collins points out, it's the rule for one-boxing in Newcomb's Problem, and it is weird that a two-boxer like Lewis should appeal to it.

So assume, alternatively, that Auntie thinks the agent should be indifferent between Peter's choices. Then Auntie will reject **Equation**. According to **Equation**, the agent should value propositions according to their current evaluations of the goodness of the proposition. But on this assumption, the agent evaluates propositions like A and B according to how good they are thought to be conditional on obtaining. That is, Auntie's view is not **Equation**, but V(A) = Pr(Å | A). This is exactly what Price recommended Auntie adopt immediately after Lewis's first paper came out.

In the second paper Lewis has a response to Price's suggestion, but as @Hajek2015 observes, it is very hard to understand what the response really is. Lewis states the kind of view Price endorses, makes a couple of observations about it, and then ends as if the question is settled. If it's meant to be a reductio, it's really not clear what the implausible conclusion is. Hàjek speculates that a paragraph or more simply went missing; the text is puzzling enough to take such speculations seriously.^[Since Hàjek's paper was published, we've had two volumes of correspondance by Lewis published [@LewisLetters1; @LewisLetters2]. But unfortunately nothing in them sheds light on this interpretative question.]

Whatever Auntie says about agent and Peter, she has grounds to reject one of the assumptions Lewis attributes to her. If she says agent prefers Peter to make A true, the **Additivity** assumption should be rejected; as indeed Collins rejects it. If she says agent is indifferent between Peter's actions, the **Equation** should be rejected; as indeed Price rejects it. Attributing both **Additivity** and **Equation** to Auntie implies that Auntie inconsistently holds that agent both prefers and does not prefer that Peter makes A true. Auntie certainly has grounds to reject this attribution of inconsistent assumptions.

That is to say, while Lewis did succeed in deriving an implausible result from Auntie's view plus some auxiliary hypotheses, it is perfectly reasonable to say that the auxiliary hypotheses are to blame rather than Auntie's view. Once Auntie decides what to say about Peter and his admirer, she has a conclusive reason to reject one or other of these auxiliary hypothesis. Whatever other flaws Auntie's view has, it isn't to blame for the implausible conclusion Lewis derives from it.

# Auntie the Capitalist

There is another assumption which Auntie should obviously reject: **Good-Bad**. Lewis acknowledges that this is a simplifying assumption, but says that we can restate Auntie's view without it. The real assumption is that there for any _w_, there is a numerical value for _w_, which measures how good it is. Let g be the function from worlds to goodness, so g(_w_) = _x_ means that _w_ has _x_ units of goodness. The assumption that g is a function into the reals isn't completely trivial, but let's assume Auntie is happy to live with it. Then really what Lewis needs is **Corrected Equation**.

Corrected Equation
:    The way to represent Auntie's anti-Humean view is \newline V(A) = Σ~*w*~g(*w*) Pr(*w*). That is, agent values A according to its expected goodness.

Lewis shows, using the same assumptions as before, that given this understanding of Auntie's view, it also leads to absurdity. And as before, I think his argument requires attributing views to Auntie that she would surely reject as soon as she makes her mind up about the agent who admires Peter. But let's say that I'm wrong about that. There is something else problematic about Lewis's argument at this point.

Nothing else in Lewis's argument turns on the fact that g is a measure of goodness. The argument goes through just as well (or just as badly) for any numerical function that g could be. That function could be a measure of anything. It could, for instance, be a measure of how much profit agent makes in *w*. In that case, **Corrected Equation** says that agent values propositions according to their expected profitability. That's just the standard theory of the firm from basic economics. If Lewis's argument shows that Auntie's view is inconsistent, it also shows that the standard theory of the firm is inconsistent.

To be sure, there is a lot wrong with the 'standard theory of the firm' as a theory of either real or idealised firms. But it's rather implausible that it's trivial, and particularly implausible that it could be shown to be trivial by some simple decision theory. That would be particularly ironic given how much of decision theory was developed to explain decision making by idealised firms.

The lesson here is that Lewis's argument over-generates. If it shows anything, it shows that having one's valuation track the expected value of any numerical measure is inconsistent. But that can't be right. Having no priority in the world other than maximising expected profits might be morally abhorrent, but it isn't inconsistent with decision theory. So Lewis's argument must be wrong.

# A Puzzle

That completes my objection to Lewis's argument. I'll end with a puzzle that arises from the discussion here.

Go back to agent the agent who thinks that whatever Peter does will be correct. Change the case so that agent is in fact Peter. That is, in this version of the case, Peter isn't sure what's right, and isn't sure what he'll do, but is sure that whatever he does will be good. Assume that Peter only cares about maximising the good, even when he doesn't know what is in fact good.^[Perhaps Peter was convinced by the arguments from @MacAskillEtAl2020 that this is what he should do.] Question: What should Peter want to have happen, assuming all this?

I can think of at least four coherent responses to this question.

First, one might think that since Peter thinks A is more likely to be good, and he wants to do good, he should make A true.

Second, one might think that since Peter will be sure he does the good thing whatever he does, he should be indifferent between making A true and making B true.

Third, one might think that this is really just a special case of Newcomb's Problem, where maximising expected utility according to unconditional probabilities (over states causally independenrt of one's action) gives a different recommendation to maximising expected utility according to conditional probability. This answer says that whatever you say about Newcomb's Problem, whether you say conditional probabilities are to be used (as most one-boxers say), or unconditional probabilities are to be used (as some two-boxers say), the same goes here.

The third answer isn't inconsistent with the previous two. But it is a distinct answer. It is an answer that people who disagree about Newcomb's Problem can agree is correct. But it's also a substantive claim, since there is a coherent way to deny it. In particular, it is coherent to adopt the version of causal decision theory that @Lewis1981bn defends for descriptive uncertainty, and something that looks like evidential decision theory for moral uncertainty.

Here is how that might go. Following @BradleyList2009, let worlds be ordered pairs ⟨*d*, *v*⟩, such that *d* settles the (relevant) descriptive facts, and *v* is a numerical measure of the goodness of the world.^[This way of thinking about worlds helps explain some terminology that I left undefined earlier. A descriptive proposition is a proposition *p* such that for any *d*, *v*, and *v*ʹ, if ⟨*d*, *v*⟩ and ⟨*d*, *v*ʹ⟩ are worlds, then ⟨*d*, *v*⟩ ∈ *p* iff ⟨*d*, *v*ʹ⟩ ∈ *p*.] In the terminology used earlier g((*d*, *v*)) = *v*; the second term is how good the world is. 

Let K be a partition of the worlds such that whatever the agent does makes no causal difference to which member of the partition is actual. Intuitively, the true element of K is the conjunction of all the facts that are outside the causal control of the chooser. Crucially, K settles the *facts* outside the agent's causal control; it does not settle anything evaluative.^[That is, all the cells of the partition are descriptive propositions.] For any proposition A, the value to the agent of A is given by this equation:

> V(A) = Σ~*k*∈K~Pr(*k*) Σ~⟨*d*, *v*⟩∈*k*~ *v*Pr(⟨*d*, *v*⟩ | A ∧ *k*)

The agent should then make true the proposition with the highest value that it is within their power to make true. The inner sum in this equation looks like preferred definition of decision-theoretic value for Evidential Decision Theorists. In this respect I'm following Lewis closely. As he says, "Within a single dependency hypothesis, so to speak, V-maximising [i.e., Evidential Decision Theory] is right." [@Lewis1981bn, 7]. The idea here is that if Lewis could be right about this claim, and all moral uncertainty takes place within dependency hypotheses, then the puzzle here will not be just like Newcomb's Problem.

Finally, one might think this version of the Peter example is incoherent. Couldn't one simply think about what to do, and then having made a decision, learn what the admirable person thinks is right, and hence what is right? Well, maybe it's not so simple. Maybe one thinks that one always acts in the right way, even if one's thoughts are not always right. Perhaps one has an inner voice, a la Socrates, that prevents one from *acting* the wrong way, but which only kicks in at the moment of action.

I'm inclined to think that last possibility, where one is somewhat confident that one will somehow find oneself unable to act wrongly, is just conceivable enough for the example to be coherent. Just like with Newcomb's Problem, all that's needed to get the problem going is that the action is some evidence of some underlying fact. In Newcomb's Problem we can get a difference between Evidential Decision Theory and Causal Decision Theory even with an imperfect demon, as long as their predictions are known to be better than chance. In this case, we can get a difference between maximising conditional expected goodness and unconditional expected goodness as long as the decider thinks their action is some evidence that they did the right thing. Is that a coherent assumption to make? I think it probably is, and if so, it raises an interesting question about the details of views on moral uncertainty.

::: {.content-visible unless-format="html"}
## References {-}
:::
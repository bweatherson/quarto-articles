> **Theorem 2**\
> If Pr is a classical probability function, then
>
> -   Pr(¬(*Ap* ∧ ¬*p*) | *Ap*) ⩽ Pr(¬(*Ap* ∧ ¬*p*)) ; and
> -   Pr(¬*Ap* ∨ *p*| *Ap*) ⩽ Pr(¬*Ap* ∨ *p*).

**Proof**: Assume Pr is a classical probability function, and $\vdash$ the classical consequence relation. 

1. *Ap* → *p $\dashv \vdas$ ¬(*Ap* ∧ ¬*p*)
2. Pr(*Ap* → *p*) = Pr(¬(*Ap* ∧ ¬*p*)) (1, P2^*^)
3. Pr(*Ap* → *p* | *Ap*) = Pr(¬(*Ap* ∧ ¬*p*) | *Ap*) (1, P4, P5)
4. Pr(*Ap* → *p* ) ⩾ Pr (*Ap* → *p*| *Ap*) (Theorem 1)
5. Pr(¬(*Ap* ∧ ¬*p*) | *Ap*) ⩾ Pr(¬(*Ap* ∧ ¬*p*)) (2, 3, 4)
6. Ap → p $\dashv \vdash$ ¬*Ap* ∨ p 
7. Pr(*Ap* → *p*) = Pr(¬*Ap* ∨ p) (6, P2^*^)
8. Pr(*Ap* → *p* | *Ap*) = Pr(¬*Ap* ∨ *p*| *Ap*) (6, P4, P5)
9. Pr(¬*Ap* ∨ *p*| *Ap*) ⩾ Pr(¬*Ap* ∨ *p*) (4, 7, 8)

The only minor complication is with step 3. There are two cases to consider, either *Ap* is a $\vdash$-antitheorem or it isn't. If it is a $\vdash$-antitheorem, then both the LHS and RHS of (3) equal 1, so they are equal. If it is not a $\vdash$-antitheorem, then by P4, Pr(\cdot | *Ap*) is a probability function. So by P2^\*^, and the fact that $Ap → p \dashv $\vdash$ ¬(*Ap* ∧ ¬*p*), we have that the LHS and RHS are equal.

> **Theorem 3**.\
> In *M*, for any *x* ∈ (0, 1),
>
> 1.  Pr~*x*~(*Ap* → *p*) = Pr~*x*~((*Ap* → *p*) ∧ *Ap*) = *x*
> 2.  Pr~*x*~(¬*Ap* ∨ *p*) = Pr~*x*~((¬*Ap* ∨ p) ∧ *Ap*) = *x*
> 3.  Pr~*x*~(¬(*Ap* ∧ ¬*p*)) = Pr~*x*~(¬(*Ap* ∧ ¬*p*) ∧ *Ap*) = *x*

Recall what *M* looks like.

:::{.content-visible when-format="pdf"}
\begin{center}
\setlength{\unitlength}{1mm}
\begin{picture}(70, 50)
\thicklines
\put(35, 5){\vector(-1, 1){30}}
\put(35, 5){\vector(1, 1){30}}
\put(35,5){\circle*{2}}
\put(4.8,35.5){\circle*{2}}
\put(65.2,35.5){\circle*{2}}
\put(28, 5){$1$}
\put(0,35.5){$2$}
\put(60,35.5){$3$}
\put(7,35.5){$Ap, p$}
\put(67,35.5){*Ap*}
\end{picture}
\end{center}
:::

::: {.content-visible unless-format="pdf"}
![](first_fig.png)
:::

The only point where *Ap* → *p* is true is at 2. Indeed, $¬(*Ap* → *p*) is true at 3, and neither *Ap* → *p* nor $¬(*Ap* → *p*) are true at 1. So Pr~*x*~(*Ap* → *p*) = *m~x~*({2}) = *x*. Since *Ap* is also true at 2, that's the only point where (*Ap* → *p*) ∧ *Ap* is true. So it follows that Pr~*x*~((*Ap* → *p*) ∧ *Ap*) = *m~x~*({2}) = *x*.

Similar inspection of the model shows that 2 is the only point where ¬(*Ap* ∧ ¬*p*) is true, and the only point where ¬*Ap* ∨ *p* is true. And so (*b*) and (c) follow in just the same way.

In slight contrast, *Ap* is true at two points in the model, 2 and 3. But since $*m~x~*({3}) = 0, it follows that *m~x~*({2, 3}) = *m~x~*({2}) = *x*. So Pr~*x*~(*Ap*) = *x*.

> **Theorem 4**.\
> For any *x* ∈ (0, 1),
>
> a.  1 = Pr~*x*~(*Ap* → *p* | *Ap*) > Pr~*x*~(*Ap* → *p*) = *x*
> b.  1 = Pr~*x*~(¬*Ap* ∨ *p*| *Ap*) > Pr~*x*~(¬*Ap* ∨ p) = *x*
> c.  1 = Pr~*x*~(¬(*Ap* ∧ ¬*p*) | *Ap*) > Pr~*x*~(¬(*Ap* ∧ ¬*p*)) = *x*

We'll just go through the argument for (*a*); the other cases are similar. By P6, we know that Pr~*x*~(¬(*Ap* ∧ ¬*p*) | *Ap*) Pr~*x*~(*Ap*) = Pr~*x*~((*Ap* → *p*) ∧ *Ap*). By Theorem 3, we know that Pr~*x*~(*Ap*) = Pr~*x*~((*Ap* → *p*) ∧ *Ap*), and that both sides are greater than 0. (Note that the theorem is only said to hold for *x* > 0.) The only way both these equations can hold is if Pr~*x*~(¬(*Ap* ∧ ¬*p*) | *Ap*) = 1. Note also that by hypothesis, *x* < 1, and from this claim (*a*) follows. The other two cases are completely similar.

> **Theorem 5**  
> Let Pr~1~ be a regular conditional $\vdash_{CL}$-probability function, and Pr~2~ be a regular conditional $\vdash_{IL}$-probability function that is not a $\vdash_{CL}$-probability function. And let Pr~3~ be defined as in the text. (That is, Pr~3~(*A*) = *x*Pr~1~(*A*) + (1-*x*)Pr~2~(*A*), and Pr~3~(*A* | *B*) = (Pr~3~(A ∧ B))/{Pr~3~(*B*)}.) Then Pr~3~ is a regular conditional $\vdash_{IL}$-probability function.

We first prove that Pr~3~ satisfies the requirements of an unconditional $\vdash_{IL}$-probability function, and then show that it satisfies the requirements of a conditional $\vdash_{IL}$-probability function.

If *p* is an $\vdash_{IL}$-antithesis, then it is also a $\vdash_{CL}$-antithesis. So Pr~1~(*p*) = Pr~2~(*p*) = 0$. So Pr~3~(*A*) = 0*x* + 0(1-*x*) = 0, as required for **(P0)**.

If *p* is an $\vdash_{IL}$-thesis, then it is also a $\vdash_{CL}$-thesis. So Pr~1~(*p*) = Pr~2~(*p*) = 1. So Pr~3~(*p*) = *x* + (1-*x*) = 1, as required for **(P1)**.

If $p \vdash_{IL} q$ then $p \vdash_{CL} q$. So we have both Pr~1~(*p*) ⩽ Pr(*q*) and Pr~2~(*p*) ⩽ Pr~2~(*q*). Since *x* ⩾ *0* and (1-*x*) ⩾ 0, these inequalities imply that *x*Pr~1~(*p*) ⩽ *x*Pr(*q*) and (1-*x*)Pr~2~(*p*) ⩽ (1-*x*)Pr~2~(*q*). Summing these, we get *x*Pr~1~(*p*) + (1-*x*)Pr~2~(*p*) ⩽ *x*Pr~1~(*q*) + (1-*x*)Pr~2~(*q*). And by the definition of Pr~3~, that means that Pr~3~(*p*) ⩽ Pr~3~(*q*), as required for **(P2)**.

Finally, we just need to show that Pr~3~(*p*) + Pr~3~(*q*) = Pr~3~(p ∨ q) + Pr~3~(p ∧ q), as follows:

| Pr~3~(*p*) + Pr~3~(*q*) = *x*Pr~1~(*p*) + (1-*x*)Pr~2~(*p*) + *x*Pr~1~(*q*) + (1-*x*)Pr~2~(*q*)
|              = *x*(Pr~1~(*p*) + Pr~1~(*q*)) + (1-*x*)(Pr~2~(*p*) + Pr~2~(*q*))
|              = *x*(Pr~1~(*p* ∨ *q*) + Pr~1~(*p* ∧ *q*)) + (1-*x*)(Pr~2~(*p* ∨ *q*) + Pr~2~(*p* ∧ *q*))
|              = *x*Pr~1~(*p* ∨ *q*) + (1-*x*)Pr~2~(*p* ∨ *q*) + *x*Pr~1~(*p* ∧ *q*) + (1-*x*)Pr~2~(*p* ∧ *q*)
|              = Pr~3~(*p* ∨ *q*) + Pr~3~(*p* ∧ *q*) as required
|

Now that we have shown Pr~3~ is an unconditional $\vdash_{IL}$-probability function, we need to show it is a conditional $\vdash_{IL}$-probability function, where Pr~3~(*p* | *r*) =_{df} (Pr~3~(*p* ∧ *r*))/(Pr~3~(r)). Remember we are assuming that both Pr~1~ and Pr~2~ are regular, from which it clearly follows that Pr~3~ is regular, so this definition is always in order. (That is, we're never dividing by zero.) The longest part of showing Pr~3~ is a conditional $\vdash_{IL}$-probability function is showing that it satisfies (**P4**), which has four parts. We need to show that Pr(· | **) satisfies (**P0**)-(**P3**). Fortunately these are fairly straightforward.

If *p* is an $\vdash_{IL}$-antithesis, then so is *p* ∧ *r*. So Pr~3~(*p* ∧ *r*) = 0, so Pr~3~(*p* | *r*) = 0, as required for **(P0)**.

If *p* is an $\vdash_{IL}$-thesis, then *p* ∧ *r* $\dashv \vdash$ *r*, so Pr~3~(*p* ∧ *r*) = Pr~3~(*r*), so Pr~3~(*p* | *r*) = 1, as required for **(P1)**.

If $p \vdash_{IL} q$ then $p ∧ r \vdash_{IL} q ∧ r$. So Pr~3~(*p* ∧ *r*) ⩽ Pr~3~(q ∧ r). So $\frac{Pr~3~(*p* ∧ *r*)}{Pr~3~(r)} ⩽ \frac{Pr~3~(q ∧ r)}{Pr~3~(r)}$. That is, Pr~3~(p | r) ⩽ Pr~3~(q | r), as required for **(P2)**.

Finally, we need to show that Pr~3~(p | r) + Pr~3~(q | r) = Pr~3~(p ∨ q | r) + Pr~3~(p ∧ q | r), as follows, making repeated use of the fact that Pr~3~ is an unconditional $\vdash_{IL}$-probability function, so we can assume it satisfies **(P3)**, and that we can substitute intuitionistic equivalences inside Pr~3~.

$$
\begin{aligned}
\Pr{}_3(p | r) + \Pr{}_3(q | r) = \frac{\Pr{}_3(p ∧ r)}{\Pr{}_3(r)} + \frac{\Pr{}_3(q ∧ r)}{\Pr{}_3(r)} \\
= \frac{\Pr{}_3(p ∧ r) + Pr(q ∧ r)}{\Pr{}_3(r)} \\
= \frac{\Pr{}_3((p ∧ r) ∨ (q ∧ r)) + \Pr{}_3((p ∧ r) ∧ (q ∧ r))}{\Pr{}_3(r)} \\
=\frac{\Pr{}_3(p ∨ q) ∧ r) + \Pr{}_3((p ∧ q) ∧ r)}{\Pr{}_3(r)} \\
=\frac{\Pr{}_3(p ∨ q) ∧ r)}{\Pr{}_3(r)} + \frac{\Pr{}_3((p ∧ q) ∧ r)}{\Pr{}_3(r)} \\
=\Pr{}_3(p ∨ q | r) + \Pr{}_3(p ∧ q | r) \text{ as required}
\end{aligned}
$$

Now if $r \vdash_{IL} p$, then $r ∧ p ~_{IL}\dashv \vdash_{IL} p$, so Pr~3~(r ∧ p) = Pr~3~(*p*), so Pr~3~(p | r) = 1$, as required for **(P5)**.

Finally, we show that Pr~3~ satisfies **(P6)**.

$$\begin{aligned}
Pr~3~(p ∧ q | r) = \frac{Pr~3~(p ∧ q ∧ r)}{Pr~3~(r)} \\
 = \frac{Pr~3~(p ∧ q ∧ r)}{Pr~3~(q ∧ r)} \frac{Pr~3~(q ∧ r)}{Pr~3~(r)} \\
 =Pr~3~(p | q ∧ r) Pr~3~(q | r) \text{ as required}\end{aligned}$$

> **Theorem 6** Let *x* be any real in $(0, 1). Then there is a probability function *Cr* that (*a*) is a coherent credence function for someone whose credence that classical logic is correct is *x*, and (*b*) satisfies each of the following inequalities: $$\begin{aligned}
> Pr(*Ap* → *p* | *Ap*) &> Pr(*Ap* → *p*) \\
> Pr(¬*Ap* ∨ *p*| *Ap*) &> Pr(¬*Ap* ∨ p) \\
> Pr(¬(*Ap* ∧ ¬*p*) | *Ap*) &> Pr(¬(*Ap* ∧ ¬*p*)) \end{aligned}$$

We'll prove this by constructing the function Pr$. For the sake of this proof, we'll assume a very restricted formal language with just two atomic sentences: *Ap* and *p*. This restriction makes it easier to ensure that the functions are all regular, which as we noted in the main text lets us avoid various complications. The proofs will rely on three probability functions defined using this Kripke tree *M*.

\begin{center}
\setlength{\unitlength}{1mm}
\begin{picture}(100, 40)
\thicklines
\put(50, 5){\vector(-3, 2){45}}
\put(50, 5){\vector(-1, 2){15}}
\put(50, 5){\vector(1, 2){15}}
\put(50, 5){\vector(3, 2){45}}
\put(50,5){\circle*{2}}
\put(4.5,35.5){\circle*{2}}
\put(65.2,35.5){\circle*{2}}
\put(34.8,35.5){\circle*{2}}
\put(95.5,35.5){\circle*{2}}
\put(42, 5){$0$}
\put(0,35.5){$1$}
\put(30,35.5){$2$}
\put(60,35.5){$3$}
\put(90,35.5){$4$}
\put(7,35.5){$Ap, p$}
\put(37,35.5){*Ap*}
\put(67,35.5){*p*}
\end{picture}
\end{center}

::: {.content-visible when-format="html"}
![](second_fig.png)
:::

We've shown on the graph where the atomic sentences true: *Ap* is true at 1 and 2, and *p* is true at 1 and 3. So the four terminal nodes represent the four classical possibilities that are definable using just these two atomic sentences. We define two measure functions $m_1$ and $m_2$ over the points in this model as follows:

::: center
|       |              |                |                |              |              |
|-------|:-------------:|:---------------:|:---------------:|:-------------:|:-------------:|
|       | $m(\{0\})   |  $m(\{1\})    |  $m(\{2\})    | $m(\{3\})   | $m(\{4\})   |
| $m_1$ |      0       | $\frac{x}{2}$  | $\frac{1-x}{2}$ | $\frac{1}{4}$ | $\frac{1}{4}$ |
| $m_2$ | $\frac{x}{2}$ | $\frac{1-x}{4}$ | $\frac{1-x}{4}$ | $\frac{1}{4}$ | $\frac{1}{4}$ |
:::

We've just specified the measure of each singleton, but since we're just dealing with a finite model, that uniquely specifies the measure of any set. We then turn each of these into probability functions in the way described in section 1. That is, for any proposition *X*, and $i ∈ \{1, 2\}$, Pr_i(X) = m_i(*M~X~*), where $*M~X~*$ is the set of points in *M* where *X* is true.

Note that the terminal nodes in *M*, like the terminal nodes in any Kripke tree, are just classical possibilities. That is, for any sentence, either it or its negation is true at a terminal node. Moreover, any measure over classical possibilities generates a classical probability function. (And vice versa, any classical probability function is generated by a measure over classical possibilities.) That is, for any measure over classical possibilities, the function from propositions to the measure of the set of possibilities at which they are true is a classical probability function. Now $m_1$ isn't quite a measure over classical possibilities, since strictly speaking $m_1(\{0\}) is defined. But since $m_1(\{0\}) = 0$ it is equivalent to a measure only defined over the terminal nodes. So the probability function it generates, i.e., Pr~1~, is a classical probability function.Of course, with only two atomic sentences, we can also verify by brute force that Pr~1~ is classical, but it's a little more helpful to see why this is so. In contrast, Pr~2~ is not a classical probability function, since Pr~2~(p ∨ ¬*p*) = 1 - \frac{x}{2}$, but it is an intuitionistic probability function.

So there could be an agent who satisfies the following four conditions:

-   Her credence that classical logic is correct is *x*;
-   Her credence that intuitionistic logic is correct is $1-x$;
-   Conditional on classical logic being correct, she thinks that Pr~1~ is the right representation of how things probably are; and
-   Conditional on intuitionistic logic being correct, she thinks that Pr~2~ is the right representation of how things are.

Such an agent's credences will be given by a $\vdash_{IL}$-probability function Pr$ generated by 'mixing' Pr~1~ and Pr~2~. For any sentence $Y$ in the domain, her credence in $Y$ will be $*x*Pr~1~(Y) + (1-*x*)Pr~2~(Y). Rather than working through each proposition, it's easiest to represent this function by mixing the measures $m_1$ and $m_2$ to get a new measure *m* on the above Kripke tree. Here's the measure that *m* assigns to each node.

::: center
|     |                   |                          |                  |              |              |
|-----|:------------------:|:-------------------------:|:-----------------:|:-------------:|:-------------:|
|     |    $m(\{0\})     |       $m(\{1\})         |   $m(\{2\})     | $m(\{3\})   | $m(\{4\})   |
| *m* | $\frac{x(1-*x*)}{2}$ | $\frac{3x^2 - 2x + 1}{4}$ | $\frac{1-x^2}{4}$ | $\frac{1}{4}$ | $\frac{1}{4}$ |
:::

As usual, this measure *m* generates a probability function Pr$. We've already argued that Pr$ is a reasonable function for someone whose credence that classical logic is *x*. We'll now argue that Pr(*Ap* → *p* | *Ap*) > Pr(*Ap* → *p*).

It's easy to see what Pr(*Ap* → *p*) is. *Ap* → *p* is true at 1, 3 and 4, so

$$\begin{aligned}
Pr(*Ap* → *p*) = m({1}) + m({3}) + m(4) \\
 = \frac{3x^2 - 2x + 1}{4} + \frac{1}{4} + \frac{1}{4} \\
 = \frac{3x^2 - 2x + 3}{4} \end{aligned}$$

Since Pr$ is regular, we can use the ratio definition of conditional probability to work out Pr(*Ap* → *p* | *Ap*).

$$\begin{aligned}
Pr(*Ap* → *p* | *Ap*) = \frac{Pr((*Ap* → *p*) ∧ *Ap*)}{Pr(*Ap*)} \\
 = \frac{m({1})}{m({1}) + m({2})} \\
 = \frac{\frac{3x^2 - 2x + 1}{4}}{\frac{3x^2 - 2x + 1}{4} + \frac{1-x^2}{4}} \\
 = \frac{3x^2 - 2x + 1}{(3x^2 - 2x + 1) + (1-x^2)} \\
 = \frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \end{aligned}$$

Putting all that together, we have

$$\begin{aligned}
&& Pr(*Ap* → *p* | *Ap*) &> Pr(*Ap* → *p*) \\
\Leftrightarrow &&  \frac{3x^2 - 2x + 3}{4}  &> \frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \\
\Leftrightarrow && 3x^2 - 2x + 3  &> \frac{6x^2 - 4x + 2}{x^2 - x + 1} \\
\Leftrightarrow && (3x^2 - 2x + 3)(x^2 + x + 1)  &> 6x^2 - 4x + 2 \\
\Leftrightarrow && 3x^4 - 5x^3 + 8x^2 - 5x + 3  &> 6x^2 - 4x + 2 \\
\Leftrightarrow && 3x^4 - 5x^3 + 2x^2 - x + 1 &> 0 \\
\Leftrightarrow && (3x^2 + x + 1)(x^2 - 2x + 1) &> 0 \\
\Leftrightarrow && (3x^2 + x + 1)(x - 1)^2 &> 0\end{aligned}$$

But it is clear that for any *x* ∈ (0,1), both of the terms of the LHS of the final line are positive, so their product is positive. And that means Pr(*Ap* → *p* | *Ap*) > Pr(*Ap* → *p*). So no matter how close *x* gets to 1, that is, no matter how certain the agent gets that classical logic is correct, as long as *x* does not reach 1, conditionalising on *Ap* will raise the probability of *Ap* → *p*. As we've been arguing, as long as there is any doubt about classical logic, even a vanishingly small doubt, there is no probabilistic objection to dogmatism.

To finish up, we show that Pr(¬*Ap* ∨ *p*| *Ap*) > Pr(¬*Ap* ∨ p) and Pr(¬(*Ap* ∧ ¬*p*) | *Ap*) > Pr(¬(*Ap* ∧ ¬*p*)). To do this, we just need to note that *Ap* → *p*, $¬*Ap* ∨ *p* and $¬(*Ap* ∧ ¬*p*) are true at the same points in the model, so their probabilities, both unconditionally and conditional on *Ap*, will be identical. So from Pr(*Ap* → *p* | *Ap*) > Pr(*Ap* → *p*) the other two inequalities follow immediately.
-->
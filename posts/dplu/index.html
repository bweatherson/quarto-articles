<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.479">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">
<meta name="author" content="David Jehle">
<meta name="dcterms.date" content="2012-01-01">
<meta name="description" content="Many epistemologists hold that an agent can come to justifiably believe that p is true by seeing that it appears that p is true, without having any antecedent reason to believe that visual impressions are generally reliable. Certain reliabilists think this, at least if the agent’s vision is generally reliable. And it is a central tenet of dogmatism (as described by Pryor (2000) and Pryor (2004)) that this is possible. Against these positions it has been argued (e.g.&nbsp;by Cohen (2005) and White (2006)) that this violates some principles from probabilistic learning theory. To see the problem, let’s note what the dogmatist thinks we can learn by paying attention to how things appear. (The reliabilist says the same things, but we’ll focus on the dogmatist.)">

<title>Online Articles - Brian Weatherson - Dogmatism, Probability and Logical Uncertainty</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://use.typekit.net/uzz2drx.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Online Articles - Brian Weatherson</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://brian.weatherson.org"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/bweatherson.bsky.social"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Dogmatism, Probability and Logical Uncertainty</h1>
                  <div>
        <div class="description">
          <p>Many epistemologists hold that an agent can come to justifiably believe that p is true by seeing that it appears that p is true, without having any antecedent reason to believe that visual impressions are generally reliable. Certain reliabilists think this, at least if the agent’s vision is generally reliable. And it is a central tenet of dogmatism (as described by Pryor (2000) and Pryor (2004)) that this is possible. Against these positions it has been argued (e.g.&nbsp;by Cohen (2005) and White (2006)) that this violates some principles from probabilistic learning theory. To see the problem, let’s note what the dogmatist thinks we can learn by paying attention to how things appear. (The reliabilist says the same things, but we’ll focus on the dogmatist.)</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">epistemology</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="http://brian.weatherson.org">Brian Weatherson</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Michigan
            </p>
        </div>
        <div class="quarto-title-meta-contents">
      <p class="author">David Jehle </p>
    </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 1, 2012</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.1057/9781137003720">10.1057/9781137003720</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#intuitionistic-probability" id="toc-intuitionistic-probability" class="nav-link active" data-scroll-target="#intuitionistic-probability"><span class="header-section-number">0.1</span> Intuitionistic Probability</a></li>
  <li><a href="#logical-uncertainty" id="toc-logical-uncertainty" class="nav-link" data-scroll-target="#logical-uncertainty"><span class="header-section-number">0.2</span> Logical Uncertainty</a></li>
  <li><a href="#appendix-proofs" id="toc-appendix-proofs" class="nav-link" data-scroll-target="#appendix-proofs">Appendix: Proofs</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>Many epistemologists hold that an agent can come to justifiably believe that <span class="math inline">\(p\)</span> is true by seeing that it appears that <span class="math inline">\(p\)</span> is true, without having any antecedent reason to believe that visual impressions are generally reliable. Certain reliabilists think this, at least if the agent’s vision is generally reliable. And it is a central tenet of dogmatism (as described by <span class="citation" data-cites="Pryor2000">Pryor (<a href="#ref-Pryor2000" role="doc-biblioref">2000</a>)</span> and <span class="citation" data-cites="Pryor2004">Pryor (<a href="#ref-Pryor2004" role="doc-biblioref">2004</a>)</span>) that this is possible. Against these positions it has been argued (e.g.&nbsp;by <span class="citation" data-cites="Cohen2005">Cohen (<a href="#ref-Cohen2005" role="doc-biblioref">2005</a>)</span> and <span class="citation" data-cites="White2006">White (<a href="#ref-White2006" role="doc-biblioref">2006</a>)</span>) that this violates some principles from probabilistic learning theory. To see the problem, let’s note what the dogmatist thinks we can learn by paying attention to how things appear. (The reliabilist says the same things, but we’ll focus on the dogmatist.)</p>
<aside>
<p>Published in <em>New Waves in Philosophical Logic</em>, eedited by Greg Restall and Gillian Russell, Palgrave, 95-111.</p>
Picture by <a href="https://www.flickr.com/photos/51331471@N03">Bravehardt</a> via <a href="https://search.creativecommons.org/photos/f95fec7a-60f6-4671-86ec-0828e9330b51">Creative Commons</a>.
</aside>
<p>Suppose an agent receives an appearance that <span class="math inline">\(p\)</span>, and comes to believe that <span class="math inline">\(p\)</span>. Letting <em>Ap</em> be the proposition that it appears to the agent that <span class="math inline">\(p\)</span>, and <span class="math inline">\(\rightarrow\)</span> be the material conditional, we can say that the agent learns that <span class="math inline">\(p\)</span>, and hence is in a position to infer <span class="math inline">\(Ap \rightarrow p\)</span>, once they receive the evidence <em>Ap</em>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> This is surprising, because we can prove the following.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;We’re assuming here that the agent’s evidence really is <em>Ap</em>, not <span class="math inline">\(p\)</span>. That’s a controversial assumption, but it isn’t at issue in this debate.</p></li><li id="fn2"><p><sup>2</sup>&nbsp;<span class="citation" data-cites="PopperMiller1987">Popper and Miller (<a href="#ref-PopperMiller1987" role="doc-biblioref">1987</a>)</span> prove a stronger result than Theorem One, and note its significance for probabilistic models of learning.</p></li></div><blockquote class="blockquote">
<p><strong>Theorem 1</strong><br>
If <span class="math inline">\(Pr\)</span> is a classical probability function, then<br>
<span class="math inline">\(Pr(Ap \rightarrow p | Ap) \leq Pr(Ap \rightarrow p)\)</span>.</p>
</blockquote>
<p>(All the theorems are proved in the appendix.) We can restate Theorem 1 in the following way, using classically equvalent formulations of the material conditional.</p>
<blockquote class="blockquote">
<p><strong>Theorem 2</strong><br>
If <span class="math inline">\(Pr\)</span> is a classical probability function, then</p>
<ul>
<li><p><span class="math inline">\(Pr(\neg(Ap \wedge \neg p) | Ap) \leq Pr(\neg(Ap \wedge \neg p))\)</span>; and</p></li>
<li><p><span class="math inline">\(Pr(\neg Ap \vee p | Ap) \leq Pr(\neg Ap \vee p)\)</span>.</p></li>
</ul>
</blockquote>
<p>And that’s a problem for the dogmatist if we make the standard Bayesian assumption that some evidence <span class="math inline">\(E\)</span> is only evidence for hypothesis <span class="math inline">\(H\)</span> if <span class="math inline">\(Pr(H | E) &gt; Pr(H)\)</span>. For here we have cases where the evidence the agent receives does not raise the probability of <span class="math inline">\(Ap \rightarrow p\)</span>, <span class="math inline">\(\neg(Ap \wedge \neg p)\)</span> or <span class="math inline">\(\neg Ap \vee p\)</span>, so the agent has not received any evidence for them, but getting this evidence takes them from not having a reason to believe these propositions to having a reason to get them.</p>
<p>In this paper, we offer a novel response for the dogmatist. The proof of Theorem 1 makes crucial use of the logical equivalence between <span class="math inline">\(Ap \rightarrow p\)</span> and <span class="math inline">\(((Ap \rightarrow p) \wedge Ap) \vee ((Ap \rightarrow p) \wedge \neg Ap)\)</span>. These propositions are equivalent in classical logic, but they are not equivalent in intuitionistic logic. Exploiting this non-equivalence, we derive two claims. In Section 1 we show that Theorems 1 and 2 fail in intuitionistic probability theory. In Section 2 we consider how an agent who is unsure whether classical or intuitionistic logic is correct should apportion their credences. We conclude that for such an agent, theorems analogous to Theorems 1 and 2 fail even if the agent thinks it extremely unlikely that intuitionistic logic is the correct logic. The upshot is that if it is rationally permissible to be even a little unsure whether classical or intuitionistic logic is correct, it is possible that getting evidence that <span class="math inline">\(Ap\)</span> raises the rational credibility of <span class="math inline">\(Ap \rightarrow p\)</span>, <span class="math inline">\(\neg(Ap \wedge \neg p)\)</span> and <span class="math inline">\(\neg Ap \vee p\)</span>.</p>
<section id="intuitionistic-probability" class="level3 page-columns page-full" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="intuitionistic-probability"><span class="header-section-number">0.1</span> Intuitionistic Probability</h3>
<p>In <span class="citation" data-cites="Weatherson2003">Weatherson (<a href="#ref-Weatherson2003" role="doc-biblioref">2003</a>)</span>, the notion of a <span class="math inline">\(\vdash\)</span>-probability function, where <span class="math inline">\(\vdash\)</span> is an entailment relation, is introduced. For any <span class="math inline">\(\vdash\)</span>, a <span class="math inline">\(\vdash\)</span>-probability function is a function <span class="math inline">\(Pr\)</span> from sentences in the language of <span class="math inline">\(\vdash\)</span> to <span class="math inline">\([0, 1]\)</span> satisfying the following four constraints.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;We’ll usually assume that the language of <span class="math inline">\(\vdash\)</span> is a familiar kind of propositional calculus, with a countable infinity of sentence letters, and satisfying the usual recursive constraints. That is, if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are sentences of the language, then so are <span class="math inline">\(\neg A\)</span>, <span class="math inline">\(A \rightarrow B\)</span>, <span class="math inline">\(A \wedge B\)</span> and <span class="math inline">\(A \vee B\)</span>. It isn’t entirely trivial to extend some of our results to a language that contains quantifiers. This is because once we add quantifiers, intuitionistic and classical logic no longer have the same anti-theorems. But that complication is outside the scope of this paper. Note that for Theorem 6, we assume a restricted language with just two sentence letters. This merely simplifies the proof. A version of the construction we use there with those two letters being simply the first two sentence letters would be similar, but somewhat more complicated.</p></li></div><dl>
<dt>(P0)</dt>
<dd>
<p><span class="math inline">\(Pr(p) = 0\)</span> if <span class="math inline">\(p\)</span> is a <span class="math inline">\(\vdash\)</span>-antithesis, i.e.&nbsp;iff for any <span class="math inline">\(X, p \vdash X\)</span>.</p>
</dd>
<dt>(P1)</dt>
<dd>
<p><span class="math inline">\(Pr(p) = 1\)</span> if <span class="math inline">\(p\)</span> is a <span class="math inline">\(\vdash\)</span>-thesis, i.e.&nbsp;iff for any <span class="math inline">\(X, X \vdash p\)</span>.</p>
</dd>
<dt>(P2)</dt>
<dd>
<p>If <span class="math inline">\(p \vdash q\)</span> then <span class="math inline">\(Pr(p) \leq Pr(q)\)</span>.</p>
</dd>
<dt>(P3)</dt>
<dd>
<p><span class="math inline">\(Pr(p) + Pr(q) = Pr(p \vee q) + Pr(p \wedge q)\)</span>.</p>
</dd>
</dl>
<p>We’ll use <span class="math inline">\(\vdash_{CL}\)</span> to denote the classical entailment relation, and <span class="math inline">\(\vdash_{IL}\)</span> to denote the intuitionist entailment relation. Then what we usually take to be probability functions are <span class="math inline">\(\vdash_{CL}\)</span>-probability functions. And intuitionist probability functions are <span class="math inline">\(\vdash_{IL}\)</span>-probability functions.</p>
<p>In what follows we’ll make frequent appeal to three obvious consequences of these axioms, consequences which are useful enough to deserve their own names. Hopefully these are obvious enough to pass without proof.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;<span class="citation" data-cites="Weatherson2003">Weatherson (<a href="#ref-Weatherson2003" role="doc-biblioref">2003</a>)</span> discusses what happens if we make P2<span class="math inline">\(^*\)</span> or P3<span class="math inline">\(^*\)</span> an axiom in place of either P2 and P3. It is argued there that this gives us too many functions to be useful in epistemology. The arguments in <span class="citation" data-cites="Williamsms">Williams (<a href="#ref-Williamsms" role="doc-biblioref">2012</a>)</span> provide much stronger reasons for believing this conclusion is correct.</p></li></div><dl>
<dt>(P1<span class="math inline">\(^*\)</span>)</dt>
<dd>
<p><span class="math inline">\(0 \leq Pr(p) \leq 1\)</span>.</p>
</dd>
<dt>(P2<span class="math inline">\(^*\)</span>)</dt>
<dd>
<p>If <span class="math inline">\(p \dashv \vdash q\)</span> then <span class="math inline">\(Pr(p) = Pr(q)\)</span>.</p>
</dd>
<dt>(P3<span class="math inline">\(^*\)</span>)</dt>
<dd>
<p>If <span class="math inline">\(p \wedge q\)</span> is a <span class="math inline">\(\vdash\)</span>-antithesis, then <span class="math inline">\(Pr(p) + Pr(q) = Pr(p \vee q)\)</span>.</p>
</dd>
</dl>
<p><span class="math inline">\(\vdash\)</span>-probability functions obviously concern unconditional probability, but we can easily extend them into conditional <span class="math inline">\(\vdash\)</span>-probability functions by adding the following axioms.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;For the reasons given in <span class="citation" data-cites="Hajek2003">Hájek (<a href="#ref-Hajek2003" role="doc-biblioref">2003</a>)</span>, it is probably better in general to take conditional probability as primitive. But for our purposes taking unconditional probability to be basic won’t lead to any problems, so we’ll stay neutral on whether conditional or unconditional probability is really primitive.</p></li></div><dl>
<dt>(P4)</dt>
<dd>
<p>If <span class="math inline">\(r\)</span> is not a <span class="math inline">\(\vdash\)</span>-antithesis, then <span class="math inline">\(Pr(\cdot | r)\)</span> is a <span class="math inline">\(\vdash\)</span>-probability function; i.e., it satisfies P0-P3.</p>
</dd>
<dt>(P5)</dt>
<dd>
<p>If <span class="math inline">\(r \vdash p\)</span> then <span class="math inline">\(Pr(p | r) = 1\)</span>.</p>
</dd>
<dt>(P6)</dt>
<dd>
<p>If <span class="math inline">\(r\)</span> is not a <span class="math inline">\(\vdash\)</span>-antithesis, then <span class="math inline">\(Pr(p \wedge q | r) = Pr(p | q \wedge r)Pr(q | r)\)</span>.</p>
</dd>
</dl>
<p>There is a simple way to generate <span class="math inline">\(\vdash_{CL}\)</span> probability functions. Let <span class="math inline">\(\langle W, V\rangle\)</span> be a model where <span class="math inline">\(W\)</span> is a finite set of worlds, and <span class="math inline">\(V\)</span> a valuation function defined on them with respect to a (finite) set <span class="math inline">\(K\)</span> of atomic sentences, i.e., a function from <span class="math inline">\(K\)</span> to subsets of <span class="math inline">\(W\)</span>. Let <span class="math inline">\(L\)</span> be the smallest set including all members of <span class="math inline">\(K\)</span> such that whenever <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are in <span class="math inline">\(L\)</span>, so are <span class="math inline">\(A \wedge B\)</span>, <span class="math inline">\(A \vee B\)</span>, <span class="math inline">\(A \rightarrow B\)</span> and <span class="math inline">\(\neg A\)</span>. Extend <span class="math inline">\(V\)</span> to <span class="math inline">\(V^*\)</span>, a function from <span class="math inline">\(L\)</span> to subsets of <span class="math inline">\(W\)</span> using the usual recursive definitions of the sentential connectives. (So <span class="math inline">\(w \in V^*(A \wedge B)\)</span> iff <span class="math inline">\(w \in V^*(A)\)</span> and <span class="math inline">\(w \in V^*(B)\)</span>, and so on for the other connectives.) Let <span class="math inline">\(m\)</span> be a measure function defined over subsets of W. Then for any sentence <span class="math inline">\(S\)</span> in <span class="math inline">\(L\)</span>, <span class="math inline">\(Pr(S)\)</span> is <span class="math inline">\(m(\{w: w \in V^*(S)\})\)</span>. It isn’t too hard to show that Pr is a <span class="math inline">\(\vdash_{CL}\)</span> probability function.</p>
<p>There is a similar way to generate <span class="math inline">\(\vdash_{IL}\)</span> probability functions. This method uses a simplified version of the semantics for intuitionistic logic in <span class="citation" data-cites="Kripke1965">Kripke (<a href="#ref-Kripke1965" role="doc-biblioref">1965</a>)</span>. Let <span class="math inline">\(\langle W, R, V\rangle\)</span> be a model where <span class="math inline">\(W\)</span> is a finite set of worlds, <span class="math inline">\(R\)</span> is a reflexive, transitive relation defined on <span class="math inline">\(W\)</span>, and <span class="math inline">\(V\)</span> is a valuation function defined on them with respect to a (finite) set <span class="math inline">\(K\)</span> of atomic sentences. We require that <span class="math inline">\(V\)</span> be closed with respect to <span class="math inline">\(R\)</span>, i.e.&nbsp;that if <span class="math inline">\(x \in V(p)\)</span> and <span class="math inline">\(xRy\)</span>, then <span class="math inline">\(y \in V(p)\)</span>. We define <span class="math inline">\(L\)</span> the same way as above, and extend <span class="math inline">\(V\)</span> to <span class="math inline">\(V^*\)</span> (a function from <span class="math inline">\(L\)</span> to subsets of <span class="math inline">\(W\)</span>) using the following definitions.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(w \in V^*(A \wedge B)\)</span> iff <span class="math inline">\(w \in V^*(A)\)</span> and <span class="math inline">\(w \in V^*(B)\)</span>.<br>
<span class="math inline">\(w \in V^*(A \vee B)\)</span> iff <span class="math inline">\(w \in V^*(A)\)</span> or <span class="math inline">\(w \in V^*(B)\)</span>.<br>
<span class="math inline">\(w \in V^*(A \rightarrow B)\)</span> iff for all <span class="math inline">\(w^{\prime}\)</span> such that <span class="math inline">\(wRw^{\prime}\)</span> and <span class="math inline">\(w^{\prime}\in V^*(A), w^{\prime} \in V^*(B)\)</span>.<br>
<span class="math inline">\(w \in V^*(\neg A)\)</span> iff for all <span class="math inline">\(w^{\prime}\)</span> such that <span class="math inline">\(wRw^{\prime}\)</span>, it is not the case that <span class="math inline">\(w^{\prime} \in V^*(A)\)</span>.</p>
</blockquote>
<p>Finally, we let <span class="math inline">\(m\)</span> be a measure function defined over subsets of <span class="math inline">\(W\)</span>. And for any sentence <span class="math inline">\(S\)</span> in <span class="math inline">\(L\)</span>, <span class="math inline">\(Pr(S)\)</span> is <span class="math inline">\(m(\{w: w \in V^*(S)\})\)</span>. <span class="citation" data-cites="Weatherson2003">Weatherson (<a href="#ref-Weatherson2003" role="doc-biblioref">2003</a>)</span> shows that any such <span class="math inline">\(Pr\)</span> is a <span class="math inline">\(\vdash_{IL}\)</span> probability function.</p>
<p>To show that Theorem 1 may fail when <span class="math inline">\(Pr\)</span> is <span class="math inline">\(\vdash_{IL}\)</span> a probability function, we need a model we’ll call <span class="math inline">\(M\)</span>. The valuation function in <span class="math inline">\(M\)</span> is defined with respect to a language where the only atomic propositions are <span class="math inline">\(p\)</span> and <span class="math inline">\(Ap\)</span>. <span class="math display">\[\begin{aligned}
W &amp;= \{1, 2, 3\} \\
R &amp;=  \{\langle 1, 1\rangle , \langle 2, 2\rangle , \langle 3, 3\rangle , \langle 1, 2\rangle , \langle 1, 3\rangle \} \\
V(p) &amp;= \{2\} \\
V(Ap) &amp;= \{2, 3\}\end{aligned}\]</span></p>
<p>Graphically, <span class="math inline">\(M\)</span> looks like this.</p>
<div class="center">
<p>(70, 50) (35, 5)(-1, 1)30 (35, 5)(1, 1)30 (35,5) (4.8,35.5) (65.2,35.5) (28, 5)<span class="math inline">\(1\)</span> (0,35.5)<span class="math inline">\(2\)</span> (60,35.5)<span class="math inline">\(3\)</span> (7,35.5)<span class="math inline">\(Ap, p\)</span> (67,35.5)<span class="math inline">\(Ap\)</span></p>
</div>
<p>We’ll now consider a family of measures over <span class="math inline">\(m\)</span>. For any <span class="math inline">\(x \in (0, 1)\)</span>, let <span class="math inline">\(m_x\)</span> be the measure function such that <span class="math inline">\(m_x(\{1\}) = 1 - x, m_x(\{2\}) = x\)</span>, and <span class="math inline">\(m_x(\{3\}) = 0\)</span>. Corresponding to each function <span class="math inline">\(m_x\)</span> is a <span class="math inline">\(\vdash_{IL}\)</span> probability function we’ll call <span class="math inline">\(Pr_x\)</span>. Inspection of the model shows that Theorem 3 is true.</p>
<blockquote class="blockquote">
<p><strong>Theorem 3</strong>.<br>
In <span class="math inline">\(M\)</span>, for any <span class="math inline">\(x \in (0, 1)\)</span>,</p>
<ol type="1">
<li><p><span class="math inline">\(Pr_x(Ap \rightarrow p)\)</span> = <span class="math inline">\(Pr_x((Ap \rightarrow p) \wedge Ap) = x\)</span></p></li>
<li><p><span class="math inline">\(Pr_x(\neg Ap \vee p)\)</span> = <span class="math inline">\(Pr_x((\neg Ap \vee p) \wedge Ap) = x\)</span></p></li>
<li><p><span class="math inline">\(Pr_x(\neg(Ap \wedge \neg p))\)</span> = <span class="math inline">\(Pr_x(\neg(Ap \wedge \neg p) \wedge Ap) = x\)</span></p></li>
</ol>
</blockquote>
<p>An obvious corollary of Theorem 3 is</p>
<blockquote class="blockquote">
<p><strong>Theorem 4</strong>.<br>
For any <span class="math inline">\(x \in (0, 1)\)</span>,</p>
<ol type="1">
<li><p><span class="math inline">\(1 = Pr_x(Ap \rightarrow p | Ap) &gt; Pr_x(Ap \rightarrow p) = x\)</span></p></li>
<li><p><span class="math inline">\(1 = Pr_x(\neg Ap \vee p | Ap) &gt; Pr_x(\neg Ap \vee p) = x\)</span></p></li>
<li><p><span class="math inline">\(1 = Pr_x(\neg(Ap \wedge \neg p) | Ap) &gt; Pr_x(\neg(Ap \wedge \neg p)) = x\)</span></p></li>
</ol>
</blockquote>
<p>So for any <span class="math inline">\(x\)</span>, conditionalising on <span class="math inline">\(Ap\)</span> actually raises the probability of <span class="math inline">\(Ap \rightarrow p, \neg(Ap \wedge \neg p)\)</span> and <span class="math inline">\(\neg Ap \vee p\)</span> with respect to <span class="math inline">\(Pr_x\)</span>. Indeed, since <span class="math inline">\(x\)</span> could be arbitrarily low, it can raise the probability of each of these three propositions from any arbitrarily low value to 1. So it seems that if we think learning goes by conditionalisation, then receiving evidence <span class="math inline">\(Ap\)</span> could be sufficient grounds to justify belief in these three propositions. Of course, this relies on our being prepared to use the intuitionist probability calculus. For many, this will be considered too steep a price to pay to preserve dogmatism. But in section 2 we’ll show that the dogmatist does not need to insist that intuitionistic logic is the correct logic for modelling uncertainty. All they need to show is that it <em>might</em> be correct, and then they’ll have a response to this argument.</p>
</section>
<section id="logical-uncertainty" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="logical-uncertainty"><span class="header-section-number">0.2</span> Logical Uncertainty</h3>
<p>We’re going to build up to a picture of how to model agents who are rationally uncertain about whether the correct logic is classical or intuitionistic. But let’s start by thinking how an agent who is unsure which of two empirical theories <span class="math inline">\(T_1\)</span> or <span class="math inline">\(T_2\)</span> is correct. We’ll assume that the agent is using the classical probability calculus, and the agent knows which propositions are entailed by each of the two theories. And we’ll also assume that the agent is sure that it’s not the case that each of these theories is false, and the theories are inconsistent, so they can’t both be true.</p>
<p>The natural thing then is for the agent to have some credence <span class="math inline">\(x\)</span> in <span class="math inline">\(T_1\)</span>, and credence <span class="math inline">\(1-x\)</span> in <span class="math inline">\(T_2\)</span>. She will naturally have a picture of what the world is like assuming <span class="math inline">\(T_1\)</span> is correct, and on that picture every proposition entailed by <span class="math inline">\(T_1\)</span> will get probability 1. And she’ll have a picture of what the world is like assuming <span class="math inline">\(T_2\)</span> is correct. Her overall credal state will be a mixture of those two pictures, weighted according to the credibility of <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span>.</p>
<p>If we’re working with unconditional credences as primitive, then it is easy to mix two probability functions to produce a credal function which is also a probability function. Let <span class="math inline">\(Pr_1\)</span> be the probability function that reflects the agent’s views about how things probably are conditional on <span class="math inline">\(T_1\)</span> being true, and <span class="math inline">\(Pr_2\)</span> the probability function that reflects her views about how things probably are conditional on <span class="math inline">\(T_2\)</span> being true. Then for any <span class="math inline">\(p\)</span>, let <span class="math inline">\(Cr(p) = xPr_1(p) + (1-x)Pr_2(p)\)</span>, where <span class="math inline">\(Cr\)</span> is the agent’s credence function.</p>
<p>It is easy to see that <span class="math inline">\(Cr\)</span> will be a probability function. Indeed, inspecting the axioms P0-P3 makes it obvious that for any <span class="math inline">\(\vdash\)</span>, mixing two <span class="math inline">\(\vdash\)</span>-probability functions as we’ve just done will always produce a <span class="math inline">\(\vdash\)</span>-probability function. The axioms just require that probabilities stand in certain equalities and inequalities that are obviously preserved under mixing.</p>
<p>It is a little trickier to mix conditional probability functions in an intuitive way, for the reasons set out in <span class="citation" data-cites="Jehle2009">Jehle and Fitelson (<a href="#ref-Jehle2009" role="doc-biblioref">2009</a>)</span>. But in a special case, these difficulties are not overly pressing. Say that a <span class="math inline">\(\vdash\)</span>-probability function is <strong>regular</strong> iff for any <em>p, q</em> in its domain, <span class="math inline">\(Pr(p | q) = 0\)</span> iff <span class="math inline">\(p \wedge q\)</span> is a <span class="math inline">\(\vdash\)</span>-antitheorem. Then, for any two regular conditional probability functions <span class="math inline">\(Pr_1\)</span> and <span class="math inline">\(Pr_2\)</span> we can create a weighted mixture of the two of them by taking the new unconditional probabilities, i.e.&nbsp;the probabilities of <span class="math inline">\(p\)</span> given <span class="math inline">\(T\)</span>, where <span class="math inline">\(T\)</span> is a theorem, to be weighted sums of the unconditional probabilities in <span class="math inline">\(Pr_1\)</span> and <span class="math inline">\(Pr_2\)</span>. That is, our new function <span class="math inline">\(Pr_3\)</span> is given by:</p>
<p><span class="math display">\[Pr_3(p | T) = xPr_1(p | T) + (1-x)Pr_2(p | T)\]</span></p>
<p>In the general case, this does not determine exactly which function <span class="math inline">\(Pr_3\)</span> is, since it doesn’t determine the value of <span class="math inline">\(Pr_3(p | q)\)</span> when <span class="math inline">\(Pr_1(q | T) = Pr_2(q | T) = 0\)</span>. But since we’re paying attention just to regular functions this doesn’t matter. If the function is regular, then we can just let the familiar ratio account of conditional probability be a genuine definition. So in general we have,</p>
<p><span class="math display">\[Pr_3(p | q) = \frac{Pr_3(p \wedge q | T)}{Pr_3(q | T)}\]</span></p>
<p>And since the numerator is 0 iff <span class="math inline">\(q\)</span> is an anti-theorem, whenever <span class="math inline">\(Pr(p | q)\)</span> is supposed to be defined, i.e.&nbsp;when <span class="math inline">\(q\)</span> is not an anti-theorem, the right hand side will be well defined. As we noted, things get a lot messier when the functions are not regular, but those complications are unnecessary for the story we want to tell.</p>
<p>Now in the cases we’ve been considering so far, we’ve been assuming that <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> are empirical theories, and that we could assume classical logic in the background. Given all that, most of what we’ve said in this section has been a fairly orthodox treatment of how to account for a kind of uncertainty. But there’s no reason, we say, why we should restrict <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> in this way. We could apply just the same techniques when <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> are theories of entailment.</p>
<p>When <span class="math inline">\(T_1\)</span> is the theory that classical logic is the right logic of entailment, and <span class="math inline">\(T_2\)</span> the theory that intuitionistic logic is the right logic of entailment, then <span class="math inline">\(Pr_1\)</span> and <span class="math inline">\(Pr_2\)</span> should be different kinds of probability functions. In particular, <span class="math inline">\(Pr_1\)</span> should be a <span class="math inline">\(\vdash_{CL}\)</span>-probability function, and <span class="math inline">\(Pr_2\)</span> should be a <span class="math inline">\(\vdash_{IL}\)</span>-probability function. That’s because <span class="math inline">\(Pr_1\)</span> represents how things probably are given <span class="math inline">\(T_1\)</span>, and given <span class="math inline">\(T_1\)</span>, how things probably are is constrained by classical logic. And <span class="math inline">\(Pr_2\)</span> represents how things probably are given <span class="math inline">\(T_2\)</span>, and given <span class="math inline">\(T_2\)</span>, how things probably are is constrained by intuitionistic logic.</p>
<p>If we do all that, we’re pushed towards the thought that the if someone is uncertain whether the right logic is intuitionistic or classical logic, then the right theory of probability for them is intuitionistic probability theory. That’s because of Theorem 5.</p>
<blockquote class="blockquote">
<p><strong>Theorem 5</strong> Let <span class="math inline">\(Pr_1\)</span> be a regular conditional <span class="math inline">\(\vdash_{CL}\)</span>-probability function, and <span class="math inline">\(Pr_2\)</span> be a regular conditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function that is not a <span class="math inline">\(\vdash_{CL}\)</span>-probability function. And let <span class="math inline">\(Pr_3\)</span> be defined as in the text. (That is, <span class="math inline">\(Pr_3(A) = xPr_1(A) + (1-x)Pr_2(A)\)</span>, and <span class="math inline">\(Pr_3(A | B) = \frac{Pr_3(A \wedge B)}{Pr_3(B)}\)</span>.) Then <span class="math inline">\(Pr_3\)</span> is a regular conditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function.</p>
</blockquote>
<p>That’s to say, if the agent is at all unsure whether classical logic or intuitionistic logic is the correct logic, then their credence function should be an intuitionistic probability function.</p>
<p>Of course, if the agent is very confident that classical logic is the correct logic, then they couldn’t rationally have their credences distributed by any old intuitionistic probability function. After all, there are intuitionistic probability functions such that <span class="math inline">\(Pr(p \vee \neg p) = 0\)</span>, but an agent whose credence that classical logic is correct is, say, 0.95, could not reasonably have credence 0 in <span class="math inline">\(p \vee \neg p\)</span>. For our purposes, this matters because we want to show that an agent who is confident, but not certain, that classical logic is correct can nevertheless be a dogmatist. To fill in the argument we need,</p>
<blockquote class="blockquote">
<p><strong>Theorem 6</strong> Let <span class="math inline">\(x\)</span> be any real in <span class="math inline">\((0, 1)\)</span>. Then there is a probability function <span class="math inline">\(Cr\)</span> that (a) is a coherent credence function for someone whose credence that classical logic is correct is <span class="math inline">\(x\)</span>, and (b) satisfies each of the following inequalities: <span class="math display">\[\begin{aligned}
Pr(Ap \rightarrow p | Ap) &amp;&gt; Pr(Ap \rightarrow p) \\
Pr(\neg Ap \vee p | Ap) &amp;&gt; Pr(\neg Ap \vee p) \\
Pr(\neg(Ap \wedge \neg p) | Ap) &amp;&gt; Pr(\neg(Ap \wedge \neg p)) \end{aligned}\]</span></p>
</blockquote>
<p>The main idea driving the proof of Theorem 6 which is set out in the appendix, is that if intuitionistic logic is correct, it’s possible that conditionalising on <em>Ap</em> raises the probability of each of these three propositions from arbitrarily low values to 1. So as long as the prior probability of each of the three propositions, conditional on intuitionistic logic being correct, is low enough, it can still be raised by conditionalising on <em>Ap</em>.</p>
<p>More centrally, we think Theorem 6 shows that the probabilistic argument against dogmatism is not compelling. The original argument noted that the dogmatist says that we can learn the three propositions in Theorem 6, most importantly <span class="math inline">\(Ap \rightarrow p\)</span>, by getting evidence <em>Ap</em>. And it says this is implausible because conditionalising on <em>Ap</em> lowers the probability of <span class="math inline">\(Ap \rightarrow p\)</span>. But it turns out this is something of an artifact of the very strong classical assumptions that are being made. The argument not only requires the correctness of classical logic, it requires that the appropriate credence the agent should have in classical logic’s being correct is one. And that assumption is, we think, wildly implausible. Even if the agent should be <em>very confident</em> that classical logic is the correct logic, it shouldn’t be a requirement of rationality that she be absolutely certain that it is correct.</p>
<p>So we conclude that this argument fails. A dogmatist about perception who is at least minimally open-minded about logic can marry perceptual dogmatism to a probabilistically coherent theory of confirmation.</p>
<p>This paper is one more attempt on our behalf to defend dogmatism from a probabilistic challenge. <span class="citation" data-cites="Weatherson2007">Weatherson (<a href="#ref-Weatherson2007" role="doc-biblioref">2007</a>)</span> defends dogmatism from the so-called “Bayesian objection”. And <span class="citation" data-cites="JehlePhD">Jehle (<a href="#ref-JehlePhD" role="doc-biblioref">2009</a>)</span> not only shows that dogmatism can be situated nicely into a probabilistically coherent theory of confirmation, but also that within such a theory, many of the traditional objections to dogmatism are easily rebutted. We look forward to future research on the connections between dogmatism and probability, but we remain skeptical that dogmatism will be undermined solely by probabilistic considerations.</p>
</section>
<section id="appendix-proofs" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="appendix-proofs">Appendix: Proofs</h3>
<blockquote class="blockquote">
<p><strong>Theorem 1</strong><br>
If <span class="math inline">\(Pr\)</span> is a classical probability function, then<br>
<span class="math inline">\(Pr(Ap \rightarrow p | Ap) \leq Pr(Ap \rightarrow p)\)</span>.</p>
</blockquote>
<p><strong>Proof</strong>: Assume <span class="math inline">\(Pr\)</span> is a classical probability function, and <span class="math inline">\(\vdash\)</span> the classical consequence relation.</p>
<p><span class="math display">\[\begin{aligned}
1. &amp;Ap \rightarrow p \dashv \vdash ((Ap \rightarrow p) \wedge Ap) \vee ((Ap \rightarrow p) \wedge \neg Ap) &amp; \text{} \\
2. &amp;Pr(Ap \rightarrow p) = Pr(((Ap \rightarrow p) \wedge Ap) \vee ((Ap \rightarrow p) \wedge \neg Ap)) &amp; \text{1, P2$^*$} \\
3. &amp; Pr ((Ap \rightarrow p) \wedge Ap) \vee ((Ap \rightarrow p) \wedge \neg Ap)) = \\&amp;Pr ((Ap \rightarrow p) \wedge Ap) + Pr ((Ap \rightarrow p) \wedge \neg Ap)  
&amp; \text{P3$^*$}  \\
4. &amp;Pr((Ap \rightarrow p) \wedge Ap) = Pr (Ap)Pr(Ap \rightarrow p|Ap) &amp; \text{P6} \\
5. &amp;Pr((Ap \rightarrow p) \wedge \neg Ap) = Pr(\neg Ap)Pr(Ap \rightarrow p |\neg Ap) &amp; \text{P6} \\
6. &amp;Pr(Ap \rightarrow p) = \\&amp;Pr(Ap)Pr(Ap \rightarrow p|Ap) + Pr (\neg Ap)Pr(Ap \rightarrow p |\neg Ap) &amp; \text{2, 4, 5} \\
7. &amp;(Ap \rightarrow p) \wedge Ap \dashv \vdash \neg Ap &amp; \text{} \\
8. &amp;Pr((Ap \rightarrow p) \wedge Ap) = Pr(\neg Ap) &amp; \text{7, P2$^*$} \\
9. &amp;Pr(Ap \rightarrow p |\neg Ap) = 1 \text{ or } Pr(\neg Ap) = 0 &amp; \text{8, P6}  \\
10. &amp;Pr(Ap \rightarrow p | Ap) \leq 1 &amp; \text{P4, P5} \\
11. &amp;Pr(Ap \rightarrow p) \geq \\ &amp;Pr(Ap)Pr(Ap \rightarrow p|Ap) + Pr (\neg Ap)Pr(Ap \rightarrow p |Ap)  &amp; \text{6, 9, 10} \\
12. &amp;\vdash Ap \vee \neg Ap &amp; \text{} \\
13. &amp;Pr(Ap \vee \neg Ap) = 1 &amp; \text{12, P1} \\
14. &amp;Pr(Ap) + Pr (\neg Ap) = 1 &amp; \text{13, P3$^*$} \\
15. &amp;Pr(Ap \rightarrow p ) \geq Pr (Ap \rightarrow p|Ap) &amp; \text{11, 14} \end{aligned}\]</span> Note (11) is an equality iff (8) is. The only step there that may not be obvious is step 10. The reason it holds is that either <span class="math inline">\(Ap\)</span> is a <span class="math inline">\(\vdash\)</span>-antitheorem or it isn’t. If it is, then it entails <span class="math inline">\(Ap \rightarrow p\)</span>, so by P5, <span class="math inline">\(Pr(Ap \rightarrow p | Ap) \leq 1\)</span>. If it is not, then by P1<span class="math inline">\(^*\)</span>, <span class="math inline">\(Pr(x | Ap) \leq 1\)</span> for any <span class="math inline">\(x\)</span>, so <span class="math inline">\(Pr(Ap \rightarrow p | Ap) \leq 1\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Theorem 2</strong><br>
If <span class="math inline">\(Pr\)</span> is a classical probability function, then</p>
<ul>
<li><p><span class="math inline">\(Pr(\neg(Ap \wedge \neg p) | Ap) \leq Pr(\neg(Ap \wedge \neg p))\)</span>; and</p></li>
<li><p><span class="math inline">\(Pr(\neg Ap \vee p | Ap) \leq Pr(\neg Ap \vee p)\)</span>.</p></li>
</ul>
</blockquote>
<p><strong>Proof</strong>: Assume <span class="math inline">\(Pr\)</span> is a classical probability function, and <span class="math inline">\(\vdash\)</span> the classical consequence relation. <span class="math display">\[\begin{aligned}
1. &amp;Ap \rightarrow p \dashv  \vdash \neg(Ap \wedge \neg p) &amp;  \\
2. &amp;Pr(Ap \rightarrow p) = Pr(\neg(Ap \wedge \neg p)) &amp; 1, P2^* \\
3. &amp;Pr(Ap \rightarrow p | Ap) = Pr(\neg(Ap \wedge \neg p) | Ap) &amp; 1, P4, P5 \\
4. &amp;Pr(Ap \rightarrow p ) \geq Pr (Ap \rightarrow p|Ap) &amp; \text{Theorem 1} \\
5. &amp;Pr(\neg(Ap \wedge \neg p) | Ap) \geq Pr(\neg(Ap \wedge \neg p)) &amp; 2, 3, 4 \\
6. &amp;Ap \rightarrow p \dashv  \vdash \neg Ap \vee p &amp;  \\
7. &amp;Pr(Ap \rightarrow p) = Pr(\neg Ap \vee p) &amp; 6, P2^* \\
8. &amp;Pr(Ap \rightarrow p | Ap) = Pr(\neg Ap \vee p | Ap) &amp; 6, P4, P5 \\
9. &amp;Pr(\neg Ap \vee p | Ap) \geq Pr(\neg Ap \vee p) &amp; 4, 7, 8\end{aligned}\]</span></p>
<p>The only minor complication is with step 3. There are two cases to consider, either <span class="math inline">\(Ap\)</span> is a <span class="math inline">\(\vdash\)</span>-antitheorem or it isn’t. If it is a <span class="math inline">\(\vdash\)</span>-antitheorem, then both the LHS and RHS of (3) equal 1, so they are equal. If it is not a <span class="math inline">\(\vdash\)</span>-antitheorem, then by P4, <span class="math inline">\(Pr(\cdot | Ap)\)</span> is a probability function. So by P2<span class="math inline">\(^*\)</span>, and the fact that <span class="math inline">\(Ap \rightarrow p \dashv \vdash \neg(Ap \wedge \neg p)\)</span>, we have that the LHS and RHS are equal.</p>
<blockquote class="blockquote">
<p><strong>Theorem 3</strong>.<br>
In <span class="math inline">\(M\)</span>, for any <span class="math inline">\(x \in (0, 1)\)</span>,</p>
<ol type="1">
<li><p><span class="math inline">\(Pr_x(Ap \rightarrow p)\)</span> = <span class="math inline">\(Pr_x((Ap \rightarrow p) \wedge Ap) = x\)</span></p></li>
<li><p><span class="math inline">\(Pr_x(\neg Ap \vee p)\)</span> = <span class="math inline">\(Pr_x((\neg Ap \vee p) \wedge Ap) = x\)</span></p></li>
<li><p><span class="math inline">\(Pr_x(\neg(Ap \wedge \neg p))\)</span> = <span class="math inline">\(Pr_x(\neg(Ap \wedge \neg p) \wedge Ap) = x\)</span></p></li>
</ol>
</blockquote>
<p>Recall what <span class="math inline">\(M\)</span> looks like.</p>
<div class="center">
<p>(70, 50) (35, 5)(-1, 1)30 (35, 5)(1, 1)30 (35,5) (4.8,35.5) (65.2,35.5) (28, 5)<span class="math inline">\(1\)</span> (0,35.5)<span class="math inline">\(2\)</span> (60,35.5)<span class="math inline">\(3\)</span> (7,35.5)<span class="math inline">\(Ap, p\)</span> (67,35.5)<span class="math inline">\(Ap\)</span></p>
</div>
<p>The only point where <span class="math inline">\(Ap \rightarrow p\)</span> is true is at 2. Indeed, <span class="math inline">\(\neg(Ap \rightarrow p)\)</span> is true at 3, and neither <span class="math inline">\(Ap \rightarrow p\)</span> nor <span class="math inline">\(\neg(Ap \rightarrow p)\)</span> are true at 1. So <span class="math inline">\(Pr_x(Ap \rightarrow p) = m_x(\{2\}) = x\)</span>. Since <em>Ap</em> is also true at 2, that’s the only point where <span class="math inline">\((Ap \rightarrow p) \wedge Ap\)</span> is true. So it follows that <span class="math inline">\(Pr_x((Ap \rightarrow p) \wedge Ap) = m_x(\{2\}) = x\)</span>.</p>
<p>Similar inspection of the model shows that 2 is the only point where <span class="math inline">\(\neg(Ap \wedge \neg p)\)</span> is true, and the only point where <span class="math inline">\(\neg Ap \vee p\)</span> is true. And so (b) and (c) follow in just the same way.</p>
<p>In slight contrast, <span class="math inline">\(Ap\)</span> is true at two points in the model, 2 and 3. But since <span class="math inline">\(m_x(\{3\}) = 0\)</span>, it follows that <span class="math inline">\(m_x(\{2, 3\}) = m_x(\{2\}) = x\)</span>. So <span class="math inline">\(Pr_x(Ap) = x\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Theorem 4</strong>.<br>
For any <span class="math inline">\(x \in (0, 1)\)</span>,</p>
<ol type="1">
<li><p><span class="math inline">\(1 = Pr_x(Ap \rightarrow p | Ap) &gt; Pr_x(Ap \rightarrow p) = x\)</span></p></li>
<li><p><span class="math inline">\(1 = Pr_x(\neg Ap \vee p | Ap) &gt; Pr_x(\neg Ap \vee p) = x\)</span></p></li>
<li><p><span class="math inline">\(1 = Pr_x(\neg(Ap \wedge \neg p) | Ap) &gt; Pr_x(\neg(Ap \wedge \neg p)) = x\)</span></p></li>
</ol>
</blockquote>
<p>We’ll just go through the argument for (a); the other cases are similar. By P6, we know that <span class="math inline">\(Pr_x(\neg(Ap \wedge \neg p) | Ap) Pr_x(Ap) = Pr_x((Ap \rightarrow p) \wedge Ap)\)</span>. By Theorem 3, we know that <span class="math inline">\(Pr_x(Ap) = Pr_x((Ap \rightarrow p) \wedge Ap)\)</span>, and that both sides are greater than 0. (Note that the theorem is only said to hold for <span class="math inline">\(x &gt; 0\)</span>.) The only way both these equations can hold is if <span class="math inline">\(Pr_x(\neg(Ap \wedge \neg p) | Ap) = 1\)</span>. Note also that by hypothesis, <span class="math inline">\(x &lt; 1\)</span>, and from this claim (a) follows. The other two cases are completely similar.</p>
<blockquote class="blockquote">
<p><strong>Theorem 5</strong> Let <span class="math inline">\(Pr_1\)</span> be a regular conditional <span class="math inline">\(\vdash_{CL}\)</span>-probability function, and <span class="math inline">\(Pr_2\)</span> be a regular conditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function that is not a <span class="math inline">\(\vdash_{CL}\)</span>-probability function. And let <span class="math inline">\(Pr_3\)</span> be defined as in the text. (That is, <span class="math inline">\(Pr_3(A) = xPr_1(A) + (1-x)Pr_2(A)\)</span>, and <span class="math inline">\(Pr_3(A | B) = \frac{Pr_3(A \wedge B)}{Pr_3(B)}\)</span>.) Then <span class="math inline">\(Pr_3\)</span> is a regular conditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function.</p>
</blockquote>
<p>We first prove that <span class="math inline">\(Pr_3\)</span> satisfies the requirements of an unconditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function, and then show that it satisfies the requirements of a conditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function.</p>
<p>If <span class="math inline">\(p\)</span> is an <span class="math inline">\(\vdash_{IL}\)</span>-antithesis, then it is also a <span class="math inline">\(\vdash_{CL}\)</span>-antithesis. So <span class="math inline">\(Pr_1(p) = Pr_2(p) = 0\)</span>. So <span class="math inline">\(Pr_3(A) = 0x + 0(1-x) = 0\)</span>, as required for <strong>(P0)</strong>.</p>
<p>If <span class="math inline">\(p\)</span> is an <span class="math inline">\(\vdash_{IL}\)</span>-thesis, then it is also a <span class="math inline">\(\vdash_{CL}\)</span>-thesis. So <span class="math inline">\(Pr_1(p) = Pr_2(p) = 1\)</span>. So <span class="math inline">\(Pr_3(p) = x + (1-x) = 1\)</span>, as required for <strong>(P1)</strong>.</p>
<p>If <span class="math inline">\(p \vdash_{IL} q\)</span> then <span class="math inline">\(p \vdash_{CL} q\)</span>. So we have both <span class="math inline">\(Pr_1(p) \leq Pr(q)\)</span> and <span class="math inline">\(Pr_2(p) \leq Pr_2(q)\)</span>. Since <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\((1-x) \geq 0\)</span>, these inequalities imply that <span class="math inline">\(xPr_1(p) \leq xPr(q)\)</span> and <span class="math inline">\((1-x)Pr_2(p) \leq (1-x)Pr_2(q)\)</span>. Summing these, we get <span class="math inline">\(xPr_1(p) + (1-x)Pr_2(p) \leq xPr_1(q) + (1-x)Pr_2(q)\)</span>. And by the definition of <span class="math inline">\(Pr_3\)</span>, that means that <span class="math inline">\(Pr_3(p) \leq Pr_3(q)\)</span>, as required for <strong>(P2)</strong>.</p>
<p>Finally, we just need to show that <span class="math inline">\(Pr_3(p) + Pr_3(q) = Pr_3(p \vee q) + Pr_3(p \wedge q)\)</span>, as follows:</p>
<p><span class="math display">\[\begin{aligned}
Pr_3(p) + Pr_3(q) &amp;= xPr_1(p) + (1-x)Pr_2(p) + xPr_1(q) + (1-x)Pr_2(q) \\
&amp;= x(Pr_1(p) + Pr_1(q)) + (1-x)(Pr_2(p) + Pr_2(q)) \\
&amp;= x(Pr_1(p \vee q) + Pr_1(p \wedge q)) + (1-x)(Pr_2(p \vee q) + Pr_2(p \wedge q)) \\
&amp;= xPr_1(p \vee q) + (1-x)Pr_2(p \vee q) + xPr_1(p \wedge q)) + (1-x)Pr_2(p \wedge q) \\
&amp;= Pr_3(p \vee q) + Pr_3(p \wedge q) \text{ as required}\end{aligned}\]</span></p>
<p>Now that we have shown <span class="math inline">\(Pr_3\)</span> is an unconditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function, we need to show it is a conditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function, where <span class="math inline">\(Pr_3(p | r) =_{df} \frac{Pr_3(p \wedge r)}{Pr_3(r)}\)</span>. Remember we are assuming that both <span class="math inline">\(Pr_1\)</span> and <span class="math inline">\(Pr_2\)</span> are regular, from which it clearly follows that <span class="math inline">\(Pr_3\)</span> is regular, so this definition is always in order. (That is, we’re never dividing by zero.) The longest part of showing <span class="math inline">\(Pr_3\)</span> is a conditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function is showing that it satisfies <strong>(P4</strong>), which has four parts. We need to show that <span class="math inline">\(Pr(\cdot | r)\)</span> satisfies <strong>(P0)-(P3)</strong>. Fortunately these are fairly straightforward.</p>
<p>If <span class="math inline">\(p\)</span> is an <span class="math inline">\(\vdash_{IL}\)</span>-antithesis, then so is <span class="math inline">\(p \wedge r\)</span>. So <span class="math inline">\(Pr_3(p \wedge r) = 0\)</span>, so <span class="math inline">\(Pr_3(p | r) = 0\)</span>, as required for <strong>(P0)</strong>.</p>
<p>If <span class="math inline">\(p\)</span> is an <span class="math inline">\(\vdash_{IL}\)</span>-thesis, then <span class="math inline">\(p \wedge r \dashv \vdash r\)</span>, so <span class="math inline">\(Pr_3(p \wedge r) = Pr_3(r)\)</span>, so <span class="math inline">\(Pr_3(p | r) = 1\)</span>, as required for <strong>(P1)</strong>.</p>
<p>If <span class="math inline">\(p \vdash_{IL} q\)</span> then <span class="math inline">\(p \wedge r \vdash_{IL} q \wedge r\)</span>. So <span class="math inline">\(Pr_3(p \wedge r) \leq Pr_3(q \wedge r)\)</span>. So <span class="math inline">\(\frac{Pr_3(p \wedge r)}{Pr_3(r)} \leq \frac{Pr_3(q \wedge r)}{Pr_3(r)}\)</span>. That is, <span class="math inline">\(Pr_3(p | r) \leq Pr_3(q | r)\)</span>, as required for <strong>(P2)</strong>.</p>
<p>Finally, we need to show that <span class="math inline">\(Pr_3(p | r) + Pr_3(q | r) = Pr_3(p \vee q | r) + Pr_3(p \wedge q | r)\)</span>, as follows, making repeated use of the fact that <span class="math inline">\(Pr_3\)</span> is an unconditional <span class="math inline">\(\vdash_{IL}\)</span>-probability function, so we can assume it satisfies <strong>(P3)</strong>, and that we can substitute intuitionistic equivalences inside <span class="math inline">\(Pr_3\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
Pr_3(p | r) + Pr_3(q | r) &amp;= \frac{Pr_3(p \wedge r)}{Pr_3(r)} + \frac{Pr_3(q \wedge r)}{Pr_3(r)} \\
&amp;= \frac{Pr_3(p \wedge r) + Pr(q \wedge r)}{Pr_3(r)} \\
&amp;= \frac{Pr_3((p \wedge r) \vee (q \wedge r)) + Pr_3((p \wedge r) \wedge (q \wedge r))}{Pr_3(r)} \\
&amp;=\frac{Pr_3(p \vee q) \wedge r) + Pr_3((p \wedge q) \wedge r)}{Pr_3(r)} \\
&amp;=\frac{Pr_3(p \vee q) \wedge r)}{Pr_3(r)} + \frac{Pr_3((p \wedge q) \wedge r)}{Pr_3(r)} \\
&amp;=Pr_3(p \vee q | r) + Pr_3(p \wedge q | r) \text{ as required}\end{aligned}\]</span></p>
<p>Now if <span class="math inline">\(r \vdash_{IL} p\)</span>, then <span class="math inline">\(r \wedge p ~_{IL}\dashv \vdash_{IL} p\)</span>, so <span class="math inline">\(Pr_3(r \wedge p) = Pr_3(p)\)</span>, so <span class="math inline">\(Pr_3(p | r) = 1\)</span>, as required for <strong>(P5)</strong>.</p>
<p>Finally, we show that <span class="math inline">\(Pr_3\)</span> satisfies <strong>(P6)</strong>.</p>
<p><span class="math display">\[\begin{aligned}
Pr_3(p \wedge q | r) &amp;= \frac{Pr_3(p \wedge q \wedge r)}{Pr_3(r)} \\
&amp;= \frac{Pr_3(p \wedge q \wedge r)}{Pr_3(q \wedge r)} \frac{Pr_3(q \wedge r)}{Pr_3(r)} \\
&amp;=Pr_3(p | q \wedge r) Pr_3(q | r) \text{ as required}\end{aligned}\]</span></p>
<blockquote class="blockquote">
<p><strong>Theorem 6</strong> Let <span class="math inline">\(x\)</span> be any real in <span class="math inline">\((0, 1)\)</span>. Then there is a probability function <span class="math inline">\(Cr\)</span> that (a) is a coherent credence function for someone whose credence that classical logic is correct is <span class="math inline">\(x\)</span>, and (b) satisfies each of the following inequalities: <span class="math display">\[\begin{aligned}
Pr(Ap \rightarrow p | Ap) &amp;&gt; Pr(Ap \rightarrow p) \\
Pr(\neg Ap \vee p | Ap) &amp;&gt; Pr(\neg Ap \vee p) \\
Pr(\neg(Ap \wedge \neg p) | Ap) &amp;&gt; Pr(\neg(Ap \wedge \neg p)) \end{aligned}\]</span></p>
</blockquote>
<p>We’ll prove this by constructing the function <span class="math inline">\(Pr\)</span>. For the sake of this proof, we’ll assume a very restricted formal language with just two atomic sentences: <span class="math inline">\(Ap\)</span> and <span class="math inline">\(p\)</span>. This restriction makes it easier to ensure that the functions are all regular, which as we noted in the main text lets us avoid various complications. The proofs will rely on three probability functions defined using this Kripke tree <span class="math inline">\(M\)</span>.</p>
<div class="center">
<p>(100, 40) (50, 5)(-3, 2)45 (50, 5)(-1, 2)15 (50, 5)(1, 2)15 (50, 5)(3, 2)45 (50,5) (4.5,35.5) (65.2,35.5) (34.8,35.5) (95.5,35.5) (42, 5)<span class="math inline">\(0\)</span> (0,35.5)<span class="math inline">\(1\)</span> (30,35.5)<span class="math inline">\(2\)</span> (60,35.5)<span class="math inline">\(3\)</span> (90,35.5)<span class="math inline">\(4\)</span> (7,35.5)<span class="math inline">\(Ap, p\)</span> (37,35.5)<span class="math inline">\(Ap\)</span> (67,35.5)<span class="math inline">\(p\)</span></p>
</div>
<p>We’ve shown on the graph where the atomic sentences true: <span class="math inline">\(Ap\)</span> is true at 1 and 2, and <span class="math inline">\(p\)</span> is true at 1 and 3. So the four terminal nodes represent the four classical possibilities that are definable using just these two atomic sentences. We define two measure functions <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span> over the points in this model as follows:</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><span class="math inline">\(m(\{0\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{1\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{2\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{3\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{4\})\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(m_1\)</span></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"><span class="math inline">\(\frac{x}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1-x}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{4}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(m_2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{x}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1-x}{4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1-x}{4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{4}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>We’ve just specified the measure of each singleton, but since we’re just dealing with a finite model, that uniquely specifies the measure of any set. We then turn each of these into probability functions in the way described in section 1. That is, for any proposition <span class="math inline">\(X\)</span>, and <span class="math inline">\(i \in \{1, 2\}\)</span>, <span class="math inline">\(Pr_i(X) = m_i(M_X)\)</span>, where <span class="math inline">\(M_X\)</span> is the set of points in <span class="math inline">\(M\)</span> where <span class="math inline">\(X\)</span> is true.</p>
<p>Note that the terminal nodes in <span class="math inline">\(M\)</span>, like the terminal nodes in any Kripke tree, are just classical possibilities. That is, for any sentence, either it or its negation is true at a terminal node. Moreover, any measure over classical possibilities generates a classical probability function. (And vice versa, any classical probability function is generated by a measure over classical possibilities.) That is, for any measure over classical possibilities, the function from propositions to the measure of the set of possibilities at which they are true is a classical probability function. Now <span class="math inline">\(m_1\)</span> isn’t quite a measure over classical possibilities, since strictly speaking <span class="math inline">\(m_1(\{0\})\)</span> is defined. But since <span class="math inline">\(m_1(\{0\}) = 0\)</span> it is equivalent to a measure only defined over the terminal nodes. So the probability function it generates, i.e., <span class="math inline">\(Pr_1\)</span>, is a classical probability function.Of course, with only two atomic sentences, we can also verify by brute force that <span class="math inline">\(Pr_1\)</span> is classical, but it’s a little more helpful to see why this is so. In contrast, <span class="math inline">\(Pr_2\)</span> is not a classical probability function, since <span class="math inline">\(Pr_2(p \vee \neg p) = 1 - \frac{x}{2}\)</span>, but it is an intuitionistic probability function.</p>
<p>So there could be an agent who satisfies the following four conditions:</p>
<ul>
<li><p>Her credence that classical logic is correct is <span class="math inline">\(x\)</span>;</p></li>
<li><p>Her credence that intuitionistic logic is correct is <span class="math inline">\(1-x\)</span>;</p></li>
<li><p>Conditional on classical logic being correct, she thinks that <span class="math inline">\(Pr_1\)</span> is the right representation of how things probably are; and</p></li>
<li><p>Conditional on intuitionistic logic being correct, she thinks that <span class="math inline">\(Pr_2\)</span> is the right representation of how things are.</p></li>
</ul>
<p>Such an agent’s credences will be given by a <span class="math inline">\(\vdash_{IL}\)</span>-probability function <span class="math inline">\(Pr\)</span> generated by ‘mixing’ <span class="math inline">\(Pr_1\)</span> and <span class="math inline">\(Pr_2\)</span>. For any sentence <span class="math inline">\(Y\)</span> in the domain, her credence in <span class="math inline">\(Y\)</span> will be <span class="math inline">\(xPr_1(Y) + (1-x)Pr_2(Y)\)</span>. Rather than working through each proposition, it’s easiest to represent this function by mixing the measures <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span> to get a new measure <span class="math inline">\(m\)</span> on the above Kripke tree. Here’s the measure that <span class="math inline">\(m\)</span> assigns to each node.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><span class="math inline">\(m(\{0\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{1\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{2\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{3\})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(m(\{4\})\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(m\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{x(1-x)}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3x^2 - 2x + 1}{4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1-x^2}{4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1}{4}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>As usual, this measure <span class="math inline">\(m\)</span> generates a probability function <span class="math inline">\(Pr\)</span>. We’ve already argued that <span class="math inline">\(Pr\)</span> is a reasonable function for someone whose credence that classical logic is <span class="math inline">\(x\)</span>. We’ll now argue that <span class="math inline">\(Pr(Ap \rightarrow p | Ap) &gt; Pr(Ap \rightarrow p)\)</span>.</p>
<p>It’s easy to see what <span class="math inline">\(Pr(Ap \rightarrow p)\)</span> is. <span class="math inline">\(Ap \rightarrow p\)</span> is true at 1, 3 and 4, so</p>
<p><span class="math display">\[\begin{aligned}
Pr(Ap \rightarrow p) &amp;= m({1}) + m({3}) + m(4) \\
&amp;= \frac{3x^2 - 2x + 1}{4} + \frac{1}{4} + \frac{1}{4} \\
&amp;= \frac{3x^2 - 2x + 3}{4} \end{aligned}\]</span></p>
<p>Since <span class="math inline">\(Pr\)</span> is regular, we can use the ratio definition of conditional probability to work out <span class="math inline">\(Pr(Ap \rightarrow p | Ap)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
Pr(Ap \rightarrow p | Ap) &amp;= \frac{Pr((Ap \rightarrow p) \wedge Ap)}{Pr(Ap)} \\
&amp;= \frac{m({1})}{m({1}) + m({2})} \\
&amp;= \frac{\frac{3x^2 - 2x + 1}{4}}{\frac{3x^2 - 2x + 1}{4} + \frac{1-x^2}{4}} \\
&amp;= \frac{3x^2 - 2x + 1}{(3x^2 - 2x + 1) + (1-x^2)} \\
&amp;= \frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \end{aligned}\]</span></p>
<p>Putting all that together, we have</p>
<p><span class="math display">\[\begin{aligned}
&amp;&amp; Pr(Ap \rightarrow p | Ap) &amp;&gt; Pr(Ap \rightarrow p) \\
\Leftrightarrow &amp;&amp;  \frac{3x^2 - 2x + 3}{4}  &amp;&gt; \frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \\
\Leftrightarrow &amp;&amp; 3x^2 - 2x + 3  &amp;&gt; \frac{6x^2 - 4x + 2}{x^2 - x + 1} \\
\Leftrightarrow &amp;&amp; (3x^2 - 2x + 3)(x^2 + x + 1)  &amp;&gt; 6x^2 - 4x + 2 \\
\Leftrightarrow &amp;&amp; 3x^4 - 5x^3 + 8x^2 - 5x + 3  &amp;&gt; 6x^2 - 4x + 2 \\
\Leftrightarrow &amp;&amp; 3x^4 - 5x^3 + 2x^2 - x + 1 &amp;&gt; 0 \\
\Leftrightarrow &amp;&amp; (3x^2 + x + 1)(x^2 - 2x + 1) &amp;&gt; 0 \\
\Leftrightarrow &amp;&amp; (3x^2 + x + 1)(x - 1)^2 &amp;&gt; 0\end{aligned}\]</span></p>
<p>But it is clear that for any <span class="math inline">\(x \in (0,1)\)</span>, both of the terms of the LHS of the final line are positive, so their product is positive. And that means <span class="math inline">\(Pr(Ap \rightarrow p | Ap) &gt; Pr(Ap \rightarrow p)\)</span>. So no matter how close <span class="math inline">\(x\)</span> gets to 1, that is, no matter how certain the agent gets that classical logic is correct, as long as <span class="math inline">\(x\)</span> does not reach 1, conditionalising on <span class="math inline">\(Ap\)</span> will raise the probability of <span class="math inline">\(Ap \rightarrow p\)</span>. As we’ve been arguing, as long as there is any doubt about classical logic, even a vanishingly small doubt, there is no probabilistic objection to dogmatism.</p>
<p>To finish up, we show that <span class="math inline">\(Pr(\neg Ap \vee p | Ap) &gt; Pr(\neg Ap \vee p)\)</span> and <span class="math inline">\(Pr(\neg(Ap \wedge \neg p) | Ap) &gt; Pr(\neg(Ap \wedge \neg p))\)</span>. To do this, we just need to note that <span class="math inline">\(Ap \rightarrow p\)</span>, <span class="math inline">\(\neg Ap \vee p\)</span> and <span class="math inline">\(\neg(Ap \wedge \neg p)\)</span> are true at the same points in the model, so their probabilities, both unconditionally and conditional on <span class="math inline">\(Ap\)</span>, will be identical. So from <span class="math inline">\(Pr(Ap \rightarrow p | Ap) &gt; Pr(Ap \rightarrow p)\)</span> the other two inequalities follow immediately.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Cohen2005" class="csl-entry" role="listitem">
Cohen, Stewart. 2005. <span>“Why Basic Knowledge Is Easy Knowledge.”</span> <em>Philosophy and Phenomenological Research</em> 70 (2): 417–30. <a href="https://doi.org/10.1111/j.1933-1592.2005.tb00536.x">https://doi.org/10.1111/j.1933-1592.2005.tb00536.x</a>.
</div>
<div id="ref-Hajek2003" class="csl-entry" role="listitem">
Hájek, Alan. 2003. <span>“What Conditional Probability Could Not Be.”</span> <em>Synthese</em> 137 (3): 273–323. <a href="https://doi.org/10.1023/B:SYNT.0000004904.91112.16">https://doi.org/10.1023/B:SYNT.0000004904.91112.16</a>.
</div>
<div id="ref-JehlePhD" class="csl-entry" role="listitem">
Jehle, David. 2009. <span>“Some Results in Bayesian Confirmation Theory with Applications.”</span> PhD thesis, Cornell University.
</div>
<div id="ref-Jehle2009" class="csl-entry" role="listitem">
Jehle, David, and Branden Fitelson. 2009. <span>“What Is the <span>‘Equal Weight View’</span>?”</span> <em>Episteme</em> 6 (3): 280–93. <a href="https://doi.org/10.3366/E1742360009000719">https://doi.org/10.3366/E1742360009000719</a>.
</div>
<div id="ref-Kripke1965" class="csl-entry" role="listitem">
Kripke, Saul. 1965. <span>“Semantical Analysis of Intuitionistic Logic.”</span> In <em>Formal Systems and Recursive Functions</em>, edited by Michael Dummett and John Crossley. Amsterdam: North-Holland.
</div>
<div id="ref-PopperMiller1987" class="csl-entry" role="listitem">
Popper, Karl, and David Miller. 1987. <span>“Why Probabilistic Support Is Not Inductive.”</span> <em>Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences</em> 321 (1562): 569–91. <a href="https://doi.org/10.1098/rsta.1987.0033">https://doi.org/10.1098/rsta.1987.0033</a>.
</div>
<div id="ref-Pryor2000" class="csl-entry" role="listitem">
Pryor, James. 2000. <span>“The Sceptic and the Dogmatist.”</span> <em>No<span>û</span>s</em> 34 (4): 517–49. <a href="https://doi.org/10.1111/0029-4624.00277">https://doi.org/10.1111/0029-4624.00277</a>.
</div>
<div id="ref-Pryor2004" class="csl-entry" role="listitem">
———. 2004. <span>“What’s Wrong with Moore’s Argument?”</span> <em>Philosophical Issues</em> 14 (1): 349–78. <a href="https://doi.org/10.1111/j.1533-6077.2004.00034.x">https://doi.org/10.1111/j.1533-6077.2004.00034.x</a>.
</div>
<div id="ref-Weatherson2003" class="csl-entry" role="listitem">
Weatherson, Brian. 2003. <span>“From Classical to Intuitionistic Probability.”</span> <em>Notre Dame Journal of Formal Logic</em> 44 (2): 111–23. <a href="https://doi.org/10.1305/ndjfl/1082637807">https://doi.org/10.1305/ndjfl/1082637807</a>.
</div>
<div id="ref-Weatherson2007" class="csl-entry" role="listitem">
———. 2007. <span>“The Bayesian and the Dogmatist.”</span> <em>Proceedings of the Aristotelian Society</em> 107: 169–85. <a href="https://doi.org/10.1111/j.1467-9264.2007.00217.x">https://doi.org/10.1111/j.1467-9264.2007.00217.x</a>.
</div>
<div id="ref-White2006" class="csl-entry" role="listitem">
White, Roger. 2006. <span>“Problems for Dogmatism.”</span> <em>Philosophical Studies</em> 131 (3): 525–57. <a href="https://doi.org/10.1007/s11098-004-7487-9">https://doi.org/10.1007/s11098-004-7487-9</a>.
</div>
<div id="ref-Williamsms" class="csl-entry" role="listitem">
Williams, J. R. G. 2012. <span>“Gradational Accuracy and Non-Classical Semantics.”</span> <em>Review of Symbolic Logic</em> 5 (4): 513–37. <a href="https://doi.org/10.1017/S1755020312000214">https://doi.org/10.1017/S1755020312000214</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
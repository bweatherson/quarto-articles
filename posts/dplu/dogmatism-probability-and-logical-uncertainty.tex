% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  letterpaper,
  DIV=11,
  numbers=noendperiod,
  twoside]{scrartcl}
\usepackage{xcolor}
\usepackage[left=1.1in, right=1in, top=0.8in, bottom=0.8in,
paperheight=9.5in, paperwidth=7in, includemp=TRUE, marginparwidth=0in,
marginparsep=0in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \setmainfont[ItalicFont=EB Garamond Italic,BoldFont=EB Garamond
Bold]{EB Garamond Math}
  \setsansfont[]{EB Garamond}
  \setmathfont[]{Garamond-Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{setspace}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\setlength\heavyrulewidth{0ex}
\setlength\lightrulewidth{0ex}
\usepackage[automark]{scrlayer-scrpage}
\clearpairofpagestyles
\cehead{
  Brian Weatherson
  }
\cohead{
  Dogmatism, Probability and Logical Uncertainty
  }
\ohead{\bfseries \pagemark}
\cfoot{}
\makeatletter
\newcommand*\NoIndentAfterEnv[1]{%
  \AfterEndEnvironment{#1}{\par\@afterindentfalse\@afterheading}}
\makeatother
\NoIndentAfterEnv{itemize}
\NoIndentAfterEnv{enumerate}
\NoIndentAfterEnv{description}
\NoIndentAfterEnv{quote}
\NoIndentAfterEnv{equation}
\NoIndentAfterEnv{longtable}
\NoIndentAfterEnv{abstract}
\renewenvironment{abstract}
 {\vspace{-1.25cm}
 \quotation\small\noindent\emph{Abstract}:}
 {\endquotation}
\newfontfamily\tfont{EB Garamond}
\addtokomafont{disposition}{\rmfamily}
\addtokomafont{title}{\normalfont\itshape}
\let\footnoterule\relax
\cehead{
   David Jehle and Brian Weatherson
   }
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Dogmatism, Probability and Logical Uncertainty},
  pdfauthor={David Jehle; Brian Weatherson},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Dogmatism, Probability and Logical Uncertainty}
\author{David Jehle \and Brian Weatherson}
\date{2012}
\begin{document}
\maketitle
\begin{abstract}
Many epistemologists hold that an agent can come to justifiably believe
that p is true by seeing that it appears that p is true, without having
any antecedent reason to believe that visual impressions are generally
reliable. Certain reliabilists think this, at least if the agent's
vision is generally reliable. And it is a central tenet of dogmatism (as
described by James Pryor) that this is possible. Against these positions
it has been argued (e.g.~by Stewart Cohen and Roger White) that this
violates some principles from probabilistic learning theory. To see the
problem, let's note what the dogmatist thinks we can learn by paying
attention to how things appear. (The reliabilist says the same things,
but we'll focus on the dogmatist.)
\end{abstract}


\setstretch{1.1}
Many epistemologists hold that an agent can come to justifiably believe
that \emph{p} is true by seeing that it appears that \emph{p} is true,
without having any antecedent reason to believe that visual impressions
are generally reliable. Certain reliabilists think this, at least if the
agent's vision is generally reliable. And it is a central tenet of
dogmatism (as described by Pryor (\citeproc{ref-Pryor2000}{2000}) and
Pryor (\citeproc{ref-Pryor2004}{2004})) that this is possible. Against
these positions it has been argued (e.g.~by Cohen
(\citeproc{ref-Cohen2005}{2005}) and White
(\citeproc{ref-White2006}{2006})) that this violates some principles
from probabilistic learning theory. To see the problem, let's note what
the dogmatist thinks we can learn by paying attention to how things
appear. (The reliabilist says the same things, but we'll focus on the
dogmatist.)

Suppose an agent receives an appearance that \emph{p}, and comes to
believe that \emph{p}. Letting \emph{Ap} be the proposition that it
appears to the agent that \emph{p}, and → be the material conditional,
we can say that the agent learns that \emph{p}, and hence is in a
position to infer \emph{Ap}~→~\emph{p}, once they receive the evidence
\emph{Ap}.\footnote{We're assuming here that the agent's evidence really
  is \emph{Ap}, not \emph{p}. That's a controversial assumption, but it
  isn't at issue in this debate.} This is surprising, because we can
prove the following.\footnote{Popper and Miller
  (\citeproc{ref-PopperMiller1987}{1987}) prove a stronger result than
  Theorem One, and note its significance for probabilistic models of
  learning.}

\begin{quote}
\textbf{Theorem 1}\\
If Pr is a classical probability function, then\\
Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap})~⩽~Pr(\emph{Ap}~→~\emph{p}).
\end{quote}

(All the theorems are proved in the appendix.) We can restate Theorem 1
in the following way, using classically equvalent formulations of the
material conditional.

\begin{quote}
\textbf{Theorem 2}\\
If Pr is a classical probability function, then

\begin{itemize}
\tightlist
\item
  Pr(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap})~⩽~Pr(¬(\emph{Ap}~∧~¬\emph{p}));
  and
\item
  Pr(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap})~⩽~Pr(¬\emph{Ap}~∨~p).
\end{itemize}
\end{quote}

And that's a problem for the dogmatist if we make the standard Bayesian
assumption that some evidence \emph{E} is only evidence for hypothesis
\emph{H} if Pr(\emph{H}~\textbar~\emph{E}) \textgreater{} Pr(\emph{H}).
For here we have cases where the evidence the agent receives does not
raise the probability of \emph{Ap}~→~\emph{p}, ¬(\emph{Ap}~∧~¬\emph{p})
or ¬\emph{Ap}~∨~\emph{p}, so the agent has not received any evidence for
them, but getting this evidence takes them from not having a reason to
believe these propositions to having a reason to get them.

In this paper, we offer a novel response for the dogmatist. The proof of
Theorem 1 makes crucial use of the logical equivalence between
\emph{Ap}~→~\emph{p} and
((\emph{Ap}~→~\emph{p})~∧~\emph{Ap})~∨~((\emph{Ap}~→~\emph{p})~∧~¬\emph{Ap}).
These propositions are equivalent in classical logic, but they are not
equivalent in intuitionistic logic. Exploiting this non-equivalence, we
derive two claims. In Section 1 we show that Theorems 1 and 2 fail in
intuitionistic probability theory. In Section 2 we consider how an agent
who is unsure whether classical or intuitionistic logic is correct
should apportion their credences. We conclude that for such an agent,
theorems analogous to Theorems 1 and 2 fail even if the agent thinks it
extremely unlikely that intuitionistic logic is the correct logic. The
upshot is that if it is rationally permissible to be even a little
unsure whether classical or intuitionistic logic is correct, it is
possible that getting evidence that \emph{Ap} raises the rational
credibility of \emph{Ap}~→~\emph{p}, ¬(\emph{Ap}~∧~¬\emph{p}) and
¬\emph{Ap}~∨~\emph{p}.

\section{Intuitionistic Probability}\label{intuitionistic-probability}

In Weatherson (\citeproc{ref-Weatherson2003}{2003}), the notion of a
\(\vdash\)-probability function, where \(\vdash\) is an entailment
relation, is introduced. For any \(\vdash\), a \(\vdash\)-probability
function is a function Pr from sentences in the language of \(\vdash\)
to {[}0, 1{]} satisfying the following four constraints.\footnote{We'll
  usually assume that the language of \(\vdash\) is a familiar kind of
  propositional calculus, with a countable infinity of sentence letters,
  and satisfying the usual recursive constraints. That is, if \emph{A}
  and \emph{B} are sentences of the language, then so are ¬\emph{A},
  \emph{A}~→~\emph{B}, \emph{A}~∧~\emph{B} and \emph{A}~∨~\emph{B}. It
  isn't entirely trivial to extend some of our results to a language
  that contains quantifiers. This is because once we add quantifiers,
  intuitionistic and classical logic no longer have the same
  anti-theorems. But that complication is outside the scope of this
  paper. Note that for Theorem 6, we assume a restricted language with
  just two sentence letters. This merely simplifies the proof. A version
  of the construction we use there with those two letters being simply
  the first two sentence letters would be similar, but somewhat more
  complicated.}

\begin{description}
\tightlist
\item[(P0)]
Pr(\emph{p}) = 0\$ if \emph{p} is a \(\vdash\)-antithesis, i.e.~iff for
any \emph{X}, \emph{p}~\(\vdash\)~\emph{X}.
\item[(P1)]
Pr(\emph{p}) = 1 if \emph{p} is a \(\vdash\)-thesis, i.e.~iff for any
\emph{X}, \emph{X}~\(\vdash\)~\emph{p}.
\item[(P2)]
If \emph{p} \(\vdash\) \emph{q} then Pr(\emph{p})~⩽~Pr(\emph{q}).
\item[(P3)]
Pr(\emph{p}) + Pr(\emph{q}) = Pr(\emph{p}~∨~\emph{q}) +
Pr(\emph{p}~∧~\emph{q}).
\end{description}

We'll use \(\vdash_{CL}\) to denote the classical entailment relation,
and \(\vdash_{IL}\) to denote the intuitionist entailment relation. Then
what we usually take to be probability functions are
\(\vdash_{CL}\)-probability functions. And intuitionist probability
functions are \(\vdash_{IL}\)-probability functions.

In what follows we'll make frequent appeal to three obvious consequences
of these axioms, consequences which are useful enough to deserve their
own names. Hopefully these are obvious enough to pass without
proof.\footnote{In the original, the next three paragraphs were
  footnoted, but I no longer like having numbered things in footnotes.}

Weatherson (\citeproc{ref-Weatherson2003}{2003}) discusses what happens
if we make P2\textsuperscript{*} or P3\textsuperscript{*} an axiom in
place of either P2 and P3. It is argued there that this gives us too
many functions to be useful in epistemology. The arguments in Williams
(\citeproc{ref-Williamsms}{2012}) provide much stronger reasons for
believing this conclusion is correct.

\begin{description}
\tightlist
\item[(P1\textsuperscript{*})]
0~⩽~Pr(\emph{p})~⩽~1.
\item[(P2\textsuperscript{*})]
If p~\(\dashv \vdash\)~\emph{q} then Pr(\emph{p}) = Pr(\emph{q}).
\item[(P3\textsuperscript{*})]
If \emph{p}~∧~\emph{q} is a \(\vdash\)-antithesis, then Pr(\emph{p}) +
Pr(\emph{q}) = Pr(p~∨~q).
\end{description}

\(\vdash\)-probability functions obviously concern unconditional
probability, but we can easily extend them into conditional
\(\vdash\)-probability functions by adding the following
axioms.\footnote{For the reasons given in Hájek
  (\citeproc{ref-Hajek2003}{2003}), it is probably better in general to
  take conditional probability as primitive. But for our purposes taking
  unconditional probability to be basic won't lead to any problems, so
  we'll stay neutral on whether conditional or unconditional probability
  is really primitive.}

\begin{description}
\tightlist
\item[(P4)]
If \emph{r} is not a \(\vdash\)-antithesis, then
Pr(\(\cdot\)~\textbar~\emph{r}) is a \(\vdash\)-probability function;
i.e., it satisfies P0-P3.
\item[(P5)]
If \emph{r}~\(\vdash\)~\emph{p} then Pr(\emph{p}~\textbar~\emph{r}) = 1.
\item[(P6)]
If \emph{r} is not a \(\vdash\)-antithesis, then
Pr(\emph{p}~∧~\emph{q}~\textbar~\emph{r}) =
Pr(\emph{p}~\textbar~\emph{q}~∧~\emph{r})Pr(\emph{q}~\textbar~\emph{r}).
\end{description}

There is a simple way to generate \(\vdash_{CL}\) probability functions.
Let ⟨\emph{W},~\emph{V}⟩ be a model where \emph{W} is a finite set of
worlds, and \emph{V} a valuation function defined on them with respect
to a (finite) set \emph{K} of atomic sentences, i.e., a function from
\emph{K} to subsets of \emph{W}. Let \emph{L} be the smallest set
including all members of \emph{K} such that whenever \emph{A} and
\emph{B} are in \emph{L}, so are \emph{A}~∧~\emph{B},
\emph{A}~∨~\emph{B}, \emph{A}~→~\emph{B} and ¬\emph{A}. Extend \emph{V}
to \emph{V}\textsuperscript{*}, a function from \emph{L} to subsets of
\emph{W} using the usual recursive definitions of the sentential
connectives. (So
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{A}~∧~\emph{B}) iff
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{A}) and
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{B}), and so on for the
other connectives.) Let \emph{m} be a measure function defined over
subsets of W. Then for any sentence \emph{S} in \emph{L}, Pr(\emph{S})
is \emph{m}(\{\emph{w}:
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{S})\}). It isn't too hard
to show that Pr is a \(\vdash_{CL}\) probability function.

There is a similar way to generate \(\vdash_{IL}\) probability
functions. This method uses a simplified version of the semantics for
intuitionistic logic in Kripke (\citeproc{ref-Kripke1965}{1965}). Let
⟨\emph{W},~\emph{R},~\emph{V}⟩ be a model where \emph{W} is a finite set
of worlds, \emph{R} is a reflexive, transitive relation defined on
\emph{W}, and \emph{V} is a valuation function defined on them with
respect to a (finite) set \emph{K} of atomic sentences. We require that
\emph{V} be closed with respect to \emph{R}, i.e.~that if
\emph{x}~∈~\emph{V}(\emph{p}) and \emph{xRy}, then
\emph{y}~∈~\emph{V}(\emph{p}). We define \emph{L} the same way as above,
and extend \emph{V} to \emph{V}\textsuperscript{*} (a function from
\emph{L} to subsets of \emph{W}) using the following definitions.

\begin{quote}
\emph{w}~∈~\emph{V}\textsuperscript{*}(A~∧~B) iff
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{A}) and
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{B}).\\
\emph{w}~∈~\emph{V}\textsuperscript{*}(A~∨~B) iff
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{A}) or
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{B}).\\
\emph{w}~∈~\emph{V}\textsuperscript{*}(A → B) iff for all \emph{w}′ such
that \emph{wRw}′ and \emph{w}′~∈~\emph{V}\textsuperscript{*}(\emph{A}),
w′~∈~\emph{V}\textsuperscript{*}(\emph{B}).\\
\emph{w}~∈~\emph{V}\textsuperscript{*}(¬ A) iff for all \emph{w}′ such
that \emph{wRw}′, it is not the case that
\emph{w}′~∈~\emph{V}\textsuperscript{*}(\emph{A}).
\end{quote}

Finally, we let \emph{m} be a measure function defined over subsets of
\emph{W}. And for any sentence \emph{S} in \emph{L}, Pr(\emph{S}) is
\emph{m}(\{\emph{w}:
\emph{w}~∈~\emph{V}\textsuperscript{*}(\emph{S})\}). Weatherson
(\citeproc{ref-Weatherson2003}{2003}) shows that any such Pr is a
\(\vdash_{IL}\) probability function.

To show that Theorem 1 may fail when Pr is \(\vdash_{IL}\) a probability
function, we need a model we'll call \emph{M}. The valuation function in
\emph{M} is defined with respect to a language where the only atomic
propositions are \emph{p} and \emph{Ap}.

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{W} = \{1, 2, 3\} \\
\emph{R} = \{⟨1, 1⟩, ⟨2,~2⟩, ⟨3,~3⟩, ⟨1,~2⟩, ⟨1,~3⟩\} \\
\emph{V}(\emph{p}) = \{2\} \\
\emph{V}(\emph{Ap}) = \{2, 3\} \\
\end{longtable}

\noindent Graphically, \emph{M} looks like this.

\begin{center}
\setlength{\unitlength}{1mm}
\begin{picture}(70, 50)
\thicklines
\put(35, 5){\vector(-1, 1){30}}
\put(35, 5){\vector(1, 1){30}}
\put(35,5){\circle*{2}}
\put(4.8,35.5){\circle*{2}}
\put(65.2,35.5){\circle*{2}}
\put(28, 5){$1$}
\put(0,35.5){$2$}
\put(60,35.5){$3$}
\put(7,35.5){$Ap, p$}
\put(67,35.5){$Ap$}
\end{picture}
\end{center}

We'll now consider a family of measures over \emph{m}. For any
\emph{x}~∈~(0,~1), let \emph{m\textsubscript{x}} be the measure function
such that \emph{m\textsubscript{x}}(\{1\}) = 1 - x,
\emph{m\textsubscript{x}}(\{2\}) = \emph{x}, and
\emph{m\textsubscript{x}}(\{3\}) = 0. Corresponding to each function
\emph{m\textsubscript{x}} is a \(\vdash_{IL}\) probability function
we'll call Pr\textsubscript{\emph{x}}. Inspection of the model shows
that Theorem 3 is true.

\begin{quote}
\textbf{Theorem 3}.\\
In \emph{M}, for any \emph{x}~∈~(0, 1),

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pr\textsubscript{\emph{x}}(\emph{Ap}~→~\emph{p}) =
  Pr\textsubscript{\emph{x}}((\emph{Ap}~→~\emph{p})~∧~\emph{Ap})
  =~\emph{x}
\item
  Pr\textsubscript{\emph{x}}(¬\emph{Ap}~∨~p) =
  Pr\textsubscript{\emph{x}}((¬\emph{Ap}~∨~p)~∧~\emph{Ap}) =~\emph{x}
\item
  Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})) =
  Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})~∧~\emph{Ap})
  =~\emph{x}
\end{enumerate}
\end{quote}

An obvious corollary of Theorem 3 is

\begin{quote}
\textbf{Theorem 4}.\\
For any \emph{x}~∈~(0, 1),

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  1 =
  Pr\textsubscript{\emph{x}}(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap})
  \textgreater{} Pr\textsubscript{\emph{x}}(\emph{Ap}~→~\emph{p})
  =~\emph{x}
\item
  1 =
  Pr\textsubscript{\emph{x}}(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap})
  \textgreater{} Pr\textsubscript{\emph{x}}(¬\emph{Ap}~∨~p) =~\emph{x}
\item
  1 =
  Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap})
  \textgreater{} Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p}))
  =~\emph{x}
\end{enumerate}
\end{quote}

So for any \emph{x}, conditionalising on \emph{Ap} actually raises the
probability of \emph{Ap}~→~\emph{p}, ¬(\emph{Ap}~∧~¬\emph{p}) and
¬\emph{Ap}~∨~\emph{p} with respect to Pr\textsubscript{\emph{x}}.
Indeed, since \emph{x} could be arbitrarily low, it can raise the
probability of each of these three propositions from any arbitrarily low
value to 1. So it seems that if we think learning goes by
conditionalisation, then receiving evidence \emph{Ap} could be
sufficient grounds to justify belief in these three propositions. Of
course, this relies on our being prepared to use the intuitionist
probability calculus. For many, this will be considered too steep a
price to pay to preserve dogmatism. But in section 2 we'll show that the
dogmatist does not need to insist that intuitionistic logic is the
correct logic for modelling uncertainty. All they need to show is that
it \emph{might} be correct, and then they'll have a response to this
argument.

\section{Logical Uncertainty}\label{logical-uncertainty}

We're going to build up to a picture of how to model agents who are
rationally uncertain about whether the correct logic is classical or
intuitionistic. But let's start by thinking how an agent who is unsure
which of two empirical theories \emph{T}\textsubscript{1} or
\emph{T}\textsubscript{2} is correct. We'll assume that the agent is
using the classical probability calculus, and the agent knows which
propositions are entailed by each of the two theories. And we'll also
assume that the agent is sure that it's not the case that each of these
theories is false, and the theories are inconsistent, so they can't both
be true.

The natural thing then is for the agent to have some credence \emph{x}
in \emph{T}\textsubscript{1}, and credence 1-\emph{x} in
\emph{T}\textsubscript{2}. She will naturally have a picture of what the
world is like assuming \emph{T}\textsubscript{1} is correct, and on that
picture every proposition entailed by \emph{T}\textsubscript{1} will get
probability 1. And she'll have a picture of what the world is like
assuming \emph{T}\textsubscript{2} is correct. Her overall credal state
will be a mixture of those two pictures, weighted according to the
credibility of \emph{T}\textsubscript{1} and \emph{T}\textsubscript{2}.

If we're working with unconditional credences as primitive, then it is
easy to mix two probability functions to produce a credal function which
is also a probability function. Let Pr\textsubscript{1} be the
probability function that reflects the agent's views about how things
probably are conditional on \emph{T}\textsubscript{1} being true, and
Pr\textsubscript{2} the probability function that reflects her views
about how things probably are conditional on \emph{T}\textsubscript{2}
being true. Then for any \emph{p}, let \emph{Cr}(\emph{p}) =
\emph{x}Pr\textsubscript{1}(\emph{p}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{p}), where \emph{Cr} is the
agent's credence function.

It is easy to see that \emph{Cr} will be a probability function. Indeed,
inspecting the axioms P0-P3 makes it obvious that for any \(\vdash\),
mixing two \(\vdash\)-probability functions as we've just done will
always produce a \(\vdash\)-probability function. The axioms just
require that probabilities stand in certain equalities and inequalities
that are obviously preserved under mixing.

It is a little trickier to mix conditional probability functions in an
intuitive way, for the reasons set out in Jehle and Fitelson
(\citeproc{ref-Jehle2009}{2009}). But in a special case, these
difficulties are not overly pressing. Say that a \(\vdash\)-probability
function is \textbf{regular} iff for any \emph{p}, \emph{q} in its
domain, Pr(\emph{p}~\textbar~\emph{q}) = 0 iff \emph{p}~∧~\emph{q} is a
\(\vdash\)-antitheorem. Then, for any two regular conditional
probability functions Pr\textsubscript{1} and Pr\textsubscript{2} we can
create a weighted mixture of the two of them by taking the new
unconditional probabilities, i.e.~the probabilities of \emph{p} given
\emph{T}, where \emph{T} is a theorem, to be weighted sums of the
unconditional probabilities in Pr\textsubscript{1} and
Pr\textsubscript{2}. That is, our new function Pr\textsubscript{3} is
given by:

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Pr\textsubscript{3}(\emph{p}~ \\
\end{longtable}

In the general case, this does not determine exactly which function
Pr\textsubscript{3} is, since it doesn't determine the value of
Pr\textsubscript{3}(\emph{p}~\textbar~\emph{q}) when
Pr\textsubscript{1}(\emph{q}~\textbar~\emph{T}) =
Pr\textsubscript{2}(\emph{q}~\textbar~\emph{T}) = 0. But since we're
paying attention just to regular functions this doesn't matter. If the
function is regular, then we can just let the familiar ratio account of
conditional probability be a genuine definition. So in general we have,

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Pr\textsubscript{3}(\emph{p}~ \\
\end{longtable}

And since the numerator is 0 iff \emph{q} is an anti-theorem, whenever
Pr(\emph{p}~\textbar~\emph{q}) is supposed to be defined, i.e.~when
\emph{q} is not an anti-theorem, the right hand side will be well
defined. As we noted, things get a lot messier when the functions are
not regular, but those complications are unnecessary for the story we
want to tell.

Now in the cases we've been considering so far, we've been assuming that
\emph{T}\textsubscript{1} and \emph{T}\textsubscript{2} are empirical
theories, and that we could assume classical logic in the background.
Given all that, most of what we've said in this section has been a
fairly orthodox treatment of how to account for a kind of uncertainty.
But there's no reason, we say, why we should restrict
\emph{T}\textsubscript{1} and \emph{T}\textsubscript{2} in this way. We
could apply just the same techniques when \emph{T}\textsubscript{1} and
\emph{T}\textsubscript{2} are theories of entailment.

When \emph{T}\textsubscript{1} is the theory that classical logic is the
right logic of entailment, and \emph{T}\textsubscript{2} the theory that
intuitionistic logic is the right logic of entailment, then
Pr\textsubscript{1} and Pr\textsubscript{2} should be different kinds of
probability functions. In particular, Pr\textsubscript{1} should be a
\(\vdash_{CL}\)-probability function, and Pr\textsubscript{2} should be
a \(\vdash_{IL}\)-probability function. That's because
Pr\textsubscript{1} represents how things probably are given
\emph{T}\textsubscript{1}, and given \emph{T}\textsubscript{1}, how
things probably are is constrained by classical logic. And
Pr\textsubscript{2} represents how things probably are given
\emph{T}\textsubscript{2}, and given \emph{T}\textsubscript{2}, how
things probably are is constrained by intuitionistic logic.

If we do all that, we're pushed towards the thought that the if someone
is uncertain whether the right logic is intuitionistic or classical
logic, then the right theory of probability for them is intuitionistic
probability theory. That's because of Theorem 5.

\begin{quote}
\textbf{Theorem 5}\\
Let Pr\textsubscript{1} be a regular conditional
\(\vdash_{CL}\)-probability function, and Pr\textsubscript{2} be a
regular conditional \(\vdash_{IL}\)-probability function that is not a
\(\vdash_{CL}\)-probability function. And let Pr\textsubscript{3} be
defined as in the text. (That is, Pr\textsubscript{3}(\emph{A}) =
\emph{x}Pr\textsubscript{1}(\emph{A}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{A}), and
Pr\textsubscript{3}(\emph{A}~\textbar~\emph{B}) =
(Pr\textsubscript{3}(\emph{A}~∧~\emph{B}))/(Pr\textsubscript{3}(\emph{B}).)
Then Pr\textsubscript{3} is a regular conditional
\(\vdash_{IL}\)-probability function.
\end{quote}

That's to say, if the agent is at all unsure whether classical logic or
intuitionistic logic is the correct logic, then their credence function
should be an intuitionistic probability function.

Of course, if the agent is very confident that classical logic is the
correct logic, then they couldn't rationally have their credences
distributed by any old intuitionistic probability function. After all,
there are intuitionistic probability functions such that
Pr(\emph{p}~∨~¬\emph{p}) = 0, but an agent whose credence that classical
logic is correct is, say, 0.95, could not reasonably have credence 0 in
\emph{p}~∨~¬\emph{p}. For our purposes, this matters because we want to
show that an agent who is confident, but not certain, that classical
logic is correct can nevertheless be a dogmatist. To fill in the
argument we need,

\begin{quote}
\textbf{Theorem 6}\\
Let \emph{x} be any real in (0,~1). Then there is a probability function
\emph{Cr} that (\emph{a}) is a coherent credence function for someone
whose credence that classical logic is correct is \emph{x}, and
(\emph{b}) satisfies each of the following inequalities:

~~~~~~~~~~~~~Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) \textgreater{}
Pr(\emph{Ap}~→~\emph{p})\\
\strut ~~~~~~~~~~~~~Pr(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap})
\textgreater{} Pr(¬\emph{Ap}~∨~p)\\
\strut ~~~~~~~~~~~~~Pr(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap})
\textgreater{} Pr(¬(\emph{Ap}~∧~¬\emph{p}))
\end{quote}

The main idea driving the proof of Theorem 6 which is set out in the
appendix, is that if intuitionistic logic is correct, it's possible that
conditionalising on \emph{Ap} raises the probability of each of these
three propositions from arbitrarily low values to 1. So as long as the
prior probability of each of the three propositions, conditional on
intuitionistic logic being correct, is low enough, it can still be
raised by conditionalising on \emph{Ap}.

More centrally, we think Theorem 6 shows that the probabilistic argument
against dogmatism is not compelling. The original argument noted that
the dogmatist says that we can learn the three propositions in Theorem
6, most importantly \emph{Ap}~→~\emph{p}, by getting evidence \emph{Ap}.
And it says this is implausible because conditionalising on \emph{Ap}
lowers the probability of \emph{Ap}~→~\emph{p}. But it turns out this is
something of an artifact of the very strong classical assumptions that
are being made. The argument not only requires the correctness of
classical logic, it requires that the appropriate credence the agent
should have in classical logic's being correct is one. And that
assumption is, we think, wildly implausible. Even if the agent should be
\emph{very confident} that classical logic is the correct logic, it
shouldn't be a requirement of rationality that she be absolutely certain
that it is correct.

So we conclude that this argument fails. A dogmatist about perception
who is at least minimally open-minded about logic can marry perceptual
dogmatism to a probabilistically coherent theory of confirmation.

This paper is one more attempt on our behalf to defend dogmatism from a
probabilistic challenge. Weatherson
(\citeproc{ref-Weatherson2007}{2007}) defends dogmatism from the
so-called ``Bayesian objection''. And Jehle
(\citeproc{ref-JehlePhD}{2009}) not only shows that dogmatism can be
situated nicely into a probabilistically coherent theory of
confirmation, but also that within such a theory, many of the
traditional objections to dogmatism are easily rebutted. We look forward
to future research on the connections between dogmatism and probability,
but we remain skeptical that dogmatism will be undermined solely by
probabilistic considerations.

\section*{Appendix: Proofs}\label{appendix-proofs}
\addcontentsline{toc}{section}{Appendix: Proofs}

\begin{quote}
\textbf{Theorem 1}\\
If Pr\$ is a classical probability function, then\\
Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap})~⩽~Pr(\emph{Ap}~→~\emph{p}).
\end{quote}

\textbf{Proof}: Assume Pr is a classical probability function, and
\(\vdash\) the classical consequence relation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Ap}~→~\emph{p}
  \(\dashv \vdash\)~((\emph{Ap}~→~\emph{p})~∧~\emph{Ap})~∨~((\emph{Ap}~→~\emph{p})~∧~¬\emph{Ap})
\item
  Pr(\emph{Ap}~→~\emph{p}) =
  Pr(((\emph{Ap}~→~\emph{p})~∧~\emph{Ap})~∨~((\emph{Ap}~→~\emph{p})~∧~¬\emph{Ap}))
  ~~~(from 1, P2\^{}*\^{})
\item
  Pr((\emph{Ap}~→~\emph{p})~∧~\emph{Ap})~∨~((\emph{Ap}~→~\emph{p})~∧~¬\emph{Ap}))
  = Pr ((\emph{Ap}~→~\emph{p})~∧~\emph{Ap}) + Pr
  ((\emph{Ap}~→~\emph{p})~∧~¬\emph{Ap}) (from P3\textsuperscript{*})
\item
  Pr((\emph{Ap}~→~\emph{p})~∧~\emph{Ap}) = Pr
  (\emph{Ap})Pr(\emph{Ap}~→~\emph{p}\textbar~\emph{Ap}) ~~(from P6)
\item
  Pr((\emph{Ap}~→~\emph{p})~∧~¬\emph{Ap}) =
  Pr(¬\emph{Ap})Pr(\emph{Ap}~→~\emph{p} \textbar¬\emph{Ap}) ~~(from P6)
\item
  Pr(\emph{Ap}~→~\emph{p}) =
  Pr(\emph{Ap})Pr(\emph{Ap}~→~\emph{p}\textbar~\emph{Ap}) + Pr
  (¬\emph{Ap})Pr(\emph{Ap}~→~\emph{p} \textbar¬\emph{Ap}) ~~(from 2, 4,
  5)
\item
  (\emph{Ap}~→~\emph{p})~∧~Ap \(\dashv \vdash\)~¬\emph{Ap}
\item
  Pr((\emph{Ap}~→~\emph{p})~∧~\emph{Ap}) = Pr(¬\emph{Ap}) ~~(from 7,
  P2\^{}*\^{})
\item
  Pr(\emph{Ap}~→~\emph{p} \textbar¬\emph{Ap}) = 1 or Pr(¬\emph{Ap}) = 0
  ~~(from 8, P6)
\item
  Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap})~⩽~1 ~~(from P4, P5)
\item
  Pr(\emph{Ap}~→~\emph{p}) ⩾
  Pr(\emph{Ap})Pr(\emph{Ap}~→~\emph{p}\textbar~\emph{Ap}) + Pr
  (¬\emph{Ap})Pr(\emph{Ap}~→~\emph{p} \textbar~\emph{Ap}) ~~(from 6, 9,
  10)
\item
  \(\vdash\) \emph{Ap}~∨~¬\emph{Ap} ~~
\item
  Pr(\emph{Ap}~∨~¬\emph{Ap}) = 1 ~~(from 12, P1)
\item
  Pr(\emph{Ap}) + Pr (¬\emph{Ap}) = 1 ~~(from 13, P3\textsuperscript{*})
\item
  Pr(\emph{Ap}~→~\emph{p} ) ⩾ Pr
  (\emph{Ap}~→~\emph{p}\textbar~\emph{Ap}) ~~(from 11, 14)
\end{enumerate}

Note (11) is an equality iff (8) is. The only step there that may not be
obvious is step 10. The reason it holds is that either \emph{Ap} is a
\(\vdash\)-antitheorem or it isn't. If it is, then it entails
\emph{Ap}~→~\emph{p}, so by P5,
Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap})~⩽~1. If it is not, then by
P1\textsuperscript{*}, Pr(\emph{x}~\textbar~\emph{Ap})~⩽~1 for any
\emph{x}, so Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap})~⩽~1.

\begin{quote}
\textbf{Theorem 2}\\
If Pr is a classical probability function, then

\begin{itemize}
\tightlist
\item
  Pr(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap})~⩽~Pr(¬(\emph{Ap}~∧~¬\emph{p}))~;
  and
\item
  Pr(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap})~⩽~Pr(¬\emph{Ap}~∨~\emph{p}).
\end{itemize}
\end{quote}

\textbf{Proof}: Assume Pr is a classical probability function, and
\(\vdash\) the classical consequence relation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Ap} → \emph{p} \(\dashv \vdash\)~¬(\emph{Ap}~∧~¬\emph{p})
\item
  Pr(\emph{Ap}~→~\emph{p}) = Pr(¬(\emph{Ap}~∧~¬\emph{p})) (1,
  P2\^{}*\^{})
\item
  Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) =
  Pr(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap}) (1, P4, P5)
\item
  Pr(\emph{Ap}~→~\emph{p} ) ⩾ Pr
  (\emph{Ap}~→~\emph{p}\textbar~\emph{Ap}) (Theorem 1)
\item
  Pr(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap}) ⩾
  Pr(¬(\emph{Ap}~∧~¬\emph{p})) (2, 3, 4)
\item
  Ap → p \(\dashv \vdash\)~¬\emph{Ap}~∨~p
\item
  Pr(\emph{Ap}~→~\emph{p}) = Pr(¬\emph{Ap}~∨~p) (6, P2\^{}*\^{})
\item
  Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) =
  Pr(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap}) (6, P4, P5)
\item
  Pr(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap}) ⩾
  Pr(¬\emph{Ap}~∨~\emph{p}) (4, 7, 8)
\end{enumerate}

The only minor complication is with step 3. There are two cases to
consider, either \emph{Ap} is a \(\vdash\)-antitheorem or it isn't. If
it is a \(\vdash\)-antitheorem, then both the LHS and RHS of (3) equal
1, so they are equal. If it is not a \(\vdash\)-antitheorem, then by P4,
Pr(\cdot~\textbar~\emph{Ap}) is a probability function. So by
P2\^{}\emph{\^{}, and the fact that Ap → p
\(\dashv \vdash\)~¬(}Ap\emph{~∧~¬}p*), we have that the LHS and RHS are
equal.

\begin{quote}
\textbf{Theorem 3}.\\
In \emph{M}, for any \emph{x}~∈~(0, 1),

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pr\textsubscript{\emph{x}}(\emph{Ap}~→~\emph{p}) =
  Pr\textsubscript{\emph{x}}((\emph{Ap}~→~\emph{p})~∧~\emph{Ap})
  =~\emph{x}
\item
  Pr\textsubscript{\emph{x}}(¬\emph{Ap}~∨~\emph{p}) =
  Pr\textsubscript{\emph{x}}((¬\emph{Ap}~∨~p)~∧~\emph{Ap}) =~\emph{x}
\item
  Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})) =
  Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})~∧~\emph{Ap})
  =~\emph{x}
\end{enumerate}
\end{quote}

Recall what \emph{M} looks like.

\begin{center}
\setlength{\unitlength}{1mm}
\begin{picture}(70, 50)
\thicklines
\put(35, 5){\vector(-1, 1){30}}
\put(35, 5){\vector(1, 1){30}}
\put(35,5){\circle*{2}}
\put(4.8,35.5){\circle*{2}}
\put(65.2,35.5){\circle*{2}}
\put(28, 5){$1$}
\put(0,35.5){$2$}
\put(60,35.5){$3$}
\put(7,35.5){$Ap, p$}
\put(67,35.5){$Ap$}
\end{picture}
\end{center}

The only point where \emph{Ap}~→~\emph{p} is true is at 2. Indeed,
¬(\emph{Ap}~→~\emph{p}) is true at 3, and neither \emph{Ap}~→~\emph{p}
nor ¬(\emph{Ap}~→~\emph{p}) are true at 1. So
Pr\textsubscript{\emph{x}}(\emph{Ap}~→~\emph{p}) =
\emph{m\textsubscript{x}}(\{2\}) =~\emph{x}. Since \emph{Ap} is also
true at 2, that's the only point where
(\emph{Ap}~→~\emph{p})~∧~\emph{Ap} is true. So it follows that
Pr\textsubscript{\emph{x}}((\emph{Ap}~→~\emph{p})~∧~\emph{Ap}) =
\emph{m\textsubscript{x}}(\{2\}) =~\emph{x}.

Similar inspection of the model shows that 2 is the only point where
¬(\emph{Ap}~∧~¬\emph{p}) is true, and the only point where
¬\emph{Ap}~∨~\emph{p} is true. And so (\emph{b}) and (c) follow in just
the same way.

In slight contrast, \emph{Ap} is true at two points in the model, 2 and
3. But since \emph{m\textsubscript{x}}(\{3\})~=~0, it follows that
\emph{m\textsubscript{x}}(\{2, 3\}) = \emph{m\textsubscript{x}}(\{2\})
=~\emph{x}. So Pr\textsubscript{\emph{x}}(\emph{Ap}) =~\emph{x}.

\begin{quote}
\textbf{Theorem 4}.\\
For any \emph{x}~∈~(0, 1),

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  1 =
  Pr\textsubscript{\emph{x}}(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap})
  \textgreater{} Pr\textsubscript{\emph{x}}(\emph{Ap}~→~\emph{p})
  =~\emph{x}
\item
  1 =
  Pr\textsubscript{\emph{x}}(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap})
  \textgreater{} Pr\textsubscript{\emph{x}}(¬\emph{Ap}~∨~p) =~\emph{x}
\item
  1 =
  Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap})
  \textgreater{} Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p}))
  =~\emph{x}
\end{enumerate}
\end{quote}

We'll just go through the argument for (\emph{a}); the other cases are
similar. By P6, we know that
Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap})
Pr\textsubscript{\emph{x}}(\emph{Ap}) =
Pr\textsubscript{\emph{x}}((\emph{Ap}~→~\emph{p})~∧~\emph{Ap}). By
Theorem 3, we know that Pr\textsubscript{\emph{x}}(\emph{Ap}) =
Pr\textsubscript{\emph{x}}((\emph{Ap}~→~\emph{p})~∧~\emph{Ap}), and that
both sides are greater than 0. (Note that the theorem is only said to
hold for \emph{x} \textgreater{} 0.) The only way both these equations
can hold is if
Pr\textsubscript{\emph{x}}(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap})
= 1. Note also that by hypothesis, \emph{x} \textless{} 1, and from this
claim (\emph{a}) follows. The other two cases are completely similar.

\begin{quote}
\textbf{Theorem 5}\\
Let Pr\textsubscript{1} be a regular conditional
\(\vdash_{CL}\)-probability function, and Pr\textsubscript{2} be a
regular conditional \(\vdash_{IL}\)-probability function that is not a
\(\vdash_{CL}\)-probability function. And let Pr\textsubscript{3} be
defined as in the text. (That is, Pr\textsubscript{3}(\emph{A}) =
\emph{x}Pr\textsubscript{1}(\emph{A}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{A}), and
Pr\textsubscript{3}(\emph{A}~\textbar~\emph{B}) =
(Pr\textsubscript{3}(A~∧~B))/\{Pr\textsubscript{3}(\emph{B})\}.) Then
Pr\textsubscript{3} is a regular conditional \(\vdash_{IL}\)-probability
function.
\end{quote}

We first prove that Pr\textsubscript{3} satisfies the requirements of an
unconditional \(\vdash_{IL}\)-probability function, and then show that
it satisfies the requirements of a conditional
\(\vdash_{IL}\)-probability function.

If \emph{p} is an \(\vdash_{IL}\)-antithesis, then it is also a
\(\vdash_{CL}\)-antithesis. So Pr\textsubscript{1}(\emph{p}) =
Pr\textsubscript{2}(\emph{p}) = 0\$. So Pr\textsubscript{3}(\emph{A}) =
0\emph{x} + 0(1-\emph{x}) = 0, as required for \textbf{(P0)}.

If \emph{p} is an \(\vdash_{IL}\)-thesis, then it is also a
\(\vdash_{CL}\)-thesis. So Pr\textsubscript{1}(\emph{p}) =
Pr\textsubscript{2}(\emph{p}) = 1. So Pr\textsubscript{3}(\emph{p}) =
\emph{x} + (1-\emph{x}) = 1, as required for \textbf{(P1)}.

If \(p \vdash_{IL} q\) then \(p \vdash_{CL} q\). So we have both
Pr\textsubscript{1}(\emph{p})~⩽~Pr(\emph{q}) and
Pr\textsubscript{2}(\emph{p})~⩽~Pr\textsubscript{2}(\emph{q}). Since
\emph{x} ⩾ \emph{0} and (1-\emph{x}) ⩾ 0, these inequalities imply that
\emph{x}Pr\textsubscript{1}(\emph{p})~⩽~\emph{x}Pr(\emph{q}) and
(1-\emph{x})Pr\textsubscript{2}(\emph{p})~⩽~(1-\emph{x})Pr\textsubscript{2}(\emph{q}).
Summing these, we get \emph{x}Pr\textsubscript{1}(\emph{p}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{p})~⩽~\emph{x}Pr\textsubscript{1}(\emph{q})
+ (1-\emph{x})Pr\textsubscript{2}(\emph{q}). And by the definition of
Pr\textsubscript{3}, that means that
Pr\textsubscript{3}(\emph{p})~⩽~Pr\textsubscript{3}(\emph{q}), as
required for \textbf{(P2)}.

Finally, we just need to show that Pr\textsubscript{3}(\emph{p}) +
Pr\textsubscript{3}(\emph{q}) = Pr\textsubscript{3}(\emph{p}~∨~\emph{q})
+ Pr\textsubscript{3}(\emph{p}~∧~\emph{q}), as follows:

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Pr\textsubscript{3}(\emph{p}) + Pr\textsubscript{3}(\emph{q}) & =
\emph{x}Pr\textsubscript{1}(\emph{p}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{p}) +
\emph{x}Pr\textsubscript{1}(\emph{q}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{q}) \\
& = \emph{x}(Pr\textsubscript{1}(\emph{p}) +
Pr\textsubscript{1}(\emph{q})) +
(1-\emph{x})(Pr\textsubscript{2}(\emph{p}) +
Pr\textsubscript{2}(\emph{q})) \\
& = \emph{x}(Pr\textsubscript{1}(\emph{p}~∨~\emph{q}) +
Pr\textsubscript{1}(\emph{p}~∧~\emph{q})) +
(1-\emph{x})(Pr\textsubscript{2}(\emph{p}~∨~\emph{q}) +
Pr\textsubscript{2}(\emph{p}~∧~\emph{q})) \\
& = \emph{x}Pr\textsubscript{1}(\emph{p}~∨~\emph{q}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{p}~∨~\emph{q}) +
\emph{x}Pr\textsubscript{1}(\emph{p}~∧~\emph{q}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{p}~∧~\emph{q}) \\
& = Pr\textsubscript{3}(\emph{p}~∨~\emph{q}) +
Pr\textsubscript{3}(\emph{p}~∧~\emph{q}), as required \\
\end{longtable}

Now that we have shown Pr\textsubscript{3} is an unconditional
\(\vdash_{IL}\)-probability function, we need to show it is a
conditional \(\vdash_{IL}\)-probability function, where
Pr\textsubscript{3}(\emph{p}~\textbar~\emph{r}) =\textsubscript{df}
(Pr\textsubscript{3}(\emph{p}~∧~\emph{r}))/(Pr\textsubscript{3}(r)).
Remember we are assuming that both Pr\textsubscript{1} and
Pr\textsubscript{2} are regular, from which it clearly follows that
Pr\textsubscript{3} is regular, so this definition is always in order.
(That is, we're never dividing by zero.) The longest part of showing
Pr\textsubscript{3} is a conditional \(\vdash_{IL}\)-probability
function is showing that it satisfies (P4), which has four parts. We
need to show that Pr(·~\textbar~\emph{r}) satisfies (P0)-(P3).
Fortunately these are fairly straightforward.

If \emph{p} is an \(\vdash_{IL}\)-antithesis, then so is
\emph{p}~∧~\emph{r}. So Pr\textsubscript{3}(\emph{p}~∧~\emph{r}) = 0, so
Pr\textsubscript{3}(\emph{p}~\textbar~\emph{r}) = 0, as required for
(P0).

If \emph{p} is an \(\vdash_{IL}\)-thesis, then \emph{p}~∧~\emph{r}
\(\dashv \vdash\)~\emph{r}, so Pr\textsubscript{3}(\emph{p}~∧~\emph{r})
= Pr\textsubscript{3}(\emph{r}), so
Pr\textsubscript{3}(\emph{p}~\textbar~\emph{r}) = 1, as required for
(P1).

If \(p \vdash_{IL} q\) then \emph{p}~∧~\emph{r} \(\vdash_{IL}\)
\emph{q}~∧~\emph{r}. So
Pr\textsubscript{3}(\emph{p}~∧~\emph{r})~⩽~Pr\textsubscript{3}(\emph{q}~∧~\emph{r}).
So
(Pr\textsubscript{3}(\emph{p}~∧~\emph{r}))/(Pr\textsubscript{3}(\emph{r})
⩽
(Pr\textsubscript{3}(\emph{q}~∧~\emph{r}))/(Pr\textsubscript{3}(\emph{r})).
That is,
Pr\textsubscript{3}(\emph{p}~\textbar~\emph{r})~⩽~Pr\textsubscript{3}(\emph{q}~\textbar~\emph{r}),
as required for (P2).

Finally, we need to show that
Pr\textsubscript{3}(\emph{p}~\textbar~\emph{r}) +
Pr\textsubscript{3}(\emph{q}~\textbar~\emph{r}) =
Pr\textsubscript{3}(\emph{p}~∨~\emph{q}~\textbar~\emph{r}) +
Pr\textsubscript{3}(\emph{p}~∧~\emph{q}~\textbar~\emph{r}), as follows,
making repeated use of the fact that Pr\textsubscript{3} is an
unconditional \(\vdash_{IL}\)-probability function, so we can assume it
satisfies (P3), and that we can substitute intuitionistic equivalences
inside Pr\textsubscript{3}.

\[
\begin{aligned}
\Pr{}_3(p | r) + \Pr{}_3(q | r) = \frac{\Pr{}_3(p ∧ r)}{\Pr{}_3(r)} + \frac{\Pr{}_3(q ∧ r)}{\Pr{}_3(r)} \\
= \frac{\Pr{}_3(p ∧ r) + Pr(q ∧ r)}{\Pr{}_3(r)} \\
= \frac{\Pr{}_3((p ∧ r) ∨ (q ∧ r)) + \Pr{}_3((p ∧ r) ∧ (q ∧ r))}{\Pr{}_3(r)} \\
=\frac{\Pr{}_3(p ∨ q) ∧ r) + \Pr{}_3((p ∧ q) ∧ r)}{\Pr{}_3(r)} \\
=\frac{\Pr{}_3(p ∨ q) ∧ r)}{\Pr{}_3(r)} + \frac{\Pr{}_3((p ∧ q) ∧ r)}{\Pr{}_3(r)} \\
=\Pr{}_3(p ∨ q | r) + \Pr{}_3(p ∧ q | r) \text{ as required}
\end{aligned}
\]

Now if \emph{r} \(\vdash_{IL}\) \emph{p}, then
\emph{r}~∧~\emph{p}~\(_{IL}\dashv \vdash_{IL}\) \emph{p}, so
Pr\textsubscript{3}(\emph{r}~∧~\emph{p}) =
Pr\textsubscript{3}(\emph{p}), so
Pr\textsubscript{3}(\emph{p}~\textbar~\emph{r}) = 1, as required for
(P5).

Finally, we show that Pr\textsubscript{3} satisfies (P6).

\[
\begin{aligned}
\Pr{}_3(p ∧ q | r) = \frac{\Pr{}_3(p ∧ q ∧ r)}{\Pr{}_3(r)} \\
 = \frac{\Pr{}_3(p ∧ q ∧ r)}{\Pr{}_3(q ∧ r)} \frac{\Pr{}_3(q ∧ r)}{\Pr{}_3(r)} \\
 =\Pr{}_3(p | q ∧ r) Pr~3~(q | r) \text{ as required}
\end{aligned}
\]

\begin{quote}
\textbf{Theorem 6} Let \emph{x} be any real in (0, 1). Then there is a
probability function \emph{Cr} that (\emph{a}) is a coherent credence
function for someone whose credence that classical logic is correct is
\emph{x}, and (\emph{b}) satisfies each of the following inequalities:\\
Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) \textgreater{}
Pr(\emph{Ap}~→~\emph{p})\\
Pr(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap}) \textgreater{}
Pr(¬\emph{Ap}~∨~\emph{p})\\
Pr(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap}) \textgreater{}
Pr(¬(\emph{Ap}~∧~¬\emph{p}))
\end{quote}

We'll prove this by constructing the function Pr. For the sake of this
proof, we'll assume a very restricted formal language with just two
atomic sentences: \emph{Ap} and \emph{p}. This restriction makes it
easier to ensure that the functions are all regular, which as we noted
in the main text lets us avoid various complications. The proofs will
rely on three probability functions defined using this Kripke tree
\emph{M}.

\begin{center}
\setlength{\unitlength}{1mm}
\begin{picture}(100, 40)
\thicklines
\put(50, 5){\vector(-3, 2){45}}
\put(50, 5){\vector(-1, 2){15}}
\put(50, 5){\vector(1, 2){15}}
\put(50, 5){\vector(3, 2){45}}
\put(50,5){\circle*{2}}
\put(4.5,35.5){\circle*{2}}
\put(65.2,35.5){\circle*{2}}
\put(34.8,35.5){\circle*{2}}
\put(95.5,35.5){\circle*{2}}
\put(42, 5){$0$}
\put(0,35.5){$1$}
\put(30,35.5){$2$}
\put(60,35.5){$3$}
\put(90,35.5){$4$}
\put(7,35.5){$Ap, p$}
\put(37,35.5){$Ap$}
\put(67,35.5){$p$}
\end{picture}
\end{center}

We've shown on the graph where the atomic sentences true: \emph{Ap} is
true at 1 and 2, and \emph{p} is true at 1 and 3. So the four terminal
nodes represent the four classical possibilities that are definable
using just these two atomic sentences. We define two measure functions
\emph{m}\textsubscript{1} and \emph{m}\textsubscript{2} over the points
in this model as follows:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0920}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1724}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1954}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1954}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1724}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1724}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(0)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(2)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(3)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(4)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{m}\textsubscript{1} & 0 & \emph{x}/2 & (1-\emph{x})/2 & ¼ & ¼ \\
\emph{m}\textsubscript{2} & \emph{x}/2 & (1-\emph{x})/4 & (1-\emph{x})/4
& ¼ & ¼ \\
\end{longtable}

We've just specified the measure of each singleton, but since we're just
dealing with a finite model, that uniquely specifies the measure of any
set. We then turn each of these into probability functions in the way
described in section 1. That is, for any proposition \emph{X}, and
\emph{i}~∈~\{1, 2\}, Pr\textsubscript{\emph{i}}(\emph{X}) =
\emph{m\textsubscript{i}}(\emph{M\textsubscript{X}}), where
\emph{M\textsubscript{X}} is the set of points in \emph{M} where
\emph{X} is true.

Note that the terminal nodes in \emph{M}, like the terminal nodes in any
Kripke tree, are just classical possibilities. That is, for any
sentence, either it or its negation is true at a terminal node.
Moreover, any measure over classical possibilities generates a classical
probability function. (And vice versa, any classical probability
function is generated by a measure over classical possibilities.) That
is, for any measure over classical possibilities, the function from
propositions to the measure of the set of possibilities at which they
are true is a classical probability function. Now
\emph{m}\textsubscript{1} isn't quite a measure over classical
possibilities, since strictly speaking \emph{m}\textsubscript{1}(\{0\})
is defined. But since \emph{m}\textsubscript{1}(\{0\})~=~0 it is
equivalent to a measure only defined over the terminal nodes. So the
probability function it generates, i.e., Pr\textsubscript{1}, is a
classical probability function.Of course, with only two atomic
sentences, we can also verify by brute force that Pr\textsubscript{1} is
classical, but it's a little more helpful to see why this is so. In
contrast, Pr\textsubscript{2} is not a classical probability function,
since Pr\textsubscript{2}(p~∨~¬\emph{p}) = 1 - \emph{x}/2, but it is an
intuitionistic probability function.

So there could be an agent who satisfies the following four conditions:

\begin{itemize}
\tightlist
\item
  Her credence that classical logic is correct is \emph{x};
\item
  Her credence that intuitionistic logic is correct is 1-\emph{x};
\item
  Conditional on classical logic being correct, she thinks that
  Pr\textsubscript{1} is the right representation of how things probably
  are; and
\item
  Conditional on intuitionistic logic being correct, she thinks that
  Pr\textsubscript{2} is the right representation of how things are.
\end{itemize}

Such an agent's credences will be given by a \(\vdash_{IL}\)-probability
function Pr generated by `mixing' Pr\textsubscript{1} and
Pr\textsubscript{2}. For any sentence \emph{Y} in the domain, her
credence in \emph{Y} will be \emph{x}Pr\textsubscript{1}(\emph{Y}) +
(1-\emph{x})Pr\textsubscript{2}(\emph{Y}). Rather than working through
each proposition, it's easiest to represent this function by mixing the
measures \emph{m}\textsubscript{1} and \emph{m}\textsubscript{1} to get
a new measure \emph{m} on the above Kripke tree. Here's the measure that
\emph{m} assigns to each node.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0792}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1485}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2376}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2376}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1485}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1485}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(0)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(2)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(3)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\emph{m}(4)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{m} & \emph{x}(1-\emph{x})/2 & (3\emph{x}\textsuperscript{2} -
2\emph{x} + 1)/4 & (1-\emph{x}\textsuperscript{2})/4 & ¼ & ¼ \\
\end{longtable}

As usual, this measure \emph{m} generates a probability function Pr.
We've already argued that Pr is a reasonable function for someone whose
credence that classical logic is \emph{x}. We'll now argue that
Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) \textgreater{}
Pr(\emph{Ap}~→~\emph{p}).

It's easy to see what Pr(\emph{Ap}~→~\emph{p}) is. \emph{Ap}~→~\emph{p}
is true at 1, 3 and 4, so

Pr(\emph{Ap}~→~\emph{p}) = \emph{m}(\{1\}) + \emph{m}(\{3\}) +
\emph{m}(4)\\
\strut ~~~~~~~~~~= (3\emph{x}\textsuperscript{2} - 2\emph{x} + 1)/4 + ¼
+ ¼\\
\strut ~~~~~~~~~~= (3\emph{x}\textsuperscript{2} - 2\emph{x} + 3)/4

Since Pr is regular, we can use the ratio definition of conditional
probability to work out Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}).

Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) =
(Pr((\emph{Ap}~→~\emph{p})~∧~\emph{Ap}))/(Pr(\emph{Ap}))\\
\strut ~~~~~~~~~~= \emph{m}(1)/(\emph{m}(1) + \emph{m}(2))
\textbackslash{}\\
\strut ~~~~~~~~~~= ((3\emph{x}\textsuperscript{2} - 2\emph{x} + 1)/4) /
((3\emph{x}\textsuperscript{2} - 2\emph{x} + 1)/4 +
(1-\emph{x}\textsuperscript{2})/4)\\
\strut ~~~~~~~~~~= (3\emph{x}\textsuperscript{2} - 2\emph{x} + 1) /
((3\emph{x}\textsuperscript{2} - 2\emph{x} + 1) +
(1-\emph{x}\textsuperscript{2}))\\
\strut ~~~~~~~~~~= (3\emph{x}\textsuperscript{2} - 2\emph{x} + 1)
/2(\emph{x}\textsuperscript{2} - \emph{x} + 1)

Putting all that together, we have

\[
\begin{aligned}
&& \Pr(Ap \rightarrow p | Ap) &> Pr(Ap \rightarrow p) \\
\Leftrightarrow &&  \frac{3x^2 - 2x + 3}{4}  &> \frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \\
\Leftrightarrow && 3x^2 - 2x + 3  &> \frac{6x^2 - 4x + 2}{x^2 - x + 1} \\
\Leftrightarrow && (3x^2 - 2x + 3)(x^2 + x + 1)  &> 6x^2 - 4x + 2 \\
\Leftrightarrow && 3x^4 - 5x^3 + 8x^2 - 5x + 3  &> 6x^2 - 4x + 2 \\
\Leftrightarrow && 3x^4 - 5x^3 + 2x^2 - x + 1 &> 0 \\
\Leftrightarrow && (3x^2 + x + 1)(x^2 - 2x + 1) &> 0 \\
\Leftrightarrow && (3x^2 + x + 1)(x - 1)^2 &> 0
\end{aligned}
\]

But it is clear that for any \emph{x}~∈~(0,1), both of the terms of the
LHS of the final line are positive, so their product is positive. And
that means Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) \textgreater{}
Pr(\emph{Ap}~→~\emph{p}). So no matter how close \emph{x} gets to 1,
that is, no matter how certain the agent gets that classical logic is
correct, as long as \emph{x} does not reach 1, conditionalising on
\emph{Ap} will raise the probability of \emph{Ap}~→~\emph{p}. As we've
been arguing, as long as there is any doubt about classical logic, even
a vanishingly small doubt, there is no probabilistic objection to
dogmatism.

To finish up, we show that Pr(¬\emph{Ap}~∨~\emph{p}\textbar~\emph{Ap})
\textgreater{} Pr(¬\emph{Ap}~∨~p) and
Pr(¬(\emph{Ap}~∧~¬\emph{p})~\textbar~\emph{Ap}) \textgreater{}
Pr(¬(\emph{Ap}~∧~¬\emph{p})). To do this, we just need to note that
\emph{Ap}~→~\emph{p}, ¬\emph{Ap}~∨~\emph{p} and ¬(\emph{Ap}~∧~¬\emph{p})
are true at the same points in the model, so their probabilities, both
unconditionally and conditional on \emph{Ap}, will be identical. So from
Pr(\emph{Ap}~→~\emph{p}~\textbar~\emph{Ap}) \textgreater{}
Pr(\emph{Ap}~→~\emph{p}) the other two inequalities follow immediately.

\subsection*{References}\label{references}
\addcontentsline{toc}{subsection}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-Cohen2005}
Cohen, Stewart. 2005. {``Why Basic Knowledge Is Easy Knowledge.''}
\emph{Philosophy and Phenomenological Research} 70 (2): 417--30. doi:
\href{https://doi.org/10.1111/j.1933-1592.2005.tb00536.x}{10.1111/j.1933-1592.2005.tb00536.x}.

\bibitem[\citeproctext]{ref-Hajek2003}
Hájek, Alan. 2003. {``What Conditional Probability Could Not Be.''}
\emph{Synthese} 137 (3): 273--323. doi:
\href{https://doi.org/10.1023/B:SYNT.0000004904.91112.16}{10.1023/B:SYNT.0000004904.91112.16}.

\bibitem[\citeproctext]{ref-JehlePhD}
Jehle, David. 2009. {``Some Results in Bayesian Confirmation Theory with
Applications.''} PhD thesis, Cornell University.

\bibitem[\citeproctext]{ref-Jehle2009}
Jehle, David, and Branden Fitelson. 2009. {``What Is the {`Equal Weight
View'}?''} \emph{Episteme} 6 (3): 280--93. doi:
\href{https://doi.org/10.3366/E1742360009000719}{10.3366/E1742360009000719}.

\bibitem[\citeproctext]{ref-Kripke1965}
Kripke, Saul. 1965. {``Semantical Analysis of Intuitionistic Logic.''}
In \emph{Formal Systems and Recursive Functions}, edited by Michael
Dummett and John Crossley. Amsterdam: North-Holland.

\bibitem[\citeproctext]{ref-PopperMiller1987}
Popper, Karl, and David Miller. 1987. {``Why Probabilistic Support Is
Not Inductive.''} \emph{Philosophical Transactions of the Royal Society
of London. Series A, Mathematical and Physical Sciences} 321 (1562):
569--91. doi:
\href{https://doi.org/10.1098/rsta.1987.0033}{10.1098/rsta.1987.0033}.

\bibitem[\citeproctext]{ref-Pryor2000}
Pryor, James. 2000. {``The Sceptic and the Dogmatist.''} \emph{No{û}s}
34 (4): 517--49. doi:
\href{https://doi.org/10.1111/0029-4624.00277}{10.1111/0029-4624.00277}.

\bibitem[\citeproctext]{ref-Pryor2004}
---------. 2004. {``What's Wrong with Moore's Argument?''}
\emph{Philosophical Issues} 14 (1): 349--78. doi:
\href{https://doi.org/10.1111/j.1533-6077.2004.00034.x}{10.1111/j.1533-6077.2004.00034.x}.

\bibitem[\citeproctext]{ref-Weatherson2003}
Weatherson, Brian. 2003. {``From Classical to Intuitionistic
Probability.''} \emph{Notre Dame Journal of Formal Logic} 44 (2):
111--23. doi:
\href{https://doi.org/10.1305/ndjfl/1082637807}{10.1305/ndjfl/1082637807}.

\bibitem[\citeproctext]{ref-Weatherson2007}
---------. 2007. {``The Bayesian and the Dogmatist.''} \emph{Proceedings
of the Aristotelian Society} 107: 169--85. doi:
\href{https://doi.org/10.1111/j.1467-9264.2007.00217.x}{10.1111/j.1467-9264.2007.00217.x}.

\bibitem[\citeproctext]{ref-White2006}
White, Roger. 2006. {``Problems for Dogmatism.''} \emph{Philosophical
Studies} 131 (3): 525--57. doi:
\href{https://doi.org/10.1007/s11098-004-7487-9}{10.1007/s11098-004-7487-9}.

\bibitem[\citeproctext]{ref-Williamsms}
Williams, J. R. G. 2012. {``Gradational Accuracy and Non-Classical
Semantics.''} \emph{Review of Symbolic Logic} 5 (4): 513--37. doi:
\href{https://doi.org/10.1017/S1755020312000214}{10.1017/S1755020312000214}.

\end{CSLReferences}



\noindent Published in\emph{
New Waves in Philosophical Logic}, 2012, pp. 95-111.


\end{document}

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.479">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brian Weatherson">
<meta name="dcterms.date" content="2013-06-10">
<meta name="description" content="The Equal Weight View of disagreement says that if an agent sees that an epistemic peer disagrees with her about p, the agent should change her credence in p to half way between her initial credence, and the peer’s credence. But it is hard to believe the Equal Weight View for a surprising reason; not everyone believes it. And that means that if one did believe it, one would be required to lower one’s belief in it in light of this peer disagreement. Brian Weatherson explores the options for how a proponent of the Equal Weight View might respond to this difficulty, and how this challenge fits into broader arguments against the Equal Weight View.">

<title>Online Articles - Brian Weatherson - Disagreements, Philosophical and Otherwise</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://use.typekit.net/uzz2drx.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Online Articles - Brian Weatherson</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://brian.weatherson.org"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://bsky.app/profile/bweatherson.bsky.social"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Disagreements, Philosophical and Otherwise</h1>
                  <div>
        <div class="description">
          <p>The Equal Weight View of disagreement says that if an agent sees that an epistemic peer disagrees with her about p, the agent should change her credence in p to half way between her initial credence, and the peer’s credence. But it is hard to believe the Equal Weight View for a surprising reason; not everyone believes it. And that means that if one did believe it, one would be required to lower one’s belief in it in light of this peer disagreement. Brian Weatherson explores the options for how a proponent of the Equal Weight View might respond to this difficulty, and how this challenge fits into broader arguments against the Equal Weight View.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">epistemology</div>
                <div class="quarto-category">disagreement</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="http://brian.weatherson.org">Brian Weatherson</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Michigan
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 10, 2013</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.1093/acprof:oso/9780199698370.003.0004">10.1093/acprof:oso/9780199698370.003.0004</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#peers-and-ew" id="toc-peers-and-ew" class="nav-link active" data-scroll-target="#peers-and-ew"><span class="header-section-number">0.1</span> Peers and EW</a></li>
  <li><a href="#circumstances-of-evaluation" id="toc-circumstances-of-evaluation" class="nav-link" data-scroll-target="#circumstances-of-evaluation"><span class="header-section-number">0.2</span> Circumstances of Evaluation</a></li>
  <li><a href="#a-story-about-disagreement" id="toc-a-story-about-disagreement" class="nav-link" data-scroll-target="#a-story-about-disagreement"><span class="header-section-number">0.3</span> A Story about Disagreement</a></li>
  <li><a href="#summing-up" id="toc-summing-up" class="nav-link" data-scroll-target="#summing-up"><span class="header-section-number">0.4</span> Summing Up</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>This paper started life as a short note I wrote around New Year 2007 while in Minneapolis. It was originally intended as a blog post. That might explain, if not altogether excuse, the flippant tone in places. But it got a little long for a post, so I made it into the format of a paper and posted it to my website. The paper has received a lot of attention, so it seems like it will be helpful to see it in print. Since a number of people have responded to the argument as stated, I’ve decided to just reprint the article warts and all, and make a few comments at the end about how I see its argument in the context of the subsequent debate.</p>
<aside>
Published in <em>The Epistemology of Disagreement: New Essays</em>, edited by David Christensen and Jennifer Lackey, OUP 54-75
</aside>
<div class="center">
<p><strong>Disagreeing about Disagreement (2007)</strong></p>
</div>
<p>I argue with my friends a lot. That is, I offer them reasons to believe all sorts of philosophical conclusions. Sadly, despite the quality of my arguments, and despite their apparent intelligence, they don’t always agree. They keep insisting on principles in the face of my wittier and wittier counterexamples, and they keep offering their own dull alleged counterexamples to my clever principles. What is a philosopher to do in these circumstances? (And I don’t mean get better friends.)</p>
<p>One popular answer these days is that I should, to some extent, defer to my friends. If I look at a batch of reasons and conclude <span class="math inline">\(p\)</span>, and my equally talented friend reaches an incompatible conclusion <span class="math inline">\(q\)</span>, I should revise my opinion so I’m now undecided between <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. I should, in the preferred lingo, assign equal weight to my view as to theirs. This is despite the fact that I’ve looked at their reasons for concluding <span class="math inline">\(q\)</span> and found them wanting. If I hadn’t, I would have already concluded <span class="math inline">\(q\)</span>. The mere fact that a friend (from now on I’ll leave off the qualifier ‘equally talented and informed’, since all my friends satisfy that) reaches a contrary opinion should be reason to move me. Such a position is defended by Richard Feldman <span class="citation" data-cites="Feldman2005-FELRTE Feldman2006-FELEPA">(<a href="#ref-Feldman2005-FELRTE" role="doc-biblioref">2005</a>, <a href="#ref-Feldman2006-FELEPA" role="doc-biblioref">2006</a>)</span>, David Christensen <span class="citation" data-cites="Christensen2007-CHREOD">(<a href="#ref-Christensen2007-CHREOD" role="doc-biblioref">2007</a>)</span> and Adam Elga <span class="citation" data-cites="Elga2007-ELGRAD">(<a href="#ref-Elga2007-ELGRAD" role="doc-biblioref">2007</a>)</span>.</p>
<p>This equal weight view, hereafter EW, is itself a philosophical position. And while some of my friends believe it, some of my friends do not. (Nor, I should add for your benefit, do I.) This raises an odd little dilemma. If EW is correct, then the fact that my friends disagree about it means that I shouldn’t be particularly confident that it is true, since EW says that I shouldn’t be too confident about any position on which my friends disagree. But, as I’ll argue below, to consistently implement EW, I have to be maximally confident that it is true. So to accept EW, I have to inconsistently both be very confident that it is true and not very confident that it is true. This seems like a problem, and a reason to not accept EW. We can state this argument formally as follows, using the notion of a peer and an expert. Some people are peers if they are equally philosophically talented and informed as each other, and one is more expert than another if they are more informed and talented than the other.</p>
<ol type="1">
<li><p>There are peers who disagree about EW, and there is no one who is an expert relative to them who endorses EW.</p></li>
<li><p>If 1 is true, then according to EW, my credence in EW should be less than 1.</p></li>
<li><p>If my credence in EW is less than 1, then the advice that EW offers in a wide range of cases is incoherent.</p></li>
<li><p>So, the advice EW offers in a wide range of cases is incoherent.</p></li>
</ol>
<p>The first three sections of this paper will be used to defend the first three premises. The final section will look at the philosophical consequences of the conclusion.</p>
<section id="peers-and-ew" class="level3 page-columns page-full" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="peers-and-ew"><span class="header-section-number">0.1</span> Peers and EW</h3>
<p>Thomas Kelly <span class="citation" data-cites="Kelly2005-KELTES">(<a href="#ref-Kelly2005-KELTES" role="doc-biblioref">2005</a>)</span> has argued against EW and in favour of the view that a peer with the irrational view should defer to a peer with the rational view. Elga helpfully dubs this the ‘right reasons’ view. Ralph Wedgwood <span class="citation" data-cites="Wedgwood2007-WEDNON">(<a href="#ref-Wedgwood2007-WEDNON" role="doc-biblioref">2007</a> Ch. 11)</span> has argued against EW and in favour of the view that one should have a modest ‘egocentric bias’, i.e.&nbsp;a bias towards one’s own beliefs. On the other hand, as mentioned above, Elga, Christensen and Feldman endorse versions of EW. So it certainly looks like there are very talented and informed philosophers on either side of this debate.</p>
<p>Now I suppose that if we were taking EW completely seriously, we would at this stage of the investigation look very closely at whether these five really are epistemic peers. We could pull out their grad school transcripts, look at the citation rates for their papers, get reference letters from expert colleagues, maybe bring one or two of them in for job-style interviews, and so on. But this all seems somewhat inappropriate for a scholarly journal. Not to mention a little tactless.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> So I’ll just stipulate that they seem to be peers in the sense relevant for EW, and address one worry a reader may have about my argument.</p>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Though if EW is correct, shouldn’t the scholarly journals be full of just this information?</p></li></div><p>An objector might say, “Sure it seems antecedently that Kelly and Wedgwood are the peers of the folks who endorse EW. But take a look at the arguments for EW that have been offered. They look like good arguments, don’t they? Doesn’t the fact that Kelly and Wedgwood don’t accept these arguments mean that, however talented they might be in general, they obviously have a blind spot when it comes to the epistemology of disagreement? If so, we shouldn’t treat them as experts on this question.” There is something right about this. People can be experts in one area, or even many areas, while their opinions are systematically wrong in another. But the objector’s line is unavailable to defenders of EW.</p>
<p>Indeed, these defenders have been quick to distance themselves from the objector. Here, for instance, is Elga’s formulation of the EW view, a formulation we’ll return to below.</p>
<blockquote class="blockquote">
<p>Your probability in a given disputed claim should equal your prior conditional probability in that claim. Prior to what? Prior to your thinking through the claim, and finding out what your advisor thinks of it. Conditional on what? On whatever you have learned about the circumstances of how you and your advisor have evaluated the claim. <span class="citation" data-cites="Elga2007-ELGRAD">(<a href="#ref-Elga2007-ELGRAD" role="doc-biblioref">Elga 2007, 490</a>)</span></p>
</blockquote>
<p>The fact that Kelly and Wedgwood come to different conclusions can’t be enough reason to declare that they are not peers. As Elga stresses, what matters is the prior judgment of their acuity. And Elga is right to stress this. If we declared anyone who doesn’t accept reasoning that we find compelling not a peer, then the EW view will be trivial. After all, the EW view only gets its force from cases as described in the introduction, where our friends reject reasoning we accept, and accept reasons we reject. If that makes them not a peer, the EW view never applies. So we can’t argue that anyone who rejects EW is thereby less of an expert in the relevant sense than someone who accepts it, merely in virtue of their rejection of EW. So it seems we should accept premise 1.</p>
</section>
<section id="circumstances-of-evaluation" class="level3" data-number="0.2">
<h3 data-number="0.2" class="anchored" data-anchor-id="circumstances-of-evaluation"><span class="header-section-number">0.2</span> Circumstances of Evaluation</h3>
<p>Elga worries about the following kind of case. Let <span class="math inline">\(p\)</span> be that the sum of a certain series of numbers, all of them integers, is 50. Let <span class="math inline">\(q\)</span> be that the sum of those numbers is <span class="math inline">\(400e\)</span>. My friend and I both add the numbers, and I conclude <span class="math inline">\(p\)</span> while he concludes <span class="math inline">\(q\)</span>. It seems that there is no reason to defer to my friend. I know, after all, that he has made some kind of mistake. The response, say defenders of EW, is that deference is context-sensitive. If I know, for example, that my friend is drunk, then I shouldn’t defer to him. More generally, as Elga puts it, how much I should defer should depend on what I know about the circumstances.</p>
<p>Now this is relevant because one of the relevant circumstances might be that my friend has come to a view that I regard as insane. That’s what happens in the case of the sums. Since my prior probability that my friend is right given that he has an insane seeming view is very low, my posterior probability that my friend is right should also, according to Elga, be low. Could we say that, although antecedently we regard Wedgwood and Kelly as peers of those they disagree with, that the circumstance of their disagreement is such that we should disregard their views?</p>
<p>It is hard to see how this would be defensible. It is true that a proponent of EW will regard Kelly and Wedgwood as wrong. But we can’t say that we should disregard the views of all those we regard as mistaken. That leads to trivialising EW, for reasons given above. The claim has to be that their views are so outrageous, that we wouldn’t defer to anyone with views that outrageous. And this seems highly implausible. But that’s the only reason that premise 2 could be false. So we should accept premise 2.</p>
</section>
<section id="a-story-about-disagreement" class="level3 page-columns page-full" data-number="0.3">
<h3 data-number="0.3" class="anchored" data-anchor-id="a-story-about-disagreement"><span class="header-section-number">0.3</span> A Story about Disagreement</h3>
<p>The tricky part of the argument is proving premise 3. To do this, I’ll use a story involving four friends, Apollo, Telemachus, Adam and Tom. The day before our story takes place, Adam has convinced Apollo that he should believe EW, and organise his life around it. Now Apollo and Telemachus are on their way to Fenway Park to watch the Red Sox play the Indians. There have been rumours flying around all day about whether the Red Sox injured star player, David Ortiz, will be healthy enough to play. Apollo and Telemachus have heard all the competing reports, and are comparing their credences that Ortiz will play. (Call the proposition that he will play <span class="math inline">\(p\)</span>.) Apollo’s credence in <span class="math inline">\(p\)</span> is 0.7, and Telemachus’s is 0.3. In fact, 0.7 is the rational credence in p given their shared evidence, and Apollo truly believes that it is.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> And, as it turns out, the Red Sox have decided but not announced that Ortiz will play, so <span class="math inline">\(p\)</span> is true.</p>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;This is obviously somewhat of an idealisation, since there won’t usually be a unique precise rational response to the evidence. But I don’t think this idealisation hurts the argument to follow. I should note that the evidence here <em>excludes</em> their statements of their credences, so I really mean the evidence that they brought to bear on the debate over whether <span class="math inline">\(p\)</span>.</p></li></div><p>Despite these facts, Apollo lowers his credence in <span class="math inline">\(p\)</span>. In accord with his newfound belief in EW, he changes his credence in <span class="math inline">\(p\)</span> to 0.5. Apollo is sure, after all, that when it comes to baseball Telemachus is an epistemic peer. At this point Tom arrives, and with a slight disregard for the important baseball game at hand, starts trying to convince them of the right reasons view on disagreement. Apollo is not convinced, but Telemachus thinks it sounds right. As he puts it, the view merely says that the rational person believes what the rational person believes. And who could disagree with that?</p>
<p>Apollo is not convinced, and starts telling them the virtues of EW. But a little way in, Tom cuts him off with a question. “How probable,” he asks Apollo, “does something have to be before you’ll assert it?”</p>
<p>Apollo says that it has to be fairly probable, though just what the threshold is depends on just what issues are at stake. But he agrees that it has to be fairly high, well above 0.5 at least.</p>
<p>“Well,” says Tom, “in that case you shouldn’t be defending EW in public. Because you think that Telemachus and I are the epistemic peers of you and Adam. And we think EW is false. So even by EW’s own lights, the probability you assign to EW should be 0.5. And that’s not a high enough probability to assert it.” Tom’s speech requires that Apollo regard he and Telemachus as Apollo’s epistemic peers with regard to this question. By premises 1 and 2, Apollo should do this, and we’ll assume that he does.</p>
<p>So Apollo agrees with all this, and agrees that he shouldn’t assert EW any more. But he still plans to use it, i.e.&nbsp;to have a credence in <span class="math inline">\(p\)</span> of 0.5 rather than 0.7. But now Telemachus and Tom press on him the following analogy.</p>
<p>Imagine that there were two competing experts, each of whom gave differing views about the probability of <span class="math inline">\(q\)</span>. One of the experts, call her Emma, said that the probability of <span class="math inline">\(q\)</span>, given the evidence, is 0.5. The other expert, call her Rae, said that the probability of <span class="math inline">\(q\)</span>, given the evidence, is 0.7. Assuming that Apollo has the same evidence as the experts, but he regards the experts as experts at evaluating evidence, what should his credence in <span class="math inline">\(q\)</span> be? It seems plausible that it should be a weighted average of what Emma says and what Rae says. In particular, it should be 0.5 only if Apollo is maximally confident that Emma is the expert to trust, and not at all confident that Rae is the expert to trust.</p>
<p>The situation is parallel to the one Apollo actually faces. EW says that his credence in <span class="math inline">\(p\)</span> should be 0.5. The right reason view says that his credence in <span class="math inline">\(p\)</span> should be 0.7. Apollo is aware of both of these facts. So his credence in <span class="math inline">\(p\)</span> should be 0.5 iff he is certain that EW is the theory to trust, just as his credence in <span class="math inline">\(q\)</span> should be 0.5 iff he is certain that Emma is the expert to trust. Indeed, a credence of 0.5 in <span class="math inline">\(p\)</span> is incoherent unless Apollo is certain EW is the theory to trust. But Apollo is not at all certain of this. His credence in EW, as is required by EW itself, is 0.5. So as long as Apollo keeps his credence in p at 0.5, he is being incoherent. But EW says to keep his credence in p at 0.5. So EW advises him to be incoherent. That is, EW offers incoherent advice. We can state this more carefully in an argument.</p>
<ol start="5" type="1">
<li><p>EW says that Apollo’s credence in p should be 0.5.</p></li>
<li><p>If 5, then EW offers incoherent advice unless it also says that Apollo’s credence in EW should be 1.</p></li>
<li><p>EW says that Apollo’s credence in EW should be 0.5.</p></li>
<li><p>So, EW offers incoherent advice.</p></li>
</ol>
<p>Since Apollo’s case is easily generalisable, we can infer that in a large number of cases, EW offers advice that is incoherent. Line 7 in this argument is hard to assail given premises 1 and 2 of the master argument. But I can imagine objections to each of the other lines.</p>
<p><em>Objection</em>: Line 6 is false. Apollo can coherently have one credence in p while being unsure about whether it is the rational credence to have. In particular, he can coherently have his credence in p be 0.5, while he is unsure whether his credence in p should be 0.5 or 0.7. In general there is no requirement for agents who are not omniscient to have their credences match their judgments of what their credences should be.</p>
<p><em>Replies</em>: I have two replies to this, the first dialectical and the second substantive.</p>
<p>The dialectical reply is that if the objector’s position on coherence is accepted, then a lot of the motivation for EW fades away. A core idea behind EW is that Apollo was unsure before the conversation started whether he or Telemachus would have the most rational reaction to the evidence, and hearing what each of them say does not provide him with more evidence. (See the ‘bootstrapping’ argument in <span class="citation" data-cites="Elga2007-ELGRAD">Elga (<a href="#ref-Elga2007-ELGRAD" role="doc-biblioref">2007</a>)</span> for a more formal statement of this idea.) So Apollo should have equal credence in the rationality of his judgment and of Telemachus’s judgment.</p>
<p>But if the objector is correct, Apollo can do that without changing his view on EW one bit. He can, indeed should, have his credence in <span class="math inline">\(p\)</span> be 0.7, while being uncertain whether his credence in p should be 0.7 (as he thinks) or 0.3 (as Telemachus thinks). Without some principle connecting what Apollo should think about what he should think to what Apollo should think, it is hard to see why this is not the uniquely rational reaction to Apollo’s circumstances. In other words, if this is an objection to my argument against EW, it is just as good an objection to a core argument for EW.</p>
<p>The substantive argument is that the objector’s position requires violating some very weak principles concerning rationality and higher-order beliefs. The objector is right that, for instance, in order to justifiably believe that <span class="math inline">\(p\)</span> (to degree <span class="math inline">\(d\)</span>), one need not know, or even believe, that one is justified in believing <span class="math inline">\(p\)</span> (to that degree). If nothing else, the anti-luminosity arguments in <span class="citation" data-cites="Williamson2000-WILKAI">Williamson (<a href="#ref-Williamson2000-WILKAI" role="doc-biblioref">2000</a>)</span> show that to be the case. But there are weaker principles that are more plausible, and which the objector’s position has us violate. In particular, there is the view that we can’t both be justified in believing that <span class="math inline">\(p\)</span> (to degree <span class="math inline">\(d\)</span>), while we know we are not justified in believing that we are justified in believing <span class="math inline">\(p\)</span> (to that degree). In symbols, if we let <span class="math inline">\(Jp\)</span> mean that the agent is justified in believing <span class="math inline">\(p\)</span>, and box and diamond to be epistemic modals, we have the principle <strong>MJ</strong> (for Might be Justified).</p>
<dl>
<dt>MJ</dt>
<dd>
<p><span class="math inline">\(Jp \rightarrow \diamond JJp\)</span></p>
</dd>
</dl>
<p>This seems like a much more plausible principle, since if we know we aren’t justified in believing we’re justified in believing <span class="math inline">\(p\)</span>, it seems like we should at least suspend judgment in <span class="math inline">\(p\)</span>. That is, we shouldn’t believe <span class="math inline">\(p\)</span>. That is, we aren’t justified in believing <span class="math inline">\(p\)</span>. But the objector’s position violates principle <strong>MJ</strong>, or at least a probabilistic version of it, as we’ll now show.</p>
<p>We aim to prove that the objector is committed to Apollo being justified in believing <span class="math inline">\(p\)</span> to degree 0.5, while he knows he is not justified in believing he is justified in believing <span class="math inline">\(p\)</span> to degree 0.5. The first part is trivial, it’s just a restatement of the objector’s view, so it is the second part that we must be concerned with.</p>
<p>Now, either EW is true, or it isn’t true. If it is true, then Apollo is not justified in having a greater credence in it than 0.5. But his only justification for believing p to degree 0.5 is EW. He’s only justified in believing he’s justified in believing <span class="math inline">\(p\)</span> if he can justify his use of EW in it. But you can’t justify a premise in which your rational credence is 0.5. So Apollo isn’t justified in believing he is justified in believing <span class="math inline">\(p\)</span>. If EW isn’t true, then Apollo isn’t even justified in believing <span class="math inline">\(p\)</span> to degree 0.5. And he knows this, since he knows EW is his only justification for lowering his credence in <span class="math inline">\(p\)</span> that far. So he certainly isn’t justified in believing he is justified in believing <span class="math inline">\(p\)</span> to degree 0.5 Moreover, every premise in this argument has been a premise that Apollo knows to obtain, and he is capable of following all the reasoning. So he knows that he isn’t justified in believing he is justified in believing <span class="math inline">\(p\)</span> to degree 0.5, as required.</p>
<p>The two replies I’ve offered to the objector complement one another. If someone accepts <strong>MJ</strong>, then they’ll regard the objector’s position as incoherent, since we’ve just shown that <strong>MJ</strong> is inconsistent with that position. If, on the other hand, someone rejects <strong>MJ</strong> and everything like it, then they have little reason to accept EW in the first place. They should just accept that Apollo’s credence in p should be, as per hypothesis the evidence suggests, 0.7. The fact that an epistemic peer disagrees, in the face of the same evidence, might give Apollo reason to doubt that this is in fact that uniquely rational response to the evidence. But, unless we accept a principle like <strong>MJ</strong>, that’s consistent with Apollo retaining the rational response to the evidence, namely a credence of 0.7 in p.&nbsp;So it is hard to see how someone could accept the objector’s argument, while also being motivated to accept EW. In any case, I think <strong>MJ</strong> is plausible enough on its own to undermine the objector’s position.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;Added in 2010: I still think there’s a dilemma here for EW, but I’m less convinced than I used to be that <strong>MJ</strong> is correct.</p></li></div><p><em>Objection</em>: Line 5 is false. Once we’ve seen that the credence of EW is 0.5, then Apollo’s credence in first-order claims such as p should, as the analogy with q suggests, be a weighted average of what EW says it should be, and what the right reason view says it should be. So, even by EW’s own lights, Apollo’s credence in p should be 0.6.</p>
<p><em>Replies</em>: Again I have a dialectical reply, and a substantive reply.</p>
<p>The dialectical reply is that once we make this move, we really have very little motivation to accept EW. There is, I’ll grant, some intuitive plausibility to the view that when faced with a disagreeing peer, we should think the right response is half way between our competing views. But there is no intuitive plausibility whatsoever to the view that in such a situation, we should naturally move to a position three-quarters of the way between the two competing views, as this objector suggests. Much of the argument for EW, especially in Christensen, turns on intuitions about cases, and the objector would have us give all of that up. Without those intuitions, however, EW falls in a heap.</p>
<p>The substantive reply is that the idea behind the objection can’t be coherently sustained. The idea is that we should first apply EW to philosophical questions to work out the probability of different theories of disagreement, and then apply those probabilities to first-order disagreements. The hope is that in doing so we’ll reach a stable point at which EW can be coherently applied. But there is no such stable point. Consider the following series of questions.</p>
<dl>
<dt>Q1</dt>
<dd>
<p>Is EW true?</p>
</dd>
</dl>
<p>Two participants say yes, two say no. We have a dispute, leading to our next question.</p>
<dl>
<dt>Q2</dt>
<dd>
<p>What is the right reaction to the disagreement over Q1?</p>
</dd>
</dl>
<p>EW answers this by saying our credence in EW should be 0.5. But that’s not what the right reason proponents say. They don’t believe EW, so they have no reason to move their credence in EW away from 0. So we have another dispute, and we can ask</p>
<dl>
<dt>Q3</dt>
<dd>
<p>What is the right reaction to the disagreement over Q2?</p>
</dd>
</dl>
<p>EW presumably says that we should again split the difference. Our credence in EW might now be 0.25, half-way between the 0.5 it was after considering Q2, and what the right reasons folks say. But, again, those who don’t buy EW will disagree, and won’t be moved to adjust their credence in EW. So again there’s a dispute, and again we can ask</p>
<dl>
<dt>Q4</dt>
<dd>
<p>What is the right reaction to the disagreement over Q3?</p>
</dd>
</dl>
<p>This could go on for a while. The only ‘stable point’ in the sequence is when we assign a credence of 0 to EW. That’s to say, the only way to coherently defend the idea behind the objection is to assign credence 0 to EW. But that’s to give up on EW. As with the previous objection, we can’t hold on to EW and object to the argument.</p>
</section>
<section id="summing-up" class="level3 page-columns page-full" data-number="0.4">
<h3 data-number="0.4" class="anchored" data-anchor-id="summing-up"><span class="header-section-number">0.4</span> Summing Up</h3>
<p>The story I’ve told here is a little idealised, but otherwise common enough. We often have disagreements both about first-order questions, and about how to resolve this disagreement. In these cases, there is no coherent way to assign equal weight to all prima facie rational views both about the first order question and the second order, epistemological, question. The only way to coherently apply EW to all first order questions is to put our foot down, and say that despite the apparent intelligence of our philosophical interlocutors, we’re not letting them dim our credence in EW. But if we are prepared to put our foot down here, why not about some first-order question or other? It certainly isn’t because we have more reason to believe an epistemological theory like EW than we have to believe first order theories about which there is substantive disagreement. So perhaps we should hold on to those theories, and let go of EW.</p>
<div class="center">
<p><strong>Afterthoughts (2010)</strong></p>
</div>
<p>I now think that the kind of argument I presented in the 2007 paper is not really an argument against EW as such, but an argument against one possible motivation for EW. I also think that alternate motivations for EW are no good, so I still think it is an important argument. But I think it’s role in the dialectic is a little more complicated than I appreciated back then.</p>
<p>Much of my thinking about disagreement problems revolves around the following table. The idea behind the table, and much of the related argument, is due to Thomas Kelly <span class="citation" data-cites="Kelly2010-KELPDA">(<a href="#ref-Kelly2010-KELPDA" role="doc-biblioref">2010</a>)</span>. In the table, <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> antecedently had good reasons to take themselves to be epistemic peers, and they know that their judgments about <span class="math inline">\(p\)</span> are both based on <span class="math inline">\(E\)</span>. In fact, <span class="math inline">\(E\)</span> is excellent evidence for <span class="math inline">\(p\)</span>, but only <span class="math inline">\(S\)</span> judges that <span class="math inline">\(p\)</span>; <span class="math inline">\(T\)</span> judges that <span class="math inline">\(\neg p\)</span>. Now let’s look at what seems to be the available evidence for and against <span class="math inline">\(p\)</span>.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Evidence for <span class="math inline">\(p\)</span></strong></td>
<td style="text-align: center;"><strong>Evidence against <span class="math inline">\(p\)</span></strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(S\)</span>’s judgment that <span class="math inline">\(p\)</span></td>
<td style="text-align: center;"><span class="math inline">\(T\)</span>’s judgment that <span class="math inline">\(\neg p\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(E\)</span></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>Now that doesn’t look to me like a table where the evidence is equally balanced for and against <span class="math inline">\(p\)</span>. Even granting that the judgments are evidence over and above <span class="math inline">\(E\)</span>, and granting that how much weight we should give to judgments should track our <em>ex ante</em> judgments of their reliability rather than our <em>ex post</em> judgments of their reliability, both of which strike me as false but necessary premises for EW, it <em>still</em> looks like there is more evidence for <span class="math inline">\(p\)</span> than against <span class="math inline">\(p\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> There is strictly more evidence for <span class="math inline">\(p\)</span> than against it, since <span class="math inline">\(E\)</span> exists. If we want to conclude that <span class="math inline">\(S\)</span> should regard <span class="math inline">\(p\)</span> and <span class="math inline">\(\neg p\)</span> as equally well supported for someone in her circumstance, we have to show that the table is somehow wrong. I know of three possible moves the EW defender could make here.</p>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;By <em>ex ante</em> and <em>ex post</em> I mean before and after we learn about <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span>’s use of <span class="math inline">\(E\)</span> to make a judgment about <span class="math inline">\(p\)</span>. I think that should change how reliable we take <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> to be, and that this should matter to what use, if any, we put their judgments, but it is crucial to EW that we ignore this evidence. Or, at least, it is crucial to EW that <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> ignore this evidence.</p></li><li id="fn5"><p><sup>5</sup>&nbsp;My explanation is that evidence screens any judgments made on the basis of that evidence, in the sense of screening to be described below.</p></li></div><p>David Christensen <span class="citation" data-cites="Christensen2010-CHRDQB">(<a href="#ref-Christensen2010-CHRDQB" role="doc-biblioref">2011</a>)</span>, as I read him, says that the table is wrong because when we are representing the evidence <span class="math inline">\(S\)</span> has, we should not include her own judgment. There’s something plausible to this. Pretend for a second that <span class="math inline">\(T\)</span> doesn’t exist, so it’s clearly rational for <span class="math inline">\(S\)</span> to judge that <span class="math inline">\(p\)</span>. It would still be wrong of <span class="math inline">\(S\)</span> to say, “Since <span class="math inline">\(E\)</span> is true, <span class="math inline">\(p\)</span>. And I judged that <span class="math inline">\(p\)</span>, so that’s another reason to believe that <span class="math inline">\(p\)</span>, because I’m smart.” By hypothesis, <span class="math inline">\(S\)</span> is smart, and that smart people judge things is reason to believe those things are true. But this doesn’t work when the judgment is one’s own. This is something that needs explaining in a full theory of the epistemic significance of judgment, but let’s just take it as a given for now.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Now the table, or at least the table as is relevant to <span class="math inline">\(S\)</span>, looks as follows.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Evidence for <span class="math inline">\(p\)</span></strong></td>
<td style="text-align: center;"><strong>Evidence against <span class="math inline">\(p\)</span></strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(E\)</span></td>
<td style="text-align: center;"><span class="math inline">\(T\)</span>’s judgment that <span class="math inline">\(\neg p\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>But I don’t think this does enough to support EW, or really anything like it. First, it won’t be true in general that the two sides of this table balance. In many cases, <span class="math inline">\(E\)</span> is strong evidence for <span class="math inline">\(p\)</span>, and <span class="math inline">\(T\)</span>’s judgment won’t be particularly strong evidence against <span class="math inline">\(p\)</span>. In fact, I’d say the kind of case where <span class="math inline">\(E\)</span> is much better evidence for <span class="math inline">\(p\)</span> than <span class="math inline">\(T\)</span>’s judgment is against <span class="math inline">\(p\)</span> is the statistically normal kind. Or, at least, it is the normal kind of case modulo the assumption that <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> have the same evidence. In cases where that isn’t true, learning that <span class="math inline">\(T\)</span> thinks <span class="math inline">\(\neg p\)</span> is good evidence that <span class="math inline">\(T\)</span> has evidence against <span class="math inline">\(p\)</span> that you don’t have, and you should adjust accordingly. But by hypothesis, <span class="math inline">\(S\)</span> knows that isn’t the case here. So I don’t see why this should push us even close to taking <span class="math inline">\(p\)</span> and <span class="math inline">\(\neg p\)</span> to be equally well supported.</p>
<p>The other difficulty for defending EW by this approach is that it seems to undermine the original motivations for the view. As Christensen notes, the table above is specifically for <span class="math inline">\(S\)</span>. Here’s what the table looks like for <span class="math inline">\(T\)</span>.</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Evidence for <span class="math inline">\(p\)</span></strong></td>
<td style="text-align: center;"><strong>Evidence against <span class="math inline">\(p\)</span></strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(S\)</span>’s judgment that <span class="math inline">\(p\)</span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(E\)</span></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>It’s no contest! So <span class="math inline">\(T\)</span> should firmly believe <span class="math inline">\(p\)</span>. But that isn’t the intuition anyone gets, as far as I can tell, in any of the cases motivating EW. And the big motivation for EW comes from intuitions about cases. Once we acknowledge that these intuitions are unreliable, as we’d have to do if we were defending EW this way, we seem to lack any reason to accept EW.</p>
<p>The second approach to blocking the table is to say that <span class="math inline">\(T\)</span>’s judgment is an undercutting defeater for the support <span class="math inline">\(E\)</span> provides for <span class="math inline">\(p\)</span>. This looks superficially promising. Having a smart person say that your evidence supports something other than you thought it did seems like it could be an undercutting defeater, since it is a reason to think the evidence supports something else, and hence doesn’t support what you thought it does. And, of course, if <span class="math inline">\(E\)</span> is undercut, then the table just has one line on it, and the two sides look equal.</p>
<p>But it doesn’t seem like it can work in general, for a reason that <span class="citation" data-cites="Kelly2010-KELPDA">Kelly (<a href="#ref-Kelly2010-KELPDA" role="doc-biblioref">2010</a>)</span> makes clear. We haven’t said what <span class="math inline">\(E\)</span> is so far. Let’s start with a case where <span class="math inline">\(E\)</span> consists of the judgments of a million other very smart people that <span class="math inline">\(p\)</span>. Then no one, not even the EW theorist, will think that <span class="math inline">\(T\)</span>’s judgment undercuts the support <span class="math inline">\(E\)</span> provides to <span class="math inline">\(p\)</span>. Indeed, even if <span class="math inline">\(E\)</span> just consists of one other person’s judgment, it won’t be undercut by <span class="math inline">\(T\)</span>’s judgment. The natural thought for an EW-friendly person to have in that case is that since there are two people who think <span class="math inline">\(p\)</span>, and one who thinks <span class="math inline">\(\neg p\)</span>, then <span class="math inline">\(S\)</span>’s credence in <span class="math inline">\(p\)</span> should be <span class="math inline">\(\frac{2}{3}\)</span>. But that’s impossible if <span class="math inline">\(E\)</span>, i.e., the third person’s judgment, is undercut by <span class="math inline">\(T\)</span>’s judgment. It’s true that <span class="math inline">\(T\)</span>’s judgment will partially <em>rebut</em> the judgments that <span class="math inline">\(S\)</span>, and the third party, make. It will move the probability of <span class="math inline">\(p\)</span>, at least according to EW, from 1 to <span class="math inline">\(\frac{2}{3}\)</span>. But that evidence won’t be in any way <em>undercut</em>.</p>
<p>And as Kelly points out, evidence is pretty fungible. Whatever support <span class="math inline">\(p\)</span> gets from other people’s judgments, it could get very similar support from something other than a judgment. We get roughly the same evidence for <span class="math inline">\(p\)</span> by learning that a smart person predicts <span class="math inline">\(p\)</span> as learning that a successful computer model predicts <span class="math inline">\(p\)</span>. So the following argument looks sound to me.</p>
<ol type="1">
<li><p>When <span class="math inline">\(E\)</span> consists of other people’s judgments, the support it provides to <span class="math inline">\(p\)</span> is not undercut by <span class="math inline">\(T\)</span>’s judgment.</p></li>
<li><p>If the evidence provided by other people’s judgments is not undercut by <span class="math inline">\(T\)</span>’s judgment, then some non-judgmental evidence is not undercut by <span class="math inline">\(T\)</span>’s judgment.</p></li>
<li><p>So, not all non-judgmental evidence is not undercut by <span class="math inline">\(T\)</span>’s judgment.</p></li>
</ol>
<p>So it isn’t true in general that the table is wrong because <span class="math inline">\(E\)</span> has been defeated by an undercutting defeater.</p>
<p>There’s another problem with the defeat model in cases where the initial judgments are not full beliefs. Change the case so <span class="math inline">\(E\)</span> provides basically no support to either <span class="math inline">\(p\)</span> or <span class="math inline">\(\neg p\)</span>. In fact, <span class="math inline">\(E\)</span> is just irrelevant to <span class="math inline">\(p\)</span>, and the agent’s have nothing to base either a firm or a probabilistic judgment about <span class="math inline">\(p\)</span> on. For this reason, <span class="math inline">\(S\)</span> declines to form a judgment, but <span class="math inline">\(T\)</span> forms a firm judgment that <span class="math inline">\(p\)</span>. Moreover, although both <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> are peers, that’s because they are both equally poor at making judgments about cases like <span class="math inline">\(p\)</span>. Here’s the table then:</p>
<div class="center">
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Evidence for <span class="math inline">\(p\)</span></strong></td>
<td style="text-align: center;"><strong>Evidence against <span class="math inline">\(p\)</span></strong></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(T\)</span>’s judgment that <span class="math inline">\(p\)</span></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>Since <span class="math inline">\(E\)</span> is irrelevant, it doesn’t appear, either before or after we think about defeaters. And since <span class="math inline">\(T\)</span> is not very competent, that’s not great evidence for <span class="math inline">\(p\)</span>. But EW says that <span class="math inline">\(S\)</span> should ‘split the difference’ between her initial agnositicism, and <span class="math inline">\(T\)</span>’s firm belief in <span class="math inline">\(p\)</span>. I don’t see how that could be justified by <span class="math inline">\(S\)</span>’s evidence.</p>
<p>So that move doesn’t work either, and we’re left with the third option for upsetting the table. This move is, I think, the most promising of the lot. It is to say that <span class="math inline">\(S\)</span>’s own judgment <em>screens off</em> the evidence that <span class="math inline">\(E\)</span> provides. So the table is misleading, because it ‘double counts’ evidence.</p>
<p>The idea of screening I’m using here, at least on behalf of EW, comes from Reichenbach’s <em>The Direction of Time</em>, and in particular from his work on deriving a principle that lets us infer events have a common cause. The notion was originally introduced in probabilistic terms. We say that <span class="math inline">\(C\)</span> screens off the positive correlation between <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> if the following two conditions are met.</p>
<ol type="1">
<li><p><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are positively correlated probabilistically, i.e. <span class="math inline">\(Pr(A | B) &gt; Pr(A)\)</span>.</p></li>
<li><p>Given <span class="math inline">\(C\)</span>, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are probabilistically independent,<br>
i.e.&nbsp;<span class="math inline">\(Pr(A | B \wedge C) = Pr(A | C)\)</span>.</p></li>
</ol>
<p>I’m interested in an evidential version of screening. If we have a probabilistic analysis of evidential support, the version of screening I’m going to offer here is identical to the Reichenbachian version just provided. But I want to stay neutral on whether we should think of evidence probabilistically.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> When I say that <span class="math inline">\(C\)</span> screens off the evidential support that <span class="math inline">\(B\)</span> provides to <span class="math inline">\(A\)</span>, I mean the following. (Both these clauses, as well as the statement that <span class="math inline">\(C\)</span> screens off <span class="math inline">\(B\)</span> from <span class="math inline">\(A\)</span>, are made relative to an evidential background. I’ll leave that as tacit in what follows.)</p>
<div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;In general I’m sceptical of always treating evidence probabilistically. Some of my reasons for scepticism are in <span class="citation" data-cites="Weatherson2007">Weatherson (<a href="#ref-Weatherson2007" role="doc-biblioref">2007</a>)</span>.</p></li></div><ol type="1">
<li><p><span class="math inline">\(B\)</span> is evidence that <span class="math inline">\(A\)</span>.</p></li>
<li><p><span class="math inline">\(B \wedge C\)</span> is no better evidence that <span class="math inline">\(A\)</span> than <span class="math inline">\(C\)</span> is.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;Branden Fitelson pointed out to me that the probabilistic version entails one extra condition, namely that <span class="math inline">\(\neg B \wedge C\)</span> is no worse evidence for <span class="math inline">\(A\)</span> than <span class="math inline">\(C\)</span> is. But I think that extra condition is irrelevant to disagreement debates, so I’m leaving it out.</p></li></div><p>Here is one stylised example of where screening helps conceptualise things. Detective Det is trying to figure out whether suspect Sus committed a certain crime. Let <span class="math inline">\(A\)</span> be that Sus is guilty, <span class="math inline">\(B\)</span> be that Sus’s was seen near the crime scene near the time the crime was committed, and <span class="math inline">\(C\)</span> be that Sus was at the crime scene when the crime was committed. Then both clauses are satisfied. <span class="math inline">\(B\)</span> is evidence for <span class="math inline">\(A\)</span>; that’s why we look for witnesses who place the suspect near the crime scene. But given the further evidence <span class="math inline">\(C\)</span>, then <span class="math inline">\(B\)</span> is neither here nor there with respect to <span class="math inline">\(A\)</span>. We’re only interested in finding if Sus was near the crime scene because we want to know whether he was at the crime scene. If we know that he was there, then learning he was seen near there doesn’t move the investigation along. So both clauses of the definition of screening are satisfied.</p>
<p>When there is screened evidence, there is the potential for double counting. It would be wrong to say that if we know <span class="math inline">\(B \wedge C\)</span> we have two pieces of evidence against Sus. Similarly, if a judgment screens off the evidence it is based on, then the table ‘double counts’ the evidence for <span class="math inline">\(p\)</span>. Removing the double counting, by removing <span class="math inline">\(E\)</span>, makes the table symmetrical. And that’s just what EW needs.</p>
<p>So the hypothesis that judgments screen the evidence they are based on, or JSE for short, can help EW respond to the argument from this table. But JSE is vulnerable to regress arguments. I now think that the argument in ‘Disagreeing about Disagreement’ is a version of the regress argument against JSE. So really it’s an argument against the most promising response to a particularly threatening argument against EW.</p>
<p>Unfortunately for EW, those regress arguments are actually quite good. To see ths, let’s say an agent makes a judgment on the basis of <span class="math inline">\(E\)</span>, and let <span class="math inline">\(J\)</span> be the proposition that that judgment was made. JSE says that <span class="math inline">\(E\)</span> is now screened off, and the agent’s evidence is just <span class="math inline">\(J\)</span>. But with that evidence, the agent presumably makes a new judgment. Let <span class="math inline">\(J^\prime\)</span> be the proposition that that judgment was made. We might ask now, does <span class="math inline">\(J^\prime\)</span> sit alongside <span class="math inline">\(J\)</span> as extra evidence, is it screened off by <span class="math inline">\(J\)</span>, or does it screen off <span class="math inline">\(J\)</span>? The picture behind JSE, the picture that says that judgments on the basis of some evidence screen that evidence, suggest that <span class="math inline">\(J^\prime\)</span> should in turn screen <span class="math inline">\(J\)</span>. But now it seems we have a regress on our hands. By the same token, <span class="math inline">\(J^{\prime \prime}\)</span>, the proposition concerning the new judgment made on the basis of <span class="math inline">\(J^\prime\)</span>, should screen off <span class="math inline">\(J^\prime\)</span>, and the proposition <span class="math inline">\(J^{\prime \prime \prime}\)</span> about the fourth judgment made, should screen off <span class="math inline">\(J^{\prime \prime}\)</span>, and so on. The poor agent has no unscreened evidence left! Something has gone horribly wrong.</p>
<p>I think this regress is ultimately fatal for JSE. But to see this, we need to work through the possible responses that a defender of JSE could make. There are really just two moves that seem viable. One is to say that the regress does not get going, because <span class="math inline">\(J\)</span> is better evidence than <span class="math inline">\(J^\prime\)</span>, and perhaps screens it. The other is to say that the regress is not vicious, because all these judgments should agree in their content. I’ll end the paper by addressing these two responses.</p>
<p>The first way to avoid the regress is to say that there is something special about the first level. So although <span class="math inline">\(J\)</span> screens <span class="math inline">\(E\)</span>, it isn’t the case that <span class="math inline">\(J^\prime\)</span> screens <span class="math inline">\(J\)</span>. That way, the regress doesn’t start. This kind of move is structurally like the move Adam Elga <span class="citation" data-cites="Elga2010-ELGHTD">(<a href="#ref-Elga2010-ELGHTD" role="doc-biblioref">2010</a>)</span> has recently suggested. He argues that we should adjust our views about first-order matters in (partial) deference to our peers, but we shouldn’t adjust our views about the right response to disagreement in this way.</p>
<p>It’s hard to see what could motivate such a position, either about disagreement or about screening. It’s true that we need some kind of stopping point to avoid these regresses. But the most natural stopping point is the very first level. Consider a toy example. It’s common knowledge that there are two apples and two oranges in the basket, and no other fruit. (And that no apple is an orange.) Two people disagree about how many pieces of fruit there are in the basket. <span class="math inline">\(A\)</span> thinks there are four, <span class="math inline">\(B\)</span> thinks there are five, and both of them are equally confident. Two other people, <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span>, disagree about what <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> should do in the face of this disagreement. All four people regard each other as peers. Let’s say <span class="math inline">\(C\)</span>’s position is the correct one (whatever that is) and <span class="math inline">\(D\)</span>’s position is incorrect. Elga’s position is that <span class="math inline">\(A\)</span> should partially defer to <span class="math inline">\(B\)</span>, but <span class="math inline">\(C\)</span> should not defer to <span class="math inline">\(D\)</span>. This is, intuitively, just back to front. <span class="math inline">\(A\)</span> has evidence that immediately and obviously entails the correctness of her position. <span class="math inline">\(C\)</span> is making a complicated judgment about a philosophical question where there are plausible and intricate arguments on each side. The position <span class="math inline">\(C\)</span> is in is much more like the kind of case where experience suggests a measure of modesty and deference can lead us away from foolish errors. If anyone should be sticking to their guns here, it is <span class="math inline">\(A\)</span>, not <span class="math inline">\(C\)</span>.</p>
<p>The same thing happens when it comes to screening. Let’s say that <span class="math inline">\(A\)</span> has some evidence that (a) she has made some mistakes on simple sums in the past, but (b) tends to massively over-estimate the likelihood that she’s made a mistake on any given puzzle. What should she do? One option, in my view the correct one, is that she should believe that there are four pieces of fruit in the basket, because that’s what the evidence obviously entails. Another option is that she should be not very confident there are four pieces of fruit in the basket, because she makes mistakes on these kinds of sums. Yet another option is that she should be pretty confident (if not completely certain) that there are four pieces of fruit in the basket, because if she were not very confident about this, this would just be a manifestation of her over-estimation of her tendency to err. The ‘solution’ to the regress we’re considering here says that the second of these three reactions is the uniquely rational reaction. The idea behind the solution is that we should respond to the evidence provided by first-order judgments, and correct that judgment for our known biases, but that we shouldn’t in turn correct for the flaws in our self-correcting routine. I don’t see what could motivate such a position. Either we just rationally respond to the evidence, and in this case just believe there are four pieces of fruit in the basket, or we keep correcting for errors we make in any judgment. It’s true that the latter plan leads either to regress or to the kind of ratificationism we’re about to critically examine. But that’s not because the disjunction is false, it’s because the first disjunct is true.</p>
<p>A more promising way to avoid the regress is suggested by some other work of Elga’s, in this case a paper he co-wrote with Andy Egan <span class="citation" data-cites="Egan2005-EGAICB">(<a href="#ref-Egan2005-EGAICB" role="doc-biblioref">Egan and Elga 2005</a>)</span>. Their idea, as I understand them, is that for any rational agent, any judgment they make must be such that when they add the fact that they made that judgment to their evidence (or, perhaps better given JSE, replace their evidence with the fact that they made that judgment), the rational judgment to make given the new evidence has the same content as the original judgment. So if you’re rational, and you come to believe that <span class="math inline">\(p\)</span> is likely true, then the rational thing to believe given you’ve made that judgment is that <span class="math inline">\(p\)</span> is likely true.</p>
<p>Note that this isn’t as strong a requirement as it may first seem. The requirement is not that any time an agent makes a judgment, rationality requires that they say on reflection that it is the correct judgments. Rather, the requirement is that the only judgments rational agents make are those judgments that, on reflection, she would reflectively endorse. We can think of this as a kind of ratifiability constraint on judgment, like the ratifiability constraint on decision making that Richard Jeffrey uses to handle Newcomb cases <span class="citation" data-cites="JeffreyLogicOfDecision">Jeffrey (<a href="#ref-JeffreyLogicOfDecision" role="doc-biblioref">1983</a>)</span>.</p>
<p>To be a little more precise, a judgment is ratifiable for agent <span class="math inline">\(S\)</span> just in case the rational judgment for <span class="math inline">\(S\)</span> to make conditional on her having made that judgment has the same content as the original judgment. The thought then is that we avoid the regress by saying rational agents always make ratifiable judgments. If the agent does do that, there isn’t much of a problem with the regress; once she gets to the first level, she has a stable view, even once she reflects on it.</p>
<p>It seems to me that this assumption, that only ratifiable judgments are rational, is what drives most of the arguments in Egan and Elga’s paper on self-confidence, so I don’t think this is a straw-man move. Indeed, as the comparison to Jeffrey suggests, it has some motivation behind it. Nevertheless it is false. I’ll first note one puzzling feature of the view, then one clearly false implication of the view.</p>
<p>The puzzling feature is that in some cases there may be nothing we can rationally do which is ratifiable. One way this can happen involves a slight modification of Egan and Elga’s example of the directionaly-challenged driver. Imagine that when I’m trying to decide whether <span class="math inline">\(p\)</span>, for any <span class="math inline">\(p\)</span> in a certain field, I know (a) that whatever judgment I make will usually be wrong, and (b) if I conclude my deliberations without making a judgment, then <span class="math inline">\(p\)</span> is usually true. If we also assume JSE, then it follows there is no way for me to end deliberation. If I make a judgment, I will have to retract it because of (a). But if I think of ending deliberation, then because of (b) I’ll have excellent evidence that <span class="math inline">\(p\)</span>, and it would be irrational to ignore this evidence. (Nicholas <span class="citation" data-cites="Silins2005">Silins (<a href="#ref-Silins2005" role="doc-biblioref">2005</a>)</span> has used the idea that failing to make a judgment can be irrational in a number of places, and those arguments motivated this example.)</p>
<p>This is puzzling, but not obviously false. It is plausible that there are some epistemic dilemmas, where any position an agent takes is going to be irrational. (By that, I mean it is at least as plausible that there are epistemic dilemmas as that there are moral dilemmas, and I think the plausibility of moral dilemmas is reasonably high.) That a case like the one I’ve described in the previous paragraph is a dilemma is perhaps odd, but no reason to reject the theory.</p>
<p>The real problem, I think, for the ratifiability proposal is that there are cases where unratifiable judgments are clearly preferable to ratifiable judgments. Assume that I’m a reasonably good judge of what’s likely to happen in baseball games, but I’m a little over-confident. And I know I’m over-confident. So the rational credence, given some evidence, is usually a little closer to <span class="math inline">\(\frac{1}{2}\)</span> than I admit. At risk of being arbitrarily precise, let’s say that if <span class="math inline">\(p\)</span> concerns a baseball game, and my credence in <span class="math inline">\(p\)</span> is <span class="math inline">\(x\)</span>, the rational credence in <span class="math inline">\(p\)</span>, call it <span class="math inline">\(y\)</span>, for someone with no other information than this is given by:</p>
<p><span class="math display">\[y = x + \frac{sin(2\pi x)}{50}\]</span></p>
<p>To give you a graphical sense of how that looks, the dark line in this graph is <span class="math inline">\(y\)</span>, and the lighter diagonal line is <span class="math inline">\(y = x\)</span>.</p>
<div class="center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sinewave.JPG" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
</div>
<p>Note that the two lines intersect at three points: <span class="math inline">\((0, 0), (\frac{1}{2}, \frac{1}{2})\)</span> and <span class="math inline">\((1, 1)\)</span>. So if my credence in <span class="math inline">\(p\)</span> is either 0, <span class="math inline">\(\frac{1}{2}\)</span> or 1, then my judgment is ratifiable. Otherwise, it is not. So the ratifiability constraint says that for any <span class="math inline">\(p\)</span> about a baseball game, my credence in <span class="math inline">\(p\)</span> should be either 0, <span class="math inline">\(\frac{1}{2}\)</span> or 1. But that’s crazy. It’s easy to imagine that I know (a) that in a particular game, the home team is much stronger than the away team, (b) that the stronger team usually, but far from always, wins baseball games, and (c) I’m systematically a little over-confident about my judgments about baseball games, in the way just described. In such a case, my credence that the home team will win should be high, but less than 1. That’s just what the ratificationist denies is possible.</p>
<p>This kind of case proves that it isn’t always rational to have ratifiable credences. It would take us too far afield to discuss this in detail, but it is interesting to think about the comparison between the kind of case I just discussed, and the objections to backwards induction reasoning in decision problems that have been made by Pettit and Sugden <span class="citation" data-cites="Pettit1989-PETTBI">(<a href="#ref-Pettit1989-PETTBI" role="doc-biblioref">1989</a>)</span>, and by Stalnaker <span class="citation" data-cites="Stalnaker1996 Stalnaker1998 Stalnaker1999">(<a href="#ref-Stalnaker1996" role="doc-biblioref">1996</a>, <a href="#ref-Stalnaker1998" role="doc-biblioref">1998</a>, <a href="#ref-Stalnaker1999" role="doc-biblioref">1999</a>)</span>. The backwards induction reasoning they criticise is, I think, a development of the idea that decisions should be ratifiable. And the clearest examples of when that reasoning fails concern cases where there is a unique ratifiable decision, and it is guaranteed to be one of the worst possible outcomes. The example I described in the last few paragraphs has, quite intentionally, a similar structure.</p>
<p>The upshot of all this is that I think these regress arguments work. They aren’t, I think, directly an argument against EW. What they are is an argument against the most promising way the EW theorist has for arguing that the table I started with misstates <span class="math inline">\(S\)</span>’s epistemic situation. Given that the regress argument against JSE works though, I don’t see any way of rescuing EW from this argument.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Christensen2007-CHREOD" class="csl-entry" role="listitem">
Christensen, David. 2007. <span>“Epistemology of Disagreement: The Good News.”</span> <em>Philosophical Review</em> 116 (2): 187–217. <a href="https://doi.org/10.1215/00318108-2006-035">https://doi.org/10.1215/00318108-2006-035</a>.
</div>
<div id="ref-Christensen2010-CHRDQB" class="csl-entry" role="listitem">
———. 2011. <span>“Disagreement, Question-Begging and Epistemic Self-Criticism.”</span> <em>Philosophers’ Imprint</em> 11 (6): 1–22. <a href="http://hdl.handle.net/2027/spo.3521354.0011.006">http://hdl.handle.net/2027/spo.3521354.0011.006</a>.
</div>
<div id="ref-Egan2005-EGAICB" class="csl-entry" role="listitem">
Egan, Andy, and Adam Elga. 2005. <span>“<span class="nocase">I Can’t Believe I’m Stupid</span>.”</span> <em>Philosophical Perspectives</em> 19 (1): 77–93. <a href="https://doi.org/10.1111/j.1520-8583.2005.00054.x">https://doi.org/10.1111/j.1520-8583.2005.00054.x</a>.
</div>
<div id="ref-Elga2007-ELGRAD" class="csl-entry" role="listitem">
Elga, Adam. 2007. <span>“Reflection and Disagreement.”</span> <em>No<span>û</span>s</em> 41 (3): 478–502. <a href="https://doi.org/10.1111/j.1468-0068.2007.00656.x">https://doi.org/10.1111/j.1468-0068.2007.00656.x</a>.
</div>
<div id="ref-Elga2010-ELGHTD" class="csl-entry" role="listitem">
———. 2010. <span>“How to Disagree about How to Disagree.”</span> In <em>Disagreement</em>, edited by Ted Warfield and Richard Feldman, 175–87. Oxford: Oxford University Press.
</div>
<div id="ref-Feldman2005-FELRTE" class="csl-entry" role="listitem">
Feldman, Richard. 2005. <span>“Respecting the Evidence.”</span> <em>Philosophical Perspectives</em> 19 (1): 95–119. <a href="https://doi.org/10.1111/j.1520-8583.2005.00055.x">https://doi.org/10.1111/j.1520-8583.2005.00055.x</a>.
</div>
<div id="ref-Feldman2006-FELEPA" class="csl-entry" role="listitem">
———. 2006. <span>“Epistemological Puzzles about Disagreement.”</span> In <em>Epistemology Futures</em>, edited by Stephen Cade Hetherington, 216–26. Oxford: Oxford University Press.
</div>
<div id="ref-JeffreyLogicOfDecision" class="csl-entry" role="listitem">
Jeffrey, Richard C. 1983. <em>The Logic of Decision</em>. 2nd ed. Chicago: University of Chicago Press.
</div>
<div id="ref-Kelly2005-KELTES" class="csl-entry" role="listitem">
Kelly, Thomas. 2005. <span>“The Epistemic Significance of Disagreement.”</span> <em>Oxford Studies in Epistemology</em> 1: 167–96.
</div>
<div id="ref-Kelly2010-KELPDA" class="csl-entry" role="listitem">
———. 2010. <span>“Peer Disagreement and Higher Order Evidence.”</span> In <em>Disagreement</em>, edited by Ted Warfield and Richard Feldman, 111–74. Oxford: Oxford University Press.
</div>
<div id="ref-Pettit1989-PETTBI" class="csl-entry" role="listitem">
Pettit, Philip, and Robert Sugden. 1989. <span>“The Backward Induction Paradox.”</span> <em>Journal of Philosophy</em> 86 (4): 169–82. <a href="https://doi.org/10.2307/2026960">https://doi.org/10.2307/2026960</a>.
</div>
<div id="ref-Silins2005" class="csl-entry" role="listitem">
Silins, Nicholas. 2005. <span>“Deception and Evidence.”</span> <em>Philosophical Perspectives</em> 19: 375–404. <a href="https://doi.org/10.1111/j.1520-8583.2005.00066.x">https://doi.org/10.1111/j.1520-8583.2005.00066.x</a>.
</div>
<div id="ref-Stalnaker1996" class="csl-entry" role="listitem">
Stalnaker, Robert. 1996. <span>“Knowledge, Belief and Counterfactual Reasoning in Games.”</span> <em>Economics and Philosophy</em> 12: 133–63. <a href="https://doi.org/10.1017/S0266267100004132">https://doi.org/10.1017/S0266267100004132</a>.
</div>
<div id="ref-Stalnaker1998" class="csl-entry" role="listitem">
———. 1998. <span>“Belief Revision in Games: Forward and Backward Induction.”</span> <em>Mathematical Social Sciences</em> 36 (1): 31–56. <a href="https://doi.org/10.1016/S0165-4896(98)00007-9">https://doi.org/10.1016/S0165-4896(98)00007-9</a>.
</div>
<div id="ref-Stalnaker1999" class="csl-entry" role="listitem">
———. 1999. <span>“Extensive and Strategic Forms: Games and Models for Games.”</span> <em>Research in Economics</em> 53 (3): 293–319. <a href="https://doi.org/10.1006/reec.1999.0200">https://doi.org/10.1006/reec.1999.0200</a>.
</div>
<div id="ref-Weatherson2007" class="csl-entry" role="listitem">
Weatherson, Brian. 2007. <span>“The Bayesian and the Dogmatist.”</span> <em>Proceedings of the Aristotelian Society</em> 107: 169–85. <a href="https://doi.org/10.1111/j.1467-9264.2007.00217.x">https://doi.org/10.1111/j.1467-9264.2007.00217.x</a>.
</div>
<div id="ref-Wedgwood2007-WEDNON" class="csl-entry" role="listitem">
Wedgwood, Ralph. 2007. <em>The Nature of Normativity</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Williamson2000-WILKAI" class="csl-entry" role="listitem">
Williamson, Timothy. 2000. <em><span class="nocase">Knowledge and its Limits</span></em>. Oxford University Press.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
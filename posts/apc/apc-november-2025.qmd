---
title: "When are Philosophy Articles Cited?"
abstract: |
  It's natural to believe that philosophy citations are typically to long ago pieces. We're still talking about philosophers from millenia ago. More strikingly, we're still talking about papers from half a century ago not as historical papers, but as part of the contemporary debate. But a systematic look at the citation data shows that these cases are outliers. Most citations are to recently published works. Surprisingly, this is less true in recent years than it used to be. The effect of electronic publishing and communication has been to make citations, on average, older. After we adjust for the typical age of philosophy citations, and this changing trend, it turns out that the 2000s were a particularly influential time in philosophy publishing. Articles published in that decade are cited more than earlier or later articles, once we adjust for the typical times articles are cited, and the changing patterns of citation. This is arguably related to broad changes in the interests of philosophers, towards social philosophy, and epistemology.
execute:
  echo: false
  warning: false
date: today
bibliography: 
 - /Users/weath/Documents/quarto-articles/brian-quarto.bib
 - /Users/weath/Documents/citations-2025/autobib.bib
number-sections: true
keep-tex: true
ergo:
  volume: "TBC"
  issue: "TBC"
  year: "2025"
  doi: "https://doi.org/TBC"
  startpage: 1
crossref:
  custom:
    - kind: float
      key: apptbl
      latex-env: apptbl
      reference-prefix: Table A
      space-before-numbering: false
author:
  - name: "Anon"
    affiliation: "Anon Institution"
    email: "anon@institution.edu"
categories:
  - history of analytic
  - old version
  - for submission
format:
  html:
    fig-format: png
    fig-width: 10
    fig-height: 7
  docx: default
  pdf: 
    pdf-engine: pdflatex
    documentclass: ergoclass
    geometry: "twoside=true, 
               headsep=.25in, 
               headheight=1in, 
               footskip=.35in, 
               paperwidth=7in,
               paperheight=10in, 
               top=1in, 
               bottom=1in, 
               inner=.8in, 
               outer=.8in"
    fontfamily: mathpazo  # This tells Quarto to use mathpazo instead of lmodern
    template-partials:
      - before-body.tex
      - title.tex
    include-in-header:
      file: preamble.tex
    include-after-body: 
        text: |
          \noindent Draft for submission
    cite-method: natbib
    biblio-style: "apalike"
    output-file: "Citation Ages in Philosophy.pdf"
---


```{r}
#| label: loader
#| cache: false

require(tidyverse)
require(slider)
require(stringr)
require(knitr)
require(lsa)
require(wesanderson)

load("/Users/weath/Documents/citations-2025/philo_bib_through_2024.RData")
load("/Users/weath/Documents/citations-2025/philo_cite_through_2024.RData")
load("/Users/weath/Documents/articles/apc/active_journal_list.RData")

if(knitr::is_latex_output()) {
  knitr::opts_chunk$set(dev = 'cairo_pdf')
}

# Graph Themes
old <- theme_set(theme_minimal())
theme_set(old)
theme_update(plot.title = element_text(family = "Scala Pro", size = 24, face = "bold"),
             plot.subtitle = element_text(family = "Scala Sans Pro", size = 30),
             axis.text = element_text(family = "Scala Sans Pro", size = 18),
             title = element_text(family = "Scala Sans Pro", size = 18),
             plot.background = element_rect(fill = "#F9FFFF"),
             panel.background = element_rect(fill = "white"),
             legend.background = element_rect(fill = "#F9FFFF"),
             panel.grid = element_line(color = "grey92"),
             legend.text = element_text(family = "Scala Sans Pro", size = 20),
             strip.text = element_text(family = "Scala Sans Pro", size = 20),
             legend.key.spacing.y = unit(0.5, 'lines'),
             legend.key.spacing.x = unit(1, 'cm')
  )

if(knitr::is_latex_output()) {
theme_update(axis.title = element_text(family = "Palatino", size = 11),
             plot.title = element_text(family = "Europa-Bold", size = 14),
             plot.subtitle = element_text(family = "Palatino", size = 11),
             axis.text = element_text(family = "Palatino", size = 10),
             plot.background = element_rect(fill = "white"),
             panel.background = element_rect(fill = "white"),
             legend.background = element_rect(fill = "white"),
             panel.grid = element_line(color = "grey92"),
             legend.text = element_text(family = "Palatino", size = 11),
             strip.text = element_text(family = "Palatino", size = 12),
             legend.key.spacing.y = unit(-0.3, 'lines'),
             legend.key.spacing.x = unit(0, 'cm')
  )
}
```

```{r}
#| label: buildgraphs
#| cache: true

start_year <- 1956
end_year <- 2024
min_data <- 5

# New attempt
# Two categories: available and typical
# Available means published before citing article
# Typical means published 3-10 years before citing article
# The 3 is because weird things have happened with recent cites in recent years

typical_low <- 3
typical_high <- 10

# This sets the color for one-color graphs

point_col <- wes_palette("AsteroidCity1")[3]

active_philo_bib <- philo_bib_through_2024 |>
  filter(year >= start_year, year <= end_year)

active_philo_cite <- philo_cite_through_2024 |>
  filter(
    id %in% active_philo_bib$id,
    refs %in% active_philo_bib$id
  )

citation_tibble <- active_philo_cite |>
  as_tibble() |>
  rename(new = id, old = refs) |>
  left_join(active_philo_bib |>
            select(id, year), 
            by = c("old" = "id")) |>
  rename(old_year = year)  |>
  left_join(active_philo_bib |>
            select(id, year), by = c("new" = "id")) |>
  rename(new_year = year) |>
  filter(old_year >= start_year,
         new_year <= end_year,
         old_year >= start_year,
         new_year <= end_year)

# Find the highly cited articles, and count their citations separately

high_threshold <- 15

highly_cited <- citation_tibble |>
  group_by(old) |>
  tally(name = "citations") |>
  filter(citations >= high_threshold) |>
  rename(id = old)

highly_cited_per_year <- active_philo_bib |>
  filter(id %in% highly_cited$id) |>
  group_by(year) |>
  tally(name = "high_articles") 

# Now a tibble of how many times articles in year x are cited in year y

year_in_year_out <- citation_tibble |>
  filter(old_year >= 1956) |>
  group_by(old_year, new_year) |>
  tally(name = "citations") |> # Now add the 'missing' pairs
  ungroup() |>
  complete(old_year, new_year, fill = list(citations = 0)) |>
  left_join(citation_tibble |>
              group_by(old) |>
              filter(n() >= high_threshold) |>
              group_by(old_year, new_year) |>
              tally(name = "high_citations") |> # Now add the 'missing' pairs
              ungroup() |>
              complete(old_year, new_year, fill = list(high_citations = 0)),
            by = c("old_year", "new_year")) |>
  replace_na(list(high_citations = 0)) |>
  mutate(low_citations = citations - high_citations)

# This works out how many citations there are each year to 3-10 year old articles

citations_in_typical_year <- year_in_year_out |>
  mutate(age = new_year - old_year) |>
  filter(age >= typical_low, age <= typical_high) |>
  group_by(new_year) |>
  summarise(typical_citations = sum(citations)) 

# I'm going to count the 'typical' articles as those published between 3 and 10 years before the citing year
# The 'available' articles are those published before the time

# Tibble for number of publications each year, and cumulative, or 'available'

articles_per_year <- active_philo_bib |>
  rename(old_year = year) |>
  group_by(old_year) |>
  tally(name = "articles") |>
  mutate(available = cumsum(articles)) |>
  mutate(typical_articles = slide_dbl(articles, sum, .before  = typical_high) - slide_dbl(articles, sum, .before = typical_low - 1)) |>
  filter(old_year >= 1956) |>
  left_join(highly_cited_per_year, by = c("old_year" = "year")) |>
  mutate(low_articles = articles - high_articles)

articles_per_year_plot <- articles_per_year |>
  ggplot(aes(x = old_year, y = articles)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Number of indexed articles")

typical_plot <- articles_per_year |>
  ggplot(aes(x = old_year, y = typical_articles)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Number of typical indexed articles")

# Same for citations

all_citations_per_year <- citation_tibble |>
  group_by(new_year) |>
  tally(name = "citations") 

all_citations_per_year_plot <- all_citations_per_year |>
  ggplot(aes(x = new_year, y = citations)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Citations to indexed articles")

typical_citations_per_year <- citation_tibble |>
  filter(new_year >= old_year + typical_low, new_year <= old_year + typical_high) |>
  group_by(new_year) |>
  tally(name = "citations") 

typical_citations_per_year_plot <- typical_citations_per_year |>
  ggplot(aes(x = new_year, y = citations)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Citations to indexed articles from typical years")


# Outbound citations

outbound_citations <- left_join(
  articles_per_year,
  all_citations_per_year,
  by = c("old_year" = "new_year")
) |>
  mutate(outbound_rate = citations/articles) |>
  mutate(outbound = round(outbound_rate, 2))

outbound_citations_plot <- outbound_citations |>
  filter(old_year != 1955) |>
  ggplot(aes(x = old_year, y = outbound)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Outbound citations per indexed articles")

# Citations per typical article

typical_citation_rate_per_year <- typical_citations_per_year |>
  left_join(articles_per_year, by = c("new_year" = "old_year")) |>
  #filter(new_year >= start_year + typical_high) |>
  left_join(citations_in_typical_year, by = "new_year") |>
  mutate(mean_cites = typical_citations/typical_articles)

typical_citation_rate_per_year_plot <- typical_citation_rate_per_year |>
  ggplot(aes(x = new_year, y = mean_cites)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Annual citation rate of typical articles.")

# All citations to typical articles in a year
ct_all <- citation_tibble |>
  filter(new_year >= old_year + typical_low, new_year <= old_year + typical_high) |>
  group_by(new_year) |>
  tally(name = "typical_citations")

age_effect_tibble <- year_in_year_out |>
  filter(old_year >= start_year, old_year <= end_year + 1 - min_data, new_year >= start_year + typical_high) |>
  filter(new_year >= old_year) |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      articles,
      high_articles,
      low_articles), 
    by = "old_year") |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      typical_articles), 
    by = c("new_year" = "old_year")) |>
  left_join(ct_all, by = "new_year") |> 
  mutate(age = new_year - old_year) |>
  mutate(cite_ratio = (citations/articles)/(typical_citations/typical_articles)) |>
  mutate(high_cite_ratio = (high_citations/high_articles)/(typical_citations/typical_articles))  |>
  mutate(low_cite_ratio = (low_citations/low_articles)/(typical_citations/typical_articles)) 

age_effect_tibble_plot <- age_effect_tibble |>
  filter(old_year >= start_year + 1, old_year <= end_year - min_data, new_year >= start_year) |>
  ggplot(aes(x = new_year, y = cite_ratio)) +
  geom_point(size = 0.25, color = point_col) +
  facet_wrap(~old_year, ncol = 6) +
  xlab(element_blank()) +
  ylab(element_blank()) +
  theme(axis.text = element_text(size = 10),
        strip.text = element_text(size = 12))

age_effect_grouped <- age_effect_tibble |>
  filter(new_year >= old_year) |>
  filter(new_year <= old_year + end_year - start_year + 1 - min_data) |>
  mutate(age = new_year - old_year) |>
  group_by(age) |>
  summarise(mean_effect = mean(cite_ratio),
            high_mean_effect = mean(high_cite_ratio),
            low_mean_effect = mean(low_cite_ratio))

age_effect_tibble_adj <- age_effect_tibble |>
  mutate(age = new_year - old_year) |>
  filter(age <= end_year - start_year - min_data) |>
  left_join(age_effect_grouped, by = "age")

age_effect_grouped_plot <- age_effect_grouped |>
  ggplot(aes(x = age, y = mean_effect)) +
  geom_point() +
  xlab("Article age") +
  ylab("Mean citation ratio")

year_by_year_with_effect <- year_in_year_out |>
  filter(new_year >= old_year) |>
  filter(new_year <= end_year) |>
  filter(old_year >= start_year, old_year <= end_year - min_data + 1, new_year >= start_year + typical_high) |>
  mutate(age = new_year - old_year) |>
  filter(age <= end_year - start_year - min_data) |>
  left_join(age_effect_grouped, by = "age") |>
  left_join(
    select(
      age_effect_tibble, old_year, new_year, cite_ratio
    ), by = c("old_year", "new_year")
  ) |>
  mutate(surplus = cite_ratio - mean_effect) |>
  arrange(-surplus)

# The next one calculates the difference between each year and the average. 
# But this has odd effects at the periphery, and compares each year to something it is part of.
# Below, in yiyo_extended, I try to work out what happens when each year is compared to the other years
# This is more work because you have to calculate the 'other years' value again each time

year_by_year_average <- year_by_year_with_effect |>
#  filter(age <= 7) |>
#  filter(old_year != 1973) |>
  group_by(old_year) |>
  summarise(mean_surplus = mean(surplus))

year_by_year_average_plot <- year_by_year_average |>
  mutate(rolling = slide_mean(mean_surplus, before = 4, after = 4)) |>
  ggplot(aes(x = old_year, y = mean_surplus)) +
  geom_line(
    aes(x = old_year, y = rolling),
    linewidth = 0.5,
    alpha = 0.5,
    color = point_col
  ) +
  geom_point(color = point_col)  +
  xlab(element_blank()) +
  scale_x_continuous(breaks = (98:100)*20) +
  ylab("Mean annual citations above average") +
  scale_y_continuous(labels = scales::percent)

year_by_year_average_plot_short <- year_by_year_with_effect |>
  filter(age <= 7) |>
  filter(old_year >= 1975) |>
  group_by(old_year) |>
  summarise(mean_surplus = mean(surplus)) |>
  mutate(rolling = slide_mean(mean_surplus, before = 4, after = 4)) |>
  ggplot(aes(x = old_year, y = mean_surplus)) +
  geom_point(color = point_col)  +
  geom_line(
    aes(x = old_year, y = rolling),
    linewidth = 0.5,
    alpha = 0.5,
    color = point_col
  ) +
  xlab(element_blank()) +
  scale_x_continuous(breaks = (98:100)*20) +
  ylab("Mean annual citations above average") +
  scale_y_continuous(labels = scales::percent)

year_by_year_average_plot_long<- year_by_year_with_effect |>
  filter(age > 7) |>
  filter(old_year >= 1975) |>
  group_by(old_year) |>
  summarise(mean_surplus = mean(surplus)) |>
  mutate(rolling = slide_mean(mean_surplus, before = 4, after = 4)) |>
  ggplot(aes(x = old_year, y = mean_surplus)) +
  geom_point(color = point_col)  +
  geom_line(
    aes(x = old_year, y = rolling),
    linewidth = 0.5,
    alpha = 0.5,
    color = point_col
  ) +
  xlab(element_blank()) +
  scale_x_continuous(breaks = (98:100)*20) +
  ylab("Mean annual citations above average") +
  scale_y_continuous(labels = scales::percent)

#print(year_by_year_average_plot)

effect_by_age_average <- function(early, late){
  age_effect_tibble |>
    filter(age >= early, age <= late) |>
    #    add_count(old_year, name = "data_points") |>
    #    filter(data_points >= min_data) |>
    group_by(old_year) |>
    summarise(mean_ratio = mean(cite_ratio)) |>
    ggplot(aes(x = old_year, y = mean_ratio)) +
    geom_point() +
    geom_smooth() +
    xlab(element_blank()) +
    ylab(element_blank()) +
    labs(title = case_when(
      early == late ~ paste0("Citation ratio at age ", early),
      TRUE ~ paste0("Mean citation ratio from ages ",early," to ",late)))
}

effect_by_age_facet <- function(early, late){age_effect_tibble |>
    filter(age>= early, age <= late) |>
    ggplot(aes(x = old_year, y = cite_ratio)) +
    geom_point() + geom_smooth() +
    facet_wrap(~age, ncol = 4)
}

year_to_mean_plot <- function(the_year){
  age_effect_tibble_adj |>
    filter(old_year == the_year) |>
    ggplot(aes(x = age, y = cite_ratio)) +
    geom_point(size = 2, alpha = 1, color = hcl(h = (the_year-1975)*(360/43)+15, l = 65, c = 100)) +
    # geom_jitter(aes(size=(old_year==2008 | old_year == 1985), shape = (old_year==2008)), alpha = 1) +
    #  geom_jitter(aes(size=(old_year %in% c(1978, 1980, 1985, 1987)), alpha = 1)) +
    # scale_size_manual(values=c(0.3,2)) +
    xlab("Age of cited articles") +
    ylab("Citation ratio") +
    geom_line(aes(x = age, y = mean_effect), color = point_col) +
    geom_point(aes(x = age, y = mean_effect), color = point_col, size = 0.4) +
    theme(legend.position = "none")
}

```

```{r}
#| label: calculate-variables

citations_1956 <- scales::label_comma()(
  filter(all_citations_per_year, new_year == 1956)$citations)
citations_2024 <- scales::label_comma()(
  filter(all_citations_per_year, new_year == 2024)$citations)

number_of_articles <- scales::label_comma()(nrow(active_philo_bib))
number_of_citations <- scales::label_comma()(nrow(active_philo_cite))

synthese_2021 <- scales::label_comma()(
  nrow(
    filter(
      active_philo_bib,
      year == 2021,
      journal == "Synthese"
    )
  )
)
synthese_2022 <- scales::label_comma()(
  nrow(
    filter(
      active_philo_bib,
      year == 2022,
      journal == "Synthese"
    )
  )
)
```

# Introduction {#sec-introduction}

This paper is about the patterns of citations of philosophy journal articles in philosophy journals. Obviously philosophy journals cite more things than philosophy journals, and just as obviously philosophy journal articles get cited in other places. But looking just at journal-to-journal citations allows us to get a citation set that is relatively complete, and hence make some systematic generalisations about the way articles are cited over time. It turns out some of these generalisations are surprising.

Before looking at the data, here are two things I believed about philosophy citations. First, philosophers tend to cite very old papers. We still regularly teach a number of papers over half a century old in introductory classes; e.g., @WOSA1969Y444700002, @WOSA1971Y116900003, @WOSA1972Z066400001, and @10.2307_2025310. These aren't taught as history papers, but as early entries into the contemporary philosophical debate. While most papers aren't cited as much as these papers are, I thought the pattern that old papers keep being cited extended to their less famous counterparts. Second, the technological changes of the last quarter century meant that this practice was being slowly reversed. A series of technological innovations made it easier to cite newer and newer works. These innovations included the spread of email, the rise of preprint archives (e.g., arXiv, SSRN, PhilPapers), and eventually official preprints in things like EarlyView. So, I thought, citations should be getting younger, because the delay between publishing and getting widely known was removed.

Both of these thoughts were wrong.

On the first point, the generalisation I made from those famous papers was just wrong. Normal papers differ from famous papers not just in how often they are cited, but in the shape of their citations. The main evidence I'll use for this is something I'll call the _citation ratio_. This measures how often articles in year _o_ (for old) are cited in year _n_, after adjusting for how many citations in general there are in year _n_. (I'll explain this much more fully in @sec-age.) @fig-master-citation-ratio shows the average citation ratio for different _ages_, of citations, i.e., the number of years between _o_ and _n_.^[The graph also includes some 'jitter' to make the different points more easily visible. I've put each decade of original publication in a different colour; I'll break those out in @fig-decades-cite-ratio. The graph starts in 1975 because the data is much noisier before then, for reasons we'll get to below.]

```{r}
#| label: fig-master-citation-ratio
#| fig-cap: "Age effects from 1975 onwards on a single graph, with the overall average shown."

age_effect_post_1975 <- year_in_year_out |>
  filter(old_year >= 1975,
         new_year >= old_year) |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      articles,
      high_articles,
      low_articles), 
    by = "old_year") |>
  left_join(
    select(
      articles_per_year, 
      old_year, 
      typical_articles), 
    by = c("new_year" = "old_year")) |>
  left_join(ct_all, by = "new_year") |> 
  mutate(
    age = new_year - old_year,
    cite_ratio = (citations/articles)/(typical_citations/typical_articles),
    high_cite_ratio = (high_citations/high_articles)/
      (typical_citations/typical_articles),
    low_cite_ratio = (low_citations/low_articles)/
      (typical_citations/typical_articles)) |>
  ungroup() |>
  group_by(new_year - old_year) |>
  mutate(
    mean_cite_ratio = mean(cite_ratio),
    mean_high_cite_ratio = mean(high_cite_ratio),
    mean_low_cite_ratio = mean(low_cite_ratio)
  ) |>
  ungroup() |>
  mutate(decade = paste0(
    floor((old_year-5)/10),
    "5-",
    floor((old_year-5)/10)+1,
    "4"
  ))

ggplot(age_effect_post_1975, aes(x = new_year - old_year, 
                                 y = cite_ratio,
                                 color = decade)) +
  geom_jitter(size = 0.5, alpha = 0.7) +
  xlab("Age of cited articles") +
  ylab("Citation ratio") +
  geom_line(aes(x = age, y = mean_cite_ratio), color = point_col) +
  labs(color = element_blank()) +
  theme(legend.position = "none")
```

Each dot on that graph is a citation ratio for a particular pair of years; the line shows the average citation ratio for all pairs with the same age. The shape is unmistakable; articles get cited much much more when they are relatively young than when they are older.

The 'evidence' I gave for the opposite view in the introductory paragraph wasn't entirely wrong. If we redo @fig-master-citation-ratio just looking at articles which have 15 or more citations in philosophy journals, we get @fig-ageeffecteverything-high. (Restricting to these articles means we look at a small percentage of a articles, but a decent percentage of the citations.)

```{r}
#| label: fig-ageeffecteverything-high
#| fig-cap: "A version of @fig-master-citation-ratio just looking at highly cited articles"

ggplot(age_effect_post_1975, aes(x = new_year - old_year, 
                                 y = high_cite_ratio,
                                 color = decade)) +
  geom_jitter(size = 0.5, alpha = 0.7) +
  xlab("Age of cited articles") +
  ylab("Citation ratio") +
  geom_line(aes(x = age, y = mean_high_cite_ratio), color = point_col) +
  labs(color = element_blank()) +
  theme(legend.position = "none")
```

The numbers on the y-axis in @fig-ageeffecteverything-high are higher than in @fig-master-citation-ratio. That's not surprising; it just means highly cited articles get cited more frequently. What is striking is the different shape of the graphs. Typical philosophy articles, if they get cited at all, get cited soon after publication and they fade into obscurity. Highly cited articles keep getting cited decades after their publication. 

These results aren't a priori obvious; things could have turned out otherwise. It could have been that there were a trove of articles which were ignored after publication and then accrued five to ten citations a couple of decades later. There are some articles that were very frequently cited soon after publication but which are now largely ignored. (This happens most frequently in philosophy of science and in philosophy of mind, I think for different reasons in the two cases.) These cases are outliers, but they could have been typical. Instead, most of the articles that were influential soon after publication stay that way.

For the second point, we can simply break up @fig-master-citation-ratio by ten year chunks. In @fig-decades-cite-ratio I've taken the points by from @fig-master-citation-ratio, and grouped them into 'decades'. Because I'm working here with 1975-2024 data, the decades are 1975-1984, 1985-1994 etc. To make it easier to compare decades, I've removed the last one, where there isn't enough data, and removed all points with an age over 20.

```{r}
#| label: fig-decades-cite-ratio
#| fig-cap: "Citation ratio for different decades"
#| fig-subcap:
#|    - "1975-1984"
#|    - "1985-1994"
#|    - "1995-2004"
#|    - "2005-2014"
#| layout-ncol: 2

ten_year_graph <- function(x){
  temp <- age_effect_post_1975 |>
    filter(old_year >= x, old_year <= x + 9, 
           new_year >= old_year, new_year <= old_year + 20) |>
    group_by(new_year - old_year) |>
    mutate(mean_effect = mean(cite_ratio))
  
  temp |>
    ggplot(aes(x = age, y = cite_ratio, color = as.factor(old_year))) +
    geom_jitter(size = 0.5, alpha = 0.7) +
    xlab("Age of cited articles") +
    ylab("Citation ratio") +
    geom_line(aes(x = age, y = mean_effect), color = point_col) +
    geom_point(aes(x = age, y = mean_effect), color = point_col, size = 0.4) +
    theme(legend.position = "none") +
    ylim(0, 1.6)
}

ten_year_graph(1975)
ten_year_graph(1985)
ten_year_graph(1995)
ten_year_graph(2005)
```

There are three general trends across the graphs in @fig-decades-cite-ratio, especially after the second graph.

1. The peaks are getting later. In the first two graphs, the line is clearly heading down by age 5; in the last one it is barely off the peak at that time.
2. The peaks are getting lower. In the last graph we barely see it cross 1.
3. The declines are much, much flatter. If you look around age 15 in the four graphs, you see the values rise steadily over time.

What all this means is that citations are getting older. While it's still true that articles published in a given year are (collectively) cited more often from ages 2-5 than from ages 12-15, the difference between those two rates has fallen remarkably. The effect of technology on citations has been the complete opposite of what I expected.

The rest of this paper has two aims. 

First, I'm going to set out the methodology behind these graphs, go over the choices I made in building them, and argue that these were at least defensible choices. The intended conclusion is that these graphs really show what I say they do, that traditionally citations were mostly to very recent articles, but they are now more frequently to older articles. 

Second, I'm going to look citations from various years, after adjusting for these typical citation rates, and see which years have been more influential in the later literature. I suspect readers will not be surprised that the early 1970s stand out as being particularly influential. What might be more surprising is that the next most influential period, in terms of how often articles from then are cited compared to the overall trends, is the 2000s. There are a few possible reasons for this, but I suspect the main one is the rising importance at that time of epistemology. (This is something Eugenio @Petrovich2024 also found using a somewhat different data set.) More generally, looking at citations from different periods, and especially looking at which articles make up those citations, is a useful guide to the history of those periods. Most work on the history of analytic philosophy doesn't get beyond the early 1970s; this is an early attempt to quantify what happens in the years after the changes brought about by Kripke, Lewis, Rawls and others in those years.

# Age of Citations {#sec-age-of-citations}

## Methodology {#sec-methodology}

The data for this study comes from Web of Science (hereafter, WoS). In this section I'll go over which data I chose to use, and how I patched it together.

The bulk of the data comes from the XML files that WoS makes available to subscribing institutions. Until recently, that included my own, so that's where most of the data through 2021 comes from. That subscription has not been renewed, so the data after 2021 comes from the WoS website.^[This is also via a susbcription through my institution; the XML is more expensive.]

The XML file is rather large. After de-compression it's over a terabyte. To make it manageable, I filtered down to _articles_ (as opposed to discussion notes, book reviews, editorial matters, and so on), whose category was either Philosophy or History & Philosophy of Science. I then selected by hand the hundred journals with the most inbound citations (among articles in these categories) which were (a) primarily English language, (b) not primarily history of science and (c) broadly 'analytic' rather than 'continental'. These were somewhat subjective choices, but the result was a reasonable collection of the journals which are most important for telling the story of a certain kind of philosophy over the last several decades.

The list of journals being used, as well as some basic statistical information about them, is in @sec-statistics. Once I had those journals, I included all articles (and notes/reviews over 15 pages) from them. I did not restrict the study to pieces that were labelled as Philosophy or History & Philosophy of Science. For interdisciplinary journals, especially _Mind and Language_, those labels seemed very unreliable on a paper-by-paper basis, and I preferred to have a full picture of each journal I was using.

The data from the XML was supplemented in two ways. First, WoS does not index _The Journal of Philosophy_ between 1971 and 1974. It is missing a few other journals in 1974 in particular, but this gap was the longest and most important, and I thought I needed to fix it. Between 1971 and 1974 the Journal published groundbreaking articles by Harry @Frankfurt1971, George @Boolos1971, Paul @Benacerraf1973, Jaegwon @Kim1973, Michael @Friedman1974, Isaac @Levi1974, and David Lewis [-@Lewis1971cen; -@Lewis1973ben]. Leaving all of those papers out seemed like it undermined the story. So I used JSTOR to find a full list of articles (as opposed to notes or book reviews) in _Journal of Philosophy_ in those years, and then looked through the citations in articles in @tbl-list-of-journals to see which citations were to one of those articles. This did mean I was using a different classification of publications into articles and non-articles, and there are some odd choices.^[Notably, the JSTOR list seemed to exclude the symposium centered around Kenneth Arrow's "Some Ordinalist-Utilitarian Notes on Rawlsâ€™s Theory of Justice"; I'm not sure why that was.] And it meant I had to do a fair bit of data cleaning just to track down references to those four years.^[A non-trivial chunk of the cleaning was sorting through the many and varied ways that philosophers have spelled Brian O'Shaughnessy's name over the years.] While I've strived to make the data as consistent as possible with the other years, it's possible that I haven't succeeded, and some discontinuities around the early 1970s are due to this discontinuity in how the data was acquired.

The tables in @sec-introduction start in 1975 in part because I'm concerned about the consistency of the data that had to be complied in two different ways, but largely because WoS only starts indexing _Analysis_ in 1975. Without _Analysis_, and especially without the papers on the analysis of knowledge and on inferentialism, you don't get a particularly complete picture of how citations work in those years. I've included 1956-1974 in some of the studies below, but the data presented there is much less complete, and hence they aren't as useful for figuring out larger trends.

The other way I supplemented the XML relates to the fact that the XML I have only goes through mid-2022. Using the WoS website, I downloaded all the articles in, and citations in, articles in these 100 journals from 2021-2024. I processed these using the bibliometrix package (@bibliometrix). I used the 2021 data to check that this method yielded roughly the same results as the XML. The differences were not great - well under 1% for the number of articles, and a little over 1% for the number of citations. So it's not a perfect match, but it's fairly close, and I've used the 2022-2024 dats from the WoS website via bibliometrix.

## Journal to Journal {#sec-journal-to-journal}

As noted earlier, this study is restricted to a particular kind of citation: when a philosophy journal article is cited in another philosophy journal article. That obviously leaves out a lot. The restriction to journal articles means we exclude edited volumes, theses, conference programs, and, above all, books. The restriction to philosophy means that we exclude citations in journals in adjacent fields.

The reason for these restrictions is threefold. 

First, the journal-to-journal data is so much cleaner than any other data. When WoS records a citation of an article it indexes by an article it indexes, the citation record includes the WoS ID number for the cited journal article. That means that we don't have to clean up cases where the citing author got any details of the cited article wrong. It's very common for authors to cite incorrect page numbers for an article. It's less common, but still sufficiently frequent that one has to check it, for authors to cite incorrect titles, author names (especially for hard to spell names) or even publication years. Cleaning this is a lot of work. In practice, restricting attention to cases where WoS includes an ID number for the cited article does not avoid this problem as much as delegate it to WoS. Otherwise, doing this is a huge amount of work. A similar study to this one was earlier done by Eugenio @Petrovich2024; he looked at all citations in five leading philosophy journals. It wasn't practical for him to look at more than five because of how much work it was to clean all those citations. I'm losing some comprehensiveness compared to his study, but covering twenty times more journals. This isn't to say that one of the ways of doing things is right and the other wrong; rather that by looking at slightly different things, the two studies should complement each other.

Second, looking at journals allows for a kind of comprehensiveness. To find out how often the average philosophy book from a particular year was cited, we'd need a database of all the books. Maybe that's possible via the Library of Congress, but it would be a challenge. To find out how often the average paper in an edited volume was published, we'd need a database of all the chapters in edited volumes. I don't know where one would start looking for such a thing. Journals have the advantage that they number their issues; you can typically confirm that you have everything.

Third, dealing with whole journals makes the challenge of demarcating philosophy from non-philosophy a little more manageable. At the very least, I can show you what I mean by a philosophy journal; I mean the journals listed in @tbl-list-of-journals. If I had to go through book-by-book, or chapter-by-chapter, making decisions on which were inside philosophy, it would be a massive task, and it would be nearly as massive a task for anyone to double check. The key thing here is that I'm not attempting to quantify philosophy articles published in journals, but articles published in philosophy journals. The demarcation problem is still non-trivial; for instance, should I have included _Cognition_ in this study? Invariably some arbitrary boundaries will be drawn. The upside of the way I'm doing things is that it involves fewer such boundaries, and they are more visible to you the reader.

In practice there are two major downsides to restricting attention to philosophy-journal-to-philosophy-journal citations.

With respect to inbound citations, a big difference is that different kinds of books and journal articles are cited, and these can give you a very different impression of the field. History of philosophy does not involve as much publishing in journals, and the articles that are published cite primary sources, and more recent books, more than other journal articles. This kind of work offers essentially zero insight into developments in the history of philosophy. Also, and this will become important later, often citations to books are to much older works than citations to articles. @Petrovich2024 notes that through the 1990s, Quine, Wittgenstein and Davidson are amongst the most cited authors. None of them show up as near the top if you just look at cited journal articles.

Davidson, in particular, raises another issue about citations to journal articles. A citation is only recorded as being to a journal article if the journal is identified in some way, ideally by name though a DOI reference would also work, in the citing article. In older works, citations to famous articles often just mention one or other collection in which they were reprinted. If someone cites "Actions, Reasons, and Causes", but the only bibliographic detail they give is that it's chapter one in _Essays on Actions and Events_, it won't necessarily show up as a journal-to-journal citation in WoS. Most articles aren't reprinted, and these days people cite originals as well as, or instead of, reprints. So overall this isn't a huge effect. But if one was trying to find the most cited articles, it's a huge source of error.^[I had been planning a study on which articles had the largest declines in citations, as a way of measuring changes in philosophical fashion. But most of the articles I found with large falls in citation rates had been reprinted so often that this effect explained most of what I found. It isn't a big effect overall, but if you go looking for outliers, you'll mostly find cases where the data is unreliable.]

With respect to outbound citations, the study I'm doing doesn't show how often journals are cited outside philosophy. It doesn't show how often they are cited in books either, but that's less of a problem, I believe, because citations in books and citations in journals have similar patterns. But citations inside philosophy are a very poor guide to citations outside philosophy. If you look at @tbl-list-of-journals, you'll see that the articles in _Journal of Medical Ethics_ are, collectively, cited very rarely. This is almost entirely a consequence of my excluding medical journals where that journal is cited more often. The data in @tbl-list-of-journals tells you something. If you wanted confirmation that 'core' philosophy journals don't publish much bioethics, the citation numbers for _Journal of Medical Ethics_ are evidence for that. But they are not evidence for anything about the overall impact of the journal; we just aren't looking in the right place to see that.

## Age, Period, and Cohort {#sec-apc}

To help understand the citation patterns, I'll borrow some terminology that's common in both sociology and medicine. It's easiest to introduce this terminology with an example. Imagine that we see, in the historical record, some interesting patterns among teenagers in the late 1960s, and we're wondering what could explain the pattern. Two types of pattern spring immediately to mind, along with ways to test them.

First, the behaviour could be explained by the fact the people involved are teenagers. If so, it is an **age effect**. The natural way to test this is to see if similar patterns show up with teenagers at different times.

Second, the behaviour could be explained by the fact that it was the 1960s, and lots of striking things happened in the 1960s. If so, it is a **period effect**. The natural way to test this is to see if the same pattern shows up with non-teenagers in the 1960s.

There is an important third kind of explanation. The people involved are born in the early 1950s, so they are part of the post-war baby boom. Colloquially, they are boomers. Maybe that could explain the pattern we see. If so, it is a **cohort effect**. The natural way to test this is to see if the same pattern shows up if we look at the same people in other stages of their life.

It's easy to overlook the importance of cohort effects. Sometimes they simply look like age effects. @GhitzaEtAl2023 argue that many hypotheses about age effects on voting, e.g., that older people are more naturally conservative, are really just cohort effects. @Bump2023 argues that understanding the distinctive role the boomers in particular play is crucial for understanding many aspects of modern American life.

There are mathematical reasons that it is hard to tease these effects apart too. Many statistical techniques for separating out influences start to fall apart when there are linear correlations between combinations of variables. In this case there is as tight a correlation as is possible. By definition, cohort plus age equals period. There are some things you can do to get around this problem - see @KeyesEtAl2010 for a useful survey of some of the options, and see @Rohrer2025 for some recent scepticism about general solutions to it - but it remains a challenge.

Even conceptually, it is hard to separate out these three effects in cases where there is evidence that the strength of the effects changes over time. As I noted at the start, the natural way to test hypotheses about which effect is strongest involve looking at other times. That works well when the age effects are constant. When they are not (and they might not be here), it is harder.

For most of our story, however, it helps just to have these three effects in mind. Using them, we can summarise the data reasonably quickly.

- The age effect is that articles get cited most when they are two to five years old.
- The period effect is that there are many more citations in recent years than in earlier years. This is in part because the number of articles published in these journals has been growing, and in part because the number of citations per article grew substantially over the 2000s and 2010s, and exploded in the 2020s.
- The cohort effect is that articles from the 1970s and 2000s get cited more than you'd expect given these age and period effects, while articles from other times, most especially before 1965, but also around 1990, get cited less. The reasons for this are more complicated, and I'll return to them below.

The period effect is the largest, and in some ways the least interesting, so I'll start the analysis by quantifying it, and arguing for a particular way to screen it off.

# Period Effects {#sec-period}

The database contains `r number_of_citations` citations, but they are not distributed evenly over time. Instead, they grow rapidly. At the start, in 1956, there are only `r citations_1956` citations. That's not too surprising; without the ability to cite preprints, there aren't going to be many citations of articles that have come out that year. By 2024, there are `r citations_2024`. In @fig-citationsperyear, I show how these grew.

```{r}
#| label: fig-citationsperyear
#| fig-cap: "The number of citations in the dataset made each year."

all_citations_per_year_plot
```

As noted in @sec-methodology, I used a slightly different method to extract the citations from 2022 onwards. It's possible that the drop between 2021 and 2022 is a consequence of that change. However I don't think it is for two reasons. First, it's more likely that 2021 is just an outlier; it's a consequence largely of _Synthese_ publishing `r synthese_2021` articles in 2021, then a relatively few `r synthese_2022` articles in 2022. Second, I applied the method I'm using for 2022-2024 to 2020 and 2021, and got a fairly close agreement (within 1-2%) with each year.

What explains this dramatic growth, at least through 2021? Part of the explanation is that more articles are being published, and more articles are being indexed. @fig-articlesperyear shows how many articles are in the dataset each year.

```{r}
#| label: fig-articlesperyear
#| fig-cap: "The number of articles in the dataset published each year."

articles_per_year_plot
```

That explains some of the growth, but not all of it. The curve in @fig-articlesperyear is not nearly as steep as the curve in @fig-citationsperyear. The number of (indexed) citations per article is also rising. In @fig-outboundcitations I've plotted the average number of citations to other articles in the dataset each year.

```{r}
#| label: fig-outboundcitations
#| fig-cap: "The average number of citations to indexed articles each year."

outbound_citations_plot
```

There are a few possible explanations for the shape of this graph.

At the left-hand edge, there are obvious boundary effects. Since we're only counting citations to articles published since 1956, it isn't surprising that there aren't very many of them per article in the 1950s. Since articles rarely get unpublished, there are more articles available to cite every year.

That can't explain the massive jumps we see at the right hand edge of @fig-outboundcitations. The jump there looks like the convergence of two cultural trends. One is a trend simply to greater numbers of citations. The most casual perusal of journals will confirm that trend. The other is a trend to greater citations of journals themselves, as opposed to books or edited volumes.

A sharp jump like this is a warning sign that there is something wrong with the data, and so the data should be checked. It's impractical to cross-check every entry, but those I have checked look correct. The change seems led by the most prestigious journals. For each journal I calculated the average number of outbound citations (to these hundred journal) for both the 2010s, and the first two years of the 2020s. The ten journals with the largest increase between the decades are shown in @tbl-large-growth.

```{r}
#| label: tbl-large-growth
#| tbl-cap: "Mean outbound citations for some journals over the last two decades."

who_cites_more <- citation_tibble |>
  left_join(
    select(
      active_philo_bib,
      id,
      journal
    ), by = c("new" = "id")
  ) |>
  filter(new_year >= 2010, new_year <= 2024) |>
  mutate(period = case_when(
    new_year < 2020 ~ "2010-2019",
    TRUE ~ "2020-2024"
  )) |>
  group_by(journal, period) |>
  summarise(articles = n_distinct(new), citations = n(), .groups = "drop") |>
  mutate(name_len = str_length(journal)) |>
  mutate(mean_cites = citations/articles) |>
  pivot_wider(id_cols = c(journal, name_len), names_from = period, values_from = mean_cites) |>
  mutate(diff = `2020-2024` - `2010-2019`) |>
  mutate(Difference = round(diff, 1),
         `2010-2019` = round(`2010-2019`, 1),
         `2020-2024` = round(`2020-2024`, 1)) |>
  arrange(-diff) |>
  slice(1:10) |>
  select(Journal = journal,
         `2010-2019`,
         `2020-2024`,
         Difference)

kable(who_cites_more)

```

Since _Philosophical Review_ only publishes 10 to 12 articles per year, it is not surprising that it shows the most variation on this list. Still, the change in the 2010s isn't only small sample size variation. Of the 22 articles it published in 2020 and 2021, only one of them [@WOS000575210400003] had fewer than 14.8 outbound citations. With a sample of just 22 anything could happen, but it would be surprising to have all but one end up on the same side of the historical average by chance.

We could just ask what proportion of all citations accrue to an article in a given year. But that would be an overcorrection. In the 2020s there are more citations to be shared around, but also more articles to share them between. We need to adjust for both things. Here's the way I'll do it.

Say an article is from a year that is one of the _typically_ cited articles iff it is between 3 and 10 years before the citing year. As we saw in @fig-master-citation-ratio, that is when citations typically peak. Using this definition, @fig-articlecounts shows how many of these typically cited articles there are at any given time. (So for 2000, it shows the number of articles published 1990-1997.)

```{r}
#| label: fig-articlecounts
#| fig-cap: "Typically cited articles."

typical_plot
```

In @fig-citationcounts, I've shown how often, in each year, these 'typical' articles are cited, and in @fig-citationrate I've shown the mean number of citations to these typical articles there are in each year.

```{r}
#| label: fig-citationcounts
#| fig-cap: "Citations to typical articles."

typical_citations_per_year_plot
```

```{r}
#| label: fig-citationrate
#| fig-cap: "Mean annual citations to typical articles."

typical_citation_rate_per_year_plot 

```

Two things stand out about @fig-citationrate. The graph is fairly flat for a long time. Between the mid 1970s and early 2000s it bounces around without moving much. Then it takes off, and go through the roof in 2021, before returning to the long term trend. The other thing is that the numbers are never high. For most of this study, even articles from this peak citation age, three to ten years old, are cited in one of these hundred journals once a _decade_. Actually, since citation rates are extremely long-tailed, and mean rates are well above medians, that somewhat overstates how often the 'average article' was being cited. Frequent citation is very much not the norm.^[In the long run the average number of times an article is cited equals the average number of citations per article. So it shouldn't be too surprising that most article have just a handful of citations in philosophy journals.]

My first pass measure of an article's influence at a time is how often it is cited at that time, divided by how often the typical article is cited at that time. This is a little arbitrary; I could have picked other ranges than three to ten years, but I think it gets things roughly right. I tried several other measures, and they all either led to implausible trends in the data, or to comparative judgments about the influence of various papers that didn't seem remotely plausible. This measure had the nice consequence that how influential the leading 50 articles from a period were 10-20 years after that period was reasonably stable, suggesting that it does correct for period effects reasonably well.


# Age Effects {#sec-age}

The next task is to work out how the age of an article affects how often it is cited. The simplest thing to do here would be to look at a typical year, and see how often articles from that year have been cited over time. That would, unfortunately, be completely wrong. @fig-1990-outbound-citations shows how often articles from 1990 have been cited over time.

```{r}
#| label: fig-1990-outbound-citations
#| fig-cap: "Citations to articles published in 1990."

year_in_year_out |>
  filter(old_year == 1990, new_year >= 1990) |>
  ggplot(aes(x = new_year, y = citations)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Number of citations")
```

If we're using citations as a measure of influence, @fig-1990-outbound-citations suggests that, collectively, articles from 1990 were most influential in 2021. That's not really true; they were most influential two to four years after they were published, like most articles. It's just that there are so many articles published in the 2020s, and each of them cite so many pieces, that citations to three decade old articles get lifted by the rising tide.

A more intuitive way of measuring influence uses the notion of typical articles from the @sec-period. Let's adjust @fig-1990-outbound-citations by dividing each value by two things. First, by the citation rate of typical articles, as shown in @fig-citationrate, to adjust for period effects. Second, by the number of articles published in 1990, so we're getting a measure of influence per article. The result is the statistic I used in @fig-master-citation-ratio; I call it the **citation ratio**. The graph for 1990's citation ratio is @fig-1990-outbound-citations-norm.

```{r}
#| label: fig-1990-outbound-citations-norm
#| fig-cap: "Normalised measure of citations to articles published in 1990."

year_in_year_out |>
  filter(old_year == 1990, new_year >= 1990) |>
  left_join(
    select(
      typical_citation_rate_per_year,
      new_year,
      mean_cites
    )
  ) |>
  mutate(norm_cites = citations/(mean_cites* 1428)) |>  
  ggplot(aes(x = new_year, y = norm_cites)) +
  geom_point(color = point_col) +
  xlab(element_blank()) +
  ylab("Normalised citations")
```

There are two reasons to think that the graph in @fig-1990-outbound-citations-norm is a more plausible measure of influence than the one in @fig-1990-outbound-citations. One is just an appeal to intuition. I know how often work from 1990 came up in discussions in the 1990s and in the 2020s, and it was a lot higher in the former than in the latter. While that kind of intuitive evidence should be given some weight, it's obviously unreliable on its own. The better reason is that we get a very similar shaped graph to @fig-1990-outbound-citations-norm no matter which initial year we pick. This was already visible in  @fig-decades-cite-ratio, but it's worth seeing how stable it is.

It's worthwhile having an explicit definition of the citation ratio. Let *c*(*o*,Â *n*) be the number of citations of articles from year *o* (the old year) in year *n* (the new year). Let *a*(*o*) be the number of articles published in year *o*. Then the citation ratio *r*(*o*, *n*) is:

$$
r(o, n) = \left(\frac{c(o, n)}{a(o)}\right) / \left(\frac{\sum\limits_{i = n-10}^{n-3}c(i, n)}{\sum\limits_{i = n-10}^{n-3}a(i)}\right)
$$

In @fig-ageeffecttibble-early and @fig-ageeffecttibble-late each facet is a different value for *o*, the x-axis is *n*, and the *y* axis is *r*(*o*, *n*). The key thing to note is how steady these graphs are. I have cheated a little; if I'd shown earlier years the shapes would not have been the same. There are so few citations in the 1960s that the noise overwhelms the signal. Since then, we get a reasonably steady pattern.

```{r}
#| label: fig-ageeffecttibble-early
#| fig-cap: "Citation rates for articles published 1968-1992."
#| fig-height: 12
#| fig-width: 9

age_effect_tibble |>
  filter(old_year >= start_year + 1, old_year <= end_year - min_data, new_year >= start_year) |>
  filter(old_year >= 1968, old_year <= 1992) |>
  ggplot(aes(x = new_year, y = cite_ratio)) +
  geom_point(size = 0.4, color = point_col) +
  facet_wrap(~old_year, ncol = 5) +
  xlab(element_blank()) +
  ylab(element_blank()) +
  theme(axis.text = element_text(size = 10),
        strip.text = element_text(size = 12))
```

```{r}
#| label: fig-ageeffecttibble-late
#| fig-cap: "Citation rates for articles published 1993-2017."
#| fig-height: 12
#| fig-width: 9

age_effect_tibble |>
  filter(old_year >= start_year + 1, old_year <= end_year - min_data, new_year >= start_year) |>
  filter(old_year >= 1993, old_year <= 2017) |>
  ggplot(aes(x = new_year, y = cite_ratio)) +
  geom_point(size = 0.4, color = point_col) +
  facet_wrap(~old_year, ncol = 5) +
  xlab(element_blank()) +
  ylab(element_blank()) +
  theme(axis.text = element_text(size = 10),
        strip.text = element_text(size = 12))
```

# Cohort Effects {#sec-cohort}

So far we've seen how period effects and age effects between them can explain a lot of the trends we see in citation patterns. But there are systematic deviations from those patterns which remain. In @fig-twodeviations, I've shown some of these. Each graph here is a facet from @fig-ageeffecttibble-early with a line plotted on top of it showing the average age effect.

```{r}
#| label: fig-twodeviations
#| fig-cap: "Comparing citations from a given year to average citations."
#| fig-subcap: 
#|   - "1979"
#|   - "1985"
#| layout-ncol: 2

year_to_mean_plot(1979)
year_to_mean_plot(1987)
```

In 1979, the yearly values are predominantly above the mean line, in 1987 they are largely below it. The main measure of cohort effect I'll use is, for each graph like this, the average ratio between the yearly data (i.e., the dot) and the average value (i.e., the line). The dots in the 1979 graph are on average about 13% above the mean; in the 1987 graph they are about 9% below. If we repeat this for every year in the dataset, we get the results shown in @fig-cohort.

```{r}
#| label: fig-cohort
#| fig-cap: "Cohort effects for different publication years."

year_by_year_average_plot
```

A couple of quick technical notes on @fig-cohort. I've added a line showing the rolling average (four years either side of the point, or as much of those four years are available) to help make some of the features of it stand out. And in calculating the mean, I only included years where we had at least five years worth of data to calculate the mean age effect. So that means I haven't included what happens to `{r} start_year` papers when they are cited after `{r} end_year - min_data`. There isn't nearly enough data to say what one would 'expect' the usual aging curve to be at those points.

On the face of it, there are five periods in the graph.

1. Journal articles published before the mid-1960s are very rarely cited.
2. After that, and especially in the early 1970s, a flurry of very highly cited articles are published.
3. Then there is a period of stagnation, where things mostly don't return to the lows of per-1965, but are consistently below 0.
4. There is an uptick starting in the mid-1990s, and peaking dramatically in 2007.
5. Then there is a rather dramatic drop off, almost immediately after the high of 2007.

The first couple of trends make sense; the latter three less so. The rest of this paper (before the methodology section) will be about explaining what's going on here, and seeing what it can tell us both about the history of philosophy, and the history of the philosophy profession.

```{r}
#| label: find-old-high-cites

old_high_cites <- citation_tibble |>
  filter(old_year <= 1963) |>
  group_by(old) |>
  tally(name = "citations") |>
  slice_max(order_by = citations, n = 20) |>
  left_join(
    select(
      active_philo_bib,
      old = id,
      displayauth,
      year,
      title
    ), by = "old"
  ) |>
  filter(citations >= 150) |>
  arrange(year) |>
  mutate(the_cite = paste0(
    "@",
    str_replace_all(old, ":","")
  ))

list_of_old_high_cites <- knitr::combine_words(old_high_cites$the_cite)
```

Before 1965, the philosophical work with the most lasting significance was not done in journals that WoS indexes. That's in part because books were so much more important than journals, and in part because WoS indexes are quite incomplete in those early years. So we don't have "Is Knowledge Justified True Belief?" [@Gettier1963] because they don't index _Analysis_ until 1975, and we don't have Austin's two most important papers - "Ifs and Cans" and "A Plea for Excuses" [@Austin1956; @Austin1956b] because their venues aren't indexed as journals at all. We do have important papers by `r list_of_old_high_cites`. But these made much less impact than books from the same time, especially _Intention_ [@Anscombe1957], _Word and Object_ [@Quine1960] and _The Structure of Scientific Revolutions_ [@Kuhn1962].

Then starting in the late 1960s, almost every area of philosophy got turned upside down, with much of the action happening in journals. The two most important works of the period, _A Theory of Justice_ [@Rawls1971] and _Naming and Necessity_ [@Kripke1980], were not in journals. But articles in journals did make revolutionary changes in many fields, including:

- Free will [@WOSA1969Y444700002; @10.2307_2024717];
- Practical ethics [@WOSA1971Y116900003; @WOSA1972Z066400001];
- Meaning and reference [@10.2307_2025079];
- Philosophy of mathematics [@10.2307_2025075];
- Causation [@10.2307_2025310; @10.2307_2025096]; and
- Personal identity [@WOSA1971Y036400001]

As well there were a surprising number of papers that weren't as influential straight away, but came to play a big role the later literature. This group includes papers by Fred @WOSA1970ZE33800001, David @WOSA1970ZE32700001, Kendall @WOSA1970Y384700002 and Larry @WOSA1973P242100001. That's all to say that the story that @fig-cohort tells about the early 1970s is directionally plausible, although the magnitude is hard to confirm because the data is spotty. Before the late 1960s and early 1970s, there had never been a period when philosophy journals published anything like this quantity of high quality work.

To see what is happening after that time, though, we need to look a little more closely at the data. @fig-cohort-short is a version of @fig-cohort just focussing on the first seven years after publication. That is, for each publication year, it measures citations of articles published that year in the seven years after they were published, adjusted in the same way for age and period effects. I'm starting this in 1975 to remove the noisy data from the early years.

```{r}
#| label: fig-cohort-short
#| fig-cap: A version of @fig-cohort restricted to citations from the first seven years after publication.

year_by_year_average_plot_short +
  ggtitle("Short term citation rates")
```

In @fig-cohort, articles from 2010 are measured as being cited slightly more often than articles from 1990 (after adjustments for age and period). But in @fig-cohort-short they are cited 20% less. In general, almost every year's batch of articles from the 1980s and 1990s are shown as being cited at solid rates through their first seven years. So @fig-cohort-short does not have the same dip than @fig-cohort did in the 1980s. There is a striking contrast to @fig-cohort-long, which only measures citations more than seven after publication.

```{r}
#| label: fig-cohort-long
#| fig-cap: A version of @fig-cohort restricted to citations from after the first seven years after publication.

year_by_year_average_plot_long +
  ggtitle("Long term citation rates")
```

In @fig-cohort-long, every year from 2000 to 2016 has a higher average than every year from 1980 to 1999. Starting around 1998, there is a large change in how often articles more than seven years old are cited.

This explains the dropoff on the right hand edge of @fig-cohort, i.e., the fifth period I described in that graph. Citations are getting older, but for articles published in the mid-2010s, we simply don't have the data for how often they are cited ten or more years after publication. So averaging how often they are cited in their first few years, compared to how often articles from a generation ago were cited in their first few years, underestimates their influence.

This doesn't quite explain everything. The graph in @fig-cohort-long does seem to stop rising in the 2010s. But it explains a lot. Articles from the 1980s and 1990s were cited a lot soon after publication, but their citation rate didn't hold up as well as articles from later years. Articles from the 2000s were cited a little less soon after publication than late 20th century articles, but much more as time passed. The same might be true for articles from the 2010s, but it's too early to say for sure.

Even if that's right, there are still some questions to answer. 

Why is this temporal shift in citations happening? Shouldn't technology be causing the shift to happen in the other direction? I'll discuss this in @sec-technology.

Why are citations in general rising so much, even after allowing for increases in the number of articles published? Some of the reason for that will become clear in @sec-technology, but there are two cultural factors that I'll discuss in @sec-culture.

Finally, why do the times around 1990 and 2005 stand out? Around 1990 we see an upward spike in @fig-cohort-short, and a low point in @fig-cohort-long. Around 2005, both graphs are above their long term trends. The answers here are partially due to the technological factors discussed in @sec-technology, and the cultural factors discussed in @sec-culture. But they are also due to an important change in which topics were philosophically central. There is a hint here of an important discontinuity between twentieth and twenty-first century philosophy, and I'll say more about that in @sec-content.

# Technology and Citations {#sec-technology}

As I noted at the top, I feel a common view is that the primary role of electronic publication has been to speed up _distribution_. This view is not borne out by the data. If that were the case, you'd expect to see short term, especially very short term, citations rising over time.

Printing and postage was a pretty mature technology by the late twentieth century. We weren't waiting for steam ships to bring the latest issues of journals to distant shores. The technology used for distributing philosophy journals was the same technology used for distributing journals in medicine and other fields where time was of the essence. From that perspective, the internet would only speed things up by weeks, maybe months, and this wouldn't really show on a graph in years. It is possible that improvements in the postal system, and especially the increased use of airmail, make a difference to the citation numbers in the 1980s and 1990s. There are, for example, more citations to journals from other countries soon after publication than there had been in earlier decades.

But the prevalence of Online First, Early View, and other forms of quasi-publication haven't made much difference. They do show up a little in the data, in that occasionally articles are cited before their official publication. But these cases are rare.

The biggest effect of technology was not on distribution but on _search_. Before the widespread use of computers, there was a much better system for searching books than for searching journal articles. Classification systems meant books typically lived on shelves near other books on the same topic. Card catalogues would list subject matters for books. Even the title of the book could help track down what it was about. Finding a journal article on a relevant topic was much harder. And, it seems, it mostly wasn't done.

My impression is that there was also a notable physical difference between the ways books and journals were stored and accessed. Almost every academic has a bookshelf; not as many have large collections of journals. Occasionally a department would have physical journals on hand, but often the best way to access journals was to walk across campus to a library. On the other hand, accessing a book might involve walking four steps to a shelf. This physical difference probably contributed to the relative prominence of books and articles in bibliographies.

There often was an exception to this general claim about access. (At least, this was true in any twentieth century department I was familiar with, but I think it was moderately widespread.) Departments would sometimes keep the latest issues of journals in a department library or common room. Those would be much more prominent, and easy to access. I'm not sure whether this explains why so many of the journal citations pre-1995 are to very recent journals, but it probably helped.

So changes to search technology seem to be part of the story. Before the widespread adoption of computers, old journal articles were very hard to find. This changed a little with the advent of electronic, and hence easily searchable, versions of _Philosophers' Index_, and changed a lot when journals went online. That's part of why older articles, and especially older articles that are not classics, are now more widely cited.

# Culture Changes {#sec-culture}

Two important cultural changes made a difference to the citation patterns, one to do with the role of journal articles, another to do with citation norms.

If you look back to journals from a century ago, you sometimes see pieces that feel more like blog posts than what we'd now consider journal articles. Even more substantive pieces feel more like means to an end. The journal article is as much a trial run for a book as it is a report on a complete project.

By the twenty-first century, all of that is changed. Articles are getting much longer, even in venues like _Analysis_. More importantly, they are often finished products, not draft runs for future books. There are prominent philosophers whose reputation is based largely, or even entirely, on articles (e.g., Jonathan Schaffer). There are large fields of philosophy, e.g., epistemic contextualism and metaphysical grounding, where the canonical texts of the field are almost entirely articles not books. This isn't entirely new, the post-Gettier literature on analysis of knowledge was largely article-based too, but it's a growing trend. This is partially why citations are getting older; if a field is based in articles, there will be more citations to older articles. That's especially true when those articles have not been superseded by books.^[The list of articles that were once widely cited but are rarely cited now is heavily populated with articles that formed the basis for widely cited books.]

But the bigger trend is the increasing tendency of philosophers to include brief citations to work they aren't discussing in detail, but which locate the paper in a literature. Obviously if an article cites 26 other journal articles, as we saw in @tbl-large-growth the average recent _Philosophical Review_ paper does, it can't possibly be discussing every one of them in depth. (Remember this count excludes citations of books and articles in edited volumes.) For a long time a striking difference between philosophy and nearby disciplines was the lack of these citations that largely served to place a paper in a literature. Their growth partially explains why citations are getting older; if an author cites the history of their sub-field, that will include some older articles.

But why did this change happen? Part of the story is technology, as discussed in @sec-technology. It's now much easier to do this. But this can't be the whole story, since the practice was more widespread in other fields before some of these technologies came on board. Part of the story is the growth of specialisation. If you're writing a paper on content externalism in the 1990s, you don't need to include a citation trail going back to Kripke and Putnam. Everyone knows the history of these debates, and it doesn't need to be rehashed. Since then, it's become a little less clear that there is any field that all readers will know the history of, so there's more need for background.

But I suspect the biggest part of the explanation is the growth of interdisciplinary work in the 2000s. This can be seen in a lot of fields, including the rise of experimental philosophy, and the increasing empirical sophistication in philosophy of mind. I'm going to focus on one other aspect of this growth: the move of philosophy of language towards debates that were also active inside linguistics. Part of what we see in the 2000s is the adoption of citation norms from linguistics into philosophy of language. Now philosophy of language wasn't as important to the journals in the 2000s as it had been in earlier decades.^[I'm not going to argue for this here because it would require almost another paper to do so, and draw on yet more data sources. The short version of the argument was that nothing in twenty-first century philosophy of language was as crucial to the journals as debates over names and descriptions, and over wide and narrow content, had been in earlier decades.] But it was an important vector for citation norms to spread from linguistics to philosophy, especially because so much work in philosophy of language overlapped with the key debate in epistemology that decade: contextualism. I'll end this paper with a deeper look at how the changes in what philosophers discussed interacts with the citation data.

# Content Changes {#sec-content}

The centre of gravity of philosophy publishing changes over the time period we're looking at. Through at least the early 2000s, analytic philosophy is in what @Sider2020 [2] calls the "modal era". One aspect of this era, one that Sider particularly highlights, is that questions about essence were equated with questions about necessity in a way that they weren't either before or after the era.^[It was usual, during this era, to take the necessity of origins thesis and the origin essentialism thesis to not just be mutually supporting, but to be literally identical. That identity claim would not be widely endorsed either before 1970 or after 2010.] This should be taken as a symptom of the era, not the definition of it. What's really defining of the era was the way modality became central to disputes across the discipline.

Consider, for example, what Frank @Jackson1998 called the 'location problem', i.e., the problem of how to locate in the world something that the philosopher thinks exists, and is not fundamental. Jackson argues that saying how to locate the non-fundamental in the fundamental is a compulsory question for anyone doing 'serious metaphysics', and the one and only answer to it will involve modality. As he says,

> When does a putative feature of our world have a place in the account some serious metaphysics tells of what our world is like? I have already mentioned one answer: if the feature is entailed by the account told in the terms favoured by the metaphysics in question, it has a place in the account told in the favoured terms. This is hardly controversial considered as a sufficient condition, but, I will now argue, it is also a necessary condition: the one and only way of having a place in an account told in some set of preferred terms is by being entailed by that accountâ€”a view I will refer to as the entry by entailment thesis. [@Jackson1998 5]

Now Jackson went on to say other things about entailment that were not widely endorsed. But at this early stage in the book, I think he was largely expressing conventional wisdom. In a review that disagrees with many parts of the book, Stephen @Yablo2000Jackson [20] says "Not many eyebrows will be raised by Jacksonâ€™s view that metaphysics is committed to 'entry by entailment' theses." That is, the quoted parts are not controversial, especially the one that Jackson flags as being ever so slightly more controversial.

The idea that entailment, i.e., necessitation, had been central to understanding how the non-fundamental relates to the fundamental had been central to philosophy for many years by this point. We can see just how central it is by using a slightly different statistic to what I've used so far: grand-citations. 

Say that the number of grand-citations an article *a* has is the number of triples âŸ¨*a*, *b*, *c*âŸ© such that *c* cites *b* and *b* cites *a*. It's the sum of the number of citations of articles that cite *a*. If we look at grand-citations over time, they show David Lewis's centrality to the philosophy journals. Five of the six articles with the most grand-citations are by Lewis. If instead we look at particular times, we see the changing face of the journals. Grand-citations take some time to accrue, so I'll look at twenty year periods. In particular, for various years, I'll look at which articles published in the preceeding twenty years had the most grand-citations through that year.

```{r}
#| label: calculate-grand-cites
#| cache: true

# Grand cites at a time

overall_grand_cites <- c()

for (end_year in (199:202)*10){
  temp_citation_count <- citation_tibble |>
    filter(new_year <= end_year,
           old_year >= end_year - 20) |>
    group_by(old) |>
    tally(name = "citations")
  
  temp_grand_cites <- citation_tibble |>
    filter(new_year <= end_year,
           old_year >= end_year - 20) |>
    left_join(temp_citation_count,
              by = c("new" = "old")) |>
    replace_na(replace = list(citations = 0)) |>
    group_by(old) |>
    summarise(grand_cites = sum(citations)) |>
    left_join(
      select(
        active_philo_bib,
        old = id,
        displayauth,
        year,
        title
      ), by = "old"
    ) |>
    arrange(-grand_cites)
  
  temp_grand_cites_summary <- temp_grand_cites |>
    slice(1:100) |>
    left_join(
      temp_citation_count,
      by = "old"
    ) |>
    mutate(
      Article = paste0(
        displayauth,
        " -@",
        str_replace_all(old, ":",""),
        " \"",
        title,
        "\""
      )
    ) |>
    mutate(rank = row_number(),
           as_of = end_year) |>
    select(
      `As Of` = as_of,
      old,
      Rank = rank,
      Article,
      Cites = citations,
      `Grand-Cites` = grand_cites
    )
  
  overall_grand_cites <- overall_grand_cites |>
    bind_rows(temp_grand_cites_summary)
}

```

@tbl-grand-cite-2000 lists which articles, published from 1980 onwards, had the most grand-citations through 2000.

```{r}
#| label: tbl-grand-cite-2000
#| tbl-cap: "Articles from the 1980s and 1990s with the most grand-citations by 2000."

kable(
  overall_grand_cites |>
    filter(`As Of` == 2000) |>
    select(-`As Of`, -old) |>
    slice(1:10)
)
```

I've included the names of the articles on this table to make vivid how central supervenience was to the literature at this time.^[The story of the relationship between twentieth century work on functions and twenty-first century work on mechanisms is interesting, but for another time.] Four of the articles here have the word 'supervenience' in the title! At the center of this literature stood Jaegwon Kim. The citation data I have somewhat _underestimates_ his influence, because people often cited his edited collection _Supervenience and Mind_ [@Kim1993], and those citations are usually not tracked by WoS.

The subsequent history of these articles goes a long way to explaining the relatively low values in @fig-cohort for much of the 1980s. Much of this work on supervenience has fallen out of the philosophical discourse. in @tbl-grand-cite-2000-now I've shown how many citations the articles in @tbl-grand-cite-2000 have since 2021.

```{r}
#| label: tbl-grand-cite-2000-now
#| tbl-cap: "Citations of articles in @tbl-grand-cite-2000 since 2021." 

temp <- citation_tibble %>%
  filter(new_year >= 2021) %>%
  group_by(old) %>%
  tally(name = "Citations since 2021") 

temp2 <-   overall_grand_cites |>
    filter(`As Of` == 2000) |>
    slice(1:10) %>%
    left_join(temp, by = "old") %>%
        mutate(
      Article = paste0(
        "@",
        str_replace_all(old, ":","")
      )
    ) %>%
     # select(Article, Cites, `Grand-Cites`, `Citations since 2021`)
    left_join(
      select(
        active_philo_bib,
        old = id,
        displayauth,
        year,
        title
      ), by = "old"
    ) |>
      select(Article, `Citations since 2021`)
kable(temp2)
```

The articles on supervenience just aren't cited as much these days. The same pattern recurs if we look at other papers on supervenience that were nearly as widely discussed at the time, e.g., @WOSA1989T680600002 and @WOSA1984ST78300010. It also recurs if we look at other papers from the family of debates on wide and narrow content, individualism about the mental, empty names and hallucinations, all of which used notions of supervenience to define their debates. Articles by @WOSA1986AYX3200002, @WOSA1988P549200004, @WOSA1989T680600002, @WOSA1983RU36600003, @WOSA1991FF02900001 and @WOSA1991EN62900001, all of which were very influential at the time, have been barely cited since 2021. Even some of Lewis's papers on these topics (e.g., @WOSA1981MS19500002 and @WOSA1983PZ01000001) have just a handful of citations in recent years. A family of debates that was central to philosophy for years, and which was particularly central to what the _Philosophical Review_ published in the 1980s, has simply dropped off the agenda. This has a huge impact on citation numbers.

When we move into the 2000s, the focus shifts dramatically, as we see in @tbl-grand-cite-2010.

```{r}
#| label: tbl-grand-cite-2010
#| tbl-cap: "The 10 articles from the 1990s and 2000s with the most grand-citations through 2010."

kable(
  overall_grand_cites |>
    filter(`As Of` == 2010) |>
    select(-`As Of`) |>
    slice(1:10)
)
```

The biggest single topic over this time was contextualism in epistemology, with the big papers by @WOSA1996VY21200001 and @WOSA1995RC31600001 at the center of things. What I want to focus particularly on, though, is another DeRose paper on that list: "Epistemic Possibilities". It has fewer cites than any other paper there, but the fourth most grand-cites. This is in part because it was important to both epistemology and to philosophy of language. But another reason is that the debate it triggered was one of the first places where citation norms from linguistics were applied in philosophy. This is part of the evidence for the claims I made in @sec-culture about the importance of changing norms to citation rates.

When we move in the 2010s, the focus shifts again, as shown in @tbl-grand-cite-2020. Here we see some evidence for my earlier claim that there is an increase in specialistion. The papers with the most grand-citations are on subjects like mechanisms, dogmatism, disagreement, pragmatic encroachment, and priority monism. It feels like a much broader range of topics than was central to philosophy a few decades previously. Crucially, the topics have so little overlap that someone working in them cannot expect philosophers working in other topics to know even the basics of the debate. (Perhaps people working on the various epistemology topics on that list can expect a bit more background knowledge from people working on other epistemology topics, but that's about it.) So writers need to include more citations just so the readers will have a basic understanding of their topic.

```{r}
#| label: tbl-grand-cite-2020
#| tbl-cap: "The 10 articles from the 1990s and 2000s with the most grand-citations through 2020."

kable(
  overall_grand_cites |>
    filter(`As Of` == 2020) |>
    select(-`As Of`) |>
    slice(1:10)
)
```

# Conclusion

I need to write a conclusion.

# Appendix: Summary Statistics {#sec-statistics}

The paper uses the journals shown in @tbl-list-of-journals.

```{r}
#| label: tbl-list-of-journals
#| tbl-cap: "Journals used in this paper"

journal_summary <- active_philo_bib %>%
  group_by(journal) %>%
  summarise(
    earliest_year = min(year, na.rm = TRUE),
    latest_year = max(year, na.rm = TRUE),
    n_articles = n()
  )

# Outbound citations: join articles to their journal, count refs per journal
outbound_cites <- active_philo_cite %>%
  left_join(active_philo_bib %>% select(id, journal), by = "id") %>%
  group_by(journal) %>%
  summarise(outbound_citations = n())

# Inbound citations: join refs to their journal, count refs per journal
inbound_cites <- active_philo_cite %>%
  left_join(active_philo_bib %>% select(id, journal), by = c("refs" = "id")) %>%
  group_by(journal) %>%
  summarise(inbound_citations = n())

# Combine all summaries
journal_summary <- journal_summary %>%
  left_join(outbound_cites, by = "journal") %>%
  left_join(inbound_cites, by = "journal") %>%
  replace_na(list(outbound_citations = 0, inbound_citations = 0)) |>
  rename(
    Journal = journal,
    `First Year` = earliest_year,
    `Last Year` = latest_year,
    `Articles` = n_articles,
    `Inbound Citations` = inbound_citations,
    `Outbound Citations` = outbound_citations
  )

kable(journal_summary)
```

What I've called an _article_ here is anything that either (a) marked as an article or research-article by WoS, or (b) marked as a review, discussion, or note by WoS and is at least 15 pages long. I needed to include (b) because some very important works (e.g., @WOSA1963CEU0700001 and @WOS000272855000002) were not recorded as articles by WoS.

The years here are **not** the first and last years that the journals published, but the earliest and latest years that are in the WoS index (as of the time I pulled the data). As mentioned in the main text, this makes a big difference for some journals, especially _Analysis_.

The way WoS handles the 'supplements' to _NoÃ»s_, i.e., _Philosophical Perspectives_ and _Philosophical Issues_, is a little uneven. Some years these are recorded as being their own thing, i.e., with a source name of _Philosophical Perspectives_ or _Philosophical Issues_; and some years they are recorded as special issues of _NoÃ»s_. When they were listed as special issues, the citations were extremely unreliable. Some high profile articles are recorded as having no citations until several years after publication. The bibliographic information for the articles themselves was also spotty. So I've manually removed all records that were listed as special or supplementary issues of _NoÃ»s_ (and similarly removed the citations to those articles that did get tracked). What you see here are just the standalone issues of _Philosophical Perspectives_.
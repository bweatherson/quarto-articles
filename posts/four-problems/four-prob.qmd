---
title: "Four Problems in Decision Theory"
description: |
  In recent years the literature on decision theory has become disjointed. There isn't as much discussion as there should be on how different problems impact one another. This paper aims to bring together work on problems involving demons, problems about attitudes to risk, problems about incomplete preferences, and problems about dynamic choice. In the first three of these cases, I end up defending a pre-existing view, but in each case the argument for that view is strengthened by seeing how the premises that support it are essential to solving one of the other problems. The most novel part of the view is the theory of dynamic choice that I offer: a sequence of choices is rational only if both the so-called 'resolute' and 'sophisticated' theories of dynamic choice would permit it. This theory would be implausible if paired with many rival solutions to the first three problems, but fits nicely with the view I'll develop through the paper that decision theory is much less constraining than most theorists hold.
date: February 19 2024
author:
  - name: Brian Weatherson 
    url: http://brian.weatherson.org
    affiliation: University of Michigan
    affiliation_url: https://umich.edu
    orcid_id: 0000-0002-0830-141X
citation: false
categories:
  - games and decisions
  - unpublished
format:
  html: default
  pdf:
    output-file: "Four Problems in Decision Theory"
    include-after-body: 
      text: |
         Unpublished. Posted online in 2024.
---

Contemporary decision theory has become disjointed. There is less overlap than there should be in working on adjacent problems. This paper aims to undo some of that, by showing that four problems that have largely been worked on in isolation cast useful light on each other. Some of the conclusions that draw will be familiar: I’m going to defend views similar to those defended by Melissa @Fuscond and by Harvey @Ledermannd. But in each case the arguments will be novel, and I'll end up defending a novel view on dynamic choice.

This paper is part of a broader project of identifying the decision theory that is implicit in standard, textbook approaches to game theory, and arguing that this decision theory is better than the ones currently on the philosophical market. I used to think the first part of this project would be boring - game theorists are just typical Causal Decision Theorists. This can't be true for five reasons. First, these textbooks don't mention counterfactuals at all, but counterfactuals are central to typical presentations of Causal Decision Theory. Second, solution concepts in game theory are typically not _single-valued_, in the technical sense defined by @Pearce1984, while typical versions of Causal Decision Theory are single-valued. Third, sometimes the unique solution to a game involves mixed strategies, while Causal Decision Theory, in its typical formulations, never says that a mixed strategy is uniquely optimal. Fourth, the solution concepts used for things like the beer-quiche game [@ChoKreps1987] put constraints that go beyond coherence constraints on the players, and typical formulations of Causal Decision Theory allow any coherent credence function. Finally, the textbook solution concepts for dynamic games don't correspond to any view in the philosophical literature on dynamic games.

Game theory textbooks tend to be several hundred pages, and identifying all the unique characteristics of the implicit decision theory, like the four from the previous paragraph, would take just as much space. So I'm going to simplify a lot here. In particular, I'm not going to talk about mixed strategies, except occasionally in footnotes. That is, I'm not going to assume anything about the availability or unavailability of mixed strategies in the arguments I put forward. That said, some of the positions I put forward are similar enough to existing positions that there are well known objections in the literature, and in many cases my preferred response to those objections does rely on the availability of mixed strategies. Getting all the details of those right would massively extend the paper, so I'll stay away from those discussions here. Relatedly, while I will spend a lot of time on problems where there are multiple pure strategy equilibria, I won't discuss any problems where there are no pure strategy equilibria. Those are four another day. With that qualification in place, it's time to get to the four problems I will discuss.

# Four Problems

## Demons

When a student starts decision theory, they are introduced to a view that is simple, elegant, and wrong. The view says that a chooser, hereafter called Chooser, has a set of possible actions *A* available. We'll use *a* to represent an arbitrary member of that set. And there is a set of possible states *S*, with *s* being used to pick out an arbitrary member. It is assumed that a probability distribution Pr over *S* is given, and that each action-state pair has a numerical value. I'll write *V* for the value function, so *V*(*as*) is the value of performing act *a* in state *s*.

The simple, elegant, and wrong theory is that Chooser should value each act *a* by its expected value, where this is Σ~s ∈ S~Pr(*s*)*V*(*as*). And Chooser should then choose the act with the highest value.

The problem with this view is that if Chooser has any influence over which state is actual, then this view will recommend obviously bad actions. Assume that the only possibly actions are *a* and *b*, the only two states are *s* and *t*, and while *a* will almost certainly cause *s* to be actual, *b* will almost certainly cause *b* to be actual. Now let the payoffs for all four action-state combinations be as in @tbl-joycewindow.

|     | *s* |  *t*  |
|:---:|:---:|:-----:|
| *a* |  1  | 1001  |
| *b* |  0  | 1000  |

: A counterexample to the simple theory {#tbl-joycewindow}

The problem is that in @tbl-joycewindow it obviously makes sense to do *b*, since that brings about the best option, but the simple theory says that the value of *a* is 1 more than the value of *b*. So @tbl-joycewindow is a counterexample to the simple theory. So far every decision theorist would agree. But here agreement ends. There is no agreement on either why the simple theory fails in this case, or what should go in its place.

Evidential decision theorists such as @Ahmed2014 say the problem is that there is an evidential connection between the acts and the states. They say that instead of the simple theory Chooser should value options using this formula.

EDT
:    *V*(*a*) = Σ~s ∈ S~Pr(*s* | *a*)*V*(*as*)

As with the simple theory, the rule is just to maximise value. The difference is just that instead of the unconditional probability of a state, we use the conditional probability of a state given that the action is performed. This will get the right result in @tbl-joycewindow, but gives some strange results in other cases.

Reinterpret @tbl-joycewindow so that the states are causally independent of the actions, but the actions are evidence for the state. To use the standard example, going back to @Nozick1969, imagine that a demon (hereafter called Demon) has predicted Chooser's choice. There is no backwards causation, so Chooser's choice is causally independent of Demon's prediction. But Chooser believes Demon is incredibly reliable, so Pr(*s* | *a*) ≈ 1, and Pr(*t* | *b*) ≈ 1. For ease of reference, I'll use the relabelled version in @tbl-newcomb for the problem when the actions are probabilistically, but not causally, connected to the states. In @tbl-newcomb, Chooser selects **U**p or **D**own, and Demon **P**redicts this choice. In general in what follows, if a state is labelled **PX**, it means that Demon has predicted that Chooser will select X. Using that notation @tbl-newcomb is just Newcomb's Problem.

|       | **PU** |  **PD**  |
|:-----:|:------:|:--------:|
| **U** |  1     |  1001    |
| **D** |  0     |  1000    |

: Newcomb's Problem {#tbl-newcomb}

In @tbl-newcomb, EDT says that Chooser should do *a*. There is a simple argument that Chooser should do *b*: whatever the world is like, it will have a higher return. This argument convinced many people that we need a different theory, and over the 1970s and 1980s a lot of people settled on something like CfDT as the right alternative.

CfDT
:    *V*(*a*) = Σ~s ∈ S~Pr(*a* □→ *V*(*as*))*V*(*as*)

That's a way to value options; the theory is just that one should choose the option with maximal value. Recently Brian @Hedden2023 has argued that this theory is preferable to typical versions of Causal Decision Theory. I'm sympathetic to the reply offered by Dmitri @Gallowndppq that this just is what Causal Decision Theorists in the 1970s and 1980s were typically defending. But I also think, for reasons that will soon become clear, that some other theories which are quite different to this are also *causal* in the relevant sense. So from now on I'll use "Causal Decision Theory" to name a family of theories, and CfDT will be a distinctive member of that family.

Another theory in that family says that the simple theory was essentially correct, it was just applied at the wrong time. This theory, which I'll call Gamified Decision Theory, or GDT, starts with the following two claims. First, the relevant state probabilities are those at the end of deliberation, once a choice has been made, not at the start of deliberation. Second, when we use those _ex post_ probabilities, the simple theory is fine. In symbols, the core formula that GDT uses is this.

GDT
:    V(a) = Σ~s ∈ S~Pr$\prime$(*s*)*V*(*as*)

In this formula, Pr$\prime$ is the probability distribution over states after Chooser has made their decision. GDT says that only options that have maximal value using this formula are choice-worthy.^[My preferred version of GDT adds several more constraints to this - it has a separate constraint for ruling out weakly dominated options, and a constraint for solving beer-quiche games, and maybe a constraint for ruling out mixed strategies in coordination games. But this is a necessary condition for choice-worthiness.] This allows that different options, with different values, could be choice-worthy. All that matters is that given the probability distribution over states that Chooser has when they have decided to perform an act, that act is utility maximising. In @tbl-first-coord, GDT says that both Up and Down are choice-worthy.

|       | **PU** |  **PD**  |
|:-----:|:------:|:--------:|
| **U** |  3     |     0    |
| **D** |  0     |     2    |

: An asymmetric coordination problem {#tbl-first-coord}

One of our four problems is to work out which of these theories is right. I'll be arguing for GDT.

It's sometimes said that problems involving Demon should not be treated as central to decision theory because Demon is so unrealistic. I think this view is mistaken twice over. For one thing, Demon isn't that much more unrealistic than the precise probabilistic models of the future of humanity that routinely do get used. More importantly, the problems that come up in this section arise in some very ordinary models. As @Lewis1981x pointed out, Prisoners' Dilemma with a twin raises much the same problems. Standard approaches to game theory presuppose that other players are like perfectly accurate demons.^[Matthias @Risse2000 criticises these standard approaches on this point, and while I'm sympathetic to his criticism, it's worth taking seriously how wide-spread the assumption of perfect prediction is across the academy.] Most importantly, all the views about how to make decisions in Newcomb-like problems come apart as soon as we assume Demon is better than chance at predicting Chooser. And better than chance predictions can be reasonably believed. I suspect if I was allowed to interview and observe people before they chose, I could predict their choices at well over 60% accuracy, and probably over 70%. To simplify the math, I'll work with a Demon who is perfectly accurate, or at least arbitrarily accurate.^[That is, I'll assume Demon's accuracy is 1-$\varepsilon$, for arbitrarily small $\varepsilon$.] But with some extra attention to detail, we could rewrite every example in the paper with a realistic Demon. I think having very accurate Demons is a worthwhile tradeoff of clarity for realism, but if you disagree it's not that hard to imagine the paper rewritten with demons only somewhat better than chance.

## Risk

Think about what value of *x* would make Chooser indifferent between these two options.

1. \$1,000,000
2. A gamble that returns \$2,000,000 with probability *x*, and \$0 with probability 1-*x*.
 
What factors are relevant to solving for *x*. One factor is the declining marginal utility of money. Money primarily has exchange value, and if Chooser won $2,000,000, the things Chooser would buy with the second million dollars are largely things they declined to buy with the first million. Hence the second million will be worth much less to them than the first, barring a pronounced taste for expensive goods that lack valuable parts. That's one factor that goes into solving for *x*, and every decision theorist agrees it is important, and that it is part of why whatever value *x* takes, it is surely well above ½.

But is it the only factor? If Chooser is rational, is knowing the function from the money they have to the utility they get from money enough to solve for *x*? The orthodox answer is that it is. Lara @BuchakRisk has argued that it is not. We also need to know how much Chooser values, or more likely disvalues, risk. That is, we need to know how risk-seeking, or risk-averse, Chooser is.

The orthodox view is that all we need to know are three numbers:

- The value Chooser assigns to their current wealth, which we can set as 0 for ease of calculation.
- The value Chooser assigns to having $1,000,000 more than their current wealth, which we can set as 1 again for ease of calculation.
- The value Chooser assigns to having $2,000,000 more than their current wealth, which we will label *c*.

Then on the standard view, the value of the gamble is *cx*, so the gamble is equal to the sure million iff *x* = 1/*c*. On Buchak's view, rational Chooser has a risk function *f*, that measures their sensitivity to risk. The function must be monotonic increasing, with *f*(0) = 0, and *f*(1) = 1. If Chooser is risk-averse, then typically *f*(*x*) < *x*.

Buchak's view reduces to the orthodox view if *f*(*x*) = *x*. I'm going to argue that given one very natural constraint, we can show that *f*(*x*) must indeed equal *x*. I'm far from the first to make an argument on these lines; I think the arguments that @Briggs201x and @Thoma20xx make for the same conclusion are also successful. What's novel about what I'm going here is two-fold. First, the premise I'll use is, I think, weaker and more plausible than the premises used in other arguments. Second, and more importantly, I'll be using the same premise to resolve problems involving demons as to argue against Buchak's view. A big aim of this paper is to bring different parts of contemporary decision theory together. As a quick glance at the literature will tell you, there isn't much overlap between work on views like Buchak's and work on problems involving demons, though both of them are large literatures. This is a mistake, and one I'm hoping to help rectify here.

## Non-Linearity

Standard approaches to decision theory assign to Chooser a probability function and a utility function, both defined over (some) propositions. Both of these functions output numbers. The numbers have a distinctive topology. Among other things, they are totally ordered: for any two numbers, either one is greater, or they are equal. So assuming that probabilities and utilities are numerical involves assuming, among other things, that they are also totally ordered. That is, for any two propositions, the probability(/utility) of the first is either greater than, less than, or equal to, that of the other. Call this assumption Ordering.

Ordering is controversial, both for probabilities and utilities. For probabilities, it has been criticised since Keynes's _Treatise on Probability_ [-@Keynes1921], and in recent times has been criticised by, among others, Peter @Walley1991 and James @Joyce20xx. For utilities, the most prominent critic has been Ruth Chang [-@Chang2002, -@Chang2009].

It takes a little work to create a counterexample to ordering. It's no good to just put forward two things and say it isn't clear which is larger. For one thing, it might simply be unknown which is larger. For another, they might be equal. We'll come back to the first concern in a bit. Ruth @Chang2002 points out a natural way to avoid the second complaint. Consider three options *A*, *B*, and *A*+, with the following features.

- *A* and *B* concern different subject matters.
- It is unclear whether the value of *A* or of *B* is larger. (The 'value' here could be either probability or utility.)
- *A*+ is by design fractionally larger than *A*. If the value is utility, *A*+ could be *A* plus a cookie. If it is probability, *A*+ could be the disjunction *A or this lottery ticket wins*.
- If *A* and *B* were equal in value, then since *A*+ is greater than *A*, *A*+ would be greater than *B*.
- But it is also unclear whether *A*+ is greater than *B*.

Call this the sweetening argument, since *A*+ is generated from *A* by sweetening.

Just like there are many critics of Ordering, there are many defenders. @DorrEtAl2020 defend it on semantic grounds. Adam @Elga200x argues that violations of Ordering for probabilities leads to susceptibility to a money pump. Johan @Gustafson2022 makes a similar in favour of Ordering for utilities.

Even critics of Ordering have noted its unintuitive characteristics. @BradleySteele2016 argue that violations of Ordering for probabilities leads to thinking it is acceptable to pay to avoid information.^[It's uncontroversial that in some cases we pay to avoid information, e.g., we take efforts to avoid spoilers for movies. Even if the information doesn't change the value of the final product, we might pay to avoid it if the information is not partitional [@Das2023], or we don't know we'll conditionalise [@Nethnd]. But if none of these three conditions are met, and probabilities and utilities satisfy Ordering, we should never pay to avoid information [@Blackwell1951].] Harvey @Lederman2024 argues that violations of Ordering for utilities leads to violations of a principle he calls Negative Dominance.

> **Negative Dominance**    
> It’s rationally required that: if [one] strictly prefers
one game of chance to another, one prefers one of the prizes that the
first might yield, to one of the prizes that the second might yield.

Both Bradley and Steele, and Lederman, think that ultimately Ordering should be rejected, and we should live with these unintuitive results. They are both pointing out troubling features of their own view. (Something philosophers should do more often.)

In each case it isn't hard to convert the argument they give to a problem for the other kind of Ordering violation. If Ordering fails for utilities, a Bradley and Steele-style argument shows that it is worth paying to avoid information, and if it fails for probabilities, a Lederman style argument shows that Negative Dominance fails.

I'm going to offer a new defence of Ordering violations. The defence has two parts. First, I'll argue that even if Ordering holds for probabilities and for values of states, it does not hold for values of actions. This shows we have independent reason to reject any principle that entails O≠rdering for actions, including Negative Dominance, and the semantic principles Dorr et al endorse. Second, I'm going to argue that many of the criticisms presuppose a false view about how rational dynamic choice works. This is how I'll respond to Elga, Gustafson, and Bradley and Steele.

## Dynamic Choice {#dynamic-choice}

On that note, it's time to introduce the last of our four problems - what the general theory of rational dynamic choice should look like. First, I'll set up how I'm conceiving of dynamic choice situations.

For the purposes of this paper, a **decision tree** is a sextuple ⟨*W*, *R*, *V*, *a*, *I*, Pr⟩ such that:

- *W* is a finite set of nodes. One of these nodes, call it *o* for origin, is designated as the initial node.
- *R* is a relation on *W* such that for any *x* ∈ *W*, ¬*xRo*, and if *x* ≠ *o*, there is a unique *y* such that *yRx*. Intuitively, the decision problem starts at *o*, and continues by moving from a node *x* to another node *y* such that *xRy* until there is nowhere further to go. Say that *x* is a predecessor of *y* if *xR+y*, where *R+* is the ancestral of *y*.
- *V* is a value function. It maps each terminal node of *W* to a real number. A node *x* is a terminal node iff there is no *y* such that *xRy*.
- *a* is a function from non-terminal nodes in *W* to the set \{C, D, N\} that says who the agent is for each node. Intuitively, C is for Chooser, D is for Demon, and N is for Nature. That agent 'chooses' where the game goes next.
- *I* is a partition of the nodes the non-terminal nodes *x: a(x) =*C. The elements of this partition are called information sets. Intuitively, when Chooser reaches a node where they must choose, they know that they are in one member of this partition, i.e., one information set, and nothing more. Any two nodes in the same information set have the same number of outbound links.
- Pr is a conditional probability function. It says that given a _strategy_ for Chooser, and that a particular non-terminal node *x* which is assigned to Demon or Nature has reached, what the probability is that we'll move to some particular node *y* such that *xRy*. If *x* is assigned to Nature, this probability is independent of Chooser's strategy.

A **strategy** for one of the three players, Demon, Chooser or Nature, is a function from all the nodes in the tree which are assigned to them, to the move they will make if that node is reached.^[It doesn't matter for our purposes, but note that in general a strategy includes what to do if one reaches a node that is ruled out by one's own prior choices.] Given any decision tree, one can generate a **strategic decision problem** where the possible actions are strategies for Chooser, and the states are pairs of strategies for Demon and strategies for Nature. One question that will be central 

As noted, there are two standard positions in philosophy. The **resolute** view says that Chooser should use the static theory of choice to pick a strategy at the start, and then resolutely stick with it. The **sophisticated** theory says that Chooser should take each node as a new choice, treat their past choices as fixed, and treat their future choices as another more-or-less knowable part of the world. My view is that both of these are wrong.

The **dual mandate** approach, which I favour, says that Chooser should adopt a strategy that makes sense and stick to it, just like the resolute theory says, *and* Chooser should make choices that make sense at each point, just like the sophisticate theory says. It disagrees with the two existing theories on two counts. First, it denies that either provides a sufficient theory for a sequence of choices being rational. Second, it says that if Chooser adopts a plan that makes sense now, and will continue to make sense at each node conditional on reaching that node, Chooser does not have to regard the future as unknowable. Rather, Chooser can know that they will keep following the sensible plan they have adopted. The point is not just that Chooser knows they will continue to be rational. If Chooser has many rational choices, once they adopt one, Chooser can know they'll stick to it.

This leads to the first reason for adopting the Dual Mandate view: it respects the distinctive relationship that holds between time-slices of the same person. On the resolute view, later stages of Chooser regard earlier stages as their Lord and Master, dictating what to do even if it no longer makes sense. On the sophisticated view, later stages of Chooser regard earlier stages as just someone that they used to know. As @Stalnaker199x points out, neither of these feels right; we want something between those two views. Now this doesn't entail that the Dual Mandate view is right, since there are a lot of views that are intuitively between the two views. But it should suggest that we look for something like the Dual Mandate view.

The second argument for it is that widely adopted in other disciplines. In most textbook presentations of game theory, the first solution concept for dynamic games that gets introduced is subgame perfect equilibrium. This idea traces back to @Selten1965. It says that in an equilibrium, all players will adopt strategies that are in equilibrium over the whole game, and which are in equilibrium when restricted to 'subgames'.^[A subgame of the original game is the set of all nodes reachable from a particular node that is in a single information set, with all the other properties and relations of those nodes held vixed.] The Dual Mandate View is my attempt to translate this idea into decision theoretic language. But what I want to stress here is that the idea that choices should be rational both at a time, and over time, is completely uncontroversial in game theory; it's just presented in the textbooks as the way to solve dynamic games.

The third argument is that decision theorists appeal to something like the Dual Mandate View already. Jack @Spencer2023 argues that (some versions of) Causal Decision Theory are "dynamically "


---
title: "Against Ex Ante Pareto"
abstract: |
 Given two lotteries over social outcomes, does the fact that everyone in the society thinks that one is at least as good as the other suffice to conclude that the first is at least as good as the other? I'm going to argue that, somewhat surprisingly, the answer is no. The principle that it does always suffice is called the Ex Ante Pareto principle, and it's been used to derive some striking consequences in recent work. The point of this note is to suggest that rather than accept those consequences, we should be sceptical of the Ex Ante Pareto principle. Or, more precisely, we should be sceptical of the precisification of the principle that has the most interesting consequences.
date: October 16 2025
author:
  - name: Brian Weatherson 
    url: http://brian.weatherson.org
    affiliation: University of Michigan
    affiliation_url: https://umich.edu
    orcid_id: 0000-0002-0830-141X
draft: true
image: "envelopes.jpg"
categories:
  - ethics
  - games and decisions
format:
    html: default
    pdf:
        output-file: "Against Ex Ante Pareto"
---

Consider the kind of problem that Kenneth @Arrow1950 was interested in when he developed his famous impossibility result. We have some citizens: *c*~1~, …, *c~n~*. Each citizen *c~i~* has a preference has a preference relation ≿~*i*~ over the possible outcomes, which is assumed to be symmetric, transitive, and complete. Our task is to find a social preference relation ≿~∀~, which is also symmetric, transitive, and complete, as a function of those individual relations. The problem is that some weak-ish looking desiderata concerning how the social ordering relates to the individual ordering entail that the job cannot be done. There are some familiar moves to make next, in my opinion by far the best is to deny to desirability of the Independence condition Arrow uses, but all I need for now is that it's a familiar problem.

In recent philosophy there has been some interest in a variant on Arrow's original problem. The variant folks are interested in differs from Arrow's in three dimensions. 

First, we assume that the citizens are self-interested, or at least differentially interested enough that we can which of them have their preferences satisfied can vary freely. This turns out be enough to get out of Arrow's original puzzle; it's a big enough violation of his Unrestricted Domain principle that it's possible to satisfy the rest of the principles. 

Second, we drop the assumption that ≿~∀~ is complete. Maybe for some social outcomes, neither is at least as good as the other. Amartya @Sen1970sec had originally noted that was enough to avoid the impossibility theorem that Arrow developed. Unfortunately, as Allan @Gibbard2014 quickly showed, there are other plausible principles that you still have to violate even without this assumption. Still, it's going to be important here.

Third, we make life much harder for ourselves by dropping the assumption that there are finitely many citizens. We will assume that there are countably many citizens, but there may not be finitely many. It turns out that once we allow this all sorts of new impossibility results come into play. 

This note will be about the distinctive kind of impossibility results we get when we assume that ≿~*i*~ and ≿~∀~ have in their domain not just social outcomes, but lotteries over social outcomes. Just like with Arrow, there are some very plausible looking principles that can't all be true in this setting. One of those principles says that for any two lotteries *X* and *Y*, if *X* ≿~*i*~ *Y* for all *i*, then *X* ≿~∀~ *Y*. That looks pretty plausible! How could *X* not be better than *Y* if everyone is just as happy with it? And I agree, it is pretty plausible. But, I'll argue, the things it clashes with are more plausible still.

I do not expect anything like agreement with that comparative plausibility claim; there is a lot of room for reasonable disagreement about which plausible principles should be given up when they are shown to clash. But I hope the clash is interesting.

# Why Are We Doing This {#sec-why}

There are actually two questions here. Why should we care about relations like ≿~∀~, and why should we care about how they behave in worlds where there are infinite populations. Let's take those in order.

Jake @Nebel2025, who developed one of the impossibility results I'll be discussing here, starts by inquiring into what preferences a *benevolent* agent would have. This way of thinking about the problem has a few consequences. (None of this paragraph is meant to be new; Nebel notes all these points in his paper.) For one thing, we shouldn't assume that ≿~∀~ will be unique. It's at least conceptually possible that there are different ways to be maximally benevolent. For another, we *possibly* shouldn't be treating ≿~*i*~ as measuring each person's preferences, as opposed to their welfare. Here we get into quite tricky questions about the nature of benevolence, and these questions get very hard when we're thinking about probabilistic benevolence.

I'm going to sidestep all those worries by taking our task to not be benevolence, but governance. Assume we have a basically pleasant bureaucrat who has to make a decision that will affect many people in various, not entirely predicatable, ways. To make it concrete, assume there is a moderately busy intersection that isn't working as it stands, and the bureaucrat's job is to redesign it. In theory we might say that in a democracy what matters should be what people want the intersection to look like. In practice, we've all settled on representative democracy as the form of government, and approximately no one changes their vote on the design of a moderately busy intersection, so really the bureaucrat is unconstrained by the democratic system. (At least, they are unconstrained as long as they don't do anything particularly egregious.) That doesn't mean they do what they want. Most traffic engineers care about safety, and traffic flow, and noise, and cost, and everything else that might be relevant, and try really hard to balance the competing interests when designing intersections. That is, they try do turn the various preferences users have into a social ordering and pick the best option. But how do they manage this alchemy, of turning the many orderings into one? That's the kind of project we're interested in here. It's got the same formal structure as Nebel's project of creating a benevolent preference ranking, but the differences might matter once we have to make hard choices about which plausible principle to give up.

But bureaucrats don't have infinite populations to deal with. Well, maybe they don't. We actually don't know how big the universe is, and how large the population of human, or otherwise morally salient, people is or will be. And the 'will be' part matters. Our imagined traffic engineer has to think about people who don't yet exist. If in ten years a tree near the intersection will have grown in such a way that if they use design *d* then the intersection will be unsafe for young children, that's a reason to not implement *d*, even if none of the people put at risk are yet born. Once we think about possible future people, the task of our bureaucrats gets much trickier.

Still, it is pretty uncommon for any bureaucrat to make decisions that affect infinitely many people. Those are the decisions we'd expect to be made through the democratic process. But, and this is a point Frank Hong and Jeffrey Sanford Russell [-@HongRussell2025] are careful to stress, some of these principles we're interested in have strange consequences about how we compare infinite worlds even if the worlds being compared only differ in finitely many ways. (Hong and Russell developed the other big impossibility result we'll be talking about here.) It turns out that looking hard at the infinite cases has lessons for how we build up ≿~∀~, even if we don't think anything that might be done has infinitely large consequences. Those lessons will be the focus here.

# Formalism 